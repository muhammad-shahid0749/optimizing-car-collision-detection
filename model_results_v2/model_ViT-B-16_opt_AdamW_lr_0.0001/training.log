2025-05-01 11:30:06,743 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:06,743 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 421,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:06,743 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:30:06,744 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 11:30:06,980 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 525.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:06,980 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:07,142 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 422,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 422,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:07,976 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:30:07,976 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:30:07,977 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 11:30:07,977 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 11:30:08,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 387.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:08,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 387.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:08,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:08,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:08,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:30:08,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:09,296 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:30:09,296 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:30:09,296 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:30:09,296 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 11:30:09,296 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 11:30:09,296 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 11:30:09,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:09,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:09,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:09,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:09,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:09,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:09,685 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:30:09,685 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:30:09,685 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:10,535 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:30:10,535 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:30:10,535 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:30:10,535 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:30:10,536 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 11:30:10,536 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 11:30:10,536 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 11:30:10,536 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:10,994 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:30:10,994 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:30:10,994 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:30:10,994 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:11,835 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:30:11,835 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:30:11,835 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:30:11,835 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:30:11,835 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:30:11,836 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 11:30:11,836 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 11:30:11,836 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 11:30:11,836 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 11:30:11,836 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:12,254 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:30:12,254 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:30:12,254 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:30:12,254 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:30:12,254 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:13,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:30:13,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:30:13,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:30:13,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:30:13,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:30:13,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:30:13,093 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:30:13,093 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:30:13,093 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:30:13,093 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:30:13,093 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:30:13,093 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:30:13,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:13,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:13,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:13,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:13,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:13,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:13,485 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:30:13,485 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:30:13,485 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:30:13,485 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:30:13,485 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:30:13,485 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:14,302 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:30:14,302 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:30:14,302 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:30:14,302 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:30:14,302 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:30:14,302 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:30:14,302 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:30:14,303 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:30:14,303 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:30:14,303 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:30:14,303 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:30:14,303 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:30:14,303 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:30:14,303 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:14,817 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:30:14,817 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:30:14,817 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:30:14,817 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:30:14,817 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:30:14,817 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:30:14,817 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 03:34:33,049 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:34:33,049 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 421,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:34:33,049 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:34:33,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 12, best validation accuracy: 0.0000
2025-05-03 03:34:33,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 03:35:09,276 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:35:53,450 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.5523
2025-05-03 03:35:53,665 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 13 completed in 80.43s - Train Loss: 0.7501, Train Acc: 0.5234, Val Loss: 0.7202, Val Acc: 0.5523
2025-05-03 03:35:54,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:35:54,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 03:36:30,678 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:37:13,286 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.5523 to 0.6295
2025-05-03 03:37:13,545 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 14 completed in 79.10s - Train Loss: 0.6555, Train Acc: 0.6169, Val Loss: 0.6484, Val Acc: 0.6295
2025-05-03 03:37:14,345 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:37:14,345 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 03:37:50,077 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:38:33,347 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.6295 to 0.6921
2025-05-03 03:38:33,611 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 15 completed in 79.27s - Train Loss: 0.5771, Train Acc: 0.7169, Val Loss: 0.6012, Val Acc: 0.6921
2025-05-03 03:38:34,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:38:34,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 03:39:09,953 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:39:54,123 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.6921 to 0.7633
2025-05-03 03:39:54,373 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 16 completed in 79.97s - Train Loss: 0.5554, Train Acc: 0.7372, Val Loss: 0.5431, Val Acc: 0.7633
2025-05-03 03:39:55,101 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:39:55,102 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 03:40:30,907 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:41:14,352 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from inf to 0.5709
2025-05-03 03:41:14,608 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 17 completed in 79.51s - Train Loss: 0.5305, Train Acc: 0.7681, Val Loss: 0.5709, Val Acc: 0.7196
2025-05-03 03:41:15,328 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:41:15,328 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 03:41:51,362 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 03:42:34,985 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.5709 to 0.5505
2025-05-03 03:42:35,254 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 18 completed in 79.93s - Train Loss: 0.5325, Train Acc: 0.7701, Val Loss: 0.5505, Val Acc: 0.7470
2025-05-03 03:42:36,015 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 03:42:36,015 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 03:43:12,131 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 03:43:56,180 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.7633 to 0.7736
2025-05-03 03:43:56,437 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 19 completed in 80.42s - Train Loss: 0.5391, Train Acc: 0.7649, Val Loss: 0.5193, Val Acc: 0.7736
2025-05-03 03:43:57,220 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 03:43:57,220 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 03:44:33,187 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 03:45:17,371 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.7736 to 0.8208
2025-05-03 03:45:17,635 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 20 completed in 80.41s - Train Loss: 0.5111, Train Acc: 0.7930, Val Loss: 0.4866, Val Acc: 0.8208
2025-05-03 03:45:18,386 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 03:45:18,386 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:29,598 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:29,598 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:29,599 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 422,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:45:29,599 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 422,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:45:29,599 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:29,599 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:29,811 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:29,811 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:29,811 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:29,811 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:41,473 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:41,473 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:41,473 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:41,473 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:41,473 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:41,473 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:52,739 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:52,739 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:52,739 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:52,739 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:16,037 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:16,037 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:16,037 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:16,037 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:16,037 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:16,037 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:16,038 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:16,038 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:16,038 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:16,038 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:16,038 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:16,038 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:27,542 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:27,542 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:27,542 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:27,542 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:27,542 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:27,542 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:27,542 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:27,543 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:27,543 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:27,543 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:27,543 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:27,543 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:27,543 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:27,543 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
