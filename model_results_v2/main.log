2025-04-30 23:40:29,480 - main - INFO - Starting training with device: cuda
2025-04-30 23:42:33,316 - main - INFO - Starting training with device: cuda
2025-04-30 23:42:33,405 - main - INFO - Class distribution:
2025-04-30 23:42:33,405 - main - INFO - Training set: Counter({1: 2336, 0: 2326})
2025-04-30 23:42:33,406 - main - INFO - Validation set: Counter({0: 588, 1: 578})
2025-04-30 23:42:33,406 - main - INFO - Test set: Counter({0: 723, 1: 286})
2025-04-30 23:42:33,406 - main - INFO - Experiment status: [] completed, 0 in progress, 756 pending
2025-04-30 23:42:33,406 - main - INFO - 
==================================================
2025-04-30 23:42:33,406 - main - INFO - Running configuration 1/756:
2025-04-30 23:42:33,406 - main - INFO - Model: ResNet50
2025-04-30 23:42:33,406 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-04-30 23:42:33,406 - main - INFO - Scheduler: StepLR
2025-04-30 23:42:33,406 - main - INFO - Loss Function: CrossEntropy
2025-04-30 23:42:33,406 - main - INFO - ==================================================
2025-04-30 23:42:33,406 - training.model_ResNet50_opt_SGD_lr_0.01_id_1 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_1
2025-04-30 23:42:33,406 - training.model_ResNet50_opt_SGD_lr_0.01_id_1 - INFO - Config: {
  "id": 1,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-04-30 23:42:33,679 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-04-30 23:42:33,680 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 1,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-04-30 23:42:33,680 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-04-30 23:42:33,680 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 1/20
2025-04-30 23:42:56,812 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-04-30 23:43:28,167 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.9314
2025-04-30 23:43:28,219 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 1 completed in 54.54s - Train Loss: 0.4599, Train Acc: 0.8546, Val Loss: 0.3828, Val Acc: 0.9314
2025-04-30 23:43:28,371 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-04-30 23:43:28,372 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 2/20
2025-04-30 23:43:51,637 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-04-30 23:44:23,221 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9314 to 0.9700
2025-04-30 23:44:23,304 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 2 completed in 54.93s - Train Loss: 0.3592, Train Acc: 0.9567, Val Loss: 0.3446, Val Acc: 0.9700
2025-04-30 23:44:23,446 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-04-30 23:44:23,446 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 3/20
2025-04-30 23:44:46,566 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-04-30 23:45:18,044 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation loss improved from inf to 0.3484
2025-04-30 23:45:18,125 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 3 completed in 54.68s - Train Loss: 0.3445, Train Acc: 0.9693, Val Loss: 0.3484, Val Acc: 0.9666
2025-04-30 23:45:18,272 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-04-30 23:45:18,273 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 4/20
2025-04-30 23:45:41,012 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-04-30 23:46:13,068 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9700 to 0.9734
2025-04-30 23:46:13,141 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 4 completed in 54.87s - Train Loss: 0.3369, Train Acc: 0.9768, Val Loss: 0.3397, Val Acc: 0.9734
2025-04-30 23:46:13,278 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-04-30 23:46:13,279 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 5/20
2025-04-30 23:46:36,357 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-04-30 23:47:07,826 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3484 to 0.3425
2025-04-30 23:47:07,918 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 5 completed in 54.64s - Train Loss: 0.3332, Train Acc: 0.9796, Val Loss: 0.3425, Val Acc: 0.9700
2025-04-30 23:47:08,063 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-04-30 23:47:08,063 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 6/20
2025-04-30 23:47:30,875 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-04-30 23:48:02,745 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9734 to 0.9760
2025-04-30 23:48:02,821 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 6 completed in 54.76s - Train Loss: 0.3291, Train Acc: 0.9848, Val Loss: 0.3373, Val Acc: 0.9760
2025-04-30 23:48:02,972 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-04-30 23:48:02,972 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 7/20
2025-04-30 23:48:26,027 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-04-30 23:48:57,940 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3425 to 0.3402
2025-04-30 23:48:58,039 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 7 completed in 55.07s - Train Loss: 0.3237, Train Acc: 0.9901, Val Loss: 0.3402, Val Acc: 0.9708
2025-04-30 23:48:58,193 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-04-30 23:48:58,194 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 8/20
2025-04-30 23:49:21,495 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-04-30 23:49:52,920 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3402 to 0.3368
2025-04-30 23:49:52,993 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 8 completed in 54.80s - Train Loss: 0.3232, Train Acc: 0.9906, Val Loss: 0.3368, Val Acc: 0.9751
2025-04-30 23:49:53,136 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-04-30 23:49:53,136 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 9/20
2025-04-30 23:50:16,340 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-04-30 23:50:47,839 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 9 completed in 54.70s - Train Loss: 0.3239, Train Acc: 0.9897, Val Loss: 0.3411, Val Acc: 0.9700
2025-04-30 23:50:48,005 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-04-30 23:50:48,006 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 10/20
2025-04-30 23:51:10,689 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-04-30 23:51:42,735 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3368 to 0.3359
2025-04-30 23:51:42,821 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 10 completed in 54.82s - Train Loss: 0.3233, Train Acc: 0.9908, Val Loss: 0.3359, Val Acc: 0.9734
2025-04-30 23:51:42,963 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-04-30 23:51:42,963 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 11/20
2025-04-30 23:52:05,974 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-04-30 23:52:37,579 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9760 to 0.9777
2025-04-30 23:52:37,675 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 11 completed in 54.71s - Train Loss: 0.3207, Train Acc: 0.9925, Val Loss: 0.3344, Val Acc: 0.9777
2025-04-30 23:52:37,818 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-04-30 23:52:37,818 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 12/20
2025-04-30 23:53:00,934 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-04-30 23:53:32,594 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9777 to 0.9837
2025-04-30 23:53:32,682 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 12 completed in 54.86s - Train Loss: 0.3192, Train Acc: 0.9940, Val Loss: 0.3284, Val Acc: 0.9837
2025-04-30 23:53:32,826 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-04-30 23:53:32,826 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 13/20
2025-04-30 23:53:55,879 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-04-30 23:54:27,851 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3359 to 0.3291
2025-04-30 23:54:27,938 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 13 completed in 55.11s - Train Loss: 0.3169, Train Acc: 0.9970, Val Loss: 0.3291, Val Acc: 0.9837
2025-04-30 23:54:28,081 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-04-30 23:54:28,082 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 14/20
2025-04-30 23:54:50,833 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-04-30 23:55:22,718 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9837 to 0.9863
2025-04-30 23:55:22,795 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 14 completed in 54.71s - Train Loss: 0.3172, Train Acc: 0.9966, Val Loss: 0.3266, Val Acc: 0.9863
2025-04-30 23:55:22,941 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-04-30 23:55:22,942 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 15/20
2025-04-30 23:55:45,957 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-04-30 23:56:18,206 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3291 to 0.3275
2025-04-30 23:56:18,288 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 15 completed in 55.35s - Train Loss: 0.3155, Train Acc: 0.9981, Val Loss: 0.3275, Val Acc: 0.9846
2025-04-30 23:56:18,431 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-04-30 23:56:18,432 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 16/20
2025-04-30 23:56:41,676 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-04-30 23:57:13,313 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9863 to 0.9871
2025-04-30 23:57:13,391 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 16 completed in 54.96s - Train Loss: 0.3157, Train Acc: 0.9981, Val Loss: 0.3260, Val Acc: 0.9871
2025-04-30 23:57:13,530 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-04-30 23:57:13,530 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 17/20
2025-04-30 23:57:35,966 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-04-30 23:58:07,994 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 17 completed in 54.46s - Train Loss: 0.3155, Train Acc: 0.9979, Val Loss: 0.3288, Val Acc: 0.9846
2025-04-30 23:58:08,146 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-04-30 23:58:08,146 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 18/20
2025-04-30 23:58:30,763 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-04-30 23:59:03,096 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 18 completed in 54.95s - Train Loss: 0.3148, Train Acc: 0.9987, Val Loss: 0.3281, Val Acc: 0.9846
2025-04-30 23:59:03,250 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-04-30 23:59:03,250 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 19/20
2025-04-30 23:59:26,092 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-04-30 23:59:57,568 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9871 to 0.9880
2025-04-30 23:59:57,646 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 19 completed in 54.40s - Train Loss: 0.3155, Train Acc: 0.9983, Val Loss: 0.3264, Val Acc: 0.9880
2025-04-30 23:59:57,786 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-04-30 23:59:57,787 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 00:00:21,010 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 00:00:52,780 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3275 to 0.3264
2025-05-01 00:00:52,867 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Epoch 20 completed in 55.08s - Train Loss: 0.3156, Train Acc: 0.9976, Val Loss: 0.3264, Val Acc: 0.9880
2025-05-01 00:00:53,005 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 00:00:53,005 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:00:53,007 - training.model_ResNet50_opt_SGD_lr_0.01_id_1 - INFO - Starting model evaluation
2025-05-01 00:01:02,984 - training.model_ResNet50_opt_SGD_lr_0.01_id_1 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9481
  Recall:    0.9580
  F1 Score:  0.9530
  IoU:       0.9103
  mAP:       0.9875
  AUC:       0.9930
2025-05-01 00:01:02,985 - training.model_ResNet50_opt_SGD_lr_0.01_id_1 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_1/final_results.json
2025-05-01 00:01:02,987 - training.model_ResNet50_opt_SGD_lr_0.01_id_1 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_1/final_results.json
2025-05-01 00:01:02,987 - main - INFO - 
Summary for configuration 1:
2025-05-01 00:01:02,987 - main - INFO - Accuracy: 0.9732
2025-05-01 00:01:02,987 - main - INFO - Precision: 0.9481
2025-05-01 00:01:02,987 - main - INFO - Recall: 0.9580
2025-05-01 00:01:02,987 - main - INFO - F1 Score: 0.9530
2025-05-01 00:01:02,987 - main - INFO - IoU: 0.9103
2025-05-01 00:01:02,987 - main - INFO - mAP: 0.9875
2025-05-01 00:01:02,987 - main - INFO - AUC: 0.9930
2025-05-01 00:01:02,987 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:01:02,987 - main - INFO - 
==================================================
2025-05-01 00:01:02,987 - main - INFO - Running configuration 2/756:
2025-05-01 00:01:02,987 - main - INFO - Model: ResNet50
2025-05-01 00:01:02,987 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:01:02,987 - main - INFO - Scheduler: StepLR
2025-05-01 00:01:02,987 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:01:02,987 - main - INFO - ==================================================
2025-05-01 00:01:02,987 - training.model_ResNet50_opt_SGD_lr_0.01_id_2 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_2
2025-05-01 00:01:02,987 - training.model_ResNet50_opt_SGD_lr_0.01_id_2 - INFO - Config: {
  "id": 2,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:01:03,160 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:01:03,161 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 2,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:01:03,161 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:01:03,219 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:01:03,219 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:01:03,221 - training.model_ResNet50_opt_SGD_lr_0.01_id_2 - INFO - Starting model evaluation
2025-05-01 00:01:10,676 - training.model_ResNet50_opt_SGD_lr_0.01_id_2 - INFO - Evaluation metrics:
  Accuracy:  0.9752
  Precision: 0.9485
  Recall:    0.9650
  F1 Score:  0.9567
  IoU:       0.9169
  mAP:       0.9897
  AUC:       0.9957
2025-05-01 00:01:10,678 - training.model_ResNet50_opt_SGD_lr_0.01_id_2 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_2/final_results.json
2025-05-01 00:01:10,680 - training.model_ResNet50_opt_SGD_lr_0.01_id_2 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_2/final_results.json
2025-05-01 00:01:10,680 - main - INFO - 
Summary for configuration 2:
2025-05-01 00:01:10,680 - main - INFO - Accuracy: 0.9752
2025-05-01 00:01:10,680 - main - INFO - Precision: 0.9485
2025-05-01 00:01:10,680 - main - INFO - Recall: 0.9650
2025-05-01 00:01:10,680 - main - INFO - F1 Score: 0.9567
2025-05-01 00:01:10,680 - main - INFO - IoU: 0.9169
2025-05-01 00:01:10,680 - main - INFO - mAP: 0.9897
2025-05-01 00:01:10,680 - main - INFO - AUC: 0.9957
2025-05-01 00:01:10,680 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:01:10,680 - main - INFO - 
==================================================
2025-05-01 00:01:10,680 - main - INFO - Running configuration 3/756:
2025-05-01 00:01:10,680 - main - INFO - Model: ResNet50
2025-05-01 00:01:10,680 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:01:10,680 - main - INFO - Scheduler: StepLR
2025-05-01 00:01:10,680 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:01:10,680 - main - INFO - ==================================================
2025-05-01 00:01:10,680 - training.model_ResNet50_opt_SGD_lr_0.01_id_3 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_3
2025-05-01 00:01:10,680 - training.model_ResNet50_opt_SGD_lr_0.01_id_3 - INFO - Config: {
  "id": 3,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:01:10,836 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:01:10,837 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 3,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:01:10,837 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:01:10,894 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:01:10,895 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:01:10,896 - training.model_ResNet50_opt_SGD_lr_0.01_id_3 - INFO - Starting model evaluation
2025-05-01 00:01:19,067 - training.model_ResNet50_opt_SGD_lr_0.01_id_3 - INFO - Evaluation metrics:
  Accuracy:  0.9742
  Precision: 0.9514
  Recall:    0.9580
  F1 Score:  0.9547
  IoU:       0.9133
  mAP:       0.9893
  AUC:       0.9951
2025-05-01 00:01:19,069 - training.model_ResNet50_opt_SGD_lr_0.01_id_3 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_3/final_results.json
2025-05-01 00:01:19,070 - training.model_ResNet50_opt_SGD_lr_0.01_id_3 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_3/final_results.json
2025-05-01 00:01:19,070 - main - INFO - 
Summary for configuration 3:
2025-05-01 00:01:19,070 - main - INFO - Accuracy: 0.9742
2025-05-01 00:01:19,070 - main - INFO - Precision: 0.9514
2025-05-01 00:01:19,070 - main - INFO - Recall: 0.9580
2025-05-01 00:01:19,070 - main - INFO - F1 Score: 0.9547
2025-05-01 00:01:19,070 - main - INFO - IoU: 0.9133
2025-05-01 00:01:19,070 - main - INFO - mAP: 0.9893
2025-05-01 00:01:19,070 - main - INFO - AUC: 0.9951
2025-05-01 00:01:19,070 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:01:19,070 - main - INFO - 
==================================================
2025-05-01 00:01:19,070 - main - INFO - Running configuration 4/756:
2025-05-01 00:01:19,070 - main - INFO - Model: ResNet50
2025-05-01 00:01:19,070 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:01:19,070 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:01:19,070 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:01:19,070 - main - INFO - ==================================================
2025-05-01 00:01:19,071 - training.model_ResNet50_opt_SGD_lr_0.01_id_4 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_4
2025-05-01 00:01:19,071 - training.model_ResNet50_opt_SGD_lr_0.01_id_4 - INFO - Config: {
  "id": 4,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:01:19,214 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:01:19,214 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 4,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:01:19,214 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:01:19,342 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:01:19,342 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:01:19,344 - training.model_ResNet50_opt_SGD_lr_0.01_id_4 - INFO - Starting model evaluation
2025-05-01 00:01:26,871 - training.model_ResNet50_opt_SGD_lr_0.01_id_4 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9481
  Recall:    0.9580
  F1 Score:  0.9530
  IoU:       0.9103
  mAP:       0.9860
  AUC:       0.9908
2025-05-01 00:01:26,885 - training.model_ResNet50_opt_SGD_lr_0.01_id_4 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_4/final_results.json
2025-05-01 00:01:26,887 - training.model_ResNet50_opt_SGD_lr_0.01_id_4 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_4/final_results.json
2025-05-01 00:01:26,887 - main - INFO - 
Summary for configuration 4:
2025-05-01 00:01:26,887 - main - INFO - Accuracy: 0.9732
2025-05-01 00:01:26,887 - main - INFO - Precision: 0.9481
2025-05-01 00:01:26,887 - main - INFO - Recall: 0.9580
2025-05-01 00:01:26,887 - main - INFO - F1 Score: 0.9530
2025-05-01 00:01:26,887 - main - INFO - IoU: 0.9103
2025-05-01 00:01:26,887 - main - INFO - mAP: 0.9860
2025-05-01 00:01:26,887 - main - INFO - AUC: 0.9908
2025-05-01 00:01:26,887 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:01:26,887 - main - INFO - 
==================================================
2025-05-01 00:01:26,887 - main - INFO - Running configuration 5/756:
2025-05-01 00:01:26,887 - main - INFO - Model: ResNet50
2025-05-01 00:01:26,887 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:01:26,887 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:01:26,887 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:01:26,887 - main - INFO - ==================================================
2025-05-01 00:01:26,887 - training.model_ResNet50_opt_SGD_lr_0.01_id_5 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_5
2025-05-01 00:01:26,887 - training.model_ResNet50_opt_SGD_lr_0.01_id_5 - INFO - Config: {
  "id": 5,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:01:27,029 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:01:27,029 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 5,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:01:27,029 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:01:27,085 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:01:27,086 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:01:27,091 - training.model_ResNet50_opt_SGD_lr_0.01_id_5 - INFO - Starting model evaluation
2025-05-01 00:01:35,786 - training.model_ResNet50_opt_SGD_lr_0.01_id_5 - INFO - Evaluation metrics:
  Accuracy:  0.9742
  Precision: 0.9483
  Recall:    0.9615
  F1 Score:  0.9549
  IoU:       0.9136
  mAP:       0.9870
  AUC:       0.9912
2025-05-01 00:01:35,788 - training.model_ResNet50_opt_SGD_lr_0.01_id_5 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_5/final_results.json
2025-05-01 00:01:35,789 - training.model_ResNet50_opt_SGD_lr_0.01_id_5 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_5/final_results.json
2025-05-01 00:01:35,789 - main - INFO - 
Summary for configuration 5:
2025-05-01 00:01:35,789 - main - INFO - Accuracy: 0.9742
2025-05-01 00:01:35,789 - main - INFO - Precision: 0.9483
2025-05-01 00:01:35,789 - main - INFO - Recall: 0.9615
2025-05-01 00:01:35,789 - main - INFO - F1 Score: 0.9549
2025-05-01 00:01:35,789 - main - INFO - IoU: 0.9136
2025-05-01 00:01:35,789 - main - INFO - mAP: 0.9870
2025-05-01 00:01:35,789 - main - INFO - AUC: 0.9912
2025-05-01 00:01:35,789 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:01:35,789 - main - INFO - 
==================================================
2025-05-01 00:01:35,790 - main - INFO - Running configuration 6/756:
2025-05-01 00:01:35,790 - main - INFO - Model: ResNet50
2025-05-01 00:01:35,790 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:01:35,790 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:01:35,790 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:01:35,790 - main - INFO - ==================================================
2025-05-01 00:01:35,790 - training.model_ResNet50_opt_SGD_lr_0.01_id_6 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_6
2025-05-01 00:01:35,790 - training.model_ResNet50_opt_SGD_lr_0.01_id_6 - INFO - Config: {
  "id": 6,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:01:35,964 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:01:35,964 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 6,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:01:35,964 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:01:36,022 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:01:36,023 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:01:36,025 - training.model_ResNet50_opt_SGD_lr_0.01_id_6 - INFO - Starting model evaluation
2025-05-01 00:01:44,932 - training.model_ResNet50_opt_SGD_lr_0.01_id_6 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9450
  Recall:    0.9615
  F1 Score:  0.9532
  IoU:       0.9106
  mAP:       0.9892
  AUC:       0.9938
2025-05-01 00:01:44,934 - training.model_ResNet50_opt_SGD_lr_0.01_id_6 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_6/final_results.json
2025-05-01 00:01:44,936 - training.model_ResNet50_opt_SGD_lr_0.01_id_6 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_6/final_results.json
2025-05-01 00:01:44,936 - main - INFO - 
Summary for configuration 6:
2025-05-01 00:01:44,936 - main - INFO - Accuracy: 0.9732
2025-05-01 00:01:44,936 - main - INFO - Precision: 0.9450
2025-05-01 00:01:44,936 - main - INFO - Recall: 0.9615
2025-05-01 00:01:44,936 - main - INFO - F1 Score: 0.9532
2025-05-01 00:01:44,936 - main - INFO - IoU: 0.9106
2025-05-01 00:01:44,936 - main - INFO - mAP: 0.9892
2025-05-01 00:01:44,936 - main - INFO - AUC: 0.9938
2025-05-01 00:01:44,936 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:01:44,936 - main - INFO - 
==================================================
2025-05-01 00:01:44,936 - main - INFO - Running configuration 7/756:
2025-05-01 00:01:44,936 - main - INFO - Model: ResNet50
2025-05-01 00:01:44,936 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:01:44,936 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:01:44,936 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:01:44,936 - main - INFO - ==================================================
2025-05-01 00:01:44,936 - training.model_ResNet50_opt_SGD_lr_0.01_id_7 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_7
2025-05-01 00:01:44,936 - training.model_ResNet50_opt_SGD_lr_0.01_id_7 - INFO - Config: {
  "id": 7,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:01:45,096 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:01:45,096 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 7,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:01:45,096 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:01:45,151 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:01:45,152 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:01:45,153 - training.model_ResNet50_opt_SGD_lr_0.01_id_7 - INFO - Starting model evaluation
2025-05-01 00:01:54,269 - training.model_ResNet50_opt_SGD_lr_0.01_id_7 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9448
  Recall:    0.9580
  F1 Score:  0.9514
  IoU:       0.9073
  mAP:       0.9848
  AUC:       0.9902
2025-05-01 00:01:54,271 - training.model_ResNet50_opt_SGD_lr_0.01_id_7 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_7/final_results.json
2025-05-01 00:01:54,272 - training.model_ResNet50_opt_SGD_lr_0.01_id_7 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_7/final_results.json
2025-05-01 00:01:54,272 - main - INFO - 
Summary for configuration 7:
2025-05-01 00:01:54,272 - main - INFO - Accuracy: 0.9722
2025-05-01 00:01:54,272 - main - INFO - Precision: 0.9448
2025-05-01 00:01:54,272 - main - INFO - Recall: 0.9580
2025-05-01 00:01:54,272 - main - INFO - F1 Score: 0.9514
2025-05-01 00:01:54,272 - main - INFO - IoU: 0.9073
2025-05-01 00:01:54,272 - main - INFO - mAP: 0.9848
2025-05-01 00:01:54,272 - main - INFO - AUC: 0.9902
2025-05-01 00:01:54,272 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:01:54,272 - main - INFO - 
==================================================
2025-05-01 00:01:54,272 - main - INFO - Running configuration 8/756:
2025-05-01 00:01:54,272 - main - INFO - Model: ResNet50
2025-05-01 00:01:54,272 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:01:54,272 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:01:54,272 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:01:54,272 - main - INFO - ==================================================
2025-05-01 00:01:54,273 - training.model_ResNet50_opt_SGD_lr_0.01_id_8 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_8
2025-05-01 00:01:54,273 - training.model_ResNet50_opt_SGD_lr_0.01_id_8 - INFO - Config: {
  "id": 8,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:01:54,437 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:01:54,437 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 8,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:01:54,437 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:01:54,493 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:01:54,494 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:01:54,496 - training.model_ResNet50_opt_SGD_lr_0.01_id_8 - INFO - Starting model evaluation
2025-05-01 00:02:03,340 - training.model_ResNet50_opt_SGD_lr_0.01_id_8 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9450
  Recall:    0.9615
  F1 Score:  0.9532
  IoU:       0.9106
  mAP:       0.9894
  AUC:       0.9934
2025-05-01 00:02:03,341 - training.model_ResNet50_opt_SGD_lr_0.01_id_8 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_8/final_results.json
2025-05-01 00:02:03,343 - training.model_ResNet50_opt_SGD_lr_0.01_id_8 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_8/final_results.json
2025-05-01 00:02:03,343 - main - INFO - 
Summary for configuration 8:
2025-05-01 00:02:03,343 - main - INFO - Accuracy: 0.9732
2025-05-01 00:02:03,343 - main - INFO - Precision: 0.9450
2025-05-01 00:02:03,343 - main - INFO - Recall: 0.9615
2025-05-01 00:02:03,343 - main - INFO - F1 Score: 0.9532
2025-05-01 00:02:03,343 - main - INFO - IoU: 0.9106
2025-05-01 00:02:03,343 - main - INFO - mAP: 0.9894
2025-05-01 00:02:03,343 - main - INFO - AUC: 0.9934
2025-05-01 00:02:03,343 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:02:03,343 - main - INFO - 
==================================================
2025-05-01 00:02:03,343 - main - INFO - Running configuration 9/756:
2025-05-01 00:02:03,343 - main - INFO - Model: ResNet50
2025-05-01 00:02:03,343 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:02:03,343 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:02:03,343 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:02:03,343 - main - INFO - ==================================================
2025-05-01 00:02:03,343 - training.model_ResNet50_opt_SGD_lr_0.01_id_9 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_9
2025-05-01 00:02:03,343 - training.model_ResNet50_opt_SGD_lr_0.01_id_9 - INFO - Config: {
  "id": 9,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:02:03,503 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:02:03,503 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 9,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:02:03,503 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:02:03,558 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:02:03,558 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:02:03,560 - training.model_ResNet50_opt_SGD_lr_0.01_id_9 - INFO - Starting model evaluation
2025-05-01 00:02:12,209 - training.model_ResNet50_opt_SGD_lr_0.01_id_9 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9412
  Recall:    0.9510
  F1 Score:  0.9461
  IoU:       0.8977
  mAP:       0.9861
  AUC:       0.9923
2025-05-01 00:02:12,211 - training.model_ResNet50_opt_SGD_lr_0.01_id_9 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_9/final_results.json
2025-05-01 00:02:12,212 - training.model_ResNet50_opt_SGD_lr_0.01_id_9 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_9/final_results.json
2025-05-01 00:02:12,212 - main - INFO - 
Summary for configuration 9:
2025-05-01 00:02:12,212 - main - INFO - Accuracy: 0.9693
2025-05-01 00:02:12,212 - main - INFO - Precision: 0.9412
2025-05-01 00:02:12,212 - main - INFO - Recall: 0.9510
2025-05-01 00:02:12,212 - main - INFO - F1 Score: 0.9461
2025-05-01 00:02:12,212 - main - INFO - IoU: 0.8977
2025-05-01 00:02:12,212 - main - INFO - mAP: 0.9861
2025-05-01 00:02:12,212 - main - INFO - AUC: 0.9923
2025-05-01 00:02:12,212 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:02:12,212 - main - INFO - 
==================================================
2025-05-01 00:02:12,212 - main - INFO - Running configuration 10/756:
2025-05-01 00:02:12,212 - main - INFO - Model: ResNet50
2025-05-01 00:02:12,212 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:02:12,212 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:02:12,212 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:02:12,212 - main - INFO - ==================================================
2025-05-01 00:02:12,213 - training.model_ResNet50_opt_SGD_lr_0.01_id_10 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_10
2025-05-01 00:02:12,213 - training.model_ResNet50_opt_SGD_lr_0.01_id_10 - INFO - Config: {
  "id": 10,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:02:12,385 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:02:12,388 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 10,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:02:12,389 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:02:12,448 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:02:12,448 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:02:12,450 - training.model_ResNet50_opt_SGD_lr_0.01_id_10 - INFO - Starting model evaluation
2025-05-01 00:02:21,145 - training.model_ResNet50_opt_SGD_lr_0.01_id_10 - INFO - Evaluation metrics:
  Accuracy:  0.9772
  Precision: 0.9550
  Recall:    0.9650
  F1 Score:  0.9600
  IoU:       0.9231
  mAP:       0.9914
  AUC:       0.9961
2025-05-01 00:02:21,146 - training.model_ResNet50_opt_SGD_lr_0.01_id_10 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_10/final_results.json
2025-05-01 00:02:21,148 - training.model_ResNet50_opt_SGD_lr_0.01_id_10 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_10/final_results.json
2025-05-01 00:02:21,148 - main - INFO - 
Summary for configuration 10:
2025-05-01 00:02:21,148 - main - INFO - Accuracy: 0.9772
2025-05-01 00:02:21,148 - main - INFO - Precision: 0.9550
2025-05-01 00:02:21,148 - main - INFO - Recall: 0.9650
2025-05-01 00:02:21,148 - main - INFO - F1 Score: 0.9600
2025-05-01 00:02:21,148 - main - INFO - IoU: 0.9231
2025-05-01 00:02:21,148 - main - INFO - mAP: 0.9914
2025-05-01 00:02:21,148 - main - INFO - AUC: 0.9961
2025-05-01 00:02:21,148 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:02:21,148 - main - INFO - 
==================================================
2025-05-01 00:02:21,148 - main - INFO - Running configuration 11/756:
2025-05-01 00:02:21,148 - main - INFO - Model: ResNet50
2025-05-01 00:02:21,148 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:02:21,148 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:02:21,148 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:02:21,148 - main - INFO - ==================================================
2025-05-01 00:02:21,148 - training.model_ResNet50_opt_SGD_lr_0.01_id_11 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_11
2025-05-01 00:02:21,148 - training.model_ResNet50_opt_SGD_lr_0.01_id_11 - INFO - Config: {
  "id": 11,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:02:21,310 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:02:21,310 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 11,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:02:21,310 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:02:21,364 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:02:21,365 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:02:21,366 - training.model_ResNet50_opt_SGD_lr_0.01_id_11 - INFO - Starting model evaluation
2025-05-01 00:02:29,937 - training.model_ResNet50_opt_SGD_lr_0.01_id_11 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9481
  Recall:    0.9580
  F1 Score:  0.9530
  IoU:       0.9103
  mAP:       0.9888
  AUC:       0.9931
2025-05-01 00:02:29,939 - training.model_ResNet50_opt_SGD_lr_0.01_id_11 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_11/final_results.json
2025-05-01 00:02:29,940 - training.model_ResNet50_opt_SGD_lr_0.01_id_11 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_11/final_results.json
2025-05-01 00:02:29,940 - main - INFO - 
Summary for configuration 11:
2025-05-01 00:02:29,940 - main - INFO - Accuracy: 0.9732
2025-05-01 00:02:29,940 - main - INFO - Precision: 0.9481
2025-05-01 00:02:29,940 - main - INFO - Recall: 0.9580
2025-05-01 00:02:29,940 - main - INFO - F1 Score: 0.9530
2025-05-01 00:02:29,940 - main - INFO - IoU: 0.9103
2025-05-01 00:02:29,940 - main - INFO - mAP: 0.9888
2025-05-01 00:02:29,940 - main - INFO - AUC: 0.9931
2025-05-01 00:02:29,940 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:02:29,940 - main - INFO - 
==================================================
2025-05-01 00:02:29,940 - main - INFO - Running configuration 12/756:
2025-05-01 00:02:29,940 - main - INFO - Model: ResNet50
2025-05-01 00:02:29,940 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 00:02:29,940 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:02:29,940 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:02:29,940 - main - INFO - ==================================================
2025-05-01 00:02:29,941 - training.model_ResNet50_opt_SGD_lr_0.01_id_12 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01_id_12
2025-05-01 00:02:29,941 - training.model_ResNet50_opt_SGD_lr_0.01_id_12 - INFO - Config: {
  "id": 12,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:02:30,106 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.01
2025-05-01 00:02:30,106 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 12,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:02:30,106 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 00:02:30,161 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 00:02:30,162 - training.model_ResNet50_opt_SGD_lr_0.01 - INFO - Training completed after 1099.19 seconds
2025-05-01 00:02:30,163 - training.model_ResNet50_opt_SGD_lr_0.01_id_12 - INFO - Starting model evaluation
2025-05-01 00:02:39,051 - training.model_ResNet50_opt_SGD_lr_0.01_id_12 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9479
  Recall:    0.9545
  F1 Score:  0.9512
  IoU:       0.9070
  mAP:       0.9886
  AUC:       0.9933
2025-05-01 00:02:39,053 - training.model_ResNet50_opt_SGD_lr_0.01_id_12 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_12/final_results.json
2025-05-01 00:02:39,054 - training.model_ResNet50_opt_SGD_lr_0.01_id_12 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.01_id_12/final_results.json
2025-05-01 00:02:39,055 - main - INFO - 
Summary for configuration 12:
2025-05-01 00:02:39,055 - main - INFO - Accuracy: 0.9722
2025-05-01 00:02:39,055 - main - INFO - Precision: 0.9479
2025-05-01 00:02:39,055 - main - INFO - Recall: 0.9545
2025-05-01 00:02:39,055 - main - INFO - F1 Score: 0.9512
2025-05-01 00:02:39,055 - main - INFO - IoU: 0.9070
2025-05-01 00:02:39,055 - main - INFO - mAP: 0.9886
2025-05-01 00:02:39,055 - main - INFO - AUC: 0.9933
2025-05-01 00:02:39,055 - main - INFO - Training time: 1099.19 seconds
2025-05-01 00:02:39,055 - main - INFO - 
==================================================
2025-05-01 00:02:39,055 - main - INFO - Running configuration 13/756:
2025-05-01 00:02:39,055 - main - INFO - Model: ResNet50
2025-05-01 00:02:39,055 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:02:39,055 - main - INFO - Scheduler: StepLR
2025-05-01 00:02:39,055 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:02:39,055 - main - INFO - ==================================================
2025-05-01 00:02:39,055 - training.model_ResNet50_opt_SGD_lr_0.001_id_13 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_13
2025-05-01 00:02:39,055 - training.model_ResNet50_opt_SGD_lr_0.001_id_13 - INFO - Config: {
  "id": 13,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:02:39,224 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:02:39,224 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 13,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:02:39,224 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 00:02:39,225 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 00:03:01,397 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 00:03:30,173 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.8070
2025-05-01 00:03:30,226 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 1 completed in 51.00s - Train Loss: 0.6197, Train Acc: 0.7415, Val Loss: 0.5385, Val Acc: 0.8070
2025-05-01 00:03:30,387 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 00:03:30,387 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 00:03:52,124 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 00:04:21,476 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.8070 to 0.8722
2025-05-01 00:04:21,569 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 2 completed in 51.18s - Train Loss: 0.5021, Train Acc: 0.8411, Val Loss: 0.4621, Val Acc: 0.8722
2025-05-01 00:04:21,715 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 00:04:21,715 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 00:04:43,613 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 00:05:12,560 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.8722 to 0.9091
2025-05-01 00:05:12,635 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 3 completed in 50.92s - Train Loss: 0.4371, Train Acc: 0.9013, Val Loss: 0.4194, Val Acc: 0.9091
2025-05-01 00:05:12,788 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 00:05:12,788 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 00:05:34,607 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 00:06:03,565 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9091 to 0.9400
2025-05-01 00:06:03,647 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 4 completed in 50.86s - Train Loss: 0.4014, Train Acc: 0.9311, Val Loss: 0.3885, Val Acc: 0.9400
2025-05-01 00:06:03,791 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 00:06:03,791 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 00:06:25,939 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 00:06:55,239 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9400 to 0.9434
2025-05-01 00:06:55,328 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 5 completed in 51.54s - Train Loss: 0.3762, Train Acc: 0.9535, Val Loss: 0.3748, Val Acc: 0.9434
2025-05-01 00:06:55,471 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 00:06:55,472 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 00:07:17,510 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 00:07:46,723 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9434 to 0.9666
2025-05-01 00:07:46,817 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 6 completed in 51.34s - Train Loss: 0.3567, Train Acc: 0.9687, Val Loss: 0.3574, Val Acc: 0.9666
2025-05-01 00:07:46,961 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 00:07:46,961 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 00:08:08,736 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 00:08:38,179 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9666 to 0.9726
2025-05-01 00:08:38,255 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 7 completed in 51.29s - Train Loss: 0.3459, Train Acc: 0.9796, Val Loss: 0.3478, Val Acc: 0.9726
2025-05-01 00:08:38,401 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 00:08:38,402 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 00:09:00,143 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 00:09:29,544 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9726 to 0.9803
2025-05-01 00:09:29,626 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 8 completed in 51.22s - Train Loss: 0.3359, Train Acc: 0.9871, Val Loss: 0.3403, Val Acc: 0.9803
2025-05-01 00:09:29,778 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 00:09:29,779 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 00:09:51,285 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 00:10:20,673 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation loss improved from inf to 0.3400
2025-05-01 00:10:20,763 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 9 completed in 50.98s - Train Loss: 0.3314, Train Acc: 0.9893, Val Loss: 0.3400, Val Acc: 0.9768
2025-05-01 00:10:20,917 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 00:10:20,917 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 00:10:42,660 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 00:11:11,930 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9803 to 0.9837
2025-05-01 00:11:12,032 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 10 completed in 51.11s - Train Loss: 0.3270, Train Acc: 0.9934, Val Loss: 0.3350, Val Acc: 0.9837
2025-05-01 00:11:12,179 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 00:11:12,180 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 00:11:33,935 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 00:12:03,552 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3400 to 0.3349
2025-05-01 00:12:03,641 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 11 completed in 51.46s - Train Loss: 0.3254, Train Acc: 0.9931, Val Loss: 0.3349, Val Acc: 0.9820
2025-05-01 00:12:03,797 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 00:12:03,797 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 00:12:25,590 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 00:12:55,181 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 12 completed in 51.38s - Train Loss: 0.3251, Train Acc: 0.9938, Val Loss: 0.3364, Val Acc: 0.9794
2025-05-01 00:12:55,352 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 00:12:55,353 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 00:13:17,354 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 00:13:46,519 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 13 completed in 51.17s - Train Loss: 0.3248, Train Acc: 0.9949, Val Loss: 0.3358, Val Acc: 0.9786
2025-05-01 00:13:46,669 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 00:13:46,670 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 00:14:08,171 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 00:14:37,503 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3349 to 0.3347
2025-05-01 00:14:37,596 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 14 completed in 50.93s - Train Loss: 0.3242, Train Acc: 0.9946, Val Loss: 0.3347, Val Acc: 0.9777
2025-05-01 00:14:37,732 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 00:14:37,733 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 00:14:59,622 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 00:15:28,940 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3347 to 0.3336
2025-05-01 00:15:29,019 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 15 completed in 51.29s - Train Loss: 0.3262, Train Acc: 0.9936, Val Loss: 0.3336, Val Acc: 0.9828
2025-05-01 00:15:29,164 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 00:15:29,165 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 00:15:51,015 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 00:16:20,706 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 16 completed in 51.54s - Train Loss: 0.3247, Train Acc: 0.9942, Val Loss: 0.3356, Val Acc: 0.9803
2025-05-01 00:16:20,864 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 00:16:20,864 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 00:16:42,657 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 00:17:12,202 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3336 to 0.3329
2025-05-01 00:17:12,292 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 17 completed in 51.43s - Train Loss: 0.3253, Train Acc: 0.9929, Val Loss: 0.3329, Val Acc: 0.9811
2025-05-01 00:17:12,439 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 00:17:12,440 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 00:17:33,987 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 00:18:03,756 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 18 completed in 51.32s - Train Loss: 0.3234, Train Acc: 0.9949, Val Loss: 0.3336, Val Acc: 0.9820
2025-05-01 00:18:03,903 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 00:18:03,904 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 00:18:25,973 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 00:18:55,344 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 19 completed in 51.44s - Train Loss: 0.3239, Train Acc: 0.9936, Val Loss: 0.3339, Val Acc: 0.9837
2025-05-01 00:18:55,494 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 00:18:55,494 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 00:19:17,076 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 00:19:46,759 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Epoch 20 completed in 51.26s - Train Loss: 0.3242, Train Acc: 0.9949, Val Loss: 0.3351, Val Acc: 0.9811
2025-05-01 00:19:46,900 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 00:19:46,901 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:19:46,903 - training.model_ResNet50_opt_SGD_lr_0.001_id_13 - INFO - Starting model evaluation
2025-05-01 00:19:55,576 - training.model_ResNet50_opt_SGD_lr_0.001_id_13 - INFO - Evaluation metrics:
  Accuracy:  0.9584
  Precision: 0.9178
  Recall:    0.9371
  F1 Score:  0.9273
  IoU:       0.8645
  mAP:       0.9829
  AUC:       0.9914
2025-05-01 00:19:55,578 - training.model_ResNet50_opt_SGD_lr_0.001_id_13 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_13/final_results.json
2025-05-01 00:19:55,579 - training.model_ResNet50_opt_SGD_lr_0.001_id_13 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_13/final_results.json
2025-05-01 00:19:55,579 - main - INFO - 
Summary for configuration 13:
2025-05-01 00:19:55,579 - main - INFO - Accuracy: 0.9584
2025-05-01 00:19:55,579 - main - INFO - Precision: 0.9178
2025-05-01 00:19:55,579 - main - INFO - Recall: 0.9371
2025-05-01 00:19:55,579 - main - INFO - F1 Score: 0.9273
2025-05-01 00:19:55,580 - main - INFO - IoU: 0.8645
2025-05-01 00:19:55,580 - main - INFO - mAP: 0.9829
2025-05-01 00:19:55,580 - main - INFO - AUC: 0.9914
2025-05-01 00:19:55,580 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:19:55,580 - main - INFO - 
==================================================
2025-05-01 00:19:55,580 - main - INFO - Running configuration 14/756:
2025-05-01 00:19:55,580 - main - INFO - Model: ResNet50
2025-05-01 00:19:55,580 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:19:55,580 - main - INFO - Scheduler: StepLR
2025-05-01 00:19:55,580 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:19:55,580 - main - INFO - ==================================================
2025-05-01 00:19:55,580 - training.model_ResNet50_opt_SGD_lr_0.001_id_14 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_14
2025-05-01 00:19:55,580 - training.model_ResNet50_opt_SGD_lr_0.001_id_14 - INFO - Config: {
  "id": 14,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:19:55,741 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:19:55,741 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 14,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:19:55,741 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:19:55,798 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:19:55,799 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:19:55,801 - training.model_ResNet50_opt_SGD_lr_0.001_id_14 - INFO - Starting model evaluation
2025-05-01 00:20:04,661 - training.model_ResNet50_opt_SGD_lr_0.001_id_14 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.9057
  Recall:    0.9406
  F1 Score:  0.9228
  IoU:       0.8567
  mAP:       0.9827
  AUC:       0.9915
2025-05-01 00:20:04,663 - training.model_ResNet50_opt_SGD_lr_0.001_id_14 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_14/final_results.json
2025-05-01 00:20:04,665 - training.model_ResNet50_opt_SGD_lr_0.001_id_14 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_14/final_results.json
2025-05-01 00:20:04,665 - main - INFO - 
Summary for configuration 14:
2025-05-01 00:20:04,665 - main - INFO - Accuracy: 0.9554
2025-05-01 00:20:04,665 - main - INFO - Precision: 0.9057
2025-05-01 00:20:04,665 - main - INFO - Recall: 0.9406
2025-05-01 00:20:04,665 - main - INFO - F1 Score: 0.9228
2025-05-01 00:20:04,665 - main - INFO - IoU: 0.8567
2025-05-01 00:20:04,665 - main - INFO - mAP: 0.9827
2025-05-01 00:20:04,665 - main - INFO - AUC: 0.9915
2025-05-01 00:20:04,665 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:20:04,665 - main - INFO - 
==================================================
2025-05-01 00:20:04,665 - main - INFO - Running configuration 15/756:
2025-05-01 00:20:04,665 - main - INFO - Model: ResNet50
2025-05-01 00:20:04,665 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:20:04,665 - main - INFO - Scheduler: StepLR
2025-05-01 00:20:04,665 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:20:04,665 - main - INFO - ==================================================
2025-05-01 00:20:04,665 - training.model_ResNet50_opt_SGD_lr_0.001_id_15 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_15
2025-05-01 00:20:04,665 - training.model_ResNet50_opt_SGD_lr_0.001_id_15 - INFO - Config: {
  "id": 15,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:20:04,844 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:20:04,844 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 15,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:20:04,844 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:20:04,913 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:20:04,914 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:20:04,916 - training.model_ResNet50_opt_SGD_lr_0.001_id_15 - INFO - Starting model evaluation
2025-05-01 00:20:13,821 - training.model_ResNet50_opt_SGD_lr_0.001_id_15 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.9057
  Recall:    0.9406
  F1 Score:  0.9228
  IoU:       0.8567
  mAP:       0.9843
  AUC:       0.9924
2025-05-01 00:20:13,823 - training.model_ResNet50_opt_SGD_lr_0.001_id_15 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_15/final_results.json
2025-05-01 00:20:13,824 - training.model_ResNet50_opt_SGD_lr_0.001_id_15 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_15/final_results.json
2025-05-01 00:20:13,824 - main - INFO - 
Summary for configuration 15:
2025-05-01 00:20:13,824 - main - INFO - Accuracy: 0.9554
2025-05-01 00:20:13,824 - main - INFO - Precision: 0.9057
2025-05-01 00:20:13,824 - main - INFO - Recall: 0.9406
2025-05-01 00:20:13,824 - main - INFO - F1 Score: 0.9228
2025-05-01 00:20:13,824 - main - INFO - IoU: 0.8567
2025-05-01 00:20:13,824 - main - INFO - mAP: 0.9843
2025-05-01 00:20:13,824 - main - INFO - AUC: 0.9924
2025-05-01 00:20:13,824 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:20:13,824 - main - INFO - 
==================================================
2025-05-01 00:20:13,824 - main - INFO - Running configuration 16/756:
2025-05-01 00:20:13,824 - main - INFO - Model: ResNet50
2025-05-01 00:20:13,824 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:20:13,824 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:20:13,824 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:20:13,824 - main - INFO - ==================================================
2025-05-01 00:20:13,824 - training.model_ResNet50_opt_SGD_lr_0.001_id_16 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_16
2025-05-01 00:20:13,825 - training.model_ResNet50_opt_SGD_lr_0.001_id_16 - INFO - Config: {
  "id": 16,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:20:13,985 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:20:13,985 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 16,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:20:13,986 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:20:14,043 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:20:14,044 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:20:14,045 - training.model_ResNet50_opt_SGD_lr_0.001_id_16 - INFO - Starting model evaluation
2025-05-01 00:20:22,849 - training.model_ResNet50_opt_SGD_lr_0.001_id_16 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.9088
  Recall:    0.9406
  F1 Score:  0.9244
  IoU:       0.8594
  mAP:       0.9823
  AUC:       0.9908
2025-05-01 00:20:22,851 - training.model_ResNet50_opt_SGD_lr_0.001_id_16 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_16/final_results.json
2025-05-01 00:20:22,852 - training.model_ResNet50_opt_SGD_lr_0.001_id_16 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_16/final_results.json
2025-05-01 00:20:22,852 - main - INFO - 
Summary for configuration 16:
2025-05-01 00:20:22,852 - main - INFO - Accuracy: 0.9564
2025-05-01 00:20:22,852 - main - INFO - Precision: 0.9088
2025-05-01 00:20:22,852 - main - INFO - Recall: 0.9406
2025-05-01 00:20:22,852 - main - INFO - F1 Score: 0.9244
2025-05-01 00:20:22,852 - main - INFO - IoU: 0.8594
2025-05-01 00:20:22,852 - main - INFO - mAP: 0.9823
2025-05-01 00:20:22,852 - main - INFO - AUC: 0.9908
2025-05-01 00:20:22,852 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:20:22,852 - main - INFO - 
==================================================
2025-05-01 00:20:22,852 - main - INFO - Running configuration 17/756:
2025-05-01 00:20:22,853 - main - INFO - Model: ResNet50
2025-05-01 00:20:22,853 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:20:22,853 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:20:22,853 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:20:22,853 - main - INFO - ==================================================
2025-05-01 00:20:22,853 - training.model_ResNet50_opt_SGD_lr_0.001_id_17 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_17
2025-05-01 00:20:22,853 - training.model_ResNet50_opt_SGD_lr_0.001_id_17 - INFO - Config: {
  "id": 17,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:20:23,018 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:20:23,018 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 17,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:20:23,018 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:20:23,076 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:20:23,076 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:20:23,078 - training.model_ResNet50_opt_SGD_lr_0.001_id_17 - INFO - Starting model evaluation
2025-05-01 00:20:31,737 - training.model_ResNet50_opt_SGD_lr_0.001_id_17 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.9116
  Recall:    0.9371
  F1 Score:  0.9241
  IoU:       0.8590
  mAP:       0.9840
  AUC:       0.9921
2025-05-01 00:20:31,738 - training.model_ResNet50_opt_SGD_lr_0.001_id_17 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_17/final_results.json
2025-05-01 00:20:31,740 - training.model_ResNet50_opt_SGD_lr_0.001_id_17 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_17/final_results.json
2025-05-01 00:20:31,740 - main - INFO - 
Summary for configuration 17:
2025-05-01 00:20:31,740 - main - INFO - Accuracy: 0.9564
2025-05-01 00:20:31,740 - main - INFO - Precision: 0.9116
2025-05-01 00:20:31,740 - main - INFO - Recall: 0.9371
2025-05-01 00:20:31,740 - main - INFO - F1 Score: 0.9241
2025-05-01 00:20:31,740 - main - INFO - IoU: 0.8590
2025-05-01 00:20:31,740 - main - INFO - mAP: 0.9840
2025-05-01 00:20:31,740 - main - INFO - AUC: 0.9921
2025-05-01 00:20:31,740 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:20:31,740 - main - INFO - 
==================================================
2025-05-01 00:20:31,740 - main - INFO - Running configuration 18/756:
2025-05-01 00:20:31,740 - main - INFO - Model: ResNet50
2025-05-01 00:20:31,740 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:20:31,740 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:20:31,740 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:20:31,740 - main - INFO - ==================================================
2025-05-01 00:20:31,740 - training.model_ResNet50_opt_SGD_lr_0.001_id_18 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_18
2025-05-01 00:20:31,740 - training.model_ResNet50_opt_SGD_lr_0.001_id_18 - INFO - Config: {
  "id": 18,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:20:31,917 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:20:31,918 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 18,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:20:31,918 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:20:31,976 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:20:31,977 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:20:31,979 - training.model_ResNet50_opt_SGD_lr_0.001_id_18 - INFO - Starting model evaluation
2025-05-01 00:20:40,614 - training.model_ResNet50_opt_SGD_lr_0.001_id_18 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.8993
  Recall:    0.9371
  F1 Score:  0.9178
  IoU:       0.8481
  mAP:       0.9830
  AUC:       0.9923
2025-05-01 00:20:40,616 - training.model_ResNet50_opt_SGD_lr_0.001_id_18 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_18/final_results.json
2025-05-01 00:20:40,618 - training.model_ResNet50_opt_SGD_lr_0.001_id_18 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_18/final_results.json
2025-05-01 00:20:40,618 - main - INFO - 
Summary for configuration 18:
2025-05-01 00:20:40,618 - main - INFO - Accuracy: 0.9524
2025-05-01 00:20:40,618 - main - INFO - Precision: 0.8993
2025-05-01 00:20:40,618 - main - INFO - Recall: 0.9371
2025-05-01 00:20:40,618 - main - INFO - F1 Score: 0.9178
2025-05-01 00:20:40,618 - main - INFO - IoU: 0.8481
2025-05-01 00:20:40,618 - main - INFO - mAP: 0.9830
2025-05-01 00:20:40,618 - main - INFO - AUC: 0.9923
2025-05-01 00:20:40,618 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:20:40,618 - main - INFO - 
==================================================
2025-05-01 00:20:40,618 - main - INFO - Running configuration 19/756:
2025-05-01 00:20:40,618 - main - INFO - Model: ResNet50
2025-05-01 00:20:40,618 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:20:40,618 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:20:40,618 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:20:40,618 - main - INFO - ==================================================
2025-05-01 00:20:40,618 - training.model_ResNet50_opt_SGD_lr_0.001_id_19 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_19
2025-05-01 00:20:40,618 - training.model_ResNet50_opt_SGD_lr_0.001_id_19 - INFO - Config: {
  "id": 19,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:20:40,778 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:20:40,778 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 19,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:20:40,778 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:20:40,833 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:20:40,834 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:20:40,836 - training.model_ResNet50_opt_SGD_lr_0.001_id_19 - INFO - Starting model evaluation
2025-05-01 00:20:49,704 - training.model_ResNet50_opt_SGD_lr_0.001_id_19 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.9091
  Recall:    0.9441
  F1 Score:  0.9262
  IoU:       0.8626
  mAP:       0.9827
  AUC:       0.9915
2025-05-01 00:20:49,718 - training.model_ResNet50_opt_SGD_lr_0.001_id_19 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_19/final_results.json
2025-05-01 00:20:49,720 - training.model_ResNet50_opt_SGD_lr_0.001_id_19 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_19/final_results.json
2025-05-01 00:20:49,720 - main - INFO - 
Summary for configuration 19:
2025-05-01 00:20:49,720 - main - INFO - Accuracy: 0.9574
2025-05-01 00:20:49,720 - main - INFO - Precision: 0.9091
2025-05-01 00:20:49,720 - main - INFO - Recall: 0.9441
2025-05-01 00:20:49,720 - main - INFO - F1 Score: 0.9262
2025-05-01 00:20:49,720 - main - INFO - IoU: 0.8626
2025-05-01 00:20:49,720 - main - INFO - mAP: 0.9827
2025-05-01 00:20:49,720 - main - INFO - AUC: 0.9915
2025-05-01 00:20:49,720 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:20:49,720 - main - INFO - 
==================================================
2025-05-01 00:20:49,720 - main - INFO - Running configuration 20/756:
2025-05-01 00:20:49,720 - main - INFO - Model: ResNet50
2025-05-01 00:20:49,720 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:20:49,720 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:20:49,720 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:20:49,720 - main - INFO - ==================================================
2025-05-01 00:20:49,720 - training.model_ResNet50_opt_SGD_lr_0.001_id_20 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_20
2025-05-01 00:20:49,720 - training.model_ResNet50_opt_SGD_lr_0.001_id_20 - INFO - Config: {
  "id": 20,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:20:49,895 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:20:49,896 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 20,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:20:49,896 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:20:49,958 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:20:49,959 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:20:49,961 - training.model_ResNet50_opt_SGD_lr_0.001_id_20 - INFO - Starting model evaluation
2025-05-01 00:20:58,749 - training.model_ResNet50_opt_SGD_lr_0.001_id_20 - INFO - Evaluation metrics:
  Accuracy:  0.9534
  Precision: 0.8970
  Recall:    0.9441
  F1 Score:  0.9199
  IoU:       0.8517
  mAP:       0.9818
  AUC:       0.9912
2025-05-01 00:20:58,750 - training.model_ResNet50_opt_SGD_lr_0.001_id_20 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_20/final_results.json
2025-05-01 00:20:58,752 - training.model_ResNet50_opt_SGD_lr_0.001_id_20 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_20/final_results.json
2025-05-01 00:20:58,752 - main - INFO - 
Summary for configuration 20:
2025-05-01 00:20:58,752 - main - INFO - Accuracy: 0.9534
2025-05-01 00:20:58,752 - main - INFO - Precision: 0.8970
2025-05-01 00:20:58,752 - main - INFO - Recall: 0.9441
2025-05-01 00:20:58,752 - main - INFO - F1 Score: 0.9199
2025-05-01 00:20:58,752 - main - INFO - IoU: 0.8517
2025-05-01 00:20:58,752 - main - INFO - mAP: 0.9818
2025-05-01 00:20:58,752 - main - INFO - AUC: 0.9912
2025-05-01 00:20:58,752 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:20:58,752 - main - INFO - 
==================================================
2025-05-01 00:20:58,752 - main - INFO - Running configuration 21/756:
2025-05-01 00:20:58,752 - main - INFO - Model: ResNet50
2025-05-01 00:20:58,752 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:20:58,752 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:20:58,752 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:20:58,752 - main - INFO - ==================================================
2025-05-01 00:20:58,752 - training.model_ResNet50_opt_SGD_lr_0.001_id_21 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_21
2025-05-01 00:20:58,752 - training.model_ResNet50_opt_SGD_lr_0.001_id_21 - INFO - Config: {
  "id": 21,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:20:58,916 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:20:58,917 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 21,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:20:58,917 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:20:58,971 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:20:58,972 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:20:58,973 - training.model_ResNet50_opt_SGD_lr_0.001_id_21 - INFO - Starting model evaluation
2025-05-01 00:21:07,664 - training.model_ResNet50_opt_SGD_lr_0.001_id_21 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9054
  Recall:    0.9371
  F1 Score:  0.9210
  IoU:       0.8535
  mAP:       0.9838
  AUC:       0.9926
2025-05-01 00:21:07,665 - training.model_ResNet50_opt_SGD_lr_0.001_id_21 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_21/final_results.json
2025-05-01 00:21:07,667 - training.model_ResNet50_opt_SGD_lr_0.001_id_21 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_21/final_results.json
2025-05-01 00:21:07,667 - main - INFO - 
Summary for configuration 21:
2025-05-01 00:21:07,667 - main - INFO - Accuracy: 0.9544
2025-05-01 00:21:07,667 - main - INFO - Precision: 0.9054
2025-05-01 00:21:07,667 - main - INFO - Recall: 0.9371
2025-05-01 00:21:07,667 - main - INFO - F1 Score: 0.9210
2025-05-01 00:21:07,667 - main - INFO - IoU: 0.8535
2025-05-01 00:21:07,667 - main - INFO - mAP: 0.9838
2025-05-01 00:21:07,667 - main - INFO - AUC: 0.9926
2025-05-01 00:21:07,667 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:21:07,667 - main - INFO - 
==================================================
2025-05-01 00:21:07,667 - main - INFO - Running configuration 22/756:
2025-05-01 00:21:07,667 - main - INFO - Model: ResNet50
2025-05-01 00:21:07,667 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:21:07,667 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:21:07,667 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:21:07,667 - main - INFO - ==================================================
2025-05-01 00:21:07,667 - training.model_ResNet50_opt_SGD_lr_0.001_id_22 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_22
2025-05-01 00:21:07,667 - training.model_ResNet50_opt_SGD_lr_0.001_id_22 - INFO - Config: {
  "id": 22,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:21:07,843 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:21:07,843 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 22,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:21:07,843 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:21:07,908 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:21:07,909 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:21:07,911 - training.model_ResNet50_opt_SGD_lr_0.001_id_22 - INFO - Starting model evaluation
2025-05-01 00:21:16,735 - training.model_ResNet50_opt_SGD_lr_0.001_id_22 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.9088
  Recall:    0.9406
  F1 Score:  0.9244
  IoU:       0.8594
  mAP:       0.9824
  AUC:       0.9906
2025-05-01 00:21:16,737 - training.model_ResNet50_opt_SGD_lr_0.001_id_22 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_22/final_results.json
2025-05-01 00:21:16,738 - training.model_ResNet50_opt_SGD_lr_0.001_id_22 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_22/final_results.json
2025-05-01 00:21:16,739 - main - INFO - 
Summary for configuration 22:
2025-05-01 00:21:16,739 - main - INFO - Accuracy: 0.9564
2025-05-01 00:21:16,739 - main - INFO - Precision: 0.9088
2025-05-01 00:21:16,739 - main - INFO - Recall: 0.9406
2025-05-01 00:21:16,739 - main - INFO - F1 Score: 0.9244
2025-05-01 00:21:16,739 - main - INFO - IoU: 0.8594
2025-05-01 00:21:16,739 - main - INFO - mAP: 0.9824
2025-05-01 00:21:16,739 - main - INFO - AUC: 0.9906
2025-05-01 00:21:16,739 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:21:16,739 - main - INFO - 
==================================================
2025-05-01 00:21:16,739 - main - INFO - Running configuration 23/756:
2025-05-01 00:21:16,739 - main - INFO - Model: ResNet50
2025-05-01 00:21:16,739 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:21:16,739 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:21:16,739 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:21:16,739 - main - INFO - ==================================================
2025-05-01 00:21:16,739 - training.model_ResNet50_opt_SGD_lr_0.001_id_23 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_23
2025-05-01 00:21:16,739 - training.model_ResNet50_opt_SGD_lr_0.001_id_23 - INFO - Config: {
  "id": 23,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:21:16,914 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:21:16,914 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 23,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:21:16,914 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:21:16,970 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:21:16,971 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:21:16,972 - training.model_ResNet50_opt_SGD_lr_0.001_id_23 - INFO - Starting model evaluation
2025-05-01 00:21:25,691 - training.model_ResNet50_opt_SGD_lr_0.001_id_23 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.9116
  Recall:    0.9371
  F1 Score:  0.9241
  IoU:       0.8590
  mAP:       0.9823
  AUC:       0.9911
2025-05-01 00:21:25,694 - training.model_ResNet50_opt_SGD_lr_0.001_id_23 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_23/final_results.json
2025-05-01 00:21:25,696 - training.model_ResNet50_opt_SGD_lr_0.001_id_23 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_23/final_results.json
2025-05-01 00:21:25,696 - main - INFO - 
Summary for configuration 23:
2025-05-01 00:21:25,696 - main - INFO - Accuracy: 0.9564
2025-05-01 00:21:25,696 - main - INFO - Precision: 0.9116
2025-05-01 00:21:25,696 - main - INFO - Recall: 0.9371
2025-05-01 00:21:25,696 - main - INFO - F1 Score: 0.9241
2025-05-01 00:21:25,696 - main - INFO - IoU: 0.8590
2025-05-01 00:21:25,696 - main - INFO - mAP: 0.9823
2025-05-01 00:21:25,696 - main - INFO - AUC: 0.9911
2025-05-01 00:21:25,696 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:21:25,696 - main - INFO - 
==================================================
2025-05-01 00:21:25,696 - main - INFO - Running configuration 24/756:
2025-05-01 00:21:25,696 - main - INFO - Model: ResNet50
2025-05-01 00:21:25,696 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 00:21:25,696 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:21:25,696 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:21:25,696 - main - INFO - ==================================================
2025-05-01 00:21:25,696 - training.model_ResNet50_opt_SGD_lr_0.001_id_24 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001_id_24
2025-05-01 00:21:25,696 - training.model_ResNet50_opt_SGD_lr_0.001_id_24 - INFO - Config: {
  "id": 24,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:21:25,874 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.001
2025-05-01 00:21:25,874 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 24,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:21:25,874 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 00:21:25,937 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 00:21:25,938 - training.model_ResNet50_opt_SGD_lr_0.001 - INFO - Training completed after 1027.53 seconds
2025-05-01 00:21:25,940 - training.model_ResNet50_opt_SGD_lr_0.001_id_24 - INFO - Starting model evaluation
2025-05-01 00:21:34,783 - training.model_ResNet50_opt_SGD_lr_0.001_id_24 - INFO - Evaluation metrics:
  Accuracy:  0.9584
  Precision: 0.9150
  Recall:    0.9406
  F1 Score:  0.9276
  IoU:       0.8650
  mAP:       0.9823
  AUC:       0.9909
2025-05-01 00:21:34,785 - training.model_ResNet50_opt_SGD_lr_0.001_id_24 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_24/final_results.json
2025-05-01 00:21:34,786 - training.model_ResNet50_opt_SGD_lr_0.001_id_24 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.001_id_24/final_results.json
2025-05-01 00:21:34,786 - main - INFO - 
Summary for configuration 24:
2025-05-01 00:21:34,786 - main - INFO - Accuracy: 0.9584
2025-05-01 00:21:34,786 - main - INFO - Precision: 0.9150
2025-05-01 00:21:34,786 - main - INFO - Recall: 0.9406
2025-05-01 00:21:34,786 - main - INFO - F1 Score: 0.9276
2025-05-01 00:21:34,786 - main - INFO - IoU: 0.8650
2025-05-01 00:21:34,786 - main - INFO - mAP: 0.9823
2025-05-01 00:21:34,786 - main - INFO - AUC: 0.9909
2025-05-01 00:21:34,786 - main - INFO - Training time: 1027.53 seconds
2025-05-01 00:21:34,786 - main - INFO - 
==================================================
2025-05-01 00:21:34,786 - main - INFO - Running configuration 25/756:
2025-05-01 00:21:34,786 - main - INFO - Model: ResNet50
2025-05-01 00:21:34,786 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:21:34,786 - main - INFO - Scheduler: StepLR
2025-05-01 00:21:34,786 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:21:34,786 - main - INFO - ==================================================
2025-05-01 00:21:34,786 - training.model_ResNet50_opt_SGD_lr_0.0001_id_25 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_25
2025-05-01 00:21:34,786 - training.model_ResNet50_opt_SGD_lr_0.0001_id_25 - INFO - Config: {
  "id": 25,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:21:34,963 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:21:34,963 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 25,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:21:34,963 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 00:21:34,964 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 00:21:56,778 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 00:22:26,291 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.6938
2025-05-01 00:22:26,343 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 1 completed in 51.38s - Train Loss: 0.6843, Train Acc: 0.5912, Val Loss: 0.6722, Val Acc: 0.6938
2025-05-01 00:22:26,513 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 00:22:26,513 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 00:22:48,553 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 00:23:17,816 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.6938 to 0.7410
2025-05-01 00:23:17,893 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 2 completed in 51.38s - Train Loss: 0.6666, Train Acc: 0.7169, Val Loss: 0.6555, Val Acc: 0.7410
2025-05-01 00:23:18,036 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 00:23:18,036 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 00:23:39,983 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 00:24:09,512 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7410 to 0.7530
2025-05-01 00:24:09,587 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 3 completed in 51.55s - Train Loss: 0.6479, Train Acc: 0.7557, Val Loss: 0.6391, Val Acc: 0.7530
2025-05-01 00:24:09,734 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 00:24:09,734 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 00:24:31,764 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 00:25:01,148 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7530 to 0.7684
2025-05-01 00:25:01,240 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 4 completed in 51.51s - Train Loss: 0.6295, Train Acc: 0.7666, Val Loss: 0.6152, Val Acc: 0.7684
2025-05-01 00:25:01,391 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 00:25:01,391 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 00:25:23,212 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 00:25:52,508 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7684 to 0.7864
2025-05-01 00:25:52,597 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 5 completed in 51.21s - Train Loss: 0.6112, Train Acc: 0.7735, Val Loss: 0.5962, Val Acc: 0.7864
2025-05-01 00:25:52,747 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 00:25:52,748 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 00:26:14,541 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 00:26:43,949 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7864 to 0.7873
2025-05-01 00:26:44,030 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 6 completed in 51.28s - Train Loss: 0.5926, Train Acc: 0.7846, Val Loss: 0.5795, Val Acc: 0.7873
2025-05-01 00:26:44,181 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 00:26:44,181 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 00:27:05,907 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 00:27:35,259 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7873 to 0.7976
2025-05-01 00:27:35,337 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 7 completed in 51.16s - Train Loss: 0.5758, Train Acc: 0.7954, Val Loss: 0.5650, Val Acc: 0.7976
2025-05-01 00:27:35,482 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 00:27:35,482 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 00:27:57,555 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 00:28:27,220 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7976 to 0.8002
2025-05-01 00:28:27,299 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 8 completed in 51.82s - Train Loss: 0.5621, Train Acc: 0.8009, Val Loss: 0.5520, Val Acc: 0.8002
2025-05-01 00:28:27,443 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 00:28:27,444 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 00:28:49,239 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 00:29:18,790 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8002 to 0.8062
2025-05-01 00:29:18,869 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 9 completed in 51.42s - Train Loss: 0.5471, Train Acc: 0.8069, Val Loss: 0.5389, Val Acc: 0.8062
2025-05-01 00:29:19,005 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 00:29:19,005 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 00:29:41,152 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 00:30:10,280 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8062 to 0.8156
2025-05-01 00:30:10,359 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 10 completed in 51.35s - Train Loss: 0.5352, Train Acc: 0.8155, Val Loss: 0.5313, Val Acc: 0.8156
2025-05-01 00:30:10,499 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 00:30:10,500 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 00:30:31,982 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 00:31:01,854 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8156 to 0.8173
2025-05-01 00:31:01,935 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 11 completed in 51.43s - Train Loss: 0.5293, Train Acc: 0.8207, Val Loss: 0.5265, Val Acc: 0.8173
2025-05-01 00:31:02,095 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 00:31:02,095 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 00:31:24,396 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 00:31:53,504 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8173 to 0.8233
2025-05-01 00:31:53,592 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 12 completed in 51.50s - Train Loss: 0.5285, Train Acc: 0.8172, Val Loss: 0.5237, Val Acc: 0.8233
2025-05-01 00:31:53,731 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 00:31:53,732 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 00:32:15,663 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 00:32:44,760 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation loss improved from inf to 0.5246
2025-05-01 00:32:44,842 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 13 completed in 51.11s - Train Loss: 0.5281, Train Acc: 0.8172, Val Loss: 0.5246, Val Acc: 0.8156
2025-05-01 00:32:44,989 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 00:32:44,989 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 00:33:07,117 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 00:33:36,432 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5246 to 0.5243
2025-05-01 00:33:36,516 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 14 completed in 51.53s - Train Loss: 0.5262, Train Acc: 0.8220, Val Loss: 0.5243, Val Acc: 0.8190
2025-05-01 00:33:36,656 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 00:33:36,657 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 00:33:58,541 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 00:34:27,877 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5243 to 0.5225
2025-05-01 00:34:27,967 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 15 completed in 51.31s - Train Loss: 0.5254, Train Acc: 0.8237, Val Loss: 0.5225, Val Acc: 0.8199
2025-05-01 00:34:28,114 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 00:34:28,114 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 00:34:50,016 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 00:35:19,526 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5225 to 0.5213
2025-05-01 00:35:19,612 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 16 completed in 51.50s - Train Loss: 0.5250, Train Acc: 0.8179, Val Loss: 0.5213, Val Acc: 0.8208
2025-05-01 00:35:19,758 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 00:35:19,758 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 00:35:41,561 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 00:36:11,126 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5213 to 0.5206
2025-05-01 00:36:11,213 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 17 completed in 51.45s - Train Loss: 0.5229, Train Acc: 0.8254, Val Loss: 0.5206, Val Acc: 0.8105
2025-05-01 00:36:11,354 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 00:36:11,355 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 00:36:33,505 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 00:37:02,892 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5206 to 0.5197
2025-05-01 00:37:02,983 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 18 completed in 51.63s - Train Loss: 0.5212, Train Acc: 0.8269, Val Loss: 0.5197, Val Acc: 0.8156
2025-05-01 00:37:03,138 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 00:37:03,139 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 00:37:24,910 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 00:37:54,811 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5197 to 0.5178
2025-05-01 00:37:54,892 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 19 completed in 51.75s - Train Loss: 0.5211, Train Acc: 0.8220, Val Loss: 0.5178, Val Acc: 0.8208
2025-05-01 00:37:55,031 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 00:37:55,032 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 00:38:16,676 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 00:38:46,580 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5178 to 0.5161
2025-05-01 00:38:46,659 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Epoch 20 completed in 51.63s - Train Loss: 0.5204, Train Acc: 0.8267, Val Loss: 0.5161, Val Acc: 0.8199
2025-05-01 00:38:46,802 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 00:38:46,802 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:38:46,804 - training.model_ResNet50_opt_SGD_lr_0.0001_id_25 - INFO - Starting model evaluation
2025-05-01 00:38:55,584 - training.model_ResNet50_opt_SGD_lr_0.0001_id_25 - INFO - Evaluation metrics:
  Accuracy:  0.8523
  Precision: 0.7338
  Recall:    0.7517
  F1 Score:  0.7427
  IoU:       0.5907
  mAP:       0.8507
  AUC:       0.9101
2025-05-01 00:38:55,586 - training.model_ResNet50_opt_SGD_lr_0.0001_id_25 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_25/final_results.json
2025-05-01 00:38:55,589 - training.model_ResNet50_opt_SGD_lr_0.0001_id_25 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_25/final_results.json
2025-05-01 00:38:55,589 - main - INFO - 
Summary for configuration 25:
2025-05-01 00:38:55,589 - main - INFO - Accuracy: 0.8523
2025-05-01 00:38:55,589 - main - INFO - Precision: 0.7338
2025-05-01 00:38:55,589 - main - INFO - Recall: 0.7517
2025-05-01 00:38:55,589 - main - INFO - F1 Score: 0.7427
2025-05-01 00:38:55,589 - main - INFO - IoU: 0.5907
2025-05-01 00:38:55,589 - main - INFO - mAP: 0.8507
2025-05-01 00:38:55,589 - main - INFO - AUC: 0.9101
2025-05-01 00:38:55,589 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:38:55,589 - main - INFO - 
==================================================
2025-05-01 00:38:55,589 - main - INFO - Running configuration 26/756:
2025-05-01 00:38:55,589 - main - INFO - Model: ResNet50
2025-05-01 00:38:55,589 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:38:55,589 - main - INFO - Scheduler: StepLR
2025-05-01 00:38:55,589 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:38:55,589 - main - INFO - ==================================================
2025-05-01 00:38:55,590 - training.model_ResNet50_opt_SGD_lr_0.0001_id_26 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_26
2025-05-01 00:38:55,590 - training.model_ResNet50_opt_SGD_lr_0.0001_id_26 - INFO - Config: {
  "id": 26,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:38:55,775 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:38:55,775 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 26,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:38:55,775 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:38:55,849 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:38:55,850 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:38:55,853 - training.model_ResNet50_opt_SGD_lr_0.0001_id_26 - INFO - Starting model evaluation
2025-05-01 00:39:04,579 - training.model_ResNet50_opt_SGD_lr_0.0001_id_26 - INFO - Evaluation metrics:
  Accuracy:  0.8494
  Precision: 0.7264
  Recall:    0.7517
  F1 Score:  0.7388
  IoU:       0.5858
  mAP:       0.8542
  AUC:       0.9125
2025-05-01 00:39:04,580 - training.model_ResNet50_opt_SGD_lr_0.0001_id_26 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_26/final_results.json
2025-05-01 00:39:04,582 - training.model_ResNet50_opt_SGD_lr_0.0001_id_26 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_26/final_results.json
2025-05-01 00:39:04,582 - main - INFO - 
Summary for configuration 26:
2025-05-01 00:39:04,582 - main - INFO - Accuracy: 0.8494
2025-05-01 00:39:04,582 - main - INFO - Precision: 0.7264
2025-05-01 00:39:04,582 - main - INFO - Recall: 0.7517
2025-05-01 00:39:04,582 - main - INFO - F1 Score: 0.7388
2025-05-01 00:39:04,582 - main - INFO - IoU: 0.5858
2025-05-01 00:39:04,582 - main - INFO - mAP: 0.8542
2025-05-01 00:39:04,582 - main - INFO - AUC: 0.9125
2025-05-01 00:39:04,582 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:39:04,582 - main - INFO - 
==================================================
2025-05-01 00:39:04,582 - main - INFO - Running configuration 27/756:
2025-05-01 00:39:04,582 - main - INFO - Model: ResNet50
2025-05-01 00:39:04,582 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:39:04,582 - main - INFO - Scheduler: StepLR
2025-05-01 00:39:04,582 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:39:04,582 - main - INFO - ==================================================
2025-05-01 00:39:04,582 - training.model_ResNet50_opt_SGD_lr_0.0001_id_27 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_27
2025-05-01 00:39:04,582 - training.model_ResNet50_opt_SGD_lr_0.0001_id_27 - INFO - Config: {
  "id": 27,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:39:04,745 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:39:04,745 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 27,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:39:04,745 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:39:04,802 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:39:04,803 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:39:04,804 - training.model_ResNet50_opt_SGD_lr_0.0001_id_27 - INFO - Starting model evaluation
2025-05-01 00:39:13,513 - training.model_ResNet50_opt_SGD_lr_0.0001_id_27 - INFO - Evaluation metrics:
  Accuracy:  0.8474
  Precision: 0.7276
  Recall:    0.7378
  F1 Score:  0.7326
  IoU:       0.5781
  mAP:       0.8494
  AUC:       0.9099
2025-05-01 00:39:13,514 - training.model_ResNet50_opt_SGD_lr_0.0001_id_27 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_27/final_results.json
2025-05-01 00:39:13,516 - training.model_ResNet50_opt_SGD_lr_0.0001_id_27 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_27/final_results.json
2025-05-01 00:39:13,516 - main - INFO - 
Summary for configuration 27:
2025-05-01 00:39:13,516 - main - INFO - Accuracy: 0.8474
2025-05-01 00:39:13,516 - main - INFO - Precision: 0.7276
2025-05-01 00:39:13,516 - main - INFO - Recall: 0.7378
2025-05-01 00:39:13,516 - main - INFO - F1 Score: 0.7326
2025-05-01 00:39:13,516 - main - INFO - IoU: 0.5781
2025-05-01 00:39:13,516 - main - INFO - mAP: 0.8494
2025-05-01 00:39:13,516 - main - INFO - AUC: 0.9099
2025-05-01 00:39:13,516 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:39:13,516 - main - INFO - 
==================================================
2025-05-01 00:39:13,516 - main - INFO - Running configuration 28/756:
2025-05-01 00:39:13,516 - main - INFO - Model: ResNet50
2025-05-01 00:39:13,516 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:39:13,516 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:39:13,516 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:39:13,516 - main - INFO - ==================================================
2025-05-01 00:39:13,516 - training.model_ResNet50_opt_SGD_lr_0.0001_id_28 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_28
2025-05-01 00:39:13,516 - training.model_ResNet50_opt_SGD_lr_0.0001_id_28 - INFO - Config: {
  "id": 28,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:39:13,693 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:39:13,693 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 28,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:39:13,693 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:39:13,750 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:39:13,750 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:39:13,752 - training.model_ResNet50_opt_SGD_lr_0.0001_id_28 - INFO - Starting model evaluation
2025-05-01 00:39:22,437 - training.model_ResNet50_opt_SGD_lr_0.0001_id_28 - INFO - Evaluation metrics:
  Accuracy:  0.8464
  Precision: 0.7235
  Recall:    0.7413
  F1 Score:  0.7323
  IoU:       0.5777
  mAP:       0.8498
  AUC:       0.9075
2025-05-01 00:39:22,439 - training.model_ResNet50_opt_SGD_lr_0.0001_id_28 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_28/final_results.json
2025-05-01 00:39:22,440 - training.model_ResNet50_opt_SGD_lr_0.0001_id_28 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_28/final_results.json
2025-05-01 00:39:22,440 - main - INFO - 
Summary for configuration 28:
2025-05-01 00:39:22,440 - main - INFO - Accuracy: 0.8464
2025-05-01 00:39:22,440 - main - INFO - Precision: 0.7235
2025-05-01 00:39:22,440 - main - INFO - Recall: 0.7413
2025-05-01 00:39:22,440 - main - INFO - F1 Score: 0.7323
2025-05-01 00:39:22,440 - main - INFO - IoU: 0.5777
2025-05-01 00:39:22,440 - main - INFO - mAP: 0.8498
2025-05-01 00:39:22,440 - main - INFO - AUC: 0.9075
2025-05-01 00:39:22,440 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:39:22,440 - main - INFO - 
==================================================
2025-05-01 00:39:22,440 - main - INFO - Running configuration 29/756:
2025-05-01 00:39:22,440 - main - INFO - Model: ResNet50
2025-05-01 00:39:22,440 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:39:22,440 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:39:22,440 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:39:22,440 - main - INFO - ==================================================
2025-05-01 00:39:22,440 - training.model_ResNet50_opt_SGD_lr_0.0001_id_29 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_29
2025-05-01 00:39:22,441 - training.model_ResNet50_opt_SGD_lr_0.0001_id_29 - INFO - Config: {
  "id": 29,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:39:22,601 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:39:22,601 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 29,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:39:22,601 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:39:22,658 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:39:22,659 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:39:22,661 - training.model_ResNet50_opt_SGD_lr_0.0001_id_29 - INFO - Starting model evaluation
2025-05-01 00:39:31,480 - training.model_ResNet50_opt_SGD_lr_0.0001_id_29 - INFO - Evaluation metrics:
  Accuracy:  0.8414
  Precision: 0.7100
  Recall:    0.7448
  F1 Score:  0.7270
  IoU:       0.5710
  mAP:       0.8461
  AUC:       0.9076
2025-05-01 00:39:31,481 - training.model_ResNet50_opt_SGD_lr_0.0001_id_29 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_29/final_results.json
2025-05-01 00:39:31,483 - training.model_ResNet50_opt_SGD_lr_0.0001_id_29 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_29/final_results.json
2025-05-01 00:39:31,483 - main - INFO - 
Summary for configuration 29:
2025-05-01 00:39:31,483 - main - INFO - Accuracy: 0.8414
2025-05-01 00:39:31,483 - main - INFO - Precision: 0.7100
2025-05-01 00:39:31,483 - main - INFO - Recall: 0.7448
2025-05-01 00:39:31,483 - main - INFO - F1 Score: 0.7270
2025-05-01 00:39:31,483 - main - INFO - IoU: 0.5710
2025-05-01 00:39:31,483 - main - INFO - mAP: 0.8461
2025-05-01 00:39:31,483 - main - INFO - AUC: 0.9076
2025-05-01 00:39:31,483 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:39:31,483 - main - INFO - 
==================================================
2025-05-01 00:39:31,483 - main - INFO - Running configuration 30/756:
2025-05-01 00:39:31,483 - main - INFO - Model: ResNet50
2025-05-01 00:39:31,483 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:39:31,483 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:39:31,483 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:39:31,483 - main - INFO - ==================================================
2025-05-01 00:39:31,483 - training.model_ResNet50_opt_SGD_lr_0.0001_id_30 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_30
2025-05-01 00:39:31,483 - training.model_ResNet50_opt_SGD_lr_0.0001_id_30 - INFO - Config: {
  "id": 30,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:39:31,731 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:39:31,731 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 30,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:39:31,731 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:39:31,790 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:39:31,791 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:39:31,793 - training.model_ResNet50_opt_SGD_lr_0.0001_id_30 - INFO - Starting model evaluation
2025-05-01 00:39:40,753 - training.model_ResNet50_opt_SGD_lr_0.0001_id_30 - INFO - Evaluation metrics:
  Accuracy:  0.8513
  Precision: 0.7361
  Recall:    0.7413
  F1 Score:  0.7387
  IoU:       0.5856
  mAP:       0.8540
  AUC:       0.9133
2025-05-01 00:39:40,754 - training.model_ResNet50_opt_SGD_lr_0.0001_id_30 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_30/final_results.json
2025-05-01 00:39:40,756 - training.model_ResNet50_opt_SGD_lr_0.0001_id_30 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_30/final_results.json
2025-05-01 00:39:40,756 - main - INFO - 
Summary for configuration 30:
2025-05-01 00:39:40,756 - main - INFO - Accuracy: 0.8513
2025-05-01 00:39:40,756 - main - INFO - Precision: 0.7361
2025-05-01 00:39:40,756 - main - INFO - Recall: 0.7413
2025-05-01 00:39:40,756 - main - INFO - F1 Score: 0.7387
2025-05-01 00:39:40,756 - main - INFO - IoU: 0.5856
2025-05-01 00:39:40,756 - main - INFO - mAP: 0.8540
2025-05-01 00:39:40,756 - main - INFO - AUC: 0.9133
2025-05-01 00:39:40,756 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:39:40,756 - main - INFO - 
==================================================
2025-05-01 00:39:40,756 - main - INFO - Running configuration 31/756:
2025-05-01 00:39:40,756 - main - INFO - Model: ResNet50
2025-05-01 00:39:40,756 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:39:40,756 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:39:40,756 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:39:40,756 - main - INFO - ==================================================
2025-05-01 00:39:40,756 - training.model_ResNet50_opt_SGD_lr_0.0001_id_31 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_31
2025-05-01 00:39:40,756 - training.model_ResNet50_opt_SGD_lr_0.0001_id_31 - INFO - Config: {
  "id": 31,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:39:40,918 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:39:40,918 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 31,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:39:40,918 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:39:40,972 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:39:40,973 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:39:40,975 - training.model_ResNet50_opt_SGD_lr_0.0001_id_31 - INFO - Starting model evaluation
2025-05-01 00:39:49,764 - training.model_ResNet50_opt_SGD_lr_0.0001_id_31 - INFO - Evaluation metrics:
  Accuracy:  0.8434
  Precision: 0.7177
  Recall:    0.7378
  F1 Score:  0.7276
  IoU:       0.5718
  mAP:       0.8529
  AUC:       0.9125
2025-05-01 00:39:49,766 - training.model_ResNet50_opt_SGD_lr_0.0001_id_31 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_31/final_results.json
2025-05-01 00:39:49,767 - training.model_ResNet50_opt_SGD_lr_0.0001_id_31 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_31/final_results.json
2025-05-01 00:39:49,767 - main - INFO - 
Summary for configuration 31:
2025-05-01 00:39:49,767 - main - INFO - Accuracy: 0.8434
2025-05-01 00:39:49,767 - main - INFO - Precision: 0.7177
2025-05-01 00:39:49,767 - main - INFO - Recall: 0.7378
2025-05-01 00:39:49,767 - main - INFO - F1 Score: 0.7276
2025-05-01 00:39:49,767 - main - INFO - IoU: 0.5718
2025-05-01 00:39:49,767 - main - INFO - mAP: 0.8529
2025-05-01 00:39:49,767 - main - INFO - AUC: 0.9125
2025-05-01 00:39:49,767 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:39:49,767 - main - INFO - 
==================================================
2025-05-01 00:39:49,767 - main - INFO - Running configuration 32/756:
2025-05-01 00:39:49,767 - main - INFO - Model: ResNet50
2025-05-01 00:39:49,768 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:39:49,768 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:39:49,768 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:39:49,768 - main - INFO - ==================================================
2025-05-01 00:39:49,768 - training.model_ResNet50_opt_SGD_lr_0.0001_id_32 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_32
2025-05-01 00:39:49,768 - training.model_ResNet50_opt_SGD_lr_0.0001_id_32 - INFO - Config: {
  "id": 32,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:39:49,945 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:39:49,945 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 32,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:39:49,946 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:39:50,002 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:39:50,003 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:39:50,005 - training.model_ResNet50_opt_SGD_lr_0.0001_id_32 - INFO - Starting model evaluation
2025-05-01 00:39:58,707 - training.model_ResNet50_opt_SGD_lr_0.0001_id_32 - INFO - Evaluation metrics:
  Accuracy:  0.8503
  Precision: 0.7304
  Recall:    0.7483
  F1 Score:  0.7392
  IoU:       0.5863
  mAP:       0.8543
  AUC:       0.9104
2025-05-01 00:39:58,708 - training.model_ResNet50_opt_SGD_lr_0.0001_id_32 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_32/final_results.json
2025-05-01 00:39:58,710 - training.model_ResNet50_opt_SGD_lr_0.0001_id_32 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_32/final_results.json
2025-05-01 00:39:58,710 - main - INFO - 
Summary for configuration 32:
2025-05-01 00:39:58,710 - main - INFO - Accuracy: 0.8503
2025-05-01 00:39:58,710 - main - INFO - Precision: 0.7304
2025-05-01 00:39:58,710 - main - INFO - Recall: 0.7483
2025-05-01 00:39:58,710 - main - INFO - F1 Score: 0.7392
2025-05-01 00:39:58,710 - main - INFO - IoU: 0.5863
2025-05-01 00:39:58,710 - main - INFO - mAP: 0.8543
2025-05-01 00:39:58,710 - main - INFO - AUC: 0.9104
2025-05-01 00:39:58,710 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:39:58,710 - main - INFO - 
==================================================
2025-05-01 00:39:58,710 - main - INFO - Running configuration 33/756:
2025-05-01 00:39:58,710 - main - INFO - Model: ResNet50
2025-05-01 00:39:58,710 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:39:58,710 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:39:58,710 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:39:58,710 - main - INFO - ==================================================
2025-05-01 00:39:58,710 - training.model_ResNet50_opt_SGD_lr_0.0001_id_33 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_33
2025-05-01 00:39:58,710 - training.model_ResNet50_opt_SGD_lr_0.0001_id_33 - INFO - Config: {
  "id": 33,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:39:58,871 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:39:58,874 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 33,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:39:58,874 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:39:58,928 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:39:58,929 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:39:58,931 - training.model_ResNet50_opt_SGD_lr_0.0001_id_33 - INFO - Starting model evaluation
2025-05-01 00:40:07,653 - training.model_ResNet50_opt_SGD_lr_0.0001_id_33 - INFO - Evaluation metrics:
  Accuracy:  0.8444
  Precision: 0.7157
  Recall:    0.7483
  F1 Score:  0.7316
  IoU:       0.5768
  mAP:       0.8480
  AUC:       0.9087
2025-05-01 00:40:07,655 - training.model_ResNet50_opt_SGD_lr_0.0001_id_33 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_33/final_results.json
2025-05-01 00:40:07,656 - training.model_ResNet50_opt_SGD_lr_0.0001_id_33 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_33/final_results.json
2025-05-01 00:40:07,656 - main - INFO - 
Summary for configuration 33:
2025-05-01 00:40:07,656 - main - INFO - Accuracy: 0.8444
2025-05-01 00:40:07,656 - main - INFO - Precision: 0.7157
2025-05-01 00:40:07,657 - main - INFO - Recall: 0.7483
2025-05-01 00:40:07,657 - main - INFO - F1 Score: 0.7316
2025-05-01 00:40:07,657 - main - INFO - IoU: 0.5768
2025-05-01 00:40:07,657 - main - INFO - mAP: 0.8480
2025-05-01 00:40:07,657 - main - INFO - AUC: 0.9087
2025-05-01 00:40:07,657 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:40:07,657 - main - INFO - 
==================================================
2025-05-01 00:40:07,657 - main - INFO - Running configuration 34/756:
2025-05-01 00:40:07,657 - main - INFO - Model: ResNet50
2025-05-01 00:40:07,657 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:40:07,657 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:40:07,657 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:40:07,657 - main - INFO - ==================================================
2025-05-01 00:40:07,657 - training.model_ResNet50_opt_SGD_lr_0.0001_id_34 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_34
2025-05-01 00:40:07,657 - training.model_ResNet50_opt_SGD_lr_0.0001_id_34 - INFO - Config: {
  "id": 34,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:40:07,833 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:40:07,833 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 34,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:40:07,833 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:40:07,896 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:40:07,897 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:40:07,899 - training.model_ResNet50_opt_SGD_lr_0.0001_id_34 - INFO - Starting model evaluation
2025-05-01 00:40:16,546 - training.model_ResNet50_opt_SGD_lr_0.0001_id_34 - INFO - Evaluation metrics:
  Accuracy:  0.8543
  Precision: 0.7372
  Recall:    0.7552
  F1 Score:  0.7461
  IoU:       0.5950
  mAP:       0.8550
  AUC:       0.9133
2025-05-01 00:40:16,550 - training.model_ResNet50_opt_SGD_lr_0.0001_id_34 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_34/final_results.json
2025-05-01 00:40:16,552 - training.model_ResNet50_opt_SGD_lr_0.0001_id_34 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_34/final_results.json
2025-05-01 00:40:16,552 - main - INFO - 
Summary for configuration 34:
2025-05-01 00:40:16,552 - main - INFO - Accuracy: 0.8543
2025-05-01 00:40:16,552 - main - INFO - Precision: 0.7372
2025-05-01 00:40:16,552 - main - INFO - Recall: 0.7552
2025-05-01 00:40:16,552 - main - INFO - F1 Score: 0.7461
2025-05-01 00:40:16,552 - main - INFO - IoU: 0.5950
2025-05-01 00:40:16,552 - main - INFO - mAP: 0.8550
2025-05-01 00:40:16,552 - main - INFO - AUC: 0.9133
2025-05-01 00:40:16,552 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:40:16,552 - main - INFO - 
==================================================
2025-05-01 00:40:16,552 - main - INFO - Running configuration 35/756:
2025-05-01 00:40:16,552 - main - INFO - Model: ResNet50
2025-05-01 00:40:16,552 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:40:16,552 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:40:16,552 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:40:16,552 - main - INFO - ==================================================
2025-05-01 00:40:16,553 - training.model_ResNet50_opt_SGD_lr_0.0001_id_35 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_35
2025-05-01 00:40:16,553 - training.model_ResNet50_opt_SGD_lr_0.0001_id_35 - INFO - Config: {
  "id": 35,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:40:16,726 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:40:16,726 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 35,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:40:16,727 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:40:16,781 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:40:16,782 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:40:16,784 - training.model_ResNet50_opt_SGD_lr_0.0001_id_35 - INFO - Starting model evaluation
2025-05-01 00:40:25,757 - training.model_ResNet50_opt_SGD_lr_0.0001_id_35 - INFO - Evaluation metrics:
  Accuracy:  0.8474
  Precision: 0.7260
  Recall:    0.7413
  F1 Score:  0.7336
  IoU:       0.5792
  mAP:       0.8500
  AUC:       0.9082
2025-05-01 00:40:25,759 - training.model_ResNet50_opt_SGD_lr_0.0001_id_35 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_35/final_results.json
2025-05-01 00:40:25,760 - training.model_ResNet50_opt_SGD_lr_0.0001_id_35 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_35/final_results.json
2025-05-01 00:40:25,760 - main - INFO - 
Summary for configuration 35:
2025-05-01 00:40:25,760 - main - INFO - Accuracy: 0.8474
2025-05-01 00:40:25,760 - main - INFO - Precision: 0.7260
2025-05-01 00:40:25,760 - main - INFO - Recall: 0.7413
2025-05-01 00:40:25,760 - main - INFO - F1 Score: 0.7336
2025-05-01 00:40:25,760 - main - INFO - IoU: 0.5792
2025-05-01 00:40:25,760 - main - INFO - mAP: 0.8500
2025-05-01 00:40:25,760 - main - INFO - AUC: 0.9082
2025-05-01 00:40:25,760 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:40:25,760 - main - INFO - 
==================================================
2025-05-01 00:40:25,760 - main - INFO - Running configuration 36/756:
2025-05-01 00:40:25,760 - main - INFO - Model: ResNet50
2025-05-01 00:40:25,760 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 00:40:25,760 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:40:25,760 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:40:25,760 - main - INFO - ==================================================
2025-05-01 00:40:25,761 - training.model_ResNet50_opt_SGD_lr_0.0001_id_36 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001_id_36
2025-05-01 00:40:25,761 - training.model_ResNet50_opt_SGD_lr_0.0001_id_36 - INFO - Config: {
  "id": 36,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:40:25,922 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_SGD_lr_0.0001
2025-05-01 00:40:25,923 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 36,
  "model_name": "ResNet50",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:40:25,923 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 00:40:25,977 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8233
2025-05-01 00:40:25,978 - training.model_ResNet50_opt_SGD_lr_0.0001 - INFO - Training completed after 1031.69 seconds
2025-05-01 00:40:25,979 - training.model_ResNet50_opt_SGD_lr_0.0001_id_36 - INFO - Starting model evaluation
2025-05-01 00:40:34,648 - training.model_ResNet50_opt_SGD_lr_0.0001_id_36 - INFO - Evaluation metrics:
  Accuracy:  0.8474
  Precision: 0.7230
  Recall:    0.7483
  F1 Score:  0.7354
  IoU:       0.5815
  mAP:       0.8513
  AUC:       0.9094
2025-05-01 00:40:34,649 - training.model_ResNet50_opt_SGD_lr_0.0001_id_36 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_36/final_results.json
2025-05-01 00:40:34,651 - training.model_ResNet50_opt_SGD_lr_0.0001_id_36 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_SGD_lr_0.0001_id_36/final_results.json
2025-05-01 00:40:34,651 - main - INFO - 
Summary for configuration 36:
2025-05-01 00:40:34,651 - main - INFO - Accuracy: 0.8474
2025-05-01 00:40:34,651 - main - INFO - Precision: 0.7230
2025-05-01 00:40:34,651 - main - INFO - Recall: 0.7483
2025-05-01 00:40:34,651 - main - INFO - F1 Score: 0.7354
2025-05-01 00:40:34,651 - main - INFO - IoU: 0.5815
2025-05-01 00:40:34,651 - main - INFO - mAP: 0.8513
2025-05-01 00:40:34,651 - main - INFO - AUC: 0.9094
2025-05-01 00:40:34,651 - main - INFO - Training time: 1031.69 seconds
2025-05-01 00:40:34,651 - main - INFO - 
==================================================
2025-05-01 00:40:34,651 - main - INFO - Running configuration 37/756:
2025-05-01 00:40:34,651 - main - INFO - Model: ResNet50
2025-05-01 00:40:34,651 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:40:34,651 - main - INFO - Scheduler: StepLR
2025-05-01 00:40:34,651 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:40:34,651 - main - INFO - ==================================================
2025-05-01 00:40:34,651 - training.model_ResNet50_opt_Adam_lr_0.01_id_37 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_37
2025-05-01 00:40:34,651 - training.model_ResNet50_opt_Adam_lr_0.01_id_37 - INFO - Config: {
  "id": 37,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:40:34,811 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:40:34,811 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 37,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:40:34,811 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 00:40:34,812 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 00:40:55,996 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 00:41:24,683 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.4966
2025-05-01 00:41:24,734 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 1 completed in 49.92s - Train Loss: 0.6947, Train Acc: 0.5300, Val Loss: 0.6958, Val Acc: 0.4966
2025-05-01 00:41:24,977 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 00:41:24,977 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 00:41:46,324 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 00:42:15,188 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation loss improved from inf to 0.7100
2025-05-01 00:42:15,276 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 2 completed in 50.30s - Train Loss: 0.6888, Train Acc: 0.5727, Val Loss: 0.7100, Val Acc: 0.4957
2025-05-01 00:42:15,489 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 00:42:15,489 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 00:42:36,834 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 00:43:05,602 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.4966 to 0.5472
2025-05-01 00:43:05,688 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 3 completed in 50.20s - Train Loss: 0.6718, Train Acc: 0.5903, Val Loss: 0.6785, Val Acc: 0.5472
2025-05-01 00:43:05,903 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 00:43:05,904 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 00:43:27,255 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 00:43:55,987 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.7100 to 0.6956
2025-05-01 00:43:56,064 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 4 completed in 50.16s - Train Loss: 0.6766, Train Acc: 0.5845, Val Loss: 0.6956, Val Acc: 0.5129
2025-05-01 00:43:56,278 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 00:43:56,278 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 00:44:17,430 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 00:44:46,358 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5472 to 0.6921
2025-05-01 00:44:46,447 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 5 completed in 50.17s - Train Loss: 0.6372, Train Acc: 0.6358, Val Loss: 0.5971, Val Acc: 0.6921
2025-05-01 00:44:46,659 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 00:44:46,659 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 00:45:08,125 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 00:45:36,803 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.6921 to 0.7238
2025-05-01 00:45:36,885 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 6 completed in 50.23s - Train Loss: 0.6039, Train Acc: 0.6825, Val Loss: 0.5836, Val Acc: 0.7238
2025-05-01 00:45:37,098 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 00:45:37,099 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 00:45:58,647 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 00:46:27,495 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.6956 to 0.5974
2025-05-01 00:46:27,581 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 7 completed in 50.48s - Train Loss: 0.5935, Train Acc: 0.6976, Val Loss: 0.5974, Val Acc: 0.6998
2025-05-01 00:46:27,791 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 00:46:27,792 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 00:46:49,487 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 00:47:18,110 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 8 completed in 50.32s - Train Loss: 0.5842, Train Acc: 0.7066, Val Loss: 0.6369, Val Acc: 0.6372
2025-05-01 00:47:18,356 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 00:47:18,356 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 00:47:40,118 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 00:48:08,730 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 9 completed in 50.37s - Train Loss: 0.5795, Train Acc: 0.7044, Val Loss: 0.6297, Val Acc: 0.6672
2025-05-01 00:48:08,976 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 00:48:08,976 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 00:48:30,491 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 00:48:58,957 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7238 to 0.7393
2025-05-01 00:48:59,042 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 10 completed in 50.07s - Train Loss: 0.5743, Train Acc: 0.7087, Val Loss: 0.5562, Val Acc: 0.7393
2025-05-01 00:48:59,256 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 00:48:59,256 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 00:49:20,450 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 00:49:49,525 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7393 to 0.7444
2025-05-01 00:49:49,621 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 11 completed in 50.36s - Train Loss: 0.5578, Train Acc: 0.7317, Val Loss: 0.5448, Val Acc: 0.7444
2025-05-01 00:49:49,842 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 00:49:49,842 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 00:50:11,625 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 00:50:40,451 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7444 to 0.7453
2025-05-01 00:50:40,523 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 12 completed in 50.68s - Train Loss: 0.5450, Train Acc: 0.7518, Val Loss: 0.5382, Val Acc: 0.7453
2025-05-01 00:50:40,727 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 00:50:40,728 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 00:51:01,674 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 00:51:30,606 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7453 to 0.7633
2025-05-01 00:51:30,695 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 13 completed in 49.97s - Train Loss: 0.5373, Train Acc: 0.7563, Val Loss: 0.5306, Val Acc: 0.7633
2025-05-01 00:51:30,907 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 00:51:30,907 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 00:51:52,529 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 00:52:21,093 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7633 to 0.7770
2025-05-01 00:52:21,184 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 14 completed in 50.28s - Train Loss: 0.5278, Train Acc: 0.7701, Val Loss: 0.5231, Val Acc: 0.7770
2025-05-01 00:52:21,396 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 00:52:21,397 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 00:52:43,133 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 00:53:12,224 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7770 to 0.7967
2025-05-01 00:53:12,303 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 15 completed in 50.91s - Train Loss: 0.5157, Train Acc: 0.7881, Val Loss: 0.5076, Val Acc: 0.7967
2025-05-01 00:53:12,517 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 00:53:12,518 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 00:53:34,125 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 00:54:03,217 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7967 to 0.8027
2025-05-01 00:54:03,311 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 16 completed in 50.79s - Train Loss: 0.5079, Train Acc: 0.7969, Val Loss: 0.5022, Val Acc: 0.8027
2025-05-01 00:54:03,527 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 00:54:03,528 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 00:54:25,134 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 00:54:54,030 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8027 to 0.8036
2025-05-01 00:54:54,120 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 17 completed in 50.59s - Train Loss: 0.5032, Train Acc: 0.8052, Val Loss: 0.5030, Val Acc: 0.8036
2025-05-01 00:54:54,332 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 00:54:54,333 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 00:55:16,040 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 00:55:44,893 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8036 to 0.8199
2025-05-01 00:55:44,975 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 18 completed in 50.64s - Train Loss: 0.4978, Train Acc: 0.8074, Val Loss: 0.4913, Val Acc: 0.8199
2025-05-01 00:55:45,191 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 00:55:45,191 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 00:56:06,760 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 00:56:35,654 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8199 to 0.8233
2025-05-01 00:56:35,742 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 19 completed in 50.55s - Train Loss: 0.4904, Train Acc: 0.8140, Val Loss: 0.4822, Val Acc: 0.8233
2025-05-01 00:56:35,967 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 00:56:35,967 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 00:56:57,086 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 00:57:26,383 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8233 to 0.8302
2025-05-01 00:57:26,473 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Epoch 20 completed in 50.51s - Train Loss: 0.4868, Train Acc: 0.8222, Val Loss: 0.4777, Val Acc: 0.8302
2025-05-01 00:57:26,688 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 00:57:26,688 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:57:26,690 - training.model_ResNet50_opt_Adam_lr_0.01_id_37 - INFO - Starting model evaluation
2025-05-01 00:57:35,497 - training.model_ResNet50_opt_Adam_lr_0.01_id_37 - INFO - Evaluation metrics:
  Accuracy:  0.7760
  Precision: 0.5739
  Recall:    0.8147
  F1 Score:  0.6734
  IoU:       0.5076
  mAP:       0.7080
  AUC:       0.8548
2025-05-01 00:57:35,498 - training.model_ResNet50_opt_Adam_lr_0.01_id_37 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_37/final_results.json
2025-05-01 00:57:35,500 - training.model_ResNet50_opt_Adam_lr_0.01_id_37 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_37/final_results.json
2025-05-01 00:57:35,500 - main - INFO - 
Summary for configuration 37:
2025-05-01 00:57:35,500 - main - INFO - Accuracy: 0.7760
2025-05-01 00:57:35,500 - main - INFO - Precision: 0.5739
2025-05-01 00:57:35,500 - main - INFO - Recall: 0.8147
2025-05-01 00:57:35,500 - main - INFO - F1 Score: 0.6734
2025-05-01 00:57:35,500 - main - INFO - IoU: 0.5076
2025-05-01 00:57:35,500 - main - INFO - mAP: 0.7080
2025-05-01 00:57:35,500 - main - INFO - AUC: 0.8548
2025-05-01 00:57:35,500 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:57:35,500 - main - INFO - 
==================================================
2025-05-01 00:57:35,500 - main - INFO - Running configuration 38/756:
2025-05-01 00:57:35,500 - main - INFO - Model: ResNet50
2025-05-01 00:57:35,500 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:57:35,500 - main - INFO - Scheduler: StepLR
2025-05-01 00:57:35,500 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:57:35,500 - main - INFO - ==================================================
2025-05-01 00:57:35,500 - training.model_ResNet50_opt_Adam_lr_0.01_id_38 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_38
2025-05-01 00:57:35,500 - training.model_ResNet50_opt_Adam_lr_0.01_id_38 - INFO - Config: {
  "id": 38,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:57:35,669 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:57:35,669 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 38,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:57:35,670 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:57:35,750 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:57:35,750 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:57:35,752 - training.model_ResNet50_opt_Adam_lr_0.01_id_38 - INFO - Starting model evaluation
2025-05-01 00:57:44,370 - training.model_ResNet50_opt_Adam_lr_0.01_id_38 - INFO - Evaluation metrics:
  Accuracy:  0.7780
  Precision: 0.5775
  Recall:    0.8077
  F1 Score:  0.6735
  IoU:       0.5077
  mAP:       0.7076
  AUC:       0.8550
2025-05-01 00:57:44,371 - training.model_ResNet50_opt_Adam_lr_0.01_id_38 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_38/final_results.json
2025-05-01 00:57:44,373 - training.model_ResNet50_opt_Adam_lr_0.01_id_38 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_38/final_results.json
2025-05-01 00:57:44,373 - main - INFO - 
Summary for configuration 38:
2025-05-01 00:57:44,373 - main - INFO - Accuracy: 0.7780
2025-05-01 00:57:44,373 - main - INFO - Precision: 0.5775
2025-05-01 00:57:44,373 - main - INFO - Recall: 0.8077
2025-05-01 00:57:44,373 - main - INFO - F1 Score: 0.6735
2025-05-01 00:57:44,373 - main - INFO - IoU: 0.5077
2025-05-01 00:57:44,373 - main - INFO - mAP: 0.7076
2025-05-01 00:57:44,373 - main - INFO - AUC: 0.8550
2025-05-01 00:57:44,373 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:57:44,373 - main - INFO - 
==================================================
2025-05-01 00:57:44,373 - main - INFO - Running configuration 39/756:
2025-05-01 00:57:44,373 - main - INFO - Model: ResNet50
2025-05-01 00:57:44,373 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:57:44,373 - main - INFO - Scheduler: StepLR
2025-05-01 00:57:44,373 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:57:44,373 - main - INFO - ==================================================
2025-05-01 00:57:44,373 - training.model_ResNet50_opt_Adam_lr_0.01_id_39 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_39
2025-05-01 00:57:44,373 - training.model_ResNet50_opt_Adam_lr_0.01_id_39 - INFO - Config: {
  "id": 39,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:57:44,542 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:57:44,542 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 39,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:57:44,542 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:57:44,622 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:57:44,623 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:57:44,625 - training.model_ResNet50_opt_Adam_lr_0.01_id_39 - INFO - Starting model evaluation
2025-05-01 00:57:53,605 - training.model_ResNet50_opt_Adam_lr_0.01_id_39 - INFO - Evaluation metrics:
  Accuracy:  0.7721
  Precision: 0.5686
  Recall:    0.8112
  F1 Score:  0.6686
  IoU:       0.5022
  mAP:       0.7102
  AUC:       0.8568
2025-05-01 00:57:53,608 - training.model_ResNet50_opt_Adam_lr_0.01_id_39 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_39/final_results.json
2025-05-01 00:57:53,609 - training.model_ResNet50_opt_Adam_lr_0.01_id_39 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_39/final_results.json
2025-05-01 00:57:53,609 - main - INFO - 
Summary for configuration 39:
2025-05-01 00:57:53,609 - main - INFO - Accuracy: 0.7721
2025-05-01 00:57:53,609 - main - INFO - Precision: 0.5686
2025-05-01 00:57:53,609 - main - INFO - Recall: 0.8112
2025-05-01 00:57:53,609 - main - INFO - F1 Score: 0.6686
2025-05-01 00:57:53,609 - main - INFO - IoU: 0.5022
2025-05-01 00:57:53,609 - main - INFO - mAP: 0.7102
2025-05-01 00:57:53,609 - main - INFO - AUC: 0.8568
2025-05-01 00:57:53,609 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:57:53,609 - main - INFO - 
==================================================
2025-05-01 00:57:53,609 - main - INFO - Running configuration 40/756:
2025-05-01 00:57:53,610 - main - INFO - Model: ResNet50
2025-05-01 00:57:53,610 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:57:53,610 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:57:53,610 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:57:53,610 - main - INFO - ==================================================
2025-05-01 00:57:53,610 - training.model_ResNet50_opt_Adam_lr_0.01_id_40 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_40
2025-05-01 00:57:53,610 - training.model_ResNet50_opt_Adam_lr_0.01_id_40 - INFO - Config: {
  "id": 40,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:57:53,787 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:57:53,787 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 40,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:57:53,788 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:57:53,869 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:57:53,870 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:57:53,871 - training.model_ResNet50_opt_Adam_lr_0.01_id_40 - INFO - Starting model evaluation
2025-05-01 00:58:02,433 - training.model_ResNet50_opt_Adam_lr_0.01_id_40 - INFO - Evaluation metrics:
  Accuracy:  0.7770
  Precision: 0.5757
  Recall:    0.8112
  F1 Score:  0.6734
  IoU:       0.5077
  mAP:       0.7158
  AUC:       0.8577
2025-05-01 00:58:02,435 - training.model_ResNet50_opt_Adam_lr_0.01_id_40 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_40/final_results.json
2025-05-01 00:58:02,436 - training.model_ResNet50_opt_Adam_lr_0.01_id_40 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_40/final_results.json
2025-05-01 00:58:02,436 - main - INFO - 
Summary for configuration 40:
2025-05-01 00:58:02,436 - main - INFO - Accuracy: 0.7770
2025-05-01 00:58:02,436 - main - INFO - Precision: 0.5757
2025-05-01 00:58:02,436 - main - INFO - Recall: 0.8112
2025-05-01 00:58:02,436 - main - INFO - F1 Score: 0.6734
2025-05-01 00:58:02,436 - main - INFO - IoU: 0.5077
2025-05-01 00:58:02,436 - main - INFO - mAP: 0.7158
2025-05-01 00:58:02,436 - main - INFO - AUC: 0.8577
2025-05-01 00:58:02,436 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:58:02,437 - main - INFO - 
==================================================
2025-05-01 00:58:02,437 - main - INFO - Running configuration 41/756:
2025-05-01 00:58:02,437 - main - INFO - Model: ResNet50
2025-05-01 00:58:02,437 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:58:02,437 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:58:02,437 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:58:02,437 - main - INFO - ==================================================
2025-05-01 00:58:02,437 - training.model_ResNet50_opt_Adam_lr_0.01_id_41 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_41
2025-05-01 00:58:02,437 - training.model_ResNet50_opt_Adam_lr_0.01_id_41 - INFO - Config: {
  "id": 41,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:58:02,605 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:58:02,605 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 41,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:58:02,605 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:58:02,686 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:58:02,687 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:58:02,689 - training.model_ResNet50_opt_Adam_lr_0.01_id_41 - INFO - Starting model evaluation
2025-05-01 00:58:11,298 - training.model_ResNet50_opt_Adam_lr_0.01_id_41 - INFO - Evaluation metrics:
  Accuracy:  0.7770
  Precision: 0.5757
  Recall:    0.8112
  F1 Score:  0.6734
  IoU:       0.5077
  mAP:       0.7176
  AUC:       0.8589
2025-05-01 00:58:11,299 - training.model_ResNet50_opt_Adam_lr_0.01_id_41 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_41/final_results.json
2025-05-01 00:58:11,301 - training.model_ResNet50_opt_Adam_lr_0.01_id_41 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_41/final_results.json
2025-05-01 00:58:11,301 - main - INFO - 
Summary for configuration 41:
2025-05-01 00:58:11,301 - main - INFO - Accuracy: 0.7770
2025-05-01 00:58:11,301 - main - INFO - Precision: 0.5757
2025-05-01 00:58:11,301 - main - INFO - Recall: 0.8112
2025-05-01 00:58:11,301 - main - INFO - F1 Score: 0.6734
2025-05-01 00:58:11,301 - main - INFO - IoU: 0.5077
2025-05-01 00:58:11,301 - main - INFO - mAP: 0.7176
2025-05-01 00:58:11,301 - main - INFO - AUC: 0.8589
2025-05-01 00:58:11,301 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:58:11,301 - main - INFO - 
==================================================
2025-05-01 00:58:11,301 - main - INFO - Running configuration 42/756:
2025-05-01 00:58:11,301 - main - INFO - Model: ResNet50
2025-05-01 00:58:11,301 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:58:11,301 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 00:58:11,301 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:58:11,301 - main - INFO - ==================================================
2025-05-01 00:58:11,301 - training.model_ResNet50_opt_Adam_lr_0.01_id_42 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_42
2025-05-01 00:58:11,301 - training.model_ResNet50_opt_Adam_lr_0.01_id_42 - INFO - Config: {
  "id": 42,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:58:11,477 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:58:11,477 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 42,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:58:11,477 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:58:11,559 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:58:11,560 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:58:11,562 - training.model_ResNet50_opt_Adam_lr_0.01_id_42 - INFO - Starting model evaluation
2025-05-01 00:58:20,372 - training.model_ResNet50_opt_Adam_lr_0.01_id_42 - INFO - Evaluation metrics:
  Accuracy:  0.7760
  Precision: 0.5739
  Recall:    0.8147
  F1 Score:  0.6734
  IoU:       0.5076
  mAP:       0.7129
  AUC:       0.8577
2025-05-01 00:58:20,386 - training.model_ResNet50_opt_Adam_lr_0.01_id_42 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_42/final_results.json
2025-05-01 00:58:20,388 - training.model_ResNet50_opt_Adam_lr_0.01_id_42 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_42/final_results.json
2025-05-01 00:58:20,388 - main - INFO - 
Summary for configuration 42:
2025-05-01 00:58:20,388 - main - INFO - Accuracy: 0.7760
2025-05-01 00:58:20,388 - main - INFO - Precision: 0.5739
2025-05-01 00:58:20,388 - main - INFO - Recall: 0.8147
2025-05-01 00:58:20,388 - main - INFO - F1 Score: 0.6734
2025-05-01 00:58:20,388 - main - INFO - IoU: 0.5076
2025-05-01 00:58:20,388 - main - INFO - mAP: 0.7129
2025-05-01 00:58:20,388 - main - INFO - AUC: 0.8577
2025-05-01 00:58:20,388 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:58:20,388 - main - INFO - 
==================================================
2025-05-01 00:58:20,388 - main - INFO - Running configuration 43/756:
2025-05-01 00:58:20,388 - main - INFO - Model: ResNet50
2025-05-01 00:58:20,388 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:58:20,388 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:58:20,388 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:58:20,388 - main - INFO - ==================================================
2025-05-01 00:58:20,388 - training.model_ResNet50_opt_Adam_lr_0.01_id_43 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_43
2025-05-01 00:58:20,388 - training.model_ResNet50_opt_Adam_lr_0.01_id_43 - INFO - Config: {
  "id": 43,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:58:20,557 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:58:20,557 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 43,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:58:20,558 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:58:20,639 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:58:20,640 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:58:20,642 - training.model_ResNet50_opt_Adam_lr_0.01_id_43 - INFO - Starting model evaluation
2025-05-01 00:58:29,523 - training.model_ResNet50_opt_Adam_lr_0.01_id_43 - INFO - Evaluation metrics:
  Accuracy:  0.7730
  Precision: 0.5707
  Recall:    0.8042
  F1 Score:  0.6676
  IoU:       0.5011
  mAP:       0.7105
  AUC:       0.8561
2025-05-01 00:58:29,525 - training.model_ResNet50_opt_Adam_lr_0.01_id_43 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_43/final_results.json
2025-05-01 00:58:29,526 - training.model_ResNet50_opt_Adam_lr_0.01_id_43 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_43/final_results.json
2025-05-01 00:58:29,526 - main - INFO - 
Summary for configuration 43:
2025-05-01 00:58:29,526 - main - INFO - Accuracy: 0.7730
2025-05-01 00:58:29,526 - main - INFO - Precision: 0.5707
2025-05-01 00:58:29,526 - main - INFO - Recall: 0.8042
2025-05-01 00:58:29,526 - main - INFO - F1 Score: 0.6676
2025-05-01 00:58:29,526 - main - INFO - IoU: 0.5011
2025-05-01 00:58:29,526 - main - INFO - mAP: 0.7105
2025-05-01 00:58:29,526 - main - INFO - AUC: 0.8561
2025-05-01 00:58:29,526 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:58:29,526 - main - INFO - 
==================================================
2025-05-01 00:58:29,527 - main - INFO - Running configuration 44/756:
2025-05-01 00:58:29,527 - main - INFO - Model: ResNet50
2025-05-01 00:58:29,527 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:58:29,527 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:58:29,527 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:58:29,527 - main - INFO - ==================================================
2025-05-01 00:58:29,527 - training.model_ResNet50_opt_Adam_lr_0.01_id_44 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_44
2025-05-01 00:58:29,527 - training.model_ResNet50_opt_Adam_lr_0.01_id_44 - INFO - Config: {
  "id": 44,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:58:29,695 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:58:29,695 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 44,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:58:29,695 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:58:29,774 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:58:29,775 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:58:29,777 - training.model_ResNet50_opt_Adam_lr_0.01_id_44 - INFO - Starting model evaluation
2025-05-01 00:58:38,363 - training.model_ResNet50_opt_Adam_lr_0.01_id_44 - INFO - Evaluation metrics:
  Accuracy:  0.7730
  Precision: 0.5690
  Recall:    0.8217
  F1 Score:  0.6724
  IoU:       0.5065
  mAP:       0.7084
  AUC:       0.8570
2025-05-01 00:58:38,364 - training.model_ResNet50_opt_Adam_lr_0.01_id_44 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_44/final_results.json
2025-05-01 00:58:38,366 - training.model_ResNet50_opt_Adam_lr_0.01_id_44 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_44/final_results.json
2025-05-01 00:58:38,366 - main - INFO - 
Summary for configuration 44:
2025-05-01 00:58:38,366 - main - INFO - Accuracy: 0.7730
2025-05-01 00:58:38,366 - main - INFO - Precision: 0.5690
2025-05-01 00:58:38,366 - main - INFO - Recall: 0.8217
2025-05-01 00:58:38,366 - main - INFO - F1 Score: 0.6724
2025-05-01 00:58:38,366 - main - INFO - IoU: 0.5065
2025-05-01 00:58:38,366 - main - INFO - mAP: 0.7084
2025-05-01 00:58:38,366 - main - INFO - AUC: 0.8570
2025-05-01 00:58:38,366 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:58:38,366 - main - INFO - 
==================================================
2025-05-01 00:58:38,366 - main - INFO - Running configuration 45/756:
2025-05-01 00:58:38,366 - main - INFO - Model: ResNet50
2025-05-01 00:58:38,366 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:58:38,366 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 00:58:38,366 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:58:38,366 - main - INFO - ==================================================
2025-05-01 00:58:38,366 - training.model_ResNet50_opt_Adam_lr_0.01_id_45 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_45
2025-05-01 00:58:38,366 - training.model_ResNet50_opt_Adam_lr_0.01_id_45 - INFO - Config: {
  "id": 45,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:58:38,539 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:58:38,540 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 45,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:58:38,540 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:58:38,618 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:58:38,619 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:58:38,621 - training.model_ResNet50_opt_Adam_lr_0.01_id_45 - INFO - Starting model evaluation
2025-05-01 00:58:47,212 - training.model_ResNet50_opt_Adam_lr_0.01_id_45 - INFO - Evaluation metrics:
  Accuracy:  0.7780
  Precision: 0.5775
  Recall:    0.8077
  F1 Score:  0.6735
  IoU:       0.5077
  mAP:       0.7099
  AUC:       0.8561
2025-05-01 00:58:47,213 - training.model_ResNet50_opt_Adam_lr_0.01_id_45 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_45/final_results.json
2025-05-01 00:58:47,215 - training.model_ResNet50_opt_Adam_lr_0.01_id_45 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_45/final_results.json
2025-05-01 00:58:47,215 - main - INFO - 
Summary for configuration 45:
2025-05-01 00:58:47,215 - main - INFO - Accuracy: 0.7780
2025-05-01 00:58:47,215 - main - INFO - Precision: 0.5775
2025-05-01 00:58:47,215 - main - INFO - Recall: 0.8077
2025-05-01 00:58:47,215 - main - INFO - F1 Score: 0.6735
2025-05-01 00:58:47,215 - main - INFO - IoU: 0.5077
2025-05-01 00:58:47,215 - main - INFO - mAP: 0.7099
2025-05-01 00:58:47,215 - main - INFO - AUC: 0.8561
2025-05-01 00:58:47,215 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:58:47,215 - main - INFO - 
==================================================
2025-05-01 00:58:47,215 - main - INFO - Running configuration 46/756:
2025-05-01 00:58:47,215 - main - INFO - Model: ResNet50
2025-05-01 00:58:47,215 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:58:47,215 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:58:47,215 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:58:47,215 - main - INFO - ==================================================
2025-05-01 00:58:47,215 - training.model_ResNet50_opt_Adam_lr_0.01_id_46 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_46
2025-05-01 00:58:47,215 - training.model_ResNet50_opt_Adam_lr_0.01_id_46 - INFO - Config: {
  "id": 46,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:58:47,397 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:58:47,397 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 46,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:58:47,397 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:58:47,476 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:58:47,477 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:58:47,479 - training.model_ResNet50_opt_Adam_lr_0.01_id_46 - INFO - Starting model evaluation
2025-05-01 00:58:56,159 - training.model_ResNet50_opt_Adam_lr_0.01_id_46 - INFO - Evaluation metrics:
  Accuracy:  0.7740
  Precision: 0.5704
  Recall:    0.8217
  F1 Score:  0.6734
  IoU:       0.5076
  mAP:       0.7117
  AUC:       0.8586
2025-05-01 00:58:56,162 - training.model_ResNet50_opt_Adam_lr_0.01_id_46 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_46/final_results.json
2025-05-01 00:58:56,164 - training.model_ResNet50_opt_Adam_lr_0.01_id_46 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_46/final_results.json
2025-05-01 00:58:56,164 - main - INFO - 
Summary for configuration 46:
2025-05-01 00:58:56,164 - main - INFO - Accuracy: 0.7740
2025-05-01 00:58:56,164 - main - INFO - Precision: 0.5704
2025-05-01 00:58:56,164 - main - INFO - Recall: 0.8217
2025-05-01 00:58:56,164 - main - INFO - F1 Score: 0.6734
2025-05-01 00:58:56,164 - main - INFO - IoU: 0.5076
2025-05-01 00:58:56,164 - main - INFO - mAP: 0.7117
2025-05-01 00:58:56,164 - main - INFO - AUC: 0.8586
2025-05-01 00:58:56,164 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:58:56,164 - main - INFO - 
==================================================
2025-05-01 00:58:56,164 - main - INFO - Running configuration 47/756:
2025-05-01 00:58:56,164 - main - INFO - Model: ResNet50
2025-05-01 00:58:56,164 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:58:56,164 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:58:56,164 - main - INFO - Loss Function: FocalLoss
2025-05-01 00:58:56,164 - main - INFO - ==================================================
2025-05-01 00:58:56,165 - training.model_ResNet50_opt_Adam_lr_0.01_id_47 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_47
2025-05-01 00:58:56,165 - training.model_ResNet50_opt_Adam_lr_0.01_id_47 - INFO - Config: {
  "id": 47,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:58:56,341 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:58:56,341 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 47,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 00:58:56,341 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:58:56,419 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:58:56,420 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:58:56,422 - training.model_ResNet50_opt_Adam_lr_0.01_id_47 - INFO - Starting model evaluation
2025-05-01 00:59:05,078 - training.model_ResNet50_opt_Adam_lr_0.01_id_47 - INFO - Evaluation metrics:
  Accuracy:  0.7780
  Precision: 0.5760
  Recall:    0.8217
  F1 Score:  0.6772
  IoU:       0.5120
  mAP:       0.7143
  AUC:       0.8579
2025-05-01 00:59:05,080 - training.model_ResNet50_opt_Adam_lr_0.01_id_47 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_47/final_results.json
2025-05-01 00:59:05,081 - training.model_ResNet50_opt_Adam_lr_0.01_id_47 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_47/final_results.json
2025-05-01 00:59:05,081 - main - INFO - 
Summary for configuration 47:
2025-05-01 00:59:05,081 - main - INFO - Accuracy: 0.7780
2025-05-01 00:59:05,081 - main - INFO - Precision: 0.5760
2025-05-01 00:59:05,081 - main - INFO - Recall: 0.8217
2025-05-01 00:59:05,081 - main - INFO - F1 Score: 0.6772
2025-05-01 00:59:05,081 - main - INFO - IoU: 0.5120
2025-05-01 00:59:05,081 - main - INFO - mAP: 0.7143
2025-05-01 00:59:05,081 - main - INFO - AUC: 0.8579
2025-05-01 00:59:05,081 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:59:05,081 - main - INFO - 
==================================================
2025-05-01 00:59:05,081 - main - INFO - Running configuration 48/756:
2025-05-01 00:59:05,081 - main - INFO - Model: ResNet50
2025-05-01 00:59:05,081 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 00:59:05,081 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 00:59:05,081 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 00:59:05,081 - main - INFO - ==================================================
2025-05-01 00:59:05,081 - training.model_ResNet50_opt_Adam_lr_0.01_id_48 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01_id_48
2025-05-01 00:59:05,081 - training.model_ResNet50_opt_Adam_lr_0.01_id_48 - INFO - Config: {
  "id": 48,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:59:05,267 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.01
2025-05-01 00:59:05,268 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 48,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 00:59:05,268 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 00:59:05,347 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8302
2025-05-01 00:59:05,348 - training.model_ResNet50_opt_Adam_lr_0.01 - INFO - Training completed after 1011.66 seconds
2025-05-01 00:59:05,350 - training.model_ResNet50_opt_Adam_lr_0.01_id_48 - INFO - Starting model evaluation
2025-05-01 00:59:14,143 - training.model_ResNet50_opt_Adam_lr_0.01_id_48 - INFO - Evaluation metrics:
  Accuracy:  0.7810
  Precision: 0.5810
  Recall:    0.8147
  F1 Score:  0.6783
  IoU:       0.5132
  mAP:       0.7088
  AUC:       0.8530
2025-05-01 00:59:14,145 - training.model_ResNet50_opt_Adam_lr_0.01_id_48 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_48/final_results.json
2025-05-01 00:59:14,146 - training.model_ResNet50_opt_Adam_lr_0.01_id_48 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.01_id_48/final_results.json
2025-05-01 00:59:14,146 - main - INFO - 
Summary for configuration 48:
2025-05-01 00:59:14,146 - main - INFO - Accuracy: 0.7810
2025-05-01 00:59:14,147 - main - INFO - Precision: 0.5810
2025-05-01 00:59:14,147 - main - INFO - Recall: 0.8147
2025-05-01 00:59:14,147 - main - INFO - F1 Score: 0.6783
2025-05-01 00:59:14,147 - main - INFO - IoU: 0.5132
2025-05-01 00:59:14,147 - main - INFO - mAP: 0.7088
2025-05-01 00:59:14,147 - main - INFO - AUC: 0.8530
2025-05-01 00:59:14,147 - main - INFO - Training time: 1011.66 seconds
2025-05-01 00:59:14,147 - main - INFO - 
==================================================
2025-05-01 00:59:14,147 - main - INFO - Running configuration 49/756:
2025-05-01 00:59:14,147 - main - INFO - Model: ResNet50
2025-05-01 00:59:14,147 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 00:59:14,147 - main - INFO - Scheduler: StepLR
2025-05-01 00:59:14,147 - main - INFO - Loss Function: CrossEntropy
2025-05-01 00:59:14,147 - main - INFO - ==================================================
2025-05-01 00:59:14,147 - training.model_ResNet50_opt_Adam_lr_0.001_id_49 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_49
2025-05-01 00:59:14,147 - training.model_ResNet50_opt_Adam_lr_0.001_id_49 - INFO - Config: {
  "id": 49,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:59:14,411 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 00:59:14,411 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 49,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 00:59:14,411 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 00:59:14,412 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 00:59:36,077 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 01:00:05,861 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.5798
2025-05-01 01:00:05,913 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 1 completed in 51.50s - Train Loss: 0.5554, Train Acc: 0.7458, Val Loss: 0.7312, Val Acc: 0.5798
2025-05-01 01:00:06,124 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 01:00:06,124 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 01:00:28,398 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 01:00:57,418 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.5798 to 0.7890
2025-05-01 01:00:57,506 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 2 completed in 51.38s - Train Loss: 0.5441, Train Acc: 0.7619, Val Loss: 0.5235, Val Acc: 0.7890
2025-05-01 01:00:57,723 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 01:00:57,723 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 01:01:19,693 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 01:01:49,032 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.7890 to 0.8130
2025-05-01 01:01:49,110 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 3 completed in 51.39s - Train Loss: 0.4989, Train Acc: 0.8095, Val Loss: 0.4927, Val Acc: 0.8130
2025-05-01 01:01:49,323 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 01:01:49,324 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 01:02:10,966 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 01:02:40,333 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8130 to 0.8268
2025-05-01 01:02:40,424 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 4 completed in 51.10s - Train Loss: 0.4925, Train Acc: 0.8157, Val Loss: 0.4846, Val Acc: 0.8268
2025-05-01 01:02:40,636 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 01:02:40,637 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 01:03:02,412 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 01:03:31,972 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8268 to 0.8473
2025-05-01 01:03:32,055 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 5 completed in 51.42s - Train Loss: 0.4764, Train Acc: 0.8278, Val Loss: 0.4596, Val Acc: 0.8473
2025-05-01 01:03:32,266 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 01:03:32,267 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 01:03:54,013 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 01:04:23,586 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8473 to 0.8825
2025-05-01 01:04:23,667 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 6 completed in 51.40s - Train Loss: 0.4621, Train Acc: 0.8421, Val Loss: 0.4292, Val Acc: 0.8825
2025-05-01 01:04:23,879 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 01:04:23,879 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 01:04:46,058 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 01:05:15,565 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation loss improved from inf to 0.5188
2025-05-01 01:05:15,642 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 7 completed in 51.76s - Train Loss: 0.4694, Train Acc: 0.8361, Val Loss: 0.5188, Val Acc: 0.7847
2025-05-01 01:05:15,851 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 01:05:15,851 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 01:05:37,923 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 01:06:07,381 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 8 completed in 51.53s - Train Loss: 0.4377, Train Acc: 0.8724, Val Loss: 0.5402, Val Acc: 0.7547
2025-05-01 01:06:07,598 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 01:06:07,599 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 01:06:29,689 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 01:06:59,197 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.5188 to 0.4589
2025-05-01 01:06:59,285 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 9 completed in 51.69s - Train Loss: 0.4381, Train Acc: 0.8715, Val Loss: 0.4589, Val Acc: 0.8439
2025-05-01 01:06:59,501 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 01:06:59,502 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 01:07:21,203 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 01:07:50,786 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 10 completed in 51.28s - Train Loss: 0.4105, Train Acc: 0.9000, Val Loss: 0.4708, Val Acc: 0.8328
2025-05-01 01:07:51,044 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 01:07:51,044 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 01:08:13,239 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 01:08:42,395 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8825 to 0.9383
2025-05-01 01:08:42,471 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 11 completed in 51.43s - Train Loss: 0.3854, Train Acc: 0.9273, Val Loss: 0.3793, Val Acc: 0.9383
2025-05-01 01:08:42,685 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 01:08:42,686 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 01:09:04,662 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 01:09:34,284 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9383 to 0.9477
2025-05-01 01:09:34,372 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 12 completed in 51.69s - Train Loss: 0.3567, Train Acc: 0.9582, Val Loss: 0.3676, Val Acc: 0.9477
2025-05-01 01:09:34,581 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 01:09:34,581 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 01:09:57,083 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 01:10:26,248 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4589 to 0.3669
2025-05-01 01:10:26,326 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 13 completed in 51.74s - Train Loss: 0.3530, Train Acc: 0.9640, Val Loss: 0.3669, Val Acc: 0.9443
2025-05-01 01:10:26,550 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 01:10:26,550 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 01:10:48,655 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 01:11:17,925 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9477 to 0.9571
2025-05-01 01:11:18,003 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 14 completed in 51.45s - Train Loss: 0.3463, Train Acc: 0.9680, Val Loss: 0.3586, Val Acc: 0.9571
2025-05-01 01:11:18,214 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 01:11:18,214 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 01:11:39,755 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 01:12:09,575 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9571 to 0.9580
2025-05-01 01:12:09,652 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 15 completed in 51.44s - Train Loss: 0.3405, Train Acc: 0.9738, Val Loss: 0.3542, Val Acc: 0.9580
2025-05-01 01:12:09,863 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 01:12:09,864 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 01:12:32,074 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 01:13:01,446 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9580 to 0.9623
2025-05-01 01:13:01,542 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 16 completed in 51.68s - Train Loss: 0.3345, Train Acc: 0.9813, Val Loss: 0.3530, Val Acc: 0.9623
2025-05-01 01:13:01,763 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 01:13:01,764 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 01:13:23,692 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 01:13:53,386 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3669 to 0.3583
2025-05-01 01:13:53,477 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 17 completed in 51.71s - Train Loss: 0.3373, Train Acc: 0.9766, Val Loss: 0.3583, Val Acc: 0.9545
2025-05-01 01:13:53,687 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 01:13:53,688 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 01:14:15,602 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 01:14:44,860 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3583 to 0.3496
2025-05-01 01:14:44,948 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 18 completed in 51.26s - Train Loss: 0.3328, Train Acc: 0.9822, Val Loss: 0.3496, Val Acc: 0.9614
2025-05-01 01:14:45,160 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 01:14:45,161 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 01:15:07,431 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 01:15:36,678 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 19 completed in 51.52s - Train Loss: 0.3302, Train Acc: 0.9837, Val Loss: 0.3549, Val Acc: 0.9537
2025-05-01 01:15:36,922 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 01:15:36,923 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 01:15:58,585 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 01:16:28,192 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Epoch 20 completed in 51.27s - Train Loss: 0.3298, Train Acc: 0.9846, Val Loss: 0.3554, Val Acc: 0.9511
2025-05-01 01:16:28,419 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 01:16:28,420 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:16:28,422 - training.model_ResNet50_opt_Adam_lr_0.001_id_49 - INFO - Starting model evaluation
2025-05-01 01:16:37,351 - training.model_ResNet50_opt_Adam_lr_0.001_id_49 - INFO - Evaluation metrics:
  Accuracy:  0.9336
  Precision: 0.8476
  Recall:    0.9336
  F1 Score:  0.8885
  IoU:       0.7994
  mAP:       0.9320
  AUC:       0.9758
2025-05-01 01:16:37,353 - training.model_ResNet50_opt_Adam_lr_0.001_id_49 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_49/final_results.json
2025-05-01 01:16:37,355 - training.model_ResNet50_opt_Adam_lr_0.001_id_49 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_49/final_results.json
2025-05-01 01:16:37,355 - main - INFO - 
Summary for configuration 49:
2025-05-01 01:16:37,355 - main - INFO - Accuracy: 0.9336
2025-05-01 01:16:37,355 - main - INFO - Precision: 0.8476
2025-05-01 01:16:37,355 - main - INFO - Recall: 0.9336
2025-05-01 01:16:37,355 - main - INFO - F1 Score: 0.8885
2025-05-01 01:16:37,355 - main - INFO - IoU: 0.7994
2025-05-01 01:16:37,355 - main - INFO - mAP: 0.9320
2025-05-01 01:16:37,355 - main - INFO - AUC: 0.9758
2025-05-01 01:16:37,355 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:16:37,355 - main - INFO - 
==================================================
2025-05-01 01:16:37,355 - main - INFO - Running configuration 50/756:
2025-05-01 01:16:37,355 - main - INFO - Model: ResNet50
2025-05-01 01:16:37,355 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:16:37,355 - main - INFO - Scheduler: StepLR
2025-05-01 01:16:37,355 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:16:37,355 - main - INFO - ==================================================
2025-05-01 01:16:37,355 - training.model_ResNet50_opt_Adam_lr_0.001_id_50 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_50
2025-05-01 01:16:37,355 - training.model_ResNet50_opt_Adam_lr_0.001_id_50 - INFO - Config: {
  "id": 50,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:16:37,524 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:16:37,524 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 50,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:16:37,524 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:16:37,604 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:16:37,604 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:16:37,606 - training.model_ResNet50_opt_Adam_lr_0.001_id_50 - INFO - Starting model evaluation
2025-05-01 01:16:46,297 - training.model_ResNet50_opt_Adam_lr_0.001_id_50 - INFO - Evaluation metrics:
  Accuracy:  0.9306
  Precision: 0.8396
  Recall:    0.9336
  F1 Score:  0.8841
  IoU:       0.7923
  mAP:       0.9240
  AUC:       0.9747
2025-05-01 01:16:46,298 - training.model_ResNet50_opt_Adam_lr_0.001_id_50 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_50/final_results.json
2025-05-01 01:16:46,300 - training.model_ResNet50_opt_Adam_lr_0.001_id_50 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_50/final_results.json
2025-05-01 01:16:46,300 - main - INFO - 
Summary for configuration 50:
2025-05-01 01:16:46,300 - main - INFO - Accuracy: 0.9306
2025-05-01 01:16:46,300 - main - INFO - Precision: 0.8396
2025-05-01 01:16:46,300 - main - INFO - Recall: 0.9336
2025-05-01 01:16:46,300 - main - INFO - F1 Score: 0.8841
2025-05-01 01:16:46,300 - main - INFO - IoU: 0.7923
2025-05-01 01:16:46,300 - main - INFO - mAP: 0.9240
2025-05-01 01:16:46,300 - main - INFO - AUC: 0.9747
2025-05-01 01:16:46,300 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:16:46,300 - main - INFO - 
==================================================
2025-05-01 01:16:46,300 - main - INFO - Running configuration 51/756:
2025-05-01 01:16:46,300 - main - INFO - Model: ResNet50
2025-05-01 01:16:46,300 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:16:46,300 - main - INFO - Scheduler: StepLR
2025-05-01 01:16:46,300 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:16:46,300 - main - INFO - ==================================================
2025-05-01 01:16:46,300 - training.model_ResNet50_opt_Adam_lr_0.001_id_51 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_51
2025-05-01 01:16:46,301 - training.model_ResNet50_opt_Adam_lr_0.001_id_51 - INFO - Config: {
  "id": 51,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:16:46,470 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:16:46,470 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 51,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:16:46,470 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:16:46,551 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:16:46,552 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:16:46,553 - training.model_ResNet50_opt_Adam_lr_0.001_id_51 - INFO - Starting model evaluation
2025-05-01 01:16:55,806 - training.model_ResNet50_opt_Adam_lr_0.001_id_51 - INFO - Evaluation metrics:
  Accuracy:  0.9277
  Precision: 0.8339
  Recall:    0.9301
  F1 Score:  0.8793
  IoU:       0.7847
  mAP:       0.9277
  AUC:       0.9767
2025-05-01 01:16:55,807 - training.model_ResNet50_opt_Adam_lr_0.001_id_51 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_51/final_results.json
2025-05-01 01:16:55,809 - training.model_ResNet50_opt_Adam_lr_0.001_id_51 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_51/final_results.json
2025-05-01 01:16:55,809 - main - INFO - 
Summary for configuration 51:
2025-05-01 01:16:55,809 - main - INFO - Accuracy: 0.9277
2025-05-01 01:16:55,809 - main - INFO - Precision: 0.8339
2025-05-01 01:16:55,809 - main - INFO - Recall: 0.9301
2025-05-01 01:16:55,809 - main - INFO - F1 Score: 0.8793
2025-05-01 01:16:55,809 - main - INFO - IoU: 0.7847
2025-05-01 01:16:55,809 - main - INFO - mAP: 0.9277
2025-05-01 01:16:55,809 - main - INFO - AUC: 0.9767
2025-05-01 01:16:55,809 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:16:55,809 - main - INFO - 
==================================================
2025-05-01 01:16:55,809 - main - INFO - Running configuration 52/756:
2025-05-01 01:16:55,809 - main - INFO - Model: ResNet50
2025-05-01 01:16:55,809 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:16:55,809 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:16:55,809 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:16:55,809 - main - INFO - ==================================================
2025-05-01 01:16:55,809 - training.model_ResNet50_opt_Adam_lr_0.001_id_52 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_52
2025-05-01 01:16:55,809 - training.model_ResNet50_opt_Adam_lr_0.001_id_52 - INFO - Config: {
  "id": 52,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:16:55,982 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:16:55,982 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 52,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:16:55,982 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:16:56,066 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:16:56,067 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:16:56,071 - training.model_ResNet50_opt_Adam_lr_0.001_id_52 - INFO - Starting model evaluation
2025-05-01 01:17:04,809 - training.model_ResNet50_opt_Adam_lr_0.001_id_52 - INFO - Evaluation metrics:
  Accuracy:  0.9336
  Precision: 0.8498
  Recall:    0.9301
  F1 Score:  0.8881
  IoU:       0.7988
  mAP:       0.9360
  AUC:       0.9775
2025-05-01 01:17:04,811 - training.model_ResNet50_opt_Adam_lr_0.001_id_52 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_52/final_results.json
2025-05-01 01:17:04,813 - training.model_ResNet50_opt_Adam_lr_0.001_id_52 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_52/final_results.json
2025-05-01 01:17:04,813 - main - INFO - 
Summary for configuration 52:
2025-05-01 01:17:04,813 - main - INFO - Accuracy: 0.9336
2025-05-01 01:17:04,813 - main - INFO - Precision: 0.8498
2025-05-01 01:17:04,813 - main - INFO - Recall: 0.9301
2025-05-01 01:17:04,813 - main - INFO - F1 Score: 0.8881
2025-05-01 01:17:04,813 - main - INFO - IoU: 0.7988
2025-05-01 01:17:04,813 - main - INFO - mAP: 0.9360
2025-05-01 01:17:04,813 - main - INFO - AUC: 0.9775
2025-05-01 01:17:04,813 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:17:04,813 - main - INFO - 
==================================================
2025-05-01 01:17:04,813 - main - INFO - Running configuration 53/756:
2025-05-01 01:17:04,813 - main - INFO - Model: ResNet50
2025-05-01 01:17:04,813 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:17:04,813 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:17:04,813 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:17:04,813 - main - INFO - ==================================================
2025-05-01 01:17:04,813 - training.model_ResNet50_opt_Adam_lr_0.001_id_53 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_53
2025-05-01 01:17:04,813 - training.model_ResNet50_opt_Adam_lr_0.001_id_53 - INFO - Config: {
  "id": 53,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:17:04,983 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:17:04,983 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 53,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:17:04,983 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:17:05,064 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:17:05,065 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:17:05,067 - training.model_ResNet50_opt_Adam_lr_0.001_id_53 - INFO - Starting model evaluation
2025-05-01 01:17:13,715 - training.model_ResNet50_opt_Adam_lr_0.001_id_53 - INFO - Evaluation metrics:
  Accuracy:  0.9267
  Precision: 0.8232
  Recall:    0.9441
  F1 Score:  0.8795
  IoU:       0.7849
  mAP:       0.9426
  AUC:       0.9794
2025-05-01 01:17:13,716 - training.model_ResNet50_opt_Adam_lr_0.001_id_53 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_53/final_results.json
2025-05-01 01:17:13,718 - training.model_ResNet50_opt_Adam_lr_0.001_id_53 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_53/final_results.json
2025-05-01 01:17:13,718 - main - INFO - 
Summary for configuration 53:
2025-05-01 01:17:13,718 - main - INFO - Accuracy: 0.9267
2025-05-01 01:17:13,718 - main - INFO - Precision: 0.8232
2025-05-01 01:17:13,718 - main - INFO - Recall: 0.9441
2025-05-01 01:17:13,718 - main - INFO - F1 Score: 0.8795
2025-05-01 01:17:13,718 - main - INFO - IoU: 0.7849
2025-05-01 01:17:13,718 - main - INFO - mAP: 0.9426
2025-05-01 01:17:13,718 - main - INFO - AUC: 0.9794
2025-05-01 01:17:13,718 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:17:13,718 - main - INFO - 
==================================================
2025-05-01 01:17:13,718 - main - INFO - Running configuration 54/756:
2025-05-01 01:17:13,718 - main - INFO - Model: ResNet50
2025-05-01 01:17:13,718 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:17:13,718 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:17:13,718 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:17:13,718 - main - INFO - ==================================================
2025-05-01 01:17:13,718 - training.model_ResNet50_opt_Adam_lr_0.001_id_54 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_54
2025-05-01 01:17:13,718 - training.model_ResNet50_opt_Adam_lr_0.001_id_54 - INFO - Config: {
  "id": 54,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:17:13,887 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:17:13,887 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 54,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:17:13,887 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:17:13,967 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:17:13,968 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:17:13,970 - training.model_ResNet50_opt_Adam_lr_0.001_id_54 - INFO - Starting model evaluation
2025-05-01 01:17:22,593 - training.model_ResNet50_opt_Adam_lr_0.001_id_54 - INFO - Evaluation metrics:
  Accuracy:  0.9326
  Precision: 0.8406
  Recall:    0.9406
  F1 Score:  0.8878
  IoU:       0.7982
  mAP:       0.9390
  AUC:       0.9772
2025-05-01 01:17:22,595 - training.model_ResNet50_opt_Adam_lr_0.001_id_54 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_54/final_results.json
2025-05-01 01:17:22,596 - training.model_ResNet50_opt_Adam_lr_0.001_id_54 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_54/final_results.json
2025-05-01 01:17:22,596 - main - INFO - 
Summary for configuration 54:
2025-05-01 01:17:22,596 - main - INFO - Accuracy: 0.9326
2025-05-01 01:17:22,596 - main - INFO - Precision: 0.8406
2025-05-01 01:17:22,596 - main - INFO - Recall: 0.9406
2025-05-01 01:17:22,596 - main - INFO - F1 Score: 0.8878
2025-05-01 01:17:22,596 - main - INFO - IoU: 0.7982
2025-05-01 01:17:22,596 - main - INFO - mAP: 0.9390
2025-05-01 01:17:22,596 - main - INFO - AUC: 0.9772
2025-05-01 01:17:22,596 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:17:22,596 - main - INFO - 
==================================================
2025-05-01 01:17:22,596 - main - INFO - Running configuration 55/756:
2025-05-01 01:17:22,596 - main - INFO - Model: ResNet50
2025-05-01 01:17:22,596 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:17:22,596 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:17:22,596 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:17:22,596 - main - INFO - ==================================================
2025-05-01 01:17:22,597 - training.model_ResNet50_opt_Adam_lr_0.001_id_55 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_55
2025-05-01 01:17:22,597 - training.model_ResNet50_opt_Adam_lr_0.001_id_55 - INFO - Config: {
  "id": 55,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:17:22,765 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:17:22,765 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 55,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:17:22,765 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:17:22,864 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:17:22,864 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:17:22,866 - training.model_ResNet50_opt_Adam_lr_0.001_id_55 - INFO - Starting model evaluation
2025-05-01 01:17:31,577 - training.model_ResNet50_opt_Adam_lr_0.001_id_55 - INFO - Evaluation metrics:
  Accuracy:  0.9306
  Precision: 0.8418
  Recall:    0.9301
  F1 Score:  0.8837
  IoU:       0.7917
  mAP:       0.9433
  AUC:       0.9793
2025-05-01 01:17:31,578 - training.model_ResNet50_opt_Adam_lr_0.001_id_55 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_55/final_results.json
2025-05-01 01:17:31,580 - training.model_ResNet50_opt_Adam_lr_0.001_id_55 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_55/final_results.json
2025-05-01 01:17:31,580 - main - INFO - 
Summary for configuration 55:
2025-05-01 01:17:31,580 - main - INFO - Accuracy: 0.9306
2025-05-01 01:17:31,580 - main - INFO - Precision: 0.8418
2025-05-01 01:17:31,580 - main - INFO - Recall: 0.9301
2025-05-01 01:17:31,580 - main - INFO - F1 Score: 0.8837
2025-05-01 01:17:31,580 - main - INFO - IoU: 0.7917
2025-05-01 01:17:31,580 - main - INFO - mAP: 0.9433
2025-05-01 01:17:31,580 - main - INFO - AUC: 0.9793
2025-05-01 01:17:31,580 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:17:31,580 - main - INFO - 
==================================================
2025-05-01 01:17:31,580 - main - INFO - Running configuration 56/756:
2025-05-01 01:17:31,580 - main - INFO - Model: ResNet50
2025-05-01 01:17:31,580 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:17:31,580 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:17:31,580 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:17:31,580 - main - INFO - ==================================================
2025-05-01 01:17:31,580 - training.model_ResNet50_opt_Adam_lr_0.001_id_56 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_56
2025-05-01 01:17:31,580 - training.model_ResNet50_opt_Adam_lr_0.001_id_56 - INFO - Config: {
  "id": 56,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:17:31,756 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:17:31,757 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 56,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:17:31,757 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:17:31,835 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:17:31,836 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:17:31,838 - training.model_ResNet50_opt_Adam_lr_0.001_id_56 - INFO - Starting model evaluation
2025-05-01 01:17:39,871 - training.model_ResNet50_opt_Adam_lr_0.001_id_56 - INFO - Evaluation metrics:
  Accuracy:  0.9326
  Precision: 0.8471
  Recall:    0.9301
  F1 Score:  0.8867
  IoU:       0.7964
  mAP:       0.9239
  AUC:       0.9733
2025-05-01 01:17:39,873 - training.model_ResNet50_opt_Adam_lr_0.001_id_56 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_56/final_results.json
2025-05-01 01:17:39,874 - training.model_ResNet50_opt_Adam_lr_0.001_id_56 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_56/final_results.json
2025-05-01 01:17:39,874 - main - INFO - 
Summary for configuration 56:
2025-05-01 01:17:39,875 - main - INFO - Accuracy: 0.9326
2025-05-01 01:17:39,875 - main - INFO - Precision: 0.8471
2025-05-01 01:17:39,875 - main - INFO - Recall: 0.9301
2025-05-01 01:17:39,875 - main - INFO - F1 Score: 0.8867
2025-05-01 01:17:39,875 - main - INFO - IoU: 0.7964
2025-05-01 01:17:39,875 - main - INFO - mAP: 0.9239
2025-05-01 01:17:39,875 - main - INFO - AUC: 0.9733
2025-05-01 01:17:39,875 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:17:39,875 - main - INFO - 
==================================================
2025-05-01 01:17:39,875 - main - INFO - Running configuration 57/756:
2025-05-01 01:17:39,875 - main - INFO - Model: ResNet50
2025-05-01 01:17:39,875 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:17:39,875 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:17:39,875 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:17:39,875 - main - INFO - ==================================================
2025-05-01 01:17:39,875 - training.model_ResNet50_opt_Adam_lr_0.001_id_57 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_57
2025-05-01 01:17:39,875 - training.model_ResNet50_opt_Adam_lr_0.001_id_57 - INFO - Config: {
  "id": 57,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:17:40,032 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:17:40,033 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 57,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:17:40,033 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:17:40,110 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:17:40,111 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:17:40,113 - training.model_ResNet50_opt_Adam_lr_0.001_id_57 - INFO - Starting model evaluation
2025-05-01 01:17:47,661 - training.model_ResNet50_opt_Adam_lr_0.001_id_57 - INFO - Evaluation metrics:
  Accuracy:  0.9366
  Precision: 0.8469
  Recall:    0.9476
  F1 Score:  0.8944
  IoU:       0.8090
  mAP:       0.9346
  AUC:       0.9790
2025-05-01 01:17:47,662 - training.model_ResNet50_opt_Adam_lr_0.001_id_57 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_57/final_results.json
2025-05-01 01:17:47,664 - training.model_ResNet50_opt_Adam_lr_0.001_id_57 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_57/final_results.json
2025-05-01 01:17:47,664 - main - INFO - 
Summary for configuration 57:
2025-05-01 01:17:47,664 - main - INFO - Accuracy: 0.9366
2025-05-01 01:17:47,664 - main - INFO - Precision: 0.8469
2025-05-01 01:17:47,664 - main - INFO - Recall: 0.9476
2025-05-01 01:17:47,664 - main - INFO - F1 Score: 0.8944
2025-05-01 01:17:47,664 - main - INFO - IoU: 0.8090
2025-05-01 01:17:47,664 - main - INFO - mAP: 0.9346
2025-05-01 01:17:47,664 - main - INFO - AUC: 0.9790
2025-05-01 01:17:47,664 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:17:47,664 - main - INFO - 
==================================================
2025-05-01 01:17:47,664 - main - INFO - Running configuration 58/756:
2025-05-01 01:17:47,664 - main - INFO - Model: ResNet50
2025-05-01 01:17:47,664 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:17:47,664 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:17:47,664 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:17:47,664 - main - INFO - ==================================================
2025-05-01 01:17:47,664 - training.model_ResNet50_opt_Adam_lr_0.001_id_58 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_58
2025-05-01 01:17:47,664 - training.model_ResNet50_opt_Adam_lr_0.001_id_58 - INFO - Config: {
  "id": 58,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:17:47,840 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:17:47,840 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 58,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:17:47,840 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:17:47,919 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:17:47,920 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:17:47,922 - training.model_ResNet50_opt_Adam_lr_0.001_id_58 - INFO - Starting model evaluation
2025-05-01 01:17:55,541 - training.model_ResNet50_opt_Adam_lr_0.001_id_58 - INFO - Evaluation metrics:
  Accuracy:  0.9326
  Precision: 0.8449
  Recall:    0.9336
  F1 Score:  0.8870
  IoU:       0.7970
  mAP:       0.9304
  AUC:       0.9752
2025-05-01 01:17:55,543 - training.model_ResNet50_opt_Adam_lr_0.001_id_58 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_58/final_results.json
2025-05-01 01:17:55,545 - training.model_ResNet50_opt_Adam_lr_0.001_id_58 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_58/final_results.json
2025-05-01 01:17:55,545 - main - INFO - 
Summary for configuration 58:
2025-05-01 01:17:55,545 - main - INFO - Accuracy: 0.9326
2025-05-01 01:17:55,545 - main - INFO - Precision: 0.8449
2025-05-01 01:17:55,545 - main - INFO - Recall: 0.9336
2025-05-01 01:17:55,545 - main - INFO - F1 Score: 0.8870
2025-05-01 01:17:55,545 - main - INFO - IoU: 0.7970
2025-05-01 01:17:55,545 - main - INFO - mAP: 0.9304
2025-05-01 01:17:55,545 - main - INFO - AUC: 0.9752
2025-05-01 01:17:55,545 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:17:55,545 - main - INFO - 
==================================================
2025-05-01 01:17:55,545 - main - INFO - Running configuration 59/756:
2025-05-01 01:17:55,545 - main - INFO - Model: ResNet50
2025-05-01 01:17:55,545 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:17:55,545 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:17:55,545 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:17:55,545 - main - INFO - ==================================================
2025-05-01 01:17:55,545 - training.model_ResNet50_opt_Adam_lr_0.001_id_59 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_59
2025-05-01 01:17:55,545 - training.model_ResNet50_opt_Adam_lr_0.001_id_59 - INFO - Config: {
  "id": 59,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:17:55,706 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:17:55,706 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 59,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:17:55,706 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:17:55,784 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:17:55,785 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:17:55,787 - training.model_ResNet50_opt_Adam_lr_0.001_id_59 - INFO - Starting model evaluation
2025-05-01 01:18:03,200 - training.model_ResNet50_opt_Adam_lr_0.001_id_59 - INFO - Evaluation metrics:
  Accuracy:  0.9366
  Precision: 0.8535
  Recall:    0.9371
  F1 Score:  0.8933
  IoU:       0.8072
  mAP:       0.9417
  AUC:       0.9799
2025-05-01 01:18:03,202 - training.model_ResNet50_opt_Adam_lr_0.001_id_59 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_59/final_results.json
2025-05-01 01:18:03,204 - training.model_ResNet50_opt_Adam_lr_0.001_id_59 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_59/final_results.json
2025-05-01 01:18:03,204 - main - INFO - 
Summary for configuration 59:
2025-05-01 01:18:03,204 - main - INFO - Accuracy: 0.9366
2025-05-01 01:18:03,204 - main - INFO - Precision: 0.8535
2025-05-01 01:18:03,204 - main - INFO - Recall: 0.9371
2025-05-01 01:18:03,204 - main - INFO - F1 Score: 0.8933
2025-05-01 01:18:03,204 - main - INFO - IoU: 0.8072
2025-05-01 01:18:03,204 - main - INFO - mAP: 0.9417
2025-05-01 01:18:03,204 - main - INFO - AUC: 0.9799
2025-05-01 01:18:03,204 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:18:03,204 - main - INFO - 
==================================================
2025-05-01 01:18:03,204 - main - INFO - Running configuration 60/756:
2025-05-01 01:18:03,204 - main - INFO - Model: ResNet50
2025-05-01 01:18:03,204 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 01:18:03,204 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:18:03,204 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:18:03,204 - main - INFO - ==================================================
2025-05-01 01:18:03,204 - training.model_ResNet50_opt_Adam_lr_0.001_id_60 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001_id_60
2025-05-01 01:18:03,204 - training.model_ResNet50_opt_Adam_lr_0.001_id_60 - INFO - Config: {
  "id": 60,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:18:03,363 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.001
2025-05-01 01:18:03,363 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 60,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:18:03,363 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 01:18:03,442 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9623
2025-05-01 01:18:03,442 - training.model_ResNet50_opt_Adam_lr_0.001 - INFO - Training completed after 1033.78 seconds
2025-05-01 01:18:03,444 - training.model_ResNet50_opt_Adam_lr_0.001_id_60 - INFO - Starting model evaluation
2025-05-01 01:18:11,126 - training.model_ResNet50_opt_Adam_lr_0.001_id_60 - INFO - Evaluation metrics:
  Accuracy:  0.9326
  Precision: 0.8449
  Recall:    0.9336
  F1 Score:  0.8870
  IoU:       0.7970
  mAP:       0.9394
  AUC:       0.9780
2025-05-01 01:18:11,140 - training.model_ResNet50_opt_Adam_lr_0.001_id_60 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_60/final_results.json
2025-05-01 01:18:11,142 - training.model_ResNet50_opt_Adam_lr_0.001_id_60 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.001_id_60/final_results.json
2025-05-01 01:18:11,142 - main - INFO - 
Summary for configuration 60:
2025-05-01 01:18:11,142 - main - INFO - Accuracy: 0.9326
2025-05-01 01:18:11,142 - main - INFO - Precision: 0.8449
2025-05-01 01:18:11,142 - main - INFO - Recall: 0.9336
2025-05-01 01:18:11,142 - main - INFO - F1 Score: 0.8870
2025-05-01 01:18:11,142 - main - INFO - IoU: 0.7970
2025-05-01 01:18:11,142 - main - INFO - mAP: 0.9394
2025-05-01 01:18:11,142 - main - INFO - AUC: 0.9780
2025-05-01 01:18:11,142 - main - INFO - Training time: 1033.78 seconds
2025-05-01 01:18:11,142 - main - INFO - 
==================================================
2025-05-01 01:18:11,142 - main - INFO - Running configuration 61/756:
2025-05-01 01:18:11,142 - main - INFO - Model: ResNet50
2025-05-01 01:18:11,142 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:18:11,142 - main - INFO - Scheduler: StepLR
2025-05-01 01:18:11,142 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:18:11,142 - main - INFO - ==================================================
2025-05-01 01:18:11,142 - training.model_ResNet50_opt_Adam_lr_0.0001_id_61 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_61
2025-05-01 01:18:11,142 - training.model_ResNet50_opt_Adam_lr_0.0001_id_61 - INFO - Config: {
  "id": 61,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:18:11,309 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:18:11,309 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 61,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:18:11,309 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 01:18:11,309 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 01:18:32,476 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 01:18:59,320 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9545
2025-05-01 01:18:59,372 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 1 completed in 48.06s - Train Loss: 0.4360, Train Acc: 0.8842, Val Loss: 0.3575, Val Acc: 0.9545
2025-05-01 01:18:59,607 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 01:18:59,607 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 01:19:20,748 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 01:19:47,776 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9545 to 0.9623
2025-05-01 01:19:47,856 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 2 completed in 48.25s - Train Loss: 0.3534, Train Acc: 0.9629, Val Loss: 0.3515, Val Acc: 0.9623
2025-05-01 01:19:48,068 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 01:19:48,068 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 01:20:08,680 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 01:20:35,909 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9623 to 0.9708
2025-05-01 01:20:35,999 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 3 completed in 47.93s - Train Loss: 0.3385, Train Acc: 0.9751, Val Loss: 0.3421, Val Acc: 0.9708
2025-05-01 01:20:36,211 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 01:20:36,212 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 01:20:56,964 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 01:21:23,704 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation loss improved from inf to 0.3550
2025-05-01 01:21:23,798 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 4 completed in 47.59s - Train Loss: 0.3365, Train Acc: 0.9773, Val Loss: 0.3550, Val Acc: 0.9614
2025-05-01 01:21:24,018 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 01:21:24,019 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 01:21:44,641 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 01:22:11,962 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3550 to 0.3455
2025-05-01 01:22:12,043 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 5 completed in 48.02s - Train Loss: 0.3330, Train Acc: 0.9803, Val Loss: 0.3455, Val Acc: 0.9674
2025-05-01 01:22:12,256 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 01:22:12,256 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 01:22:33,075 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 01:23:00,116 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 6 completed in 47.86s - Train Loss: 0.3311, Train Acc: 0.9831, Val Loss: 0.3516, Val Acc: 0.9580
2025-05-01 01:23:00,341 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 01:23:00,342 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 01:23:21,063 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 01:23:48,114 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9708 to 0.9751
2025-05-01 01:23:48,188 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 7 completed in 47.85s - Train Loss: 0.3304, Train Acc: 0.9833, Val Loss: 0.3388, Val Acc: 0.9751
2025-05-01 01:23:48,401 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 01:23:48,401 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 01:24:09,129 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 01:24:36,173 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3455 to 0.3379
2025-05-01 01:24:36,254 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 8 completed in 47.85s - Train Loss: 0.3282, Train Acc: 0.9856, Val Loss: 0.3379, Val Acc: 0.9726
2025-05-01 01:24:36,472 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 01:24:36,472 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 01:24:57,303 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 01:25:24,138 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 9 completed in 47.67s - Train Loss: 0.3291, Train Acc: 0.9848, Val Loss: 0.3449, Val Acc: 0.9674
2025-05-01 01:25:24,390 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 01:25:24,390 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 01:25:45,086 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 01:26:11,682 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9751 to 0.9811
2025-05-01 01:26:11,759 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 10 completed in 47.37s - Train Loss: 0.3280, Train Acc: 0.9863, Val Loss: 0.3328, Val Acc: 0.9811
2025-05-01 01:26:11,976 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 01:26:11,976 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 01:26:32,754 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 01:26:59,648 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3379 to 0.3328
2025-05-01 01:26:59,727 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 11 completed in 47.75s - Train Loss: 0.3254, Train Acc: 0.9884, Val Loss: 0.3328, Val Acc: 0.9803
2025-05-01 01:26:59,939 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 01:26:59,940 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 01:27:20,680 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 01:27:47,748 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3328 to 0.3314
2025-05-01 01:27:47,839 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 12 completed in 47.90s - Train Loss: 0.3195, Train Acc: 0.9942, Val Loss: 0.3314, Val Acc: 0.9803
2025-05-01 01:27:48,057 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 01:27:48,057 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 01:28:09,020 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 01:28:35,978 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9811 to 0.9828
2025-05-01 01:28:36,055 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 13 completed in 48.00s - Train Loss: 0.3187, Train Acc: 0.9946, Val Loss: 0.3285, Val Acc: 0.9828
2025-05-01 01:28:36,272 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 01:28:36,273 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 01:28:57,099 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 01:29:24,230 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9828 to 0.9863
2025-05-01 01:29:24,310 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 14 completed in 48.04s - Train Loss: 0.3175, Train Acc: 0.9964, Val Loss: 0.3263, Val Acc: 0.9863
2025-05-01 01:29:24,535 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 01:29:24,535 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 01:29:45,188 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 01:30:12,228 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3314 to 0.3279
2025-05-01 01:30:12,307 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 15 completed in 47.77s - Train Loss: 0.3166, Train Acc: 0.9968, Val Loss: 0.3279, Val Acc: 0.9846
2025-05-01 01:30:12,523 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 01:30:12,524 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 01:30:33,285 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 01:31:00,350 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 16 completed in 47.83s - Train Loss: 0.3160, Train Acc: 0.9974, Val Loss: 0.3289, Val Acc: 0.9820
2025-05-01 01:31:00,566 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 01:31:00,567 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 01:31:21,280 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 01:31:48,323 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3279 to 0.3268
2025-05-01 01:31:48,408 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 17 completed in 47.84s - Train Loss: 0.3163, Train Acc: 0.9972, Val Loss: 0.3268, Val Acc: 0.9863
2025-05-01 01:31:48,620 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 01:31:48,620 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 01:32:09,720 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 01:32:36,224 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3268 to 0.3262
2025-05-01 01:32:36,303 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 18 completed in 47.68s - Train Loss: 0.3160, Train Acc: 0.9974, Val Loss: 0.3262, Val Acc: 0.9863
2025-05-01 01:32:36,605 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 01:32:36,606 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 01:32:57,524 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 01:33:24,603 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 19 completed in 48.00s - Train Loss: 0.3143, Train Acc: 0.9994, Val Loss: 0.3271, Val Acc: 0.9854
2025-05-01 01:33:24,833 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 01:33:24,834 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 01:33:45,580 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 01:34:12,747 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Epoch 20 completed in 47.91s - Train Loss: 0.3167, Train Acc: 0.9968, Val Loss: 0.3276, Val Acc: 0.9846
2025-05-01 01:34:12,967 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 01:34:12,967 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:34:12,969 - training.model_ResNet50_opt_Adam_lr_0.0001_id_61 - INFO - Starting model evaluation
2025-05-01 01:34:20,359 - training.model_ResNet50_opt_Adam_lr_0.0001_id_61 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9368
  Recall:    0.9336
  F1 Score:  0.9352
  IoU:       0.8783
  mAP:       0.9544
  AUC:       0.9847
2025-05-01 01:34:20,361 - training.model_ResNet50_opt_Adam_lr_0.0001_id_61 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_61/final_results.json
2025-05-01 01:34:20,363 - training.model_ResNet50_opt_Adam_lr_0.0001_id_61 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_61/final_results.json
2025-05-01 01:34:20,363 - main - INFO - 
Summary for configuration 61:
2025-05-01 01:34:20,363 - main - INFO - Accuracy: 0.9633
2025-05-01 01:34:20,363 - main - INFO - Precision: 0.9368
2025-05-01 01:34:20,363 - main - INFO - Recall: 0.9336
2025-05-01 01:34:20,363 - main - INFO - F1 Score: 0.9352
2025-05-01 01:34:20,363 - main - INFO - IoU: 0.8783
2025-05-01 01:34:20,363 - main - INFO - mAP: 0.9544
2025-05-01 01:34:20,363 - main - INFO - AUC: 0.9847
2025-05-01 01:34:20,363 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:34:20,363 - main - INFO - 
==================================================
2025-05-01 01:34:20,363 - main - INFO - Running configuration 62/756:
2025-05-01 01:34:20,363 - main - INFO - Model: ResNet50
2025-05-01 01:34:20,363 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:34:20,363 - main - INFO - Scheduler: StepLR
2025-05-01 01:34:20,363 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:34:20,363 - main - INFO - ==================================================
2025-05-01 01:34:20,363 - training.model_ResNet50_opt_Adam_lr_0.0001_id_62 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_62
2025-05-01 01:34:20,363 - training.model_ResNet50_opt_Adam_lr_0.0001_id_62 - INFO - Config: {
  "id": 62,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:34:20,520 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:34:20,520 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 62,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:34:20,520 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:34:20,601 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:34:20,602 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:34:20,604 - training.model_ResNet50_opt_Adam_lr_0.0001_id_62 - INFO - Starting model evaluation
2025-05-01 01:34:28,180 - training.model_ResNet50_opt_Adam_lr_0.0001_id_62 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9343
  Recall:    0.9441
  F1 Score:  0.9391
  IoU:       0.8852
  mAP:       0.9437
  AUC:       0.9842
2025-05-01 01:34:28,183 - training.model_ResNet50_opt_Adam_lr_0.0001_id_62 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_62/final_results.json
2025-05-01 01:34:28,185 - training.model_ResNet50_opt_Adam_lr_0.0001_id_62 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_62/final_results.json
2025-05-01 01:34:28,186 - main - INFO - 
Summary for configuration 62:
2025-05-01 01:34:28,186 - main - INFO - Accuracy: 0.9653
2025-05-01 01:34:28,186 - main - INFO - Precision: 0.9343
2025-05-01 01:34:28,186 - main - INFO - Recall: 0.9441
2025-05-01 01:34:28,186 - main - INFO - F1 Score: 0.9391
2025-05-01 01:34:28,186 - main - INFO - IoU: 0.8852
2025-05-01 01:34:28,186 - main - INFO - mAP: 0.9437
2025-05-01 01:34:28,186 - main - INFO - AUC: 0.9842
2025-05-01 01:34:28,186 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:34:28,186 - main - INFO - 
==================================================
2025-05-01 01:34:28,186 - main - INFO - Running configuration 63/756:
2025-05-01 01:34:28,186 - main - INFO - Model: ResNet50
2025-05-01 01:34:28,186 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:34:28,186 - main - INFO - Scheduler: StepLR
2025-05-01 01:34:28,186 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:34:28,186 - main - INFO - ==================================================
2025-05-01 01:34:28,186 - training.model_ResNet50_opt_Adam_lr_0.0001_id_63 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_63
2025-05-01 01:34:28,186 - training.model_ResNet50_opt_Adam_lr_0.0001_id_63 - INFO - Config: {
  "id": 63,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:34:28,346 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:34:28,346 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 63,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:34:28,346 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:34:28,430 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:34:28,430 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:34:28,432 - training.model_ResNet50_opt_Adam_lr_0.0001_id_63 - INFO - Starting model evaluation
2025-05-01 01:34:35,778 - training.model_ResNet50_opt_Adam_lr_0.0001_id_63 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9441
  Recall:    0.9441
  F1 Score:  0.9441
  IoU:       0.8940
  mAP:       0.9484
  AUC:       0.9860
2025-05-01 01:34:35,780 - training.model_ResNet50_opt_Adam_lr_0.0001_id_63 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_63/final_results.json
2025-05-01 01:34:35,781 - training.model_ResNet50_opt_Adam_lr_0.0001_id_63 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_63/final_results.json
2025-05-01 01:34:35,781 - main - INFO - 
Summary for configuration 63:
2025-05-01 01:34:35,782 - main - INFO - Accuracy: 0.9683
2025-05-01 01:34:35,782 - main - INFO - Precision: 0.9441
2025-05-01 01:34:35,782 - main - INFO - Recall: 0.9441
2025-05-01 01:34:35,782 - main - INFO - F1 Score: 0.9441
2025-05-01 01:34:35,782 - main - INFO - IoU: 0.8940
2025-05-01 01:34:35,782 - main - INFO - mAP: 0.9484
2025-05-01 01:34:35,782 - main - INFO - AUC: 0.9860
2025-05-01 01:34:35,782 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:34:35,782 - main - INFO - 
==================================================
2025-05-01 01:34:35,782 - main - INFO - Running configuration 64/756:
2025-05-01 01:34:35,782 - main - INFO - Model: ResNet50
2025-05-01 01:34:35,782 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:34:35,782 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:34:35,782 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:34:35,782 - main - INFO - ==================================================
2025-05-01 01:34:35,782 - training.model_ResNet50_opt_Adam_lr_0.0001_id_64 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_64
2025-05-01 01:34:35,782 - training.model_ResNet50_opt_Adam_lr_0.0001_id_64 - INFO - Config: {
  "id": 64,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:34:35,950 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:34:35,950 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 64,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:34:35,950 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:34:36,032 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:34:36,033 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:34:36,035 - training.model_ResNet50_opt_Adam_lr_0.0001_id_64 - INFO - Starting model evaluation
2025-05-01 01:34:43,709 - training.model_ResNet50_opt_Adam_lr_0.0001_id_64 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9306
  Recall:    0.9371
  F1 Score:  0.9338
  IoU:       0.8758
  mAP:       0.9481
  AUC:       0.9846
2025-05-01 01:34:43,711 - training.model_ResNet50_opt_Adam_lr_0.0001_id_64 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_64/final_results.json
2025-05-01 01:34:43,713 - training.model_ResNet50_opt_Adam_lr_0.0001_id_64 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_64/final_results.json
2025-05-01 01:34:43,713 - main - INFO - 
Summary for configuration 64:
2025-05-01 01:34:43,713 - main - INFO - Accuracy: 0.9623
2025-05-01 01:34:43,713 - main - INFO - Precision: 0.9306
2025-05-01 01:34:43,713 - main - INFO - Recall: 0.9371
2025-05-01 01:34:43,713 - main - INFO - F1 Score: 0.9338
2025-05-01 01:34:43,713 - main - INFO - IoU: 0.8758
2025-05-01 01:34:43,713 - main - INFO - mAP: 0.9481
2025-05-01 01:34:43,713 - main - INFO - AUC: 0.9846
2025-05-01 01:34:43,713 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:34:43,713 - main - INFO - 
==================================================
2025-05-01 01:34:43,713 - main - INFO - Running configuration 65/756:
2025-05-01 01:34:43,713 - main - INFO - Model: ResNet50
2025-05-01 01:34:43,713 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:34:43,713 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:34:43,713 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:34:43,713 - main - INFO - ==================================================
2025-05-01 01:34:43,713 - training.model_ResNet50_opt_Adam_lr_0.0001_id_65 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_65
2025-05-01 01:34:43,713 - training.model_ResNet50_opt_Adam_lr_0.0001_id_65 - INFO - Config: {
  "id": 65,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:34:43,885 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:34:43,885 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 65,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:34:43,885 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:34:43,966 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:34:43,967 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:34:43,969 - training.model_ResNet50_opt_Adam_lr_0.0001_id_65 - INFO - Starting model evaluation
2025-05-01 01:34:51,522 - training.model_ResNet50_opt_Adam_lr_0.0001_id_65 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9375
  Recall:    0.9441
  F1 Score:  0.9408
  IoU:       0.8882
  mAP:       0.9527
  AUC:       0.9853
2025-05-01 01:34:51,524 - training.model_ResNet50_opt_Adam_lr_0.0001_id_65 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_65/final_results.json
2025-05-01 01:34:51,525 - training.model_ResNet50_opt_Adam_lr_0.0001_id_65 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_65/final_results.json
2025-05-01 01:34:51,525 - main - INFO - 
Summary for configuration 65:
2025-05-01 01:34:51,525 - main - INFO - Accuracy: 0.9663
2025-05-01 01:34:51,525 - main - INFO - Precision: 0.9375
2025-05-01 01:34:51,525 - main - INFO - Recall: 0.9441
2025-05-01 01:34:51,525 - main - INFO - F1 Score: 0.9408
2025-05-01 01:34:51,525 - main - INFO - IoU: 0.8882
2025-05-01 01:34:51,525 - main - INFO - mAP: 0.9527
2025-05-01 01:34:51,525 - main - INFO - AUC: 0.9853
2025-05-01 01:34:51,525 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:34:51,526 - main - INFO - 
==================================================
2025-05-01 01:34:51,526 - main - INFO - Running configuration 66/756:
2025-05-01 01:34:51,526 - main - INFO - Model: ResNet50
2025-05-01 01:34:51,526 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:34:51,526 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:34:51,526 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:34:51,526 - main - INFO - ==================================================
2025-05-01 01:34:51,526 - training.model_ResNet50_opt_Adam_lr_0.0001_id_66 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_66
2025-05-01 01:34:51,526 - training.model_ResNet50_opt_Adam_lr_0.0001_id_66 - INFO - Config: {
  "id": 66,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:34:51,687 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:34:51,687 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 66,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:34:51,687 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:34:51,768 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:34:51,769 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:34:51,770 - training.model_ResNet50_opt_Adam_lr_0.0001_id_66 - INFO - Starting model evaluation
2025-05-01 01:34:59,189 - training.model_ResNet50_opt_Adam_lr_0.0001_id_66 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9373
  Recall:    0.9406
  F1 Score:  0.9389
  IoU:       0.8849
  mAP:       0.9582
  AUC:       0.9853
2025-05-01 01:34:59,191 - training.model_ResNet50_opt_Adam_lr_0.0001_id_66 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_66/final_results.json
2025-05-01 01:34:59,192 - training.model_ResNet50_opt_Adam_lr_0.0001_id_66 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_66/final_results.json
2025-05-01 01:34:59,192 - main - INFO - 
Summary for configuration 66:
2025-05-01 01:34:59,192 - main - INFO - Accuracy: 0.9653
2025-05-01 01:34:59,192 - main - INFO - Precision: 0.9373
2025-05-01 01:34:59,192 - main - INFO - Recall: 0.9406
2025-05-01 01:34:59,192 - main - INFO - F1 Score: 0.9389
2025-05-01 01:34:59,192 - main - INFO - IoU: 0.8849
2025-05-01 01:34:59,192 - main - INFO - mAP: 0.9582
2025-05-01 01:34:59,192 - main - INFO - AUC: 0.9853
2025-05-01 01:34:59,192 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:34:59,193 - main - INFO - 
==================================================
2025-05-01 01:34:59,193 - main - INFO - Running configuration 67/756:
2025-05-01 01:34:59,193 - main - INFO - Model: ResNet50
2025-05-01 01:34:59,193 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:34:59,193 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:34:59,193 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:34:59,193 - main - INFO - ==================================================
2025-05-01 01:34:59,193 - training.model_ResNet50_opt_Adam_lr_0.0001_id_67 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_67
2025-05-01 01:34:59,193 - training.model_ResNet50_opt_Adam_lr_0.0001_id_67 - INFO - Config: {
  "id": 67,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:34:59,363 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:34:59,363 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 67,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:34:59,363 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:34:59,442 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:34:59,443 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:34:59,445 - training.model_ResNet50_opt_Adam_lr_0.0001_id_67 - INFO - Starting model evaluation
2025-05-01 01:35:06,981 - training.model_ResNet50_opt_Adam_lr_0.0001_id_67 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9343
  Recall:    0.9441
  F1 Score:  0.9391
  IoU:       0.8852
  mAP:       0.9556
  AUC:       0.9863
2025-05-01 01:35:06,982 - training.model_ResNet50_opt_Adam_lr_0.0001_id_67 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_67/final_results.json
2025-05-01 01:35:06,984 - training.model_ResNet50_opt_Adam_lr_0.0001_id_67 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_67/final_results.json
2025-05-01 01:35:06,984 - main - INFO - 
Summary for configuration 67:
2025-05-01 01:35:06,984 - main - INFO - Accuracy: 0.9653
2025-05-01 01:35:06,984 - main - INFO - Precision: 0.9343
2025-05-01 01:35:06,984 - main - INFO - Recall: 0.9441
2025-05-01 01:35:06,984 - main - INFO - F1 Score: 0.9391
2025-05-01 01:35:06,984 - main - INFO - IoU: 0.8852
2025-05-01 01:35:06,984 - main - INFO - mAP: 0.9556
2025-05-01 01:35:06,984 - main - INFO - AUC: 0.9863
2025-05-01 01:35:06,984 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:35:06,984 - main - INFO - 
==================================================
2025-05-01 01:35:06,984 - main - INFO - Running configuration 68/756:
2025-05-01 01:35:06,984 - main - INFO - Model: ResNet50
2025-05-01 01:35:06,984 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:35:06,984 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:35:06,984 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:35:06,984 - main - INFO - ==================================================
2025-05-01 01:35:06,984 - training.model_ResNet50_opt_Adam_lr_0.0001_id_68 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_68
2025-05-01 01:35:06,985 - training.model_ResNet50_opt_Adam_lr_0.0001_id_68 - INFO - Config: {
  "id": 68,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:35:07,143 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:35:07,143 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 68,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:35:07,143 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:35:07,221 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:35:07,222 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:35:07,223 - training.model_ResNet50_opt_Adam_lr_0.0001_id_68 - INFO - Starting model evaluation
2025-05-01 01:35:14,683 - training.model_ResNet50_opt_Adam_lr_0.0001_id_68 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9406
  Recall:    0.9406
  F1 Score:  0.9406
  IoU:       0.8878
  mAP:       0.9480
  AUC:       0.9841
2025-05-01 01:35:14,685 - training.model_ResNet50_opt_Adam_lr_0.0001_id_68 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_68/final_results.json
2025-05-01 01:35:14,686 - training.model_ResNet50_opt_Adam_lr_0.0001_id_68 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_68/final_results.json
2025-05-01 01:35:14,686 - main - INFO - 
Summary for configuration 68:
2025-05-01 01:35:14,686 - main - INFO - Accuracy: 0.9663
2025-05-01 01:35:14,686 - main - INFO - Precision: 0.9406
2025-05-01 01:35:14,686 - main - INFO - Recall: 0.9406
2025-05-01 01:35:14,686 - main - INFO - F1 Score: 0.9406
2025-05-01 01:35:14,686 - main - INFO - IoU: 0.8878
2025-05-01 01:35:14,686 - main - INFO - mAP: 0.9480
2025-05-01 01:35:14,686 - main - INFO - AUC: 0.9841
2025-05-01 01:35:14,686 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:35:14,686 - main - INFO - 
==================================================
2025-05-01 01:35:14,686 - main - INFO - Running configuration 69/756:
2025-05-01 01:35:14,686 - main - INFO - Model: ResNet50
2025-05-01 01:35:14,686 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:35:14,686 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:35:14,686 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:35:14,686 - main - INFO - ==================================================
2025-05-01 01:35:14,687 - training.model_ResNet50_opt_Adam_lr_0.0001_id_69 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_69
2025-05-01 01:35:14,687 - training.model_ResNet50_opt_Adam_lr_0.0001_id_69 - INFO - Config: {
  "id": 69,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:35:14,844 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:35:14,844 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 69,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:35:14,844 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:35:14,922 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:35:14,923 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:35:14,925 - training.model_ResNet50_opt_Adam_lr_0.0001_id_69 - INFO - Starting model evaluation
2025-05-01 01:35:22,496 - training.model_ResNet50_opt_Adam_lr_0.0001_id_69 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9278
  Recall:    0.9441
  F1 Score:  0.9359
  IoU:       0.8795
  mAP:       0.9585
  AUC:       0.9855
2025-05-01 01:35:22,498 - training.model_ResNet50_opt_Adam_lr_0.0001_id_69 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_69/final_results.json
2025-05-01 01:35:22,499 - training.model_ResNet50_opt_Adam_lr_0.0001_id_69 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_69/final_results.json
2025-05-01 01:35:22,499 - main - INFO - 
Summary for configuration 69:
2025-05-01 01:35:22,499 - main - INFO - Accuracy: 0.9633
2025-05-01 01:35:22,499 - main - INFO - Precision: 0.9278
2025-05-01 01:35:22,499 - main - INFO - Recall: 0.9441
2025-05-01 01:35:22,499 - main - INFO - F1 Score: 0.9359
2025-05-01 01:35:22,499 - main - INFO - IoU: 0.8795
2025-05-01 01:35:22,499 - main - INFO - mAP: 0.9585
2025-05-01 01:35:22,499 - main - INFO - AUC: 0.9855
2025-05-01 01:35:22,499 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:35:22,499 - main - INFO - 
==================================================
2025-05-01 01:35:22,499 - main - INFO - Running configuration 70/756:
2025-05-01 01:35:22,500 - main - INFO - Model: ResNet50
2025-05-01 01:35:22,500 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:35:22,500 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:35:22,500 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:35:22,500 - main - INFO - ==================================================
2025-05-01 01:35:22,500 - training.model_ResNet50_opt_Adam_lr_0.0001_id_70 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_70
2025-05-01 01:35:22,500 - training.model_ResNet50_opt_Adam_lr_0.0001_id_70 - INFO - Config: {
  "id": 70,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:35:22,667 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:35:22,667 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 70,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:35:22,667 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:35:22,746 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:35:22,746 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:35:22,748 - training.model_ResNet50_opt_Adam_lr_0.0001_id_70 - INFO - Starting model evaluation
2025-05-01 01:35:30,185 - training.model_ResNet50_opt_Adam_lr_0.0001_id_70 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9340
  Recall:    0.9406
  F1 Score:  0.9373
  IoU:       0.8820
  mAP:       0.9478
  AUC:       0.9857
2025-05-01 01:35:30,187 - training.model_ResNet50_opt_Adam_lr_0.0001_id_70 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_70/final_results.json
2025-05-01 01:35:30,189 - training.model_ResNet50_opt_Adam_lr_0.0001_id_70 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_70/final_results.json
2025-05-01 01:35:30,189 - main - INFO - 
Summary for configuration 70:
2025-05-01 01:35:30,189 - main - INFO - Accuracy: 0.9643
2025-05-01 01:35:30,189 - main - INFO - Precision: 0.9340
2025-05-01 01:35:30,189 - main - INFO - Recall: 0.9406
2025-05-01 01:35:30,189 - main - INFO - F1 Score: 0.9373
2025-05-01 01:35:30,189 - main - INFO - IoU: 0.8820
2025-05-01 01:35:30,189 - main - INFO - mAP: 0.9478
2025-05-01 01:35:30,189 - main - INFO - AUC: 0.9857
2025-05-01 01:35:30,189 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:35:30,189 - main - INFO - 
==================================================
2025-05-01 01:35:30,189 - main - INFO - Running configuration 71/756:
2025-05-01 01:35:30,189 - main - INFO - Model: ResNet50
2025-05-01 01:35:30,189 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:35:30,189 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:35:30,189 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:35:30,189 - main - INFO - ==================================================
2025-05-01 01:35:30,189 - training.model_ResNet50_opt_Adam_lr_0.0001_id_71 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_71
2025-05-01 01:35:30,189 - training.model_ResNet50_opt_Adam_lr_0.0001_id_71 - INFO - Config: {
  "id": 71,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:35:30,349 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:35:30,349 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 71,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:35:30,350 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:35:30,428 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:35:30,429 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:35:30,431 - training.model_ResNet50_opt_Adam_lr_0.0001_id_71 - INFO - Starting model evaluation
2025-05-01 01:35:38,035 - training.model_ResNet50_opt_Adam_lr_0.0001_id_71 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9408
  Recall:    0.9441
  F1 Score:  0.9424
  IoU:       0.8911
  mAP:       0.9463
  AUC:       0.9853
2025-05-01 01:35:38,037 - training.model_ResNet50_opt_Adam_lr_0.0001_id_71 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_71/final_results.json
2025-05-01 01:35:38,039 - training.model_ResNet50_opt_Adam_lr_0.0001_id_71 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_71/final_results.json
2025-05-01 01:35:38,039 - main - INFO - 
Summary for configuration 71:
2025-05-01 01:35:38,039 - main - INFO - Accuracy: 0.9673
2025-05-01 01:35:38,039 - main - INFO - Precision: 0.9408
2025-05-01 01:35:38,039 - main - INFO - Recall: 0.9441
2025-05-01 01:35:38,039 - main - INFO - F1 Score: 0.9424
2025-05-01 01:35:38,039 - main - INFO - IoU: 0.8911
2025-05-01 01:35:38,039 - main - INFO - mAP: 0.9463
2025-05-01 01:35:38,039 - main - INFO - AUC: 0.9853
2025-05-01 01:35:38,039 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:35:38,039 - main - INFO - 
==================================================
2025-05-01 01:35:38,039 - main - INFO - Running configuration 72/756:
2025-05-01 01:35:38,039 - main - INFO - Model: ResNet50
2025-05-01 01:35:38,039 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 01:35:38,039 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:35:38,039 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:35:38,039 - main - INFO - ==================================================
2025-05-01 01:35:38,039 - training.model_ResNet50_opt_Adam_lr_0.0001_id_72 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001_id_72
2025-05-01 01:35:38,039 - training.model_ResNet50_opt_Adam_lr_0.0001_id_72 - INFO - Config: {
  "id": 72,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:35:38,207 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_Adam_lr_0.0001
2025-05-01 01:35:38,207 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 72,
  "model_name": "ResNet50",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:35:38,207 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 01:35:38,286 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-01 01:35:38,287 - training.model_ResNet50_opt_Adam_lr_0.0001 - INFO - Training completed after 961.44 seconds
2025-05-01 01:35:38,289 - training.model_ResNet50_opt_Adam_lr_0.0001_id_72 - INFO - Starting model evaluation
2025-05-01 01:35:46,011 - training.model_ResNet50_opt_Adam_lr_0.0001_id_72 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9404
  Recall:    0.9371
  F1 Score:  0.9387
  IoU:       0.8845
  mAP:       0.9535
  AUC:       0.9851
2025-05-01 01:35:46,013 - training.model_ResNet50_opt_Adam_lr_0.0001_id_72 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_72/final_results.json
2025-05-01 01:35:46,015 - training.model_ResNet50_opt_Adam_lr_0.0001_id_72 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_Adam_lr_0.0001_id_72/final_results.json
2025-05-01 01:35:46,015 - main - INFO - 
Summary for configuration 72:
2025-05-01 01:35:46,015 - main - INFO - Accuracy: 0.9653
2025-05-01 01:35:46,015 - main - INFO - Precision: 0.9404
2025-05-01 01:35:46,015 - main - INFO - Recall: 0.9371
2025-05-01 01:35:46,015 - main - INFO - F1 Score: 0.9387
2025-05-01 01:35:46,015 - main - INFO - IoU: 0.8845
2025-05-01 01:35:46,015 - main - INFO - mAP: 0.9535
2025-05-01 01:35:46,015 - main - INFO - AUC: 0.9851
2025-05-01 01:35:46,015 - main - INFO - Training time: 961.44 seconds
2025-05-01 01:35:46,015 - main - INFO - 
==================================================
2025-05-01 01:35:46,015 - main - INFO - Running configuration 73/756:
2025-05-01 01:35:46,015 - main - INFO - Model: ResNet50
2025-05-01 01:35:46,015 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:35:46,015 - main - INFO - Scheduler: StepLR
2025-05-01 01:35:46,015 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:35:46,015 - main - INFO - ==================================================
2025-05-01 01:35:46,015 - training.model_ResNet50_opt_AdamW_lr_0.01_id_73 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_73
2025-05-01 01:35:46,015 - training.model_ResNet50_opt_AdamW_lr_0.01_id_73 - INFO - Config: {
  "id": 73,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:35:46,190 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:35:46,190 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 73,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:35:46,190 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 01:35:46,191 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 01:36:06,913 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 01:36:33,906 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.5069
2025-05-01 01:36:33,959 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 1 completed in 47.77s - Train Loss: 0.7030, Train Acc: 0.5249, Val Loss: 0.7563, Val Acc: 0.5069
2025-05-01 01:36:34,181 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 01:36:34,181 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 01:36:54,597 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 01:37:21,940 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.5069 to 0.5386
2025-05-01 01:37:22,021 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 2 completed in 47.84s - Train Loss: 0.6885, Train Acc: 0.5506, Val Loss: 0.6843, Val Acc: 0.5386
2025-05-01 01:37:22,244 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 01:37:22,245 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 01:37:43,150 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 01:38:10,173 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.5386 to 0.5420
2025-05-01 01:38:10,253 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 3 completed in 48.01s - Train Loss: 0.6820, Train Acc: 0.5506, Val Loss: 0.6857, Val Acc: 0.5420
2025-05-01 01:38:10,468 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 01:38:10,468 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 01:38:30,914 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 01:38:58,036 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation loss improved from inf to 0.6830
2025-05-01 01:38:58,126 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 4 completed in 47.66s - Train Loss: 0.6807, Train Acc: 0.5495, Val Loss: 0.6830, Val Acc: 0.5377
2025-05-01 01:38:58,343 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 01:38:58,343 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 01:39:19,267 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 01:39:46,107 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.5420 to 0.6003
2025-05-01 01:39:46,186 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 5 completed in 47.84s - Train Loss: 0.6780, Train Acc: 0.5622, Val Loss: 0.6618, Val Acc: 0.6003
2025-05-01 01:39:46,398 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 01:39:46,399 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 01:40:07,448 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 01:40:34,121 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6003 to 0.6089
2025-05-01 01:40:34,193 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 6 completed in 47.79s - Train Loss: 0.6691, Train Acc: 0.5832, Val Loss: 0.6621, Val Acc: 0.6089
2025-05-01 01:40:34,405 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 01:40:34,405 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 01:40:55,155 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 01:41:22,005 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 7 completed in 47.60s - Train Loss: 0.6744, Train Acc: 0.5695, Val Loss: 0.7168, Val Acc: 0.5360
2025-05-01 01:41:22,219 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 01:41:22,220 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 01:41:43,268 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 01:42:10,302 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.6830 to 0.6797
2025-05-01 01:42:10,385 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 8 completed in 48.17s - Train Loss: 0.6790, Train Acc: 0.5478, Val Loss: 0.6797, Val Acc: 0.5377
2025-05-01 01:42:10,597 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 01:42:10,598 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 01:42:31,220 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 01:42:58,275 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 9 completed in 47.68s - Train Loss: 0.6781, Train Acc: 0.5483, Val Loss: 0.6821, Val Acc: 0.6029
2025-05-01 01:42:58,517 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 01:42:58,518 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 01:43:19,016 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 01:43:46,251 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.6797 to 0.6582
2025-05-01 01:43:46,340 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 10 completed in 47.82s - Train Loss: 0.6653, Train Acc: 0.5852, Val Loss: 0.6582, Val Acc: 0.6046
2025-05-01 01:43:46,551 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 01:43:46,552 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 01:44:07,633 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 01:44:34,266 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.6582 to 0.6531
2025-05-01 01:44:34,358 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 11 completed in 47.81s - Train Loss: 0.6578, Train Acc: 0.5959, Val Loss: 0.6531, Val Acc: 0.6046
2025-05-01 01:44:34,570 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 01:44:34,570 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 01:44:55,395 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 01:45:22,369 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.6531 to 0.6506
2025-05-01 01:45:22,447 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 12 completed in 47.88s - Train Loss: 0.6555, Train Acc: 0.5955, Val Loss: 0.6506, Val Acc: 0.6072
2025-05-01 01:45:22,669 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 01:45:22,670 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 01:45:43,767 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 01:46:10,931 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6089 to 0.6106
2025-05-01 01:46:11,006 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 13 completed in 48.34s - Train Loss: 0.6581, Train Acc: 0.5888, Val Loss: 0.6510, Val Acc: 0.6106
2025-05-01 01:46:11,217 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 01:46:11,217 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 01:46:31,976 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 01:46:58,847 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6106 to 0.6166
2025-05-01 01:46:58,926 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 14 completed in 47.71s - Train Loss: 0.6536, Train Acc: 0.6002, Val Loss: 0.6458, Val Acc: 0.6166
2025-05-01 01:46:59,158 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 01:46:59,158 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 01:47:19,787 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 01:47:46,905 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6166 to 0.6244
2025-05-01 01:47:46,984 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 15 completed in 47.83s - Train Loss: 0.6496, Train Acc: 0.6109, Val Loss: 0.6434, Val Acc: 0.6244
2025-05-01 01:47:47,207 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 01:47:47,207 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 01:48:08,432 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 01:48:35,122 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.6506 to 0.6418
2025-05-01 01:48:35,209 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 16 completed in 48.00s - Train Loss: 0.6510, Train Acc: 0.6085, Val Loss: 0.6418, Val Acc: 0.6235
2025-05-01 01:48:35,422 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 01:48:35,423 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 01:48:56,160 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 01:49:22,808 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 17 completed in 47.39s - Train Loss: 0.6493, Train Acc: 0.6092, Val Loss: 0.6449, Val Acc: 0.6184
2025-05-01 01:49:23,045 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 01:49:23,046 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 01:49:43,832 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 01:50:10,790 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 18 completed in 47.74s - Train Loss: 0.6473, Train Acc: 0.6120, Val Loss: 0.6531, Val Acc: 0.6123
2025-05-01 01:50:11,029 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 01:50:11,030 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 01:50:31,433 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 01:50:58,812 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6244 to 0.6261
2025-05-01 01:50:58,888 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 19 completed in 47.86s - Train Loss: 0.6476, Train Acc: 0.6135, Val Loss: 0.6409, Val Acc: 0.6261
2025-05-01 01:50:59,098 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 01:50:59,099 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 01:51:19,910 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 01:51:46,642 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6261 to 0.6286
2025-05-01 01:51:46,717 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Epoch 20 completed in 47.62s - Train Loss: 0.6449, Train Acc: 0.6171, Val Loss: 0.6386, Val Acc: 0.6286
2025-05-01 01:51:46,935 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 01:51:46,935 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:51:46,937 - training.model_ResNet50_opt_AdamW_lr_0.01_id_73 - INFO - Starting model evaluation
2025-05-01 01:51:54,519 - training.model_ResNet50_opt_AdamW_lr_0.01_id_73 - INFO - Evaluation metrics:
  Accuracy:  0.7760
  Precision: 0.8061
  Recall:    0.2762
  F1 Score:  0.4115
  IoU:       0.2590
  mAP:       0.4687
  AUC:       0.6307
2025-05-01 01:51:54,520 - training.model_ResNet50_opt_AdamW_lr_0.01_id_73 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_73/final_results.json
2025-05-01 01:51:54,522 - training.model_ResNet50_opt_AdamW_lr_0.01_id_73 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_73/final_results.json
2025-05-01 01:51:54,522 - main - INFO - 
Summary for configuration 73:
2025-05-01 01:51:54,522 - main - INFO - Accuracy: 0.7760
2025-05-01 01:51:54,522 - main - INFO - Precision: 0.8061
2025-05-01 01:51:54,522 - main - INFO - Recall: 0.2762
2025-05-01 01:51:54,522 - main - INFO - F1 Score: 0.4115
2025-05-01 01:51:54,522 - main - INFO - IoU: 0.2590
2025-05-01 01:51:54,522 - main - INFO - mAP: 0.4687
2025-05-01 01:51:54,522 - main - INFO - AUC: 0.6307
2025-05-01 01:51:54,522 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:51:54,522 - main - INFO - 
==================================================
2025-05-01 01:51:54,522 - main - INFO - Running configuration 74/756:
2025-05-01 01:51:54,522 - main - INFO - Model: ResNet50
2025-05-01 01:51:54,522 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:51:54,522 - main - INFO - Scheduler: StepLR
2025-05-01 01:51:54,522 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:51:54,522 - main - INFO - ==================================================
2025-05-01 01:51:54,522 - training.model_ResNet50_opt_AdamW_lr_0.01_id_74 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_74
2025-05-01 01:51:54,522 - training.model_ResNet50_opt_AdamW_lr_0.01_id_74 - INFO - Config: {
  "id": 74,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:51:54,684 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:51:54,684 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 74,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:51:54,684 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:51:54,767 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:51:54,768 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:51:54,770 - training.model_ResNet50_opt_AdamW_lr_0.01_id_74 - INFO - Starting model evaluation
2025-05-01 01:52:02,185 - training.model_ResNet50_opt_AdamW_lr_0.01_id_74 - INFO - Evaluation metrics:
  Accuracy:  0.7740
  Precision: 0.7900
  Recall:    0.2762
  F1 Score:  0.4093
  IoU:       0.2573
  mAP:       0.4675
  AUC:       0.6252
2025-05-01 01:52:02,186 - training.model_ResNet50_opt_AdamW_lr_0.01_id_74 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_74/final_results.json
2025-05-01 01:52:02,188 - training.model_ResNet50_opt_AdamW_lr_0.01_id_74 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_74/final_results.json
2025-05-01 01:52:02,188 - main - INFO - 
Summary for configuration 74:
2025-05-01 01:52:02,188 - main - INFO - Accuracy: 0.7740
2025-05-01 01:52:02,188 - main - INFO - Precision: 0.7900
2025-05-01 01:52:02,188 - main - INFO - Recall: 0.2762
2025-05-01 01:52:02,188 - main - INFO - F1 Score: 0.4093
2025-05-01 01:52:02,188 - main - INFO - IoU: 0.2573
2025-05-01 01:52:02,188 - main - INFO - mAP: 0.4675
2025-05-01 01:52:02,188 - main - INFO - AUC: 0.6252
2025-05-01 01:52:02,188 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:52:02,188 - main - INFO - 
==================================================
2025-05-01 01:52:02,188 - main - INFO - Running configuration 75/756:
2025-05-01 01:52:02,188 - main - INFO - Model: ResNet50
2025-05-01 01:52:02,188 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:52:02,188 - main - INFO - Scheduler: StepLR
2025-05-01 01:52:02,188 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:52:02,188 - main - INFO - ==================================================
2025-05-01 01:52:02,188 - training.model_ResNet50_opt_AdamW_lr_0.01_id_75 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_75
2025-05-01 01:52:02,188 - training.model_ResNet50_opt_AdamW_lr_0.01_id_75 - INFO - Config: {
  "id": 75,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:52:02,358 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:52:02,358 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 75,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:52:02,358 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:52:02,441 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:52:02,442 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:52:02,444 - training.model_ResNet50_opt_AdamW_lr_0.01_id_75 - INFO - Starting model evaluation
2025-05-01 01:52:09,860 - training.model_ResNet50_opt_AdamW_lr_0.01_id_75 - INFO - Evaluation metrics:
  Accuracy:  0.7750
  Precision: 0.7921
  Recall:    0.2797
  F1 Score:  0.4134
  IoU:       0.2606
  mAP:       0.4772
  AUC:       0.6293
2025-05-01 01:52:09,861 - training.model_ResNet50_opt_AdamW_lr_0.01_id_75 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_75/final_results.json
2025-05-01 01:52:09,863 - training.model_ResNet50_opt_AdamW_lr_0.01_id_75 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_75/final_results.json
2025-05-01 01:52:09,863 - main - INFO - 
Summary for configuration 75:
2025-05-01 01:52:09,863 - main - INFO - Accuracy: 0.7750
2025-05-01 01:52:09,863 - main - INFO - Precision: 0.7921
2025-05-01 01:52:09,863 - main - INFO - Recall: 0.2797
2025-05-01 01:52:09,863 - main - INFO - F1 Score: 0.4134
2025-05-01 01:52:09,863 - main - INFO - IoU: 0.2606
2025-05-01 01:52:09,863 - main - INFO - mAP: 0.4772
2025-05-01 01:52:09,863 - main - INFO - AUC: 0.6293
2025-05-01 01:52:09,863 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:52:09,863 - main - INFO - 
==================================================
2025-05-01 01:52:09,863 - main - INFO - Running configuration 76/756:
2025-05-01 01:52:09,863 - main - INFO - Model: ResNet50
2025-05-01 01:52:09,863 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:52:09,863 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:52:09,863 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:52:09,863 - main - INFO - ==================================================
2025-05-01 01:52:09,863 - training.model_ResNet50_opt_AdamW_lr_0.01_id_76 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_76
2025-05-01 01:52:09,863 - training.model_ResNet50_opt_AdamW_lr_0.01_id_76 - INFO - Config: {
  "id": 76,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:52:10,027 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:52:10,027 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 76,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:52:10,027 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:52:10,110 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:52:10,110 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:52:10,112 - training.model_ResNet50_opt_AdamW_lr_0.01_id_76 - INFO - Starting model evaluation
2025-05-01 01:52:17,848 - training.model_ResNet50_opt_AdamW_lr_0.01_id_76 - INFO - Evaluation metrics:
  Accuracy:  0.7839
  Precision: 0.8778
  Recall:    0.2762
  F1 Score:  0.4202
  IoU:       0.2660
  mAP:       0.4790
  AUC:       0.6278
2025-05-01 01:52:17,850 - training.model_ResNet50_opt_AdamW_lr_0.01_id_76 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_76/final_results.json
2025-05-01 01:52:17,851 - training.model_ResNet50_opt_AdamW_lr_0.01_id_76 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_76/final_results.json
2025-05-01 01:52:17,851 - main - INFO - 
Summary for configuration 76:
2025-05-01 01:52:17,851 - main - INFO - Accuracy: 0.7839
2025-05-01 01:52:17,851 - main - INFO - Precision: 0.8778
2025-05-01 01:52:17,851 - main - INFO - Recall: 0.2762
2025-05-01 01:52:17,851 - main - INFO - F1 Score: 0.4202
2025-05-01 01:52:17,851 - main - INFO - IoU: 0.2660
2025-05-01 01:52:17,851 - main - INFO - mAP: 0.4790
2025-05-01 01:52:17,851 - main - INFO - AUC: 0.6278
2025-05-01 01:52:17,851 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:52:17,851 - main - INFO - 
==================================================
2025-05-01 01:52:17,851 - main - INFO - Running configuration 77/756:
2025-05-01 01:52:17,851 - main - INFO - Model: ResNet50
2025-05-01 01:52:17,851 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:52:17,851 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:52:17,851 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:52:17,851 - main - INFO - ==================================================
2025-05-01 01:52:17,852 - training.model_ResNet50_opt_AdamW_lr_0.01_id_77 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_77
2025-05-01 01:52:17,852 - training.model_ResNet50_opt_AdamW_lr_0.01_id_77 - INFO - Config: {
  "id": 77,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:52:18,019 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:52:18,019 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 77,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:52:18,020 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:52:18,101 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:52:18,102 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:52:18,103 - training.model_ResNet50_opt_AdamW_lr_0.01_id_77 - INFO - Starting model evaluation
2025-05-01 01:52:25,668 - training.model_ResNet50_opt_AdamW_lr_0.01_id_77 - INFO - Evaluation metrics:
  Accuracy:  0.7800
  Precision: 0.8404
  Recall:    0.2762
  F1 Score:  0.4158
  IoU:       0.2625
  mAP:       0.4716
  AUC:       0.6263
2025-05-01 01:52:25,670 - training.model_ResNet50_opt_AdamW_lr_0.01_id_77 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_77/final_results.json
2025-05-01 01:52:25,671 - training.model_ResNet50_opt_AdamW_lr_0.01_id_77 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_77/final_results.json
2025-05-01 01:52:25,671 - main - INFO - 
Summary for configuration 77:
2025-05-01 01:52:25,671 - main - INFO - Accuracy: 0.7800
2025-05-01 01:52:25,671 - main - INFO - Precision: 0.8404
2025-05-01 01:52:25,671 - main - INFO - Recall: 0.2762
2025-05-01 01:52:25,671 - main - INFO - F1 Score: 0.4158
2025-05-01 01:52:25,671 - main - INFO - IoU: 0.2625
2025-05-01 01:52:25,671 - main - INFO - mAP: 0.4716
2025-05-01 01:52:25,671 - main - INFO - AUC: 0.6263
2025-05-01 01:52:25,671 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:52:25,671 - main - INFO - 
==================================================
2025-05-01 01:52:25,671 - main - INFO - Running configuration 78/756:
2025-05-01 01:52:25,671 - main - INFO - Model: ResNet50
2025-05-01 01:52:25,671 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:52:25,671 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 01:52:25,671 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:52:25,671 - main - INFO - ==================================================
2025-05-01 01:52:25,671 - training.model_ResNet50_opt_AdamW_lr_0.01_id_78 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_78
2025-05-01 01:52:25,672 - training.model_ResNet50_opt_AdamW_lr_0.01_id_78 - INFO - Config: {
  "id": 78,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:52:25,920 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:52:25,920 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 78,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:52:25,920 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:52:26,003 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:52:26,003 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:52:26,005 - training.model_ResNet50_opt_AdamW_lr_0.01_id_78 - INFO - Starting model evaluation
2025-05-01 01:52:33,860 - training.model_ResNet50_opt_AdamW_lr_0.01_id_78 - INFO - Evaluation metrics:
  Accuracy:  0.7750
  Precision: 0.7980
  Recall:    0.2762
  F1 Score:  0.4104
  IoU:       0.2582
  mAP:       0.4718
  AUC:       0.6279
2025-05-01 01:52:33,861 - training.model_ResNet50_opt_AdamW_lr_0.01_id_78 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_78/final_results.json
2025-05-01 01:52:33,863 - training.model_ResNet50_opt_AdamW_lr_0.01_id_78 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_78/final_results.json
2025-05-01 01:52:33,863 - main - INFO - 
Summary for configuration 78:
2025-05-01 01:52:33,863 - main - INFO - Accuracy: 0.7750
2025-05-01 01:52:33,863 - main - INFO - Precision: 0.7980
2025-05-01 01:52:33,863 - main - INFO - Recall: 0.2762
2025-05-01 01:52:33,863 - main - INFO - F1 Score: 0.4104
2025-05-01 01:52:33,863 - main - INFO - IoU: 0.2582
2025-05-01 01:52:33,863 - main - INFO - mAP: 0.4718
2025-05-01 01:52:33,863 - main - INFO - AUC: 0.6279
2025-05-01 01:52:33,863 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:52:33,863 - main - INFO - 
==================================================
2025-05-01 01:52:33,863 - main - INFO - Running configuration 79/756:
2025-05-01 01:52:33,863 - main - INFO - Model: ResNet50
2025-05-01 01:52:33,863 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:52:33,863 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:52:33,863 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:52:33,863 - main - INFO - ==================================================
2025-05-01 01:52:33,863 - training.model_ResNet50_opt_AdamW_lr_0.01_id_79 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_79
2025-05-01 01:52:33,863 - training.model_ResNet50_opt_AdamW_lr_0.01_id_79 - INFO - Config: {
  "id": 79,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:52:34,023 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:52:34,024 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 79,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:52:34,024 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:52:34,102 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:52:34,103 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:52:34,104 - training.model_ResNet50_opt_AdamW_lr_0.01_id_79 - INFO - Starting model evaluation
2025-05-01 01:52:41,689 - training.model_ResNet50_opt_AdamW_lr_0.01_id_79 - INFO - Evaluation metrics:
  Accuracy:  0.7740
  Precision: 0.7900
  Recall:    0.2762
  F1 Score:  0.4093
  IoU:       0.2573
  mAP:       0.4755
  AUC:       0.6333
2025-05-01 01:52:41,691 - training.model_ResNet50_opt_AdamW_lr_0.01_id_79 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_79/final_results.json
2025-05-01 01:52:41,692 - training.model_ResNet50_opt_AdamW_lr_0.01_id_79 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_79/final_results.json
2025-05-01 01:52:41,692 - main - INFO - 
Summary for configuration 79:
2025-05-01 01:52:41,692 - main - INFO - Accuracy: 0.7740
2025-05-01 01:52:41,692 - main - INFO - Precision: 0.7900
2025-05-01 01:52:41,692 - main - INFO - Recall: 0.2762
2025-05-01 01:52:41,692 - main - INFO - F1 Score: 0.4093
2025-05-01 01:52:41,692 - main - INFO - IoU: 0.2573
2025-05-01 01:52:41,692 - main - INFO - mAP: 0.4755
2025-05-01 01:52:41,692 - main - INFO - AUC: 0.6333
2025-05-01 01:52:41,692 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:52:41,692 - main - INFO - 
==================================================
2025-05-01 01:52:41,692 - main - INFO - Running configuration 80/756:
2025-05-01 01:52:41,692 - main - INFO - Model: ResNet50
2025-05-01 01:52:41,692 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:52:41,692 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:52:41,692 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:52:41,692 - main - INFO - ==================================================
2025-05-01 01:52:41,693 - training.model_ResNet50_opt_AdamW_lr_0.01_id_80 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_80
2025-05-01 01:52:41,693 - training.model_ResNet50_opt_AdamW_lr_0.01_id_80 - INFO - Config: {
  "id": 80,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:52:41,851 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:52:41,851 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 80,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:52:41,851 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:52:41,930 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:52:41,930 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:52:41,932 - training.model_ResNet50_opt_AdamW_lr_0.01_id_80 - INFO - Starting model evaluation
2025-05-01 01:52:49,425 - training.model_ResNet50_opt_AdamW_lr_0.01_id_80 - INFO - Evaluation metrics:
  Accuracy:  0.7740
  Precision: 0.7900
  Recall:    0.2762
  F1 Score:  0.4093
  IoU:       0.2573
  mAP:       0.4780
  AUC:       0.6269
2025-05-01 01:52:49,426 - training.model_ResNet50_opt_AdamW_lr_0.01_id_80 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_80/final_results.json
2025-05-01 01:52:49,428 - training.model_ResNet50_opt_AdamW_lr_0.01_id_80 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_80/final_results.json
2025-05-01 01:52:49,428 - main - INFO - 
Summary for configuration 80:
2025-05-01 01:52:49,428 - main - INFO - Accuracy: 0.7740
2025-05-01 01:52:49,428 - main - INFO - Precision: 0.7900
2025-05-01 01:52:49,428 - main - INFO - Recall: 0.2762
2025-05-01 01:52:49,428 - main - INFO - F1 Score: 0.4093
2025-05-01 01:52:49,428 - main - INFO - IoU: 0.2573
2025-05-01 01:52:49,428 - main - INFO - mAP: 0.4780
2025-05-01 01:52:49,428 - main - INFO - AUC: 0.6269
2025-05-01 01:52:49,428 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:52:49,428 - main - INFO - 
==================================================
2025-05-01 01:52:49,428 - main - INFO - Running configuration 81/756:
2025-05-01 01:52:49,428 - main - INFO - Model: ResNet50
2025-05-01 01:52:49,428 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:52:49,428 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 01:52:49,428 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:52:49,428 - main - INFO - ==================================================
2025-05-01 01:52:49,428 - training.model_ResNet50_opt_AdamW_lr_0.01_id_81 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_81
2025-05-01 01:52:49,428 - training.model_ResNet50_opt_AdamW_lr_0.01_id_81 - INFO - Config: {
  "id": 81,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:52:49,586 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:52:49,586 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 81,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:52:49,586 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:52:49,664 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:52:49,665 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:52:49,667 - training.model_ResNet50_opt_AdamW_lr_0.01_id_81 - INFO - Starting model evaluation
2025-05-01 01:52:57,257 - training.model_ResNet50_opt_AdamW_lr_0.01_id_81 - INFO - Evaluation metrics:
  Accuracy:  0.7750
  Precision: 0.7921
  Recall:    0.2797
  F1 Score:  0.4134
  IoU:       0.2606
  mAP:       0.4664
  AUC:       0.6200
2025-05-01 01:52:57,258 - training.model_ResNet50_opt_AdamW_lr_0.01_id_81 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_81/final_results.json
2025-05-01 01:52:57,260 - training.model_ResNet50_opt_AdamW_lr_0.01_id_81 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_81/final_results.json
2025-05-01 01:52:57,260 - main - INFO - 
Summary for configuration 81:
2025-05-01 01:52:57,260 - main - INFO - Accuracy: 0.7750
2025-05-01 01:52:57,260 - main - INFO - Precision: 0.7921
2025-05-01 01:52:57,260 - main - INFO - Recall: 0.2797
2025-05-01 01:52:57,260 - main - INFO - F1 Score: 0.4134
2025-05-01 01:52:57,260 - main - INFO - IoU: 0.2606
2025-05-01 01:52:57,260 - main - INFO - mAP: 0.4664
2025-05-01 01:52:57,260 - main - INFO - AUC: 0.6200
2025-05-01 01:52:57,260 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:52:57,260 - main - INFO - 
==================================================
2025-05-01 01:52:57,260 - main - INFO - Running configuration 82/756:
2025-05-01 01:52:57,260 - main - INFO - Model: ResNet50
2025-05-01 01:52:57,260 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:52:57,260 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:52:57,260 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:52:57,260 - main - INFO - ==================================================
2025-05-01 01:52:57,260 - training.model_ResNet50_opt_AdamW_lr_0.01_id_82 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_82
2025-05-01 01:52:57,260 - training.model_ResNet50_opt_AdamW_lr_0.01_id_82 - INFO - Config: {
  "id": 82,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:52:57,426 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:52:57,426 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 82,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:52:57,426 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:52:57,504 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:52:57,505 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:52:57,507 - training.model_ResNet50_opt_AdamW_lr_0.01_id_82 - INFO - Starting model evaluation
2025-05-01 01:53:05,231 - training.model_ResNet50_opt_AdamW_lr_0.01_id_82 - INFO - Evaluation metrics:
  Accuracy:  0.7740
  Precision: 0.7843
  Recall:    0.2797
  F1 Score:  0.4124
  IoU:       0.2597
  mAP:       0.4756
  AUC:       0.6367
2025-05-01 01:53:05,233 - training.model_ResNet50_opt_AdamW_lr_0.01_id_82 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_82/final_results.json
2025-05-01 01:53:05,234 - training.model_ResNet50_opt_AdamW_lr_0.01_id_82 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_82/final_results.json
2025-05-01 01:53:05,234 - main - INFO - 
Summary for configuration 82:
2025-05-01 01:53:05,234 - main - INFO - Accuracy: 0.7740
2025-05-01 01:53:05,234 - main - INFO - Precision: 0.7843
2025-05-01 01:53:05,234 - main - INFO - Recall: 0.2797
2025-05-01 01:53:05,234 - main - INFO - F1 Score: 0.4124
2025-05-01 01:53:05,234 - main - INFO - IoU: 0.2597
2025-05-01 01:53:05,234 - main - INFO - mAP: 0.4756
2025-05-01 01:53:05,234 - main - INFO - AUC: 0.6367
2025-05-01 01:53:05,234 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:53:05,234 - main - INFO - 
==================================================
2025-05-01 01:53:05,234 - main - INFO - Running configuration 83/756:
2025-05-01 01:53:05,234 - main - INFO - Model: ResNet50
2025-05-01 01:53:05,234 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:53:05,234 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:53:05,234 - main - INFO - Loss Function: FocalLoss
2025-05-01 01:53:05,234 - main - INFO - ==================================================
2025-05-01 01:53:05,235 - training.model_ResNet50_opt_AdamW_lr_0.01_id_83 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_83
2025-05-01 01:53:05,235 - training.model_ResNet50_opt_AdamW_lr_0.01_id_83 - INFO - Config: {
  "id": 83,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:53:05,409 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:53:05,409 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 83,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 01:53:05,409 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:53:05,492 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:53:05,493 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:53:05,495 - training.model_ResNet50_opt_AdamW_lr_0.01_id_83 - INFO - Starting model evaluation
2025-05-01 01:53:12,958 - training.model_ResNet50_opt_AdamW_lr_0.01_id_83 - INFO - Evaluation metrics:
  Accuracy:  0.7800
  Precision: 0.8265
  Recall:    0.2832
  F1 Score:  0.4219
  IoU:       0.2673
  mAP:       0.4692
  AUC:       0.6283
2025-05-01 01:53:12,959 - training.model_ResNet50_opt_AdamW_lr_0.01_id_83 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_83/final_results.json
2025-05-01 01:53:12,961 - training.model_ResNet50_opt_AdamW_lr_0.01_id_83 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_83/final_results.json
2025-05-01 01:53:12,961 - main - INFO - 
Summary for configuration 83:
2025-05-01 01:53:12,961 - main - INFO - Accuracy: 0.7800
2025-05-01 01:53:12,961 - main - INFO - Precision: 0.8265
2025-05-01 01:53:12,961 - main - INFO - Recall: 0.2832
2025-05-01 01:53:12,961 - main - INFO - F1 Score: 0.4219
2025-05-01 01:53:12,961 - main - INFO - IoU: 0.2673
2025-05-01 01:53:12,961 - main - INFO - mAP: 0.4692
2025-05-01 01:53:12,961 - main - INFO - AUC: 0.6283
2025-05-01 01:53:12,961 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:53:12,961 - main - INFO - 
==================================================
2025-05-01 01:53:12,961 - main - INFO - Running configuration 84/756:
2025-05-01 01:53:12,961 - main - INFO - Model: ResNet50
2025-05-01 01:53:12,961 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 01:53:12,961 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 01:53:12,961 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 01:53:12,961 - main - INFO - ==================================================
2025-05-01 01:53:12,961 - training.model_ResNet50_opt_AdamW_lr_0.01_id_84 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01_id_84
2025-05-01 01:53:12,961 - training.model_ResNet50_opt_AdamW_lr_0.01_id_84 - INFO - Config: {
  "id": 84,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:53:13,125 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.01
2025-05-01 01:53:13,125 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 84,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 01:53:13,125 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 01:53:13,204 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6286
2025-05-01 01:53:13,205 - training.model_ResNet50_opt_AdamW_lr_0.01 - INFO - Training completed after 960.53 seconds
2025-05-01 01:53:13,207 - training.model_ResNet50_opt_AdamW_lr_0.01_id_84 - INFO - Starting model evaluation
2025-05-01 01:53:20,705 - training.model_ResNet50_opt_AdamW_lr_0.01_id_84 - INFO - Evaluation metrics:
  Accuracy:  0.7770
  Precision: 0.8081
  Recall:    0.2797
  F1 Score:  0.4156
  IoU:       0.2623
  mAP:       0.4701
  AUC:       0.6264
2025-05-01 01:53:20,706 - training.model_ResNet50_opt_AdamW_lr_0.01_id_84 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_84/final_results.json
2025-05-01 01:53:20,708 - training.model_ResNet50_opt_AdamW_lr_0.01_id_84 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.01_id_84/final_results.json
2025-05-01 01:53:20,708 - main - INFO - 
Summary for configuration 84:
2025-05-01 01:53:20,708 - main - INFO - Accuracy: 0.7770
2025-05-01 01:53:20,708 - main - INFO - Precision: 0.8081
2025-05-01 01:53:20,708 - main - INFO - Recall: 0.2797
2025-05-01 01:53:20,708 - main - INFO - F1 Score: 0.4156
2025-05-01 01:53:20,708 - main - INFO - IoU: 0.2623
2025-05-01 01:53:20,708 - main - INFO - mAP: 0.4701
2025-05-01 01:53:20,708 - main - INFO - AUC: 0.6264
2025-05-01 01:53:20,708 - main - INFO - Training time: 960.53 seconds
2025-05-01 01:53:20,708 - main - INFO - 
==================================================
2025-05-01 01:53:20,708 - main - INFO - Running configuration 85/756:
2025-05-01 01:53:20,708 - main - INFO - Model: ResNet50
2025-05-01 01:53:20,708 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 01:53:20,708 - main - INFO - Scheduler: StepLR
2025-05-01 01:53:20,708 - main - INFO - Loss Function: CrossEntropy
2025-05-01 01:53:20,708 - main - INFO - ==================================================
2025-05-01 01:53:20,708 - training.model_ResNet50_opt_AdamW_lr_0.001_id_85 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_85
2025-05-01 01:53:20,708 - training.model_ResNet50_opt_AdamW_lr_0.001_id_85 - INFO - Config: {
  "id": 85,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:53:20,871 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 01:53:20,871 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 85,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 01:53:20,871 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 01:53:20,871 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 01:53:41,628 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 01:54:08,664 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.6664
2025-05-01 01:54:08,716 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 1 completed in 47.84s - Train Loss: 0.5409, Train Acc: 0.7604, Val Loss: 0.6380, Val Acc: 0.6664
2025-05-01 01:54:08,943 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 01:54:08,944 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 01:54:29,690 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 01:54:56,506 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.6664 to 0.7822
2025-05-01 01:54:56,586 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 2 completed in 47.64s - Train Loss: 0.5702, Train Acc: 0.7244, Val Loss: 0.5234, Val Acc: 0.7822
2025-05-01 01:54:56,797 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 01:54:56,797 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 01:55:17,364 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 01:55:44,264 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.7822 to 0.8293
2025-05-01 01:55:44,346 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 3 completed in 47.55s - Train Loss: 0.5246, Train Acc: 0.7769, Val Loss: 0.4924, Val Acc: 0.8293
2025-05-01 01:55:44,558 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 01:55:44,558 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 01:56:04,930 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 01:56:32,108 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation loss improved from inf to 0.5297
2025-05-01 01:56:32,209 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 4 completed in 47.65s - Train Loss: 0.5139, Train Acc: 0.7930, Val Loss: 0.5297, Val Acc: 0.7702
2025-05-01 01:56:32,420 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 01:56:32,421 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 01:56:52,991 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 01:57:20,159 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.5297 to 0.4665
2025-05-01 01:57:20,263 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 5 completed in 47.84s - Train Loss: 0.4981, Train Acc: 0.8067, Val Loss: 0.4665, Val Acc: 0.8259
2025-05-01 01:57:20,475 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 01:57:20,476 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 01:57:41,323 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 01:58:08,342 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 6 completed in 47.87s - Train Loss: 0.4707, Train Acc: 0.8361, Val Loss: 0.4834, Val Acc: 0.8276
2025-05-01 01:58:08,566 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 01:58:08,566 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 01:58:29,631 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 01:58:56,518 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 7 completed in 47.95s - Train Loss: 0.4462, Train Acc: 0.8599, Val Loss: 0.4859, Val Acc: 0.8208
2025-05-01 01:58:56,737 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 01:58:56,737 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 01:59:18,089 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 01:59:44,838 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8293 to 0.8559
2025-05-01 01:59:44,915 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 8 completed in 48.18s - Train Loss: 0.4351, Train Acc: 0.8743, Val Loss: 0.4540, Val Acc: 0.8559
2025-05-01 01:59:45,140 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 01:59:45,140 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 02:00:06,068 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 02:00:32,644 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8559 to 0.8671
2025-05-01 02:00:32,721 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 9 completed in 47.58s - Train Loss: 0.4156, Train Acc: 0.8919, Val Loss: 0.4367, Val Acc: 0.8671
2025-05-01 02:00:32,931 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 02:00:32,931 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 02:00:54,299 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 02:01:20,873 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.4665 to 0.4582
2025-05-01 02:01:20,961 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 10 completed in 48.03s - Train Loss: 0.4220, Train Acc: 0.8897, Val Loss: 0.4582, Val Acc: 0.8473
2025-05-01 02:01:21,172 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 02:01:21,173 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 02:01:42,174 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 02:02:09,595 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8671 to 0.9185
2025-05-01 02:02:09,670 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 11 completed in 48.50s - Train Loss: 0.3820, Train Acc: 0.9296, Val Loss: 0.3924, Val Acc: 0.9185
2025-05-01 02:02:09,878 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 02:02:09,878 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 02:02:30,790 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 02:02:58,053 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9185 to 0.9383
2025-05-01 02:02:58,127 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 12 completed in 48.25s - Train Loss: 0.3600, Train Acc: 0.9537, Val Loss: 0.3746, Val Acc: 0.9383
2025-05-01 02:02:58,335 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 02:02:58,335 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 02:03:18,854 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 02:03:45,810 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.4582 to 0.3748
2025-05-01 02:03:45,888 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 13 completed in 47.55s - Train Loss: 0.3563, Train Acc: 0.9590, Val Loss: 0.3748, Val Acc: 0.9383
2025-05-01 02:03:46,108 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 02:03:46,109 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 02:04:07,417 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 02:04:34,113 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9383 to 0.9425
2025-05-01 02:04:34,201 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 14 completed in 48.09s - Train Loss: 0.3505, Train Acc: 0.9648, Val Loss: 0.3717, Val Acc: 0.9425
2025-05-01 02:04:34,416 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 02:04:34,416 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 02:04:55,090 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 02:05:22,033 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9425 to 0.9520
2025-05-01 02:05:22,109 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 15 completed in 47.69s - Train Loss: 0.3450, Train Acc: 0.9680, Val Loss: 0.3612, Val Acc: 0.9520
2025-05-01 02:05:22,330 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 02:05:22,331 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 02:05:43,014 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 02:06:09,962 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3748 to 0.3671
2025-05-01 02:06:10,043 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 16 completed in 47.71s - Train Loss: 0.3436, Train Acc: 0.9702, Val Loss: 0.3671, Val Acc: 0.9451
2025-05-01 02:06:10,256 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 02:06:10,256 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 02:06:31,263 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 02:06:58,216 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9520 to 0.9580
2025-05-01 02:06:58,295 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 17 completed in 48.04s - Train Loss: 0.3391, Train Acc: 0.9751, Val Loss: 0.3572, Val Acc: 0.9580
2025-05-01 02:06:58,506 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 02:06:58,506 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 02:07:19,369 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 02:07:46,318 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9580 to 0.9588
2025-05-01 02:07:46,410 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 18 completed in 47.90s - Train Loss: 0.3371, Train Acc: 0.9760, Val Loss: 0.3569, Val Acc: 0.9588
2025-05-01 02:07:46,622 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 02:07:46,622 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 02:08:07,687 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 02:08:34,369 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3671 to 0.3533
2025-05-01 02:08:34,446 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 19 completed in 47.82s - Train Loss: 0.3350, Train Acc: 0.9794, Val Loss: 0.3533, Val Acc: 0.9588
2025-05-01 02:08:34,657 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 02:08:34,658 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 02:08:55,901 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 02:09:22,679 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Epoch 20 completed in 48.02s - Train Loss: 0.3306, Train Acc: 0.9850, Val Loss: 0.3557, Val Acc: 0.9545
2025-05-01 02:09:22,906 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 02:09:22,906 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:09:22,909 - training.model_ResNet50_opt_AdamW_lr_0.001_id_85 - INFO - Starting model evaluation
2025-05-01 02:09:30,350 - training.model_ResNet50_opt_AdamW_lr_0.001_id_85 - INFO - Evaluation metrics:
  Accuracy:  0.9376
  Precision: 0.8704
  Recall:    0.9161
  F1 Score:  0.8927
  IoU:       0.8062
  mAP:       0.9381
  AUC:       0.9786
2025-05-01 02:09:30,352 - training.model_ResNet50_opt_AdamW_lr_0.001_id_85 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_85/final_results.json
2025-05-01 02:09:30,354 - training.model_ResNet50_opt_AdamW_lr_0.001_id_85 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_85/final_results.json
2025-05-01 02:09:30,354 - main - INFO - 
Summary for configuration 85:
2025-05-01 02:09:30,354 - main - INFO - Accuracy: 0.9376
2025-05-01 02:09:30,354 - main - INFO - Precision: 0.8704
2025-05-01 02:09:30,354 - main - INFO - Recall: 0.9161
2025-05-01 02:09:30,354 - main - INFO - F1 Score: 0.8927
2025-05-01 02:09:30,354 - main - INFO - IoU: 0.8062
2025-05-01 02:09:30,354 - main - INFO - mAP: 0.9381
2025-05-01 02:09:30,354 - main - INFO - AUC: 0.9786
2025-05-01 02:09:30,354 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:09:30,354 - main - INFO - 
==================================================
2025-05-01 02:09:30,354 - main - INFO - Running configuration 86/756:
2025-05-01 02:09:30,354 - main - INFO - Model: ResNet50
2025-05-01 02:09:30,354 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:09:30,354 - main - INFO - Scheduler: StepLR
2025-05-01 02:09:30,354 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:09:30,354 - main - INFO - ==================================================
2025-05-01 02:09:30,354 - training.model_ResNet50_opt_AdamW_lr_0.001_id_86 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_86
2025-05-01 02:09:30,354 - training.model_ResNet50_opt_AdamW_lr_0.001_id_86 - INFO - Config: {
  "id": 86,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:09:30,511 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:09:30,512 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 86,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:09:30,512 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:09:30,593 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:09:30,594 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:09:30,596 - training.model_ResNet50_opt_AdamW_lr_0.001_id_86 - INFO - Starting model evaluation
2025-05-01 02:09:38,489 - training.model_ResNet50_opt_AdamW_lr_0.001_id_86 - INFO - Evaluation metrics:
  Accuracy:  0.9425
  Precision: 0.8826
  Recall:    0.9196
  F1 Score:  0.9007
  IoU:       0.8193
  mAP:       0.9377
  AUC:       0.9761
2025-05-01 02:09:38,490 - training.model_ResNet50_opt_AdamW_lr_0.001_id_86 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_86/final_results.json
2025-05-01 02:09:38,492 - training.model_ResNet50_opt_AdamW_lr_0.001_id_86 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_86/final_results.json
2025-05-01 02:09:38,492 - main - INFO - 
Summary for configuration 86:
2025-05-01 02:09:38,492 - main - INFO - Accuracy: 0.9425
2025-05-01 02:09:38,492 - main - INFO - Precision: 0.8826
2025-05-01 02:09:38,492 - main - INFO - Recall: 0.9196
2025-05-01 02:09:38,492 - main - INFO - F1 Score: 0.9007
2025-05-01 02:09:38,492 - main - INFO - IoU: 0.8193
2025-05-01 02:09:38,492 - main - INFO - mAP: 0.9377
2025-05-01 02:09:38,492 - main - INFO - AUC: 0.9761
2025-05-01 02:09:38,492 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:09:38,492 - main - INFO - 
==================================================
2025-05-01 02:09:38,492 - main - INFO - Running configuration 87/756:
2025-05-01 02:09:38,492 - main - INFO - Model: ResNet50
2025-05-01 02:09:38,492 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:09:38,492 - main - INFO - Scheduler: StepLR
2025-05-01 02:09:38,492 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:09:38,492 - main - INFO - ==================================================
2025-05-01 02:09:38,492 - training.model_ResNet50_opt_AdamW_lr_0.001_id_87 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_87
2025-05-01 02:09:38,492 - training.model_ResNet50_opt_AdamW_lr_0.001_id_87 - INFO - Config: {
  "id": 87,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:09:38,650 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:09:38,650 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 87,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:09:38,650 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:09:38,731 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:09:38,732 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:09:38,734 - training.model_ResNet50_opt_AdamW_lr_0.001_id_87 - INFO - Starting model evaluation
2025-05-01 02:09:46,095 - training.model_ResNet50_opt_AdamW_lr_0.001_id_87 - INFO - Evaluation metrics:
  Accuracy:  0.9445
  Precision: 0.8859
  Recall:    0.9231
  F1 Score:  0.9041
  IoU:       0.8250
  mAP:       0.9405
  AUC:       0.9761
2025-05-01 02:09:46,097 - training.model_ResNet50_opt_AdamW_lr_0.001_id_87 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_87/final_results.json
2025-05-01 02:09:46,099 - training.model_ResNet50_opt_AdamW_lr_0.001_id_87 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_87/final_results.json
2025-05-01 02:09:46,099 - main - INFO - 
Summary for configuration 87:
2025-05-01 02:09:46,099 - main - INFO - Accuracy: 0.9445
2025-05-01 02:09:46,099 - main - INFO - Precision: 0.8859
2025-05-01 02:09:46,099 - main - INFO - Recall: 0.9231
2025-05-01 02:09:46,099 - main - INFO - F1 Score: 0.9041
2025-05-01 02:09:46,099 - main - INFO - IoU: 0.8250
2025-05-01 02:09:46,099 - main - INFO - mAP: 0.9405
2025-05-01 02:09:46,099 - main - INFO - AUC: 0.9761
2025-05-01 02:09:46,099 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:09:46,099 - main - INFO - 
==================================================
2025-05-01 02:09:46,099 - main - INFO - Running configuration 88/756:
2025-05-01 02:09:46,099 - main - INFO - Model: ResNet50
2025-05-01 02:09:46,099 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:09:46,099 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:09:46,099 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:09:46,099 - main - INFO - ==================================================
2025-05-01 02:09:46,099 - training.model_ResNet50_opt_AdamW_lr_0.001_id_88 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_88
2025-05-01 02:09:46,099 - training.model_ResNet50_opt_AdamW_lr_0.001_id_88 - INFO - Config: {
  "id": 88,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:09:46,257 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:09:46,257 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 88,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:09:46,258 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:09:46,340 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:09:46,341 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:09:46,342 - training.model_ResNet50_opt_AdamW_lr_0.001_id_88 - INFO - Starting model evaluation
2025-05-01 02:09:53,672 - training.model_ResNet50_opt_AdamW_lr_0.001_id_88 - INFO - Evaluation metrics:
  Accuracy:  0.9386
  Precision: 0.8758
  Recall:    0.9126
  F1 Score:  0.8938
  IoU:       0.8080
  mAP:       0.9451
  AUC:       0.9762
2025-05-01 02:09:53,674 - training.model_ResNet50_opt_AdamW_lr_0.001_id_88 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_88/final_results.json
2025-05-01 02:09:53,675 - training.model_ResNet50_opt_AdamW_lr_0.001_id_88 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_88/final_results.json
2025-05-01 02:09:53,675 - main - INFO - 
Summary for configuration 88:
2025-05-01 02:09:53,675 - main - INFO - Accuracy: 0.9386
2025-05-01 02:09:53,675 - main - INFO - Precision: 0.8758
2025-05-01 02:09:53,675 - main - INFO - Recall: 0.9126
2025-05-01 02:09:53,675 - main - INFO - F1 Score: 0.8938
2025-05-01 02:09:53,675 - main - INFO - IoU: 0.8080
2025-05-01 02:09:53,675 - main - INFO - mAP: 0.9451
2025-05-01 02:09:53,675 - main - INFO - AUC: 0.9762
2025-05-01 02:09:53,675 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:09:53,675 - main - INFO - 
==================================================
2025-05-01 02:09:53,675 - main - INFO - Running configuration 89/756:
2025-05-01 02:09:53,675 - main - INFO - Model: ResNet50
2025-05-01 02:09:53,675 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:09:53,675 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:09:53,675 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:09:53,675 - main - INFO - ==================================================
2025-05-01 02:09:53,676 - training.model_ResNet50_opt_AdamW_lr_0.001_id_89 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_89
2025-05-01 02:09:53,676 - training.model_ResNet50_opt_AdamW_lr_0.001_id_89 - INFO - Config: {
  "id": 89,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:09:53,843 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:09:53,843 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 89,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:09:53,843 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:09:53,925 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:09:53,926 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:09:53,928 - training.model_ResNet50_opt_AdamW_lr_0.001_id_89 - INFO - Starting model evaluation
2025-05-01 02:10:01,872 - training.model_ResNet50_opt_AdamW_lr_0.001_id_89 - INFO - Evaluation metrics:
  Accuracy:  0.9415
  Precision: 0.8847
  Recall:    0.9126
  F1 Score:  0.8985
  IoU:       0.8156
  mAP:       0.9429
  AUC:       0.9754
2025-05-01 02:10:01,874 - training.model_ResNet50_opt_AdamW_lr_0.001_id_89 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_89/final_results.json
2025-05-01 02:10:01,875 - training.model_ResNet50_opt_AdamW_lr_0.001_id_89 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_89/final_results.json
2025-05-01 02:10:01,875 - main - INFO - 
Summary for configuration 89:
2025-05-01 02:10:01,875 - main - INFO - Accuracy: 0.9415
2025-05-01 02:10:01,875 - main - INFO - Precision: 0.8847
2025-05-01 02:10:01,875 - main - INFO - Recall: 0.9126
2025-05-01 02:10:01,875 - main - INFO - F1 Score: 0.8985
2025-05-01 02:10:01,875 - main - INFO - IoU: 0.8156
2025-05-01 02:10:01,875 - main - INFO - mAP: 0.9429
2025-05-01 02:10:01,875 - main - INFO - AUC: 0.9754
2025-05-01 02:10:01,875 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:10:01,875 - main - INFO - 
==================================================
2025-05-01 02:10:01,875 - main - INFO - Running configuration 90/756:
2025-05-01 02:10:01,875 - main - INFO - Model: ResNet50
2025-05-01 02:10:01,875 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:10:01,875 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:10:01,875 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:10:01,875 - main - INFO - ==================================================
2025-05-01 02:10:01,876 - training.model_ResNet50_opt_AdamW_lr_0.001_id_90 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_90
2025-05-01 02:10:01,876 - training.model_ResNet50_opt_AdamW_lr_0.001_id_90 - INFO - Config: {
  "id": 90,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:10:02,037 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:10:02,039 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 90,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:10:02,039 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:10:02,122 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:10:02,123 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:10:02,124 - training.model_ResNet50_opt_AdamW_lr_0.001_id_90 - INFO - Starting model evaluation
2025-05-01 02:10:10,147 - training.model_ResNet50_opt_AdamW_lr_0.001_id_90 - INFO - Evaluation metrics:
  Accuracy:  0.9395
  Precision: 0.8713
  Recall:    0.9231
  F1 Score:  0.8964
  IoU:       0.8123
  mAP:       0.9463
  AUC:       0.9774
2025-05-01 02:10:10,148 - training.model_ResNet50_opt_AdamW_lr_0.001_id_90 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_90/final_results.json
2025-05-01 02:10:10,150 - training.model_ResNet50_opt_AdamW_lr_0.001_id_90 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_90/final_results.json
2025-05-01 02:10:10,150 - main - INFO - 
Summary for configuration 90:
2025-05-01 02:10:10,150 - main - INFO - Accuracy: 0.9395
2025-05-01 02:10:10,150 - main - INFO - Precision: 0.8713
2025-05-01 02:10:10,150 - main - INFO - Recall: 0.9231
2025-05-01 02:10:10,150 - main - INFO - F1 Score: 0.8964
2025-05-01 02:10:10,150 - main - INFO - IoU: 0.8123
2025-05-01 02:10:10,150 - main - INFO - mAP: 0.9463
2025-05-01 02:10:10,150 - main - INFO - AUC: 0.9774
2025-05-01 02:10:10,150 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:10:10,150 - main - INFO - 
==================================================
2025-05-01 02:10:10,150 - main - INFO - Running configuration 91/756:
2025-05-01 02:10:10,150 - main - INFO - Model: ResNet50
2025-05-01 02:10:10,150 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:10:10,150 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:10:10,150 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:10:10,150 - main - INFO - ==================================================
2025-05-01 02:10:10,150 - training.model_ResNet50_opt_AdamW_lr_0.001_id_91 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_91
2025-05-01 02:10:10,150 - training.model_ResNet50_opt_AdamW_lr_0.001_id_91 - INFO - Config: {
  "id": 91,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:10:10,309 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:10:10,309 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 91,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:10:10,309 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:10:10,388 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:10:10,389 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:10:10,391 - training.model_ResNet50_opt_AdamW_lr_0.001_id_91 - INFO - Starting model evaluation
2025-05-01 02:10:17,950 - training.model_ResNet50_opt_AdamW_lr_0.001_id_91 - INFO - Evaluation metrics:
  Accuracy:  0.9405
  Precision: 0.8767
  Recall:    0.9196
  F1 Score:  0.8976
  IoU:       0.8142
  mAP:       0.9387
  AUC:       0.9779
2025-05-01 02:10:17,964 - training.model_ResNet50_opt_AdamW_lr_0.001_id_91 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_91/final_results.json
2025-05-01 02:10:17,965 - training.model_ResNet50_opt_AdamW_lr_0.001_id_91 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_91/final_results.json
2025-05-01 02:10:17,965 - main - INFO - 
Summary for configuration 91:
2025-05-01 02:10:17,965 - main - INFO - Accuracy: 0.9405
2025-05-01 02:10:17,965 - main - INFO - Precision: 0.8767
2025-05-01 02:10:17,965 - main - INFO - Recall: 0.9196
2025-05-01 02:10:17,965 - main - INFO - F1 Score: 0.8976
2025-05-01 02:10:17,965 - main - INFO - IoU: 0.8142
2025-05-01 02:10:17,965 - main - INFO - mAP: 0.9387
2025-05-01 02:10:17,965 - main - INFO - AUC: 0.9779
2025-05-01 02:10:17,965 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:10:17,965 - main - INFO - 
==================================================
2025-05-01 02:10:17,965 - main - INFO - Running configuration 92/756:
2025-05-01 02:10:17,965 - main - INFO - Model: ResNet50
2025-05-01 02:10:17,965 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:10:17,965 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:10:17,966 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:10:17,966 - main - INFO - ==================================================
2025-05-01 02:10:17,966 - training.model_ResNet50_opt_AdamW_lr_0.001_id_92 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_92
2025-05-01 02:10:17,966 - training.model_ResNet50_opt_AdamW_lr_0.001_id_92 - INFO - Config: {
  "id": 92,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:10:18,131 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:10:18,132 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 92,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:10:18,132 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:10:18,211 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:10:18,212 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:10:18,214 - training.model_ResNet50_opt_AdamW_lr_0.001_id_92 - INFO - Starting model evaluation
2025-05-01 02:10:26,007 - training.model_ResNet50_opt_AdamW_lr_0.001_id_92 - INFO - Evaluation metrics:
  Accuracy:  0.9376
  Precision: 0.8704
  Recall:    0.9161
  F1 Score:  0.8927
  IoU:       0.8062
  mAP:       0.9451
  AUC:       0.9757
2025-05-01 02:10:26,009 - training.model_ResNet50_opt_AdamW_lr_0.001_id_92 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_92/final_results.json
2025-05-01 02:10:26,010 - training.model_ResNet50_opt_AdamW_lr_0.001_id_92 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_92/final_results.json
2025-05-01 02:10:26,011 - main - INFO - 
Summary for configuration 92:
2025-05-01 02:10:26,011 - main - INFO - Accuracy: 0.9376
2025-05-01 02:10:26,011 - main - INFO - Precision: 0.8704
2025-05-01 02:10:26,011 - main - INFO - Recall: 0.9161
2025-05-01 02:10:26,011 - main - INFO - F1 Score: 0.8927
2025-05-01 02:10:26,011 - main - INFO - IoU: 0.8062
2025-05-01 02:10:26,011 - main - INFO - mAP: 0.9451
2025-05-01 02:10:26,011 - main - INFO - AUC: 0.9757
2025-05-01 02:10:26,011 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:10:26,011 - main - INFO - 
==================================================
2025-05-01 02:10:26,011 - main - INFO - Running configuration 93/756:
2025-05-01 02:10:26,011 - main - INFO - Model: ResNet50
2025-05-01 02:10:26,011 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:10:26,011 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:10:26,011 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:10:26,011 - main - INFO - ==================================================
2025-05-01 02:10:26,011 - training.model_ResNet50_opt_AdamW_lr_0.001_id_93 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_93
2025-05-01 02:10:26,011 - training.model_ResNet50_opt_AdamW_lr_0.001_id_93 - INFO - Config: {
  "id": 93,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:10:26,173 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:10:26,173 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 93,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:10:26,174 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:10:26,253 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:10:26,253 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:10:26,255 - training.model_ResNet50_opt_AdamW_lr_0.001_id_93 - INFO - Starting model evaluation
2025-05-01 02:10:33,991 - training.model_ResNet50_opt_AdamW_lr_0.001_id_93 - INFO - Evaluation metrics:
  Accuracy:  0.9425
  Precision: 0.8826
  Recall:    0.9196
  F1 Score:  0.9007
  IoU:       0.8193
  mAP:       0.9393
  AUC:       0.9741
2025-05-01 02:10:33,992 - training.model_ResNet50_opt_AdamW_lr_0.001_id_93 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_93/final_results.json
2025-05-01 02:10:33,994 - training.model_ResNet50_opt_AdamW_lr_0.001_id_93 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_93/final_results.json
2025-05-01 02:10:33,994 - main - INFO - 
Summary for configuration 93:
2025-05-01 02:10:33,994 - main - INFO - Accuracy: 0.9425
2025-05-01 02:10:33,994 - main - INFO - Precision: 0.8826
2025-05-01 02:10:33,994 - main - INFO - Recall: 0.9196
2025-05-01 02:10:33,994 - main - INFO - F1 Score: 0.9007
2025-05-01 02:10:33,994 - main - INFO - IoU: 0.8193
2025-05-01 02:10:33,994 - main - INFO - mAP: 0.9393
2025-05-01 02:10:33,994 - main - INFO - AUC: 0.9741
2025-05-01 02:10:33,994 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:10:33,994 - main - INFO - 
==================================================
2025-05-01 02:10:33,994 - main - INFO - Running configuration 94/756:
2025-05-01 02:10:33,994 - main - INFO - Model: ResNet50
2025-05-01 02:10:33,994 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:10:33,994 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:10:33,994 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:10:33,994 - main - INFO - ==================================================
2025-05-01 02:10:33,994 - training.model_ResNet50_opt_AdamW_lr_0.001_id_94 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_94
2025-05-01 02:10:33,995 - training.model_ResNet50_opt_AdamW_lr_0.001_id_94 - INFO - Config: {
  "id": 94,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:10:34,154 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:10:34,155 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 94,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:10:34,155 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:10:34,310 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:10:34,311 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:10:34,313 - training.model_ResNet50_opt_AdamW_lr_0.001_id_94 - INFO - Starting model evaluation
2025-05-01 02:10:41,641 - training.model_ResNet50_opt_AdamW_lr_0.001_id_94 - INFO - Evaluation metrics:
  Accuracy:  0.9405
  Precision: 0.8792
  Recall:    0.9161
  F1 Score:  0.8973
  IoU:       0.8137
  mAP:       0.9460
  AUC:       0.9744
2025-05-01 02:10:41,643 - training.model_ResNet50_opt_AdamW_lr_0.001_id_94 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_94/final_results.json
2025-05-01 02:10:41,644 - training.model_ResNet50_opt_AdamW_lr_0.001_id_94 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_94/final_results.json
2025-05-01 02:10:41,644 - main - INFO - 
Summary for configuration 94:
2025-05-01 02:10:41,644 - main - INFO - Accuracy: 0.9405
2025-05-01 02:10:41,644 - main - INFO - Precision: 0.8792
2025-05-01 02:10:41,644 - main - INFO - Recall: 0.9161
2025-05-01 02:10:41,644 - main - INFO - F1 Score: 0.8973
2025-05-01 02:10:41,644 - main - INFO - IoU: 0.8137
2025-05-01 02:10:41,644 - main - INFO - mAP: 0.9460
2025-05-01 02:10:41,644 - main - INFO - AUC: 0.9744
2025-05-01 02:10:41,644 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:10:41,644 - main - INFO - 
==================================================
2025-05-01 02:10:41,644 - main - INFO - Running configuration 95/756:
2025-05-01 02:10:41,644 - main - INFO - Model: ResNet50
2025-05-01 02:10:41,644 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:10:41,644 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:10:41,644 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:10:41,644 - main - INFO - ==================================================
2025-05-01 02:10:41,645 - training.model_ResNet50_opt_AdamW_lr_0.001_id_95 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_95
2025-05-01 02:10:41,645 - training.model_ResNet50_opt_AdamW_lr_0.001_id_95 - INFO - Config: {
  "id": 95,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:10:41,819 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:10:41,819 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 95,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:10:41,819 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:10:41,907 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:10:41,908 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:10:41,910 - training.model_ResNet50_opt_AdamW_lr_0.001_id_95 - INFO - Starting model evaluation
2025-05-01 02:10:49,307 - training.model_ResNet50_opt_AdamW_lr_0.001_id_95 - INFO - Evaluation metrics:
  Accuracy:  0.9366
  Precision: 0.8627
  Recall:    0.9231
  F1 Score:  0.8919
  IoU:       0.8049
  mAP:       0.9470
  AUC:       0.9735
2025-05-01 02:10:49,309 - training.model_ResNet50_opt_AdamW_lr_0.001_id_95 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_95/final_results.json
2025-05-01 02:10:49,311 - training.model_ResNet50_opt_AdamW_lr_0.001_id_95 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_95/final_results.json
2025-05-01 02:10:49,311 - main - INFO - 
Summary for configuration 95:
2025-05-01 02:10:49,311 - main - INFO - Accuracy: 0.9366
2025-05-01 02:10:49,311 - main - INFO - Precision: 0.8627
2025-05-01 02:10:49,311 - main - INFO - Recall: 0.9231
2025-05-01 02:10:49,311 - main - INFO - F1 Score: 0.8919
2025-05-01 02:10:49,311 - main - INFO - IoU: 0.8049
2025-05-01 02:10:49,311 - main - INFO - mAP: 0.9470
2025-05-01 02:10:49,311 - main - INFO - AUC: 0.9735
2025-05-01 02:10:49,311 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:10:49,311 - main - INFO - 
==================================================
2025-05-01 02:10:49,311 - main - INFO - Running configuration 96/756:
2025-05-01 02:10:49,311 - main - INFO - Model: ResNet50
2025-05-01 02:10:49,311 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 02:10:49,311 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:10:49,311 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:10:49,311 - main - INFO - ==================================================
2025-05-01 02:10:49,311 - training.model_ResNet50_opt_AdamW_lr_0.001_id_96 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001_id_96
2025-05-01 02:10:49,311 - training.model_ResNet50_opt_AdamW_lr_0.001_id_96 - INFO - Config: {
  "id": 96,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:10:49,472 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.001
2025-05-01 02:10:49,472 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 96,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:10:49,472 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 02:10:49,550 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9588
2025-05-01 02:10:49,551 - training.model_ResNet50_opt_AdamW_lr_0.001 - INFO - Training completed after 961.81 seconds
2025-05-01 02:10:49,553 - training.model_ResNet50_opt_AdamW_lr_0.001_id_96 - INFO - Starting model evaluation
2025-05-01 02:10:56,981 - training.model_ResNet50_opt_AdamW_lr_0.001_id_96 - INFO - Evaluation metrics:
  Accuracy:  0.9336
  Precision: 0.8590
  Recall:    0.9161
  F1 Score:  0.8866
  IoU:       0.7964
  mAP:       0.9448
  AUC:       0.9785
2025-05-01 02:10:56,983 - training.model_ResNet50_opt_AdamW_lr_0.001_id_96 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_96/final_results.json
2025-05-01 02:10:56,985 - training.model_ResNet50_opt_AdamW_lr_0.001_id_96 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.001_id_96/final_results.json
2025-05-01 02:10:56,985 - main - INFO - 
Summary for configuration 96:
2025-05-01 02:10:56,985 - main - INFO - Accuracy: 0.9336
2025-05-01 02:10:56,985 - main - INFO - Precision: 0.8590
2025-05-01 02:10:56,985 - main - INFO - Recall: 0.9161
2025-05-01 02:10:56,985 - main - INFO - F1 Score: 0.8866
2025-05-01 02:10:56,985 - main - INFO - IoU: 0.7964
2025-05-01 02:10:56,985 - main - INFO - mAP: 0.9448
2025-05-01 02:10:56,985 - main - INFO - AUC: 0.9785
2025-05-01 02:10:56,985 - main - INFO - Training time: 961.81 seconds
2025-05-01 02:10:56,985 - main - INFO - 
==================================================
2025-05-01 02:10:56,985 - main - INFO - Running configuration 97/756:
2025-05-01 02:10:56,985 - main - INFO - Model: ResNet50
2025-05-01 02:10:56,985 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:10:56,985 - main - INFO - Scheduler: StepLR
2025-05-01 02:10:56,985 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:10:56,985 - main - INFO - ==================================================
2025-05-01 02:10:56,985 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_97 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_97
2025-05-01 02:10:56,985 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_97 - INFO - Config: {
  "id": 97,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:10:57,153 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:10:57,154 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 97,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:10:57,154 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 02:10:57,154 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 02:11:18,085 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 02:11:44,953 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9520
2025-05-01 02:11:45,005 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 1 completed in 47.85s - Train Loss: 0.4308, Train Acc: 0.8904, Val Loss: 0.3620, Val Acc: 0.9520
2025-05-01 02:11:45,256 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 02:11:45,256 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 02:12:05,902 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 02:12:33,096 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9520 to 0.9751
2025-05-01 02:12:33,184 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 2 completed in 47.93s - Train Loss: 0.3499, Train Acc: 0.9655, Val Loss: 0.3377, Val Acc: 0.9751
2025-05-01 02:12:33,395 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 02:12:33,395 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 02:12:54,360 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 02:13:21,200 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from inf to 0.3459
2025-05-01 02:13:21,290 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 3 completed in 47.89s - Train Loss: 0.3395, Train Acc: 0.9745, Val Loss: 0.3459, Val Acc: 0.9640
2025-05-01 02:13:21,503 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 02:13:21,503 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 02:13:42,159 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 02:14:09,063 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 4 completed in 47.56s - Train Loss: 0.3362, Train Acc: 0.9770, Val Loss: 0.3469, Val Acc: 0.9674
2025-05-01 02:14:09,306 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 02:14:09,306 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 02:14:29,986 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 02:14:56,972 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9751 to 0.9760
2025-05-01 02:14:57,047 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 5 completed in 47.74s - Train Loss: 0.3342, Train Acc: 0.9798, Val Loss: 0.3348, Val Acc: 0.9760
2025-05-01 02:14:57,253 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 02:14:57,253 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 02:15:18,204 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 02:15:44,997 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3459 to 0.3409
2025-05-01 02:15:45,079 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 6 completed in 47.83s - Train Loss: 0.3313, Train Acc: 0.9824, Val Loss: 0.3409, Val Acc: 0.9700
2025-05-01 02:15:45,292 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 02:15:45,292 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 02:16:06,204 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 02:16:33,274 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3409 to 0.3368
2025-05-01 02:16:33,367 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 7 completed in 48.07s - Train Loss: 0.3312, Train Acc: 0.9824, Val Loss: 0.3368, Val Acc: 0.9743
2025-05-01 02:16:33,586 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 02:16:33,586 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 02:16:54,203 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 02:17:21,366 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 8 completed in 47.78s - Train Loss: 0.3317, Train Acc: 0.9811, Val Loss: 0.3391, Val Acc: 0.9734
2025-05-01 02:17:21,609 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 02:17:21,609 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 02:17:42,893 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 02:18:09,466 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 9 completed in 47.86s - Train Loss: 0.3252, Train Acc: 0.9876, Val Loss: 0.3393, Val Acc: 0.9743
2025-05-01 02:18:09,694 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 02:18:09,694 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 02:18:30,282 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 02:18:57,392 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 10 completed in 47.70s - Train Loss: 0.3270, Train Acc: 0.9863, Val Loss: 0.3398, Val Acc: 0.9726
2025-05-01 02:18:57,605 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 02:18:57,606 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 02:19:18,672 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 02:19:45,539 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9760 to 0.9803
2025-05-01 02:19:45,628 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 11 completed in 48.02s - Train Loss: 0.3234, Train Acc: 0.9899, Val Loss: 0.3320, Val Acc: 0.9803
2025-05-01 02:19:45,850 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 02:19:45,850 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 02:20:06,642 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 02:20:33,531 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3368 to 0.3344
2025-05-01 02:20:33,609 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 12 completed in 47.76s - Train Loss: 0.3192, Train Acc: 0.9944, Val Loss: 0.3344, Val Acc: 0.9786
2025-05-01 02:20:33,823 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 02:20:33,823 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 02:20:54,656 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 02:21:21,975 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9803 to 0.9820
2025-05-01 02:21:22,069 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 13 completed in 48.25s - Train Loss: 0.3179, Train Acc: 0.9953, Val Loss: 0.3308, Val Acc: 0.9820
2025-05-01 02:21:22,279 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 02:21:22,280 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 02:21:43,230 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 02:22:10,237 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3344 to 0.3321
2025-05-01 02:22:10,324 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 14 completed in 48.04s - Train Loss: 0.3171, Train Acc: 0.9966, Val Loss: 0.3321, Val Acc: 0.9803
2025-05-01 02:22:10,552 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 02:22:10,552 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 02:22:31,461 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 02:22:58,813 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9820 to 0.9828
2025-05-01 02:22:58,904 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 15 completed in 48.35s - Train Loss: 0.3186, Train Acc: 0.9946, Val Loss: 0.3298, Val Acc: 0.9828
2025-05-01 02:22:59,114 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 02:22:59,115 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 02:23:20,048 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 02:23:47,068 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9828 to 0.9837
2025-05-01 02:23:47,144 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 16 completed in 48.03s - Train Loss: 0.3168, Train Acc: 0.9970, Val Loss: 0.3288, Val Acc: 0.9837
2025-05-01 02:23:47,355 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 02:23:47,356 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 02:24:08,555 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 02:24:35,049 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3321 to 0.3305
2025-05-01 02:24:35,127 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 17 completed in 47.77s - Train Loss: 0.3174, Train Acc: 0.9959, Val Loss: 0.3305, Val Acc: 0.9811
2025-05-01 02:24:35,350 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 02:24:35,351 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 02:24:55,769 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 02:25:23,061 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3305 to 0.3278
2025-05-01 02:25:23,140 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 18 completed in 47.79s - Train Loss: 0.3179, Train Acc: 0.9959, Val Loss: 0.3278, Val Acc: 0.9837
2025-05-01 02:25:23,357 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 02:25:23,358 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 02:25:44,053 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 02:26:10,987 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 19 completed in 47.63s - Train Loss: 0.3162, Train Acc: 0.9974, Val Loss: 0.3313, Val Acc: 0.9820
2025-05-01 02:26:11,230 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 02:26:11,231 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 02:26:32,215 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 02:26:59,200 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Epoch 20 completed in 47.97s - Train Loss: 0.3162, Train Acc: 0.9972, Val Loss: 0.3307, Val Acc: 0.9828
2025-05-01 02:26:59,440 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 02:26:59,440 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:26:59,442 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_97 - INFO - Starting model evaluation
2025-05-01 02:27:07,290 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_97 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9450
  Recall:    0.9615
  F1 Score:  0.9532
  IoU:       0.9106
  mAP:       0.9828
  AUC:       0.9903
2025-05-01 02:27:07,292 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_97 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_97/final_results.json
2025-05-01 02:27:07,293 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_97 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_97/final_results.json
2025-05-01 02:27:07,293 - main - INFO - 
Summary for configuration 97:
2025-05-01 02:27:07,293 - main - INFO - Accuracy: 0.9732
2025-05-01 02:27:07,293 - main - INFO - Precision: 0.9450
2025-05-01 02:27:07,293 - main - INFO - Recall: 0.9615
2025-05-01 02:27:07,293 - main - INFO - F1 Score: 0.9532
2025-05-01 02:27:07,293 - main - INFO - IoU: 0.9106
2025-05-01 02:27:07,293 - main - INFO - mAP: 0.9828
2025-05-01 02:27:07,293 - main - INFO - AUC: 0.9903
2025-05-01 02:27:07,293 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:27:07,293 - main - INFO - 
==================================================
2025-05-01 02:27:07,293 - main - INFO - Running configuration 98/756:
2025-05-01 02:27:07,293 - main - INFO - Model: ResNet50
2025-05-01 02:27:07,293 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:27:07,293 - main - INFO - Scheduler: StepLR
2025-05-01 02:27:07,293 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:27:07,293 - main - INFO - ==================================================
2025-05-01 02:27:07,294 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_98 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_98
2025-05-01 02:27:07,294 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_98 - INFO - Config: {
  "id": 98,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:27:07,459 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:27:07,459 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 98,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:27:07,459 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:27:07,538 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:27:07,539 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:27:07,541 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_98 - INFO - Starting model evaluation
2025-05-01 02:27:15,006 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_98 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9477
  Recall:    0.9510
  F1 Score:  0.9494
  IoU:       0.9037
  mAP:       0.9770
  AUC:       0.9874
2025-05-01 02:27:15,008 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_98 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_98/final_results.json
2025-05-01 02:27:15,009 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_98 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_98/final_results.json
2025-05-01 02:27:15,009 - main - INFO - 
Summary for configuration 98:
2025-05-01 02:27:15,009 - main - INFO - Accuracy: 0.9713
2025-05-01 02:27:15,009 - main - INFO - Precision: 0.9477
2025-05-01 02:27:15,009 - main - INFO - Recall: 0.9510
2025-05-01 02:27:15,009 - main - INFO - F1 Score: 0.9494
2025-05-01 02:27:15,009 - main - INFO - IoU: 0.9037
2025-05-01 02:27:15,009 - main - INFO - mAP: 0.9770
2025-05-01 02:27:15,009 - main - INFO - AUC: 0.9874
2025-05-01 02:27:15,009 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:27:15,009 - main - INFO - 
==================================================
2025-05-01 02:27:15,009 - main - INFO - Running configuration 99/756:
2025-05-01 02:27:15,009 - main - INFO - Model: ResNet50
2025-05-01 02:27:15,009 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:27:15,009 - main - INFO - Scheduler: StepLR
2025-05-01 02:27:15,009 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:27:15,009 - main - INFO - ==================================================
2025-05-01 02:27:15,010 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_99 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_99
2025-05-01 02:27:15,010 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_99 - INFO - Config: {
  "id": 99,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:27:15,170 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:27:15,170 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 99,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:27:15,170 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:27:15,250 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:27:15,251 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:27:15,253 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_99 - INFO - Starting model evaluation
2025-05-01 02:27:23,057 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_99 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9481
  Recall:    0.9580
  F1 Score:  0.9530
  IoU:       0.9103
  mAP:       0.9798
  AUC:       0.9926
2025-05-01 02:27:23,059 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_99 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_99/final_results.json
2025-05-01 02:27:23,061 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_99 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_99/final_results.json
2025-05-01 02:27:23,061 - main - INFO - 
Summary for configuration 99:
2025-05-01 02:27:23,061 - main - INFO - Accuracy: 0.9732
2025-05-01 02:27:23,061 - main - INFO - Precision: 0.9481
2025-05-01 02:27:23,061 - main - INFO - Recall: 0.9580
2025-05-01 02:27:23,061 - main - INFO - F1 Score: 0.9530
2025-05-01 02:27:23,061 - main - INFO - IoU: 0.9103
2025-05-01 02:27:23,061 - main - INFO - mAP: 0.9798
2025-05-01 02:27:23,061 - main - INFO - AUC: 0.9926
2025-05-01 02:27:23,061 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:27:23,061 - main - INFO - 
==================================================
2025-05-01 02:27:23,061 - main - INFO - Running configuration 100/756:
2025-05-01 02:27:23,061 - main - INFO - Model: ResNet50
2025-05-01 02:27:23,061 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:27:23,061 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:27:23,061 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:27:23,061 - main - INFO - ==================================================
2025-05-01 02:27:23,061 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_100 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_100
2025-05-01 02:27:23,061 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_100 - INFO - Config: {
  "id": 100,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:27:23,221 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:27:23,221 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 100,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:27:23,221 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:27:23,301 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:27:23,302 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:27:23,304 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_100 - INFO - Starting model evaluation
2025-05-01 02:27:30,702 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_100 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9510
  Recall:    0.9510
  F1 Score:  0.9510
  IoU:       0.9067
  mAP:       0.9796
  AUC:       0.9883
2025-05-01 02:27:30,704 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_100 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_100/final_results.json
2025-05-01 02:27:30,706 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_100 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_100/final_results.json
2025-05-01 02:27:30,706 - main - INFO - 
Summary for configuration 100:
2025-05-01 02:27:30,706 - main - INFO - Accuracy: 0.9722
2025-05-01 02:27:30,706 - main - INFO - Precision: 0.9510
2025-05-01 02:27:30,706 - main - INFO - Recall: 0.9510
2025-05-01 02:27:30,706 - main - INFO - F1 Score: 0.9510
2025-05-01 02:27:30,706 - main - INFO - IoU: 0.9067
2025-05-01 02:27:30,706 - main - INFO - mAP: 0.9796
2025-05-01 02:27:30,706 - main - INFO - AUC: 0.9883
2025-05-01 02:27:30,706 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:27:30,706 - main - INFO - 
==================================================
2025-05-01 02:27:30,706 - main - INFO - Running configuration 101/756:
2025-05-01 02:27:30,706 - main - INFO - Model: ResNet50
2025-05-01 02:27:30,706 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:27:30,706 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:27:30,706 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:27:30,706 - main - INFO - ==================================================
2025-05-01 02:27:30,706 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_101 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_101
2025-05-01 02:27:30,706 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_101 - INFO - Config: {
  "id": 101,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:27:30,869 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:27:30,870 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 101,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:27:30,870 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:27:30,949 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:27:30,950 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:27:30,952 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_101 - INFO - Starting model evaluation
2025-05-01 02:27:38,529 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_101 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9322
  Recall:    0.9615
  F1 Score:  0.9466
  IoU:       0.8987
  mAP:       0.9745
  AUC:       0.9879
2025-05-01 02:27:38,531 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_101 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_101/final_results.json
2025-05-01 02:27:38,532 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_101 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_101/final_results.json
2025-05-01 02:27:38,532 - main - INFO - 
Summary for configuration 101:
2025-05-01 02:27:38,532 - main - INFO - Accuracy: 0.9693
2025-05-01 02:27:38,532 - main - INFO - Precision: 0.9322
2025-05-01 02:27:38,532 - main - INFO - Recall: 0.9615
2025-05-01 02:27:38,532 - main - INFO - F1 Score: 0.9466
2025-05-01 02:27:38,532 - main - INFO - IoU: 0.8987
2025-05-01 02:27:38,532 - main - INFO - mAP: 0.9745
2025-05-01 02:27:38,532 - main - INFO - AUC: 0.9879
2025-05-01 02:27:38,532 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:27:38,532 - main - INFO - 
==================================================
2025-05-01 02:27:38,532 - main - INFO - Running configuration 102/756:
2025-05-01 02:27:38,532 - main - INFO - Model: ResNet50
2025-05-01 02:27:38,532 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:27:38,532 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:27:38,532 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:27:38,532 - main - INFO - ==================================================
2025-05-01 02:27:38,533 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_102 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_102
2025-05-01 02:27:38,533 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_102 - INFO - Config: {
  "id": 102,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:27:38,698 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:27:38,698 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 102,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:27:38,699 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:27:38,779 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:27:38,779 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:27:38,781 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_102 - INFO - Starting model evaluation
2025-05-01 02:27:46,509 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_102 - INFO - Evaluation metrics:
  Accuracy:  0.9742
  Precision: 0.9514
  Recall:    0.9580
  F1 Score:  0.9547
  IoU:       0.9133
  mAP:       0.9831
  AUC:       0.9889
2025-05-01 02:27:46,511 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_102 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_102/final_results.json
2025-05-01 02:27:46,512 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_102 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_102/final_results.json
2025-05-01 02:27:46,512 - main - INFO - 
Summary for configuration 102:
2025-05-01 02:27:46,512 - main - INFO - Accuracy: 0.9742
2025-05-01 02:27:46,512 - main - INFO - Precision: 0.9514
2025-05-01 02:27:46,512 - main - INFO - Recall: 0.9580
2025-05-01 02:27:46,513 - main - INFO - F1 Score: 0.9547
2025-05-01 02:27:46,513 - main - INFO - IoU: 0.9133
2025-05-01 02:27:46,513 - main - INFO - mAP: 0.9831
2025-05-01 02:27:46,513 - main - INFO - AUC: 0.9889
2025-05-01 02:27:46,513 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:27:46,513 - main - INFO - 
==================================================
2025-05-01 02:27:46,513 - main - INFO - Running configuration 103/756:
2025-05-01 02:27:46,513 - main - INFO - Model: ResNet50
2025-05-01 02:27:46,513 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:27:46,513 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:27:46,513 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:27:46,513 - main - INFO - ==================================================
2025-05-01 02:27:46,513 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_103 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_103
2025-05-01 02:27:46,513 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_103 - INFO - Config: {
  "id": 103,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:27:46,675 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:27:46,675 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 103,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:27:46,675 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:27:46,752 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:27:46,753 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:27:46,755 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_103 - INFO - Starting model evaluation
2025-05-01 02:27:55,165 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_103 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9544
  Recall:    0.9510
  F1 Score:  0.9527
  IoU:       0.9097
  mAP:       0.9713
  AUC:       0.9887
2025-05-01 02:27:55,167 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_103 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_103/final_results.json
2025-05-01 02:27:55,169 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_103 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_103/final_results.json
2025-05-01 02:27:55,169 - main - INFO - 
Summary for configuration 103:
2025-05-01 02:27:55,169 - main - INFO - Accuracy: 0.9732
2025-05-01 02:27:55,169 - main - INFO - Precision: 0.9544
2025-05-01 02:27:55,169 - main - INFO - Recall: 0.9510
2025-05-01 02:27:55,169 - main - INFO - F1 Score: 0.9527
2025-05-01 02:27:55,169 - main - INFO - IoU: 0.9097
2025-05-01 02:27:55,169 - main - INFO - mAP: 0.9713
2025-05-01 02:27:55,169 - main - INFO - AUC: 0.9887
2025-05-01 02:27:55,169 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:27:55,169 - main - INFO - 
==================================================
2025-05-01 02:27:55,169 - main - INFO - Running configuration 104/756:
2025-05-01 02:27:55,169 - main - INFO - Model: ResNet50
2025-05-01 02:27:55,169 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:27:55,169 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:27:55,169 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:27:55,169 - main - INFO - ==================================================
2025-05-01 02:27:55,169 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_104 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_104
2025-05-01 02:27:55,169 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_104 - INFO - Config: {
  "id": 104,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:27:55,327 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:27:55,327 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 104,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:27:55,327 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:27:55,405 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:27:55,405 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:27:55,407 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_104 - INFO - Starting model evaluation
2025-05-01 02:28:03,593 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_104 - INFO - Evaluation metrics:
  Accuracy:  0.9782
  Precision: 0.9583
  Recall:    0.9650
  F1 Score:  0.9617
  IoU:       0.9262
  mAP:       0.9871
  AUC:       0.9932
2025-05-01 02:28:03,595 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_104 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_104/final_results.json
2025-05-01 02:28:03,596 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_104 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_104/final_results.json
2025-05-01 02:28:03,596 - main - INFO - 
Summary for configuration 104:
2025-05-01 02:28:03,596 - main - INFO - Accuracy: 0.9782
2025-05-01 02:28:03,596 - main - INFO - Precision: 0.9583
2025-05-01 02:28:03,596 - main - INFO - Recall: 0.9650
2025-05-01 02:28:03,596 - main - INFO - F1 Score: 0.9617
2025-05-01 02:28:03,596 - main - INFO - IoU: 0.9262
2025-05-01 02:28:03,596 - main - INFO - mAP: 0.9871
2025-05-01 02:28:03,596 - main - INFO - AUC: 0.9932
2025-05-01 02:28:03,596 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:28:03,596 - main - INFO - 
==================================================
2025-05-01 02:28:03,596 - main - INFO - Running configuration 105/756:
2025-05-01 02:28:03,596 - main - INFO - Model: ResNet50
2025-05-01 02:28:03,596 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:28:03,596 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:28:03,596 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:28:03,596 - main - INFO - ==================================================
2025-05-01 02:28:03,597 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_105 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_105
2025-05-01 02:28:03,597 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_105 - INFO - Config: {
  "id": 105,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:28:03,763 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:28:03,763 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 105,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:28:03,763 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:28:03,841 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:28:03,842 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:28:03,844 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_105 - INFO - Starting model evaluation
2025-05-01 02:28:11,715 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_105 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9446
  Recall:    0.9545
  F1 Score:  0.9496
  IoU:       0.9040
  mAP:       0.9744
  AUC:       0.9870
2025-05-01 02:28:11,717 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_105 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_105/final_results.json
2025-05-01 02:28:11,719 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_105 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_105/final_results.json
2025-05-01 02:28:11,719 - main - INFO - 
Summary for configuration 105:
2025-05-01 02:28:11,719 - main - INFO - Accuracy: 0.9713
2025-05-01 02:28:11,719 - main - INFO - Precision: 0.9446
2025-05-01 02:28:11,719 - main - INFO - Recall: 0.9545
2025-05-01 02:28:11,719 - main - INFO - F1 Score: 0.9496
2025-05-01 02:28:11,719 - main - INFO - IoU: 0.9040
2025-05-01 02:28:11,719 - main - INFO - mAP: 0.9744
2025-05-01 02:28:11,719 - main - INFO - AUC: 0.9870
2025-05-01 02:28:11,719 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:28:11,719 - main - INFO - 
==================================================
2025-05-01 02:28:11,719 - main - INFO - Running configuration 106/756:
2025-05-01 02:28:11,719 - main - INFO - Model: ResNet50
2025-05-01 02:28:11,719 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:28:11,719 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:28:11,719 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:28:11,719 - main - INFO - ==================================================
2025-05-01 02:28:11,719 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_106 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_106
2025-05-01 02:28:11,719 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_106 - INFO - Config: {
  "id": 106,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:28:11,893 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:28:11,894 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 106,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:28:11,894 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:28:11,972 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:28:11,973 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:28:11,975 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_106 - INFO - Starting model evaluation
2025-05-01 02:28:21,052 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_106 - INFO - Evaluation metrics:
  Accuracy:  0.9742
  Precision: 0.9514
  Recall:    0.9580
  F1 Score:  0.9547
  IoU:       0.9133
  mAP:       0.9719
  AUC:       0.9890
2025-05-01 02:28:21,054 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_106 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_106/final_results.json
2025-05-01 02:28:21,055 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_106 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_106/final_results.json
2025-05-01 02:28:21,056 - main - INFO - 
Summary for configuration 106:
2025-05-01 02:28:21,056 - main - INFO - Accuracy: 0.9742
2025-05-01 02:28:21,056 - main - INFO - Precision: 0.9514
2025-05-01 02:28:21,056 - main - INFO - Recall: 0.9580
2025-05-01 02:28:21,056 - main - INFO - F1 Score: 0.9547
2025-05-01 02:28:21,056 - main - INFO - IoU: 0.9133
2025-05-01 02:28:21,056 - main - INFO - mAP: 0.9719
2025-05-01 02:28:21,056 - main - INFO - AUC: 0.9890
2025-05-01 02:28:21,056 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:28:21,056 - main - INFO - 
==================================================
2025-05-01 02:28:21,056 - main - INFO - Running configuration 107/756:
2025-05-01 02:28:21,056 - main - INFO - Model: ResNet50
2025-05-01 02:28:21,056 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:28:21,056 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:28:21,056 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:28:21,056 - main - INFO - ==================================================
2025-05-01 02:28:21,056 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_107 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_107
2025-05-01 02:28:21,056 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_107 - INFO - Config: {
  "id": 107,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:28:21,220 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:28:21,220 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 107,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:28:21,220 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:28:21,297 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:28:21,298 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:28:21,300 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_107 - INFO - Starting model evaluation
2025-05-01 02:28:29,946 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_107 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9443
  Recall:    0.9476
  F1 Score:  0.9459
  IoU:       0.8974
  mAP:       0.9681
  AUC:       0.9853
2025-05-01 02:28:29,948 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_107 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_107/final_results.json
2025-05-01 02:28:29,950 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_107 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_107/final_results.json
2025-05-01 02:28:29,950 - main - INFO - 
Summary for configuration 107:
2025-05-01 02:28:29,950 - main - INFO - Accuracy: 0.9693
2025-05-01 02:28:29,950 - main - INFO - Precision: 0.9443
2025-05-01 02:28:29,950 - main - INFO - Recall: 0.9476
2025-05-01 02:28:29,950 - main - INFO - F1 Score: 0.9459
2025-05-01 02:28:29,950 - main - INFO - IoU: 0.8974
2025-05-01 02:28:29,950 - main - INFO - mAP: 0.9681
2025-05-01 02:28:29,950 - main - INFO - AUC: 0.9853
2025-05-01 02:28:29,950 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:28:29,950 - main - INFO - 
==================================================
2025-05-01 02:28:29,950 - main - INFO - Running configuration 108/756:
2025-05-01 02:28:29,950 - main - INFO - Model: ResNet50
2025-05-01 02:28:29,950 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 02:28:29,950 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:28:29,950 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:28:29,950 - main - INFO - ==================================================
2025-05-01 02:28:29,950 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_108 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001_id_108
2025-05-01 02:28:29,950 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_108 - INFO - Config: {
  "id": 108,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:28:30,112 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet50_opt_AdamW_lr_0.0001
2025-05-01 02:28:30,112 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 108,
  "model_name": "ResNet50",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:28:30,112 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 02:28:30,188 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 02:28:30,189 - training.model_ResNet50_opt_AdamW_lr_0.0001 - INFO - Training completed after 962.05 seconds
2025-05-01 02:28:30,191 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_108 - INFO - Starting model evaluation
2025-05-01 02:28:38,738 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_108 - INFO - Evaluation metrics:
  Accuracy:  0.9752
  Precision: 0.9485
  Recall:    0.9650
  F1 Score:  0.9567
  IoU:       0.9169
  mAP:       0.9779
  AUC:       0.9915
2025-05-01 02:28:38,739 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_108 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_108/final_results.json
2025-05-01 02:28:38,741 - training.model_ResNet50_opt_AdamW_lr_0.0001_id_108 - INFO - Final results saved to model_results_v2/model_ResNet50_opt_AdamW_lr_0.0001_id_108/final_results.json
2025-05-01 02:28:38,741 - main - INFO - 
Summary for configuration 108:
2025-05-01 02:28:38,741 - main - INFO - Accuracy: 0.9752
2025-05-01 02:28:38,741 - main - INFO - Precision: 0.9485
2025-05-01 02:28:38,741 - main - INFO - Recall: 0.9650
2025-05-01 02:28:38,741 - main - INFO - F1 Score: 0.9567
2025-05-01 02:28:38,741 - main - INFO - IoU: 0.9169
2025-05-01 02:28:38,741 - main - INFO - mAP: 0.9779
2025-05-01 02:28:38,741 - main - INFO - AUC: 0.9915
2025-05-01 02:28:38,741 - main - INFO - Training time: 962.05 seconds
2025-05-01 02:28:38,741 - main - INFO - 
==================================================
2025-05-01 02:28:38,741 - main - INFO - Running configuration 109/756:
2025-05-01 02:28:38,741 - main - INFO - Model: ResNet101
2025-05-01 02:28:38,741 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:28:38,741 - main - INFO - Scheduler: StepLR
2025-05-01 02:28:38,741 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:28:38,741 - main - INFO - ==================================================
2025-05-01 02:28:38,741 - training.model_ResNet101_opt_SGD_lr_0.01_id_109 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_109
2025-05-01 02:28:38,741 - training.model_ResNet101_opt_SGD_lr_0.01_id_109 - INFO - Config: {
  "id": 109,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:28:39,151 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:28:39,151 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 109,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:28:39,151 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 02:28:39,152 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 02:29:00,120 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 02:29:28,286 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.8756
2025-05-01 02:29:28,379 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 1 completed in 49.23s - Train Loss: 0.4554, Train Acc: 0.8533, Val Loss: 0.4318, Val Acc: 0.8756
2025-05-01 02:29:28,634 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 02:29:28,634 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 02:29:49,862 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 02:30:17,077 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.8756 to 0.9237
2025-05-01 02:30:17,220 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 2 completed in 48.59s - Train Loss: 0.4088, Train Acc: 0.9030, Val Loss: 0.3875, Val Acc: 0.9237
2025-05-01 02:30:17,474 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 02:30:17,474 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 02:30:38,681 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 02:31:06,487 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9237 to 0.9297
2025-05-01 02:31:06,629 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 3 completed in 49.15s - Train Loss: 0.3780, Train Acc: 0.9341, Val Loss: 0.3816, Val Acc: 0.9297
2025-05-01 02:31:06,891 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 02:31:06,891 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 02:31:27,947 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 02:31:55,727 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9297 to 0.9408
2025-05-01 02:31:55,868 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 4 completed in 48.98s - Train Loss: 0.3717, Train Acc: 0.9397, Val Loss: 0.3684, Val Acc: 0.9408
2025-05-01 02:31:56,130 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 02:31:56,131 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 02:32:17,190 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 02:32:45,141 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9408 to 0.9434
2025-05-01 02:32:45,290 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 5 completed in 49.16s - Train Loss: 0.3632, Train Acc: 0.9494, Val Loss: 0.3634, Val Acc: 0.9434
2025-05-01 02:32:45,569 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 02:32:45,570 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 02:33:07,271 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 02:33:34,968 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9434 to 0.9683
2025-05-01 02:33:35,111 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 6 completed in 49.54s - Train Loss: 0.3534, Train Acc: 0.9586, Val Loss: 0.3446, Val Acc: 0.9683
2025-05-01 02:33:35,371 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 02:33:35,372 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 02:33:57,197 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 02:34:24,400 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from inf to 0.3527
2025-05-01 02:34:24,557 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 7 completed in 49.18s - Train Loss: 0.3470, Train Acc: 0.9648, Val Loss: 0.3527, Val Acc: 0.9588
2025-05-01 02:34:24,824 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 02:34:24,824 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 02:34:45,894 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 02:35:13,442 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3527 to 0.3469
2025-05-01 02:35:13,588 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 8 completed in 48.76s - Train Loss: 0.3480, Train Acc: 0.9642, Val Loss: 0.3469, Val Acc: 0.9666
2025-05-01 02:35:13,847 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 02:35:13,848 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 02:35:34,845 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 02:36:02,684 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3469 to 0.3454
2025-05-01 02:36:02,821 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 9 completed in 48.97s - Train Loss: 0.3442, Train Acc: 0.9685, Val Loss: 0.3454, Val Acc: 0.9666
2025-05-01 02:36:03,077 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 02:36:03,078 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 02:36:24,090 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 02:36:52,205 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 10 completed in 49.13s - Train Loss: 0.3357, Train Acc: 0.9766, Val Loss: 0.3487, Val Acc: 0.9614
2025-05-01 02:36:52,486 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 02:36:52,487 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 02:37:13,611 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 02:37:41,337 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3454 to 0.3422
2025-05-01 02:37:41,483 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 11 completed in 49.00s - Train Loss: 0.3296, Train Acc: 0.9837, Val Loss: 0.3422, Val Acc: 0.9674
2025-05-01 02:37:41,748 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 02:37:41,749 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 02:38:02,792 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 02:38:30,665 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9683 to 0.9760
2025-05-01 02:38:30,832 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 12 completed in 49.08s - Train Loss: 0.3253, Train Acc: 0.9882, Val Loss: 0.3360, Val Acc: 0.9760
2025-05-01 02:38:31,093 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 02:38:31,094 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 02:38:52,073 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 02:39:19,815 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9760 to 0.9803
2025-05-01 02:39:19,958 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 13 completed in 48.86s - Train Loss: 0.3234, Train Acc: 0.9897, Val Loss: 0.3319, Val Acc: 0.9803
2025-05-01 02:39:20,218 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 02:39:20,219 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 02:39:41,608 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 02:40:09,191 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3422 to 0.3365
2025-05-01 02:40:09,336 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 14 completed in 49.12s - Train Loss: 0.3241, Train Acc: 0.9910, Val Loss: 0.3365, Val Acc: 0.9734
2025-05-01 02:40:09,599 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 02:40:09,599 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 02:40:30,949 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 02:40:58,381 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3365 to 0.3359
2025-05-01 02:40:58,535 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 15 completed in 48.94s - Train Loss: 0.3220, Train Acc: 0.9914, Val Loss: 0.3359, Val Acc: 0.9760
2025-05-01 02:40:58,797 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 02:40:58,797 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 02:41:20,048 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 02:41:47,615 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3359 to 0.3333
2025-05-01 02:41:47,751 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 16 completed in 48.95s - Train Loss: 0.3205, Train Acc: 0.9929, Val Loss: 0.3333, Val Acc: 0.9777
2025-05-01 02:41:48,006 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 02:41:48,006 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 02:42:09,449 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 02:42:36,523 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3333 to 0.3322
2025-05-01 02:42:36,665 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 17 completed in 48.66s - Train Loss: 0.3204, Train Acc: 0.9934, Val Loss: 0.3322, Val Acc: 0.9803
2025-05-01 02:42:36,926 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 02:42:36,927 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 02:42:58,460 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 02:43:25,813 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9803 to 0.9820
2025-05-01 02:43:25,982 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 18 completed in 49.05s - Train Loss: 0.3199, Train Acc: 0.9938, Val Loss: 0.3304, Val Acc: 0.9820
2025-05-01 02:43:26,242 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 02:43:26,242 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 02:43:47,873 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 02:44:15,511 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 19 completed in 49.27s - Train Loss: 0.3194, Train Acc: 0.9940, Val Loss: 0.3324, Val Acc: 0.9811
2025-05-01 02:44:15,808 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 02:44:15,809 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 02:44:36,965 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 02:45:04,427 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3322 to 0.3311
2025-05-01 02:45:04,571 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Epoch 20 completed in 48.76s - Train Loss: 0.3184, Train Acc: 0.9951, Val Loss: 0.3311, Val Acc: 0.9786
2025-05-01 02:45:04,835 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 02:45:04,836 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:45:04,840 - training.model_ResNet101_opt_SGD_lr_0.01_id_109 - INFO - Starting model evaluation
2025-05-01 02:45:14,182 - training.model_ResNet101_opt_SGD_lr_0.01_id_109 - INFO - Evaluation metrics:
  Accuracy:  0.9752
  Precision: 0.9454
  Recall:    0.9685
  F1 Score:  0.9568
  IoU:       0.9172
  mAP:       0.9921
  AUC:       0.9957
2025-05-01 02:45:14,184 - training.model_ResNet101_opt_SGD_lr_0.01_id_109 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_109/final_results.json
2025-05-01 02:45:14,186 - training.model_ResNet101_opt_SGD_lr_0.01_id_109 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_109/final_results.json
2025-05-01 02:45:14,186 - main - INFO - 
Summary for configuration 109:
2025-05-01 02:45:14,186 - main - INFO - Accuracy: 0.9752
2025-05-01 02:45:14,186 - main - INFO - Precision: 0.9454
2025-05-01 02:45:14,186 - main - INFO - Recall: 0.9685
2025-05-01 02:45:14,186 - main - INFO - F1 Score: 0.9568
2025-05-01 02:45:14,186 - main - INFO - IoU: 0.9172
2025-05-01 02:45:14,186 - main - INFO - mAP: 0.9921
2025-05-01 02:45:14,186 - main - INFO - AUC: 0.9957
2025-05-01 02:45:14,186 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:45:14,186 - main - INFO - 
==================================================
2025-05-01 02:45:14,186 - main - INFO - Running configuration 110/756:
2025-05-01 02:45:14,186 - main - INFO - Model: ResNet101
2025-05-01 02:45:14,186 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:45:14,186 - main - INFO - Scheduler: StepLR
2025-05-01 02:45:14,186 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:45:14,186 - main - INFO - ==================================================
2025-05-01 02:45:14,186 - training.model_ResNet101_opt_SGD_lr_0.01_id_110 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_110
2025-05-01 02:45:14,186 - training.model_ResNet101_opt_SGD_lr_0.01_id_110 - INFO - Config: {
  "id": 110,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:45:14,442 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:45:14,442 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 110,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:45:14,442 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:45:14,545 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:45:14,547 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:45:14,550 - training.model_ResNet101_opt_SGD_lr_0.01_id_110 - INFO - Starting model evaluation
2025-05-01 02:45:24,267 - training.model_ResNet101_opt_SGD_lr_0.01_id_110 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9264
  Recall:    0.9685
  F1 Score:  0.9470
  IoU:       0.8994
  mAP:       0.9831
  AUC:       0.9934
2025-05-01 02:45:24,269 - training.model_ResNet101_opt_SGD_lr_0.01_id_110 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_110/final_results.json
2025-05-01 02:45:24,270 - training.model_ResNet101_opt_SGD_lr_0.01_id_110 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_110/final_results.json
2025-05-01 02:45:24,270 - main - INFO - 
Summary for configuration 110:
2025-05-01 02:45:24,270 - main - INFO - Accuracy: 0.9693
2025-05-01 02:45:24,271 - main - INFO - Precision: 0.9264
2025-05-01 02:45:24,271 - main - INFO - Recall: 0.9685
2025-05-01 02:45:24,271 - main - INFO - F1 Score: 0.9470
2025-05-01 02:45:24,271 - main - INFO - IoU: 0.8994
2025-05-01 02:45:24,271 - main - INFO - mAP: 0.9831
2025-05-01 02:45:24,271 - main - INFO - AUC: 0.9934
2025-05-01 02:45:24,271 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:45:24,271 - main - INFO - 
==================================================
2025-05-01 02:45:24,271 - main - INFO - Running configuration 111/756:
2025-05-01 02:45:24,271 - main - INFO - Model: ResNet101
2025-05-01 02:45:24,271 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:45:24,271 - main - INFO - Scheduler: StepLR
2025-05-01 02:45:24,271 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:45:24,271 - main - INFO - ==================================================
2025-05-01 02:45:24,271 - training.model_ResNet101_opt_SGD_lr_0.01_id_111 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_111
2025-05-01 02:45:24,271 - training.model_ResNet101_opt_SGD_lr_0.01_id_111 - INFO - Config: {
  "id": 111,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:45:24,531 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:45:24,531 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 111,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:45:24,531 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:45:24,633 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:45:24,635 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:45:24,638 - training.model_ResNet101_opt_SGD_lr_0.01_id_111 - INFO - Starting model evaluation
2025-05-01 02:45:34,318 - training.model_ResNet101_opt_SGD_lr_0.01_id_111 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9356
  Recall:    0.9650
  F1 Score:  0.9501
  IoU:       0.9049
  mAP:       0.9792
  AUC:       0.9927
2025-05-01 02:45:34,320 - training.model_ResNet101_opt_SGD_lr_0.01_id_111 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_111/final_results.json
2025-05-01 02:45:34,321 - training.model_ResNet101_opt_SGD_lr_0.01_id_111 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_111/final_results.json
2025-05-01 02:45:34,321 - main - INFO - 
Summary for configuration 111:
2025-05-01 02:45:34,321 - main - INFO - Accuracy: 0.9713
2025-05-01 02:45:34,321 - main - INFO - Precision: 0.9356
2025-05-01 02:45:34,321 - main - INFO - Recall: 0.9650
2025-05-01 02:45:34,321 - main - INFO - F1 Score: 0.9501
2025-05-01 02:45:34,321 - main - INFO - IoU: 0.9049
2025-05-01 02:45:34,321 - main - INFO - mAP: 0.9792
2025-05-01 02:45:34,321 - main - INFO - AUC: 0.9927
2025-05-01 02:45:34,321 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:45:34,321 - main - INFO - 
==================================================
2025-05-01 02:45:34,321 - main - INFO - Running configuration 112/756:
2025-05-01 02:45:34,321 - main - INFO - Model: ResNet101
2025-05-01 02:45:34,321 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:45:34,321 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:45:34,321 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:45:34,321 - main - INFO - ==================================================
2025-05-01 02:45:34,322 - training.model_ResNet101_opt_SGD_lr_0.01_id_112 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_112
2025-05-01 02:45:34,322 - training.model_ResNet101_opt_SGD_lr_0.01_id_112 - INFO - Config: {
  "id": 112,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:45:34,592 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:45:34,592 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 112,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:45:34,592 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:45:34,694 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:45:34,696 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:45:34,699 - training.model_ResNet101_opt_SGD_lr_0.01_id_112 - INFO - Starting model evaluation
2025-05-01 02:45:44,093 - training.model_ResNet101_opt_SGD_lr_0.01_id_112 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9418
  Recall:    0.9615
  F1 Score:  0.9516
  IoU:       0.9076
  mAP:       0.9816
  AUC:       0.9940
2025-05-01 02:45:44,094 - training.model_ResNet101_opt_SGD_lr_0.01_id_112 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_112/final_results.json
2025-05-01 02:45:44,096 - training.model_ResNet101_opt_SGD_lr_0.01_id_112 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_112/final_results.json
2025-05-01 02:45:44,096 - main - INFO - 
Summary for configuration 112:
2025-05-01 02:45:44,096 - main - INFO - Accuracy: 0.9722
2025-05-01 02:45:44,096 - main - INFO - Precision: 0.9418
2025-05-01 02:45:44,096 - main - INFO - Recall: 0.9615
2025-05-01 02:45:44,096 - main - INFO - F1 Score: 0.9516
2025-05-01 02:45:44,096 - main - INFO - IoU: 0.9076
2025-05-01 02:45:44,096 - main - INFO - mAP: 0.9816
2025-05-01 02:45:44,096 - main - INFO - AUC: 0.9940
2025-05-01 02:45:44,096 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:45:44,096 - main - INFO - 
==================================================
2025-05-01 02:45:44,096 - main - INFO - Running configuration 113/756:
2025-05-01 02:45:44,096 - main - INFO - Model: ResNet101
2025-05-01 02:45:44,096 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:45:44,096 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:45:44,096 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:45:44,096 - main - INFO - ==================================================
2025-05-01 02:45:44,097 - training.model_ResNet101_opt_SGD_lr_0.01_id_113 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_113
2025-05-01 02:45:44,097 - training.model_ResNet101_opt_SGD_lr_0.01_id_113 - INFO - Config: {
  "id": 113,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:45:44,353 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:45:44,353 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 113,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:45:44,353 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:45:44,455 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:45:44,456 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:45:44,460 - training.model_ResNet101_opt_SGD_lr_0.01_id_113 - INFO - Starting model evaluation
2025-05-01 02:45:53,726 - training.model_ResNet101_opt_SGD_lr_0.01_id_113 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9233
  Recall:    0.9685
  F1 Score:  0.9454
  IoU:       0.8964
  mAP:       0.9788
  AUC:       0.9926
2025-05-01 02:45:53,727 - training.model_ResNet101_opt_SGD_lr_0.01_id_113 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_113/final_results.json
2025-05-01 02:45:53,729 - training.model_ResNet101_opt_SGD_lr_0.01_id_113 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_113/final_results.json
2025-05-01 02:45:53,729 - main - INFO - 
Summary for configuration 113:
2025-05-01 02:45:53,729 - main - INFO - Accuracy: 0.9683
2025-05-01 02:45:53,729 - main - INFO - Precision: 0.9233
2025-05-01 02:45:53,729 - main - INFO - Recall: 0.9685
2025-05-01 02:45:53,729 - main - INFO - F1 Score: 0.9454
2025-05-01 02:45:53,729 - main - INFO - IoU: 0.8964
2025-05-01 02:45:53,729 - main - INFO - mAP: 0.9788
2025-05-01 02:45:53,729 - main - INFO - AUC: 0.9926
2025-05-01 02:45:53,729 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:45:53,729 - main - INFO - 
==================================================
2025-05-01 02:45:53,729 - main - INFO - Running configuration 114/756:
2025-05-01 02:45:53,729 - main - INFO - Model: ResNet101
2025-05-01 02:45:53,729 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:45:53,729 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 02:45:53,729 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:45:53,729 - main - INFO - ==================================================
2025-05-01 02:45:53,729 - training.model_ResNet101_opt_SGD_lr_0.01_id_114 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_114
2025-05-01 02:45:53,729 - training.model_ResNet101_opt_SGD_lr_0.01_id_114 - INFO - Config: {
  "id": 114,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:45:53,981 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:45:53,982 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 114,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:45:53,982 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:45:54,084 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:45:54,085 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:45:54,089 - training.model_ResNet101_opt_SGD_lr_0.01_id_114 - INFO - Starting model evaluation
2025-05-01 02:46:03,914 - training.model_ResNet101_opt_SGD_lr_0.01_id_114 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9324
  Recall:    0.9650
  F1 Score:  0.9485
  IoU:       0.9020
  mAP:       0.9829
  AUC:       0.9933
2025-05-01 02:46:03,916 - training.model_ResNet101_opt_SGD_lr_0.01_id_114 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_114/final_results.json
2025-05-01 02:46:03,918 - training.model_ResNet101_opt_SGD_lr_0.01_id_114 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_114/final_results.json
2025-05-01 02:46:03,918 - main - INFO - 
Summary for configuration 114:
2025-05-01 02:46:03,918 - main - INFO - Accuracy: 0.9703
2025-05-01 02:46:03,918 - main - INFO - Precision: 0.9324
2025-05-01 02:46:03,918 - main - INFO - Recall: 0.9650
2025-05-01 02:46:03,918 - main - INFO - F1 Score: 0.9485
2025-05-01 02:46:03,918 - main - INFO - IoU: 0.9020
2025-05-01 02:46:03,918 - main - INFO - mAP: 0.9829
2025-05-01 02:46:03,918 - main - INFO - AUC: 0.9933
2025-05-01 02:46:03,918 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:46:03,918 - main - INFO - 
==================================================
2025-05-01 02:46:03,918 - main - INFO - Running configuration 115/756:
2025-05-01 02:46:03,918 - main - INFO - Model: ResNet101
2025-05-01 02:46:03,918 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:46:03,918 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:46:03,918 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:46:03,918 - main - INFO - ==================================================
2025-05-01 02:46:03,918 - training.model_ResNet101_opt_SGD_lr_0.01_id_115 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_115
2025-05-01 02:46:03,918 - training.model_ResNet101_opt_SGD_lr_0.01_id_115 - INFO - Config: {
  "id": 115,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:46:04,181 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:46:04,181 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 115,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:46:04,181 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:46:04,280 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:46:04,282 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:46:04,285 - training.model_ResNet101_opt_SGD_lr_0.01_id_115 - INFO - Starting model evaluation
2025-05-01 02:46:14,058 - training.model_ResNet101_opt_SGD_lr_0.01_id_115 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9358
  Recall:    0.9685
  F1 Score:  0.9519
  IoU:       0.9082
  mAP:       0.9796
  AUC:       0.9928
2025-05-01 02:46:14,062 - training.model_ResNet101_opt_SGD_lr_0.01_id_115 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_115/final_results.json
2025-05-01 02:46:14,064 - training.model_ResNet101_opt_SGD_lr_0.01_id_115 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_115/final_results.json
2025-05-01 02:46:14,064 - main - INFO - 
Summary for configuration 115:
2025-05-01 02:46:14,064 - main - INFO - Accuracy: 0.9722
2025-05-01 02:46:14,064 - main - INFO - Precision: 0.9358
2025-05-01 02:46:14,064 - main - INFO - Recall: 0.9685
2025-05-01 02:46:14,064 - main - INFO - F1 Score: 0.9519
2025-05-01 02:46:14,064 - main - INFO - IoU: 0.9082
2025-05-01 02:46:14,064 - main - INFO - mAP: 0.9796
2025-05-01 02:46:14,064 - main - INFO - AUC: 0.9928
2025-05-01 02:46:14,064 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:46:14,064 - main - INFO - 
==================================================
2025-05-01 02:46:14,064 - main - INFO - Running configuration 116/756:
2025-05-01 02:46:14,064 - main - INFO - Model: ResNet101
2025-05-01 02:46:14,064 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:46:14,064 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:46:14,064 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:46:14,064 - main - INFO - ==================================================
2025-05-01 02:46:14,064 - training.model_ResNet101_opt_SGD_lr_0.01_id_116 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_116
2025-05-01 02:46:14,064 - training.model_ResNet101_opt_SGD_lr_0.01_id_116 - INFO - Config: {
  "id": 116,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:46:14,315 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:46:14,315 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 116,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:46:14,315 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:46:14,416 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:46:14,417 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:46:14,420 - training.model_ResNet101_opt_SGD_lr_0.01_id_116 - INFO - Starting model evaluation
2025-05-01 02:46:23,752 - training.model_ResNet101_opt_SGD_lr_0.01_id_116 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9358
  Recall:    0.9685
  F1 Score:  0.9519
  IoU:       0.9082
  mAP:       0.9798
  AUC:       0.9927
2025-05-01 02:46:23,754 - training.model_ResNet101_opt_SGD_lr_0.01_id_116 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_116/final_results.json
2025-05-01 02:46:23,756 - training.model_ResNet101_opt_SGD_lr_0.01_id_116 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_116/final_results.json
2025-05-01 02:46:23,756 - main - INFO - 
Summary for configuration 116:
2025-05-01 02:46:23,756 - main - INFO - Accuracy: 0.9722
2025-05-01 02:46:23,756 - main - INFO - Precision: 0.9358
2025-05-01 02:46:23,756 - main - INFO - Recall: 0.9685
2025-05-01 02:46:23,756 - main - INFO - F1 Score: 0.9519
2025-05-01 02:46:23,756 - main - INFO - IoU: 0.9082
2025-05-01 02:46:23,756 - main - INFO - mAP: 0.9798
2025-05-01 02:46:23,756 - main - INFO - AUC: 0.9927
2025-05-01 02:46:23,756 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:46:23,756 - main - INFO - 
==================================================
2025-05-01 02:46:23,756 - main - INFO - Running configuration 117/756:
2025-05-01 02:46:23,756 - main - INFO - Model: ResNet101
2025-05-01 02:46:23,756 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:46:23,756 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 02:46:23,756 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:46:23,756 - main - INFO - ==================================================
2025-05-01 02:46:23,756 - training.model_ResNet101_opt_SGD_lr_0.01_id_117 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_117
2025-05-01 02:46:23,756 - training.model_ResNet101_opt_SGD_lr_0.01_id_117 - INFO - Config: {
  "id": 117,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:46:24,006 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:46:24,007 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 117,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:46:24,007 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:46:24,106 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:46:24,107 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:46:24,110 - training.model_ResNet101_opt_SGD_lr_0.01_id_117 - INFO - Starting model evaluation
2025-05-01 02:46:33,445 - training.model_ResNet101_opt_SGD_lr_0.01_id_117 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9264
  Recall:    0.9685
  F1 Score:  0.9470
  IoU:       0.8994
  mAP:       0.9852
  AUC:       0.9944
2025-05-01 02:46:33,446 - training.model_ResNet101_opt_SGD_lr_0.01_id_117 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_117/final_results.json
2025-05-01 02:46:33,448 - training.model_ResNet101_opt_SGD_lr_0.01_id_117 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_117/final_results.json
2025-05-01 02:46:33,448 - main - INFO - 
Summary for configuration 117:
2025-05-01 02:46:33,448 - main - INFO - Accuracy: 0.9693
2025-05-01 02:46:33,448 - main - INFO - Precision: 0.9264
2025-05-01 02:46:33,448 - main - INFO - Recall: 0.9685
2025-05-01 02:46:33,448 - main - INFO - F1 Score: 0.9470
2025-05-01 02:46:33,448 - main - INFO - IoU: 0.8994
2025-05-01 02:46:33,448 - main - INFO - mAP: 0.9852
2025-05-01 02:46:33,448 - main - INFO - AUC: 0.9944
2025-05-01 02:46:33,448 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:46:33,448 - main - INFO - 
==================================================
2025-05-01 02:46:33,448 - main - INFO - Running configuration 118/756:
2025-05-01 02:46:33,448 - main - INFO - Model: ResNet101
2025-05-01 02:46:33,448 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:46:33,448 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:46:33,448 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:46:33,448 - main - INFO - ==================================================
2025-05-01 02:46:33,448 - training.model_ResNet101_opt_SGD_lr_0.01_id_118 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_118
2025-05-01 02:46:33,448 - training.model_ResNet101_opt_SGD_lr_0.01_id_118 - INFO - Config: {
  "id": 118,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:46:33,716 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:46:33,716 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 118,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:46:33,716 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:46:33,816 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:46:33,817 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:46:33,821 - training.model_ResNet101_opt_SGD_lr_0.01_id_118 - INFO - Starting model evaluation
2025-05-01 02:46:43,238 - training.model_ResNet101_opt_SGD_lr_0.01_id_118 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9291
  Recall:    0.9615
  F1 Score:  0.9450
  IoU:       0.8958
  mAP:       0.9786
  AUC:       0.9929
2025-05-01 02:46:43,240 - training.model_ResNet101_opt_SGD_lr_0.01_id_118 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_118/final_results.json
2025-05-01 02:46:43,243 - training.model_ResNet101_opt_SGD_lr_0.01_id_118 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_118/final_results.json
2025-05-01 02:46:43,243 - main - INFO - 
Summary for configuration 118:
2025-05-01 02:46:43,243 - main - INFO - Accuracy: 0.9683
2025-05-01 02:46:43,243 - main - INFO - Precision: 0.9291
2025-05-01 02:46:43,243 - main - INFO - Recall: 0.9615
2025-05-01 02:46:43,243 - main - INFO - F1 Score: 0.9450
2025-05-01 02:46:43,243 - main - INFO - IoU: 0.8958
2025-05-01 02:46:43,243 - main - INFO - mAP: 0.9786
2025-05-01 02:46:43,243 - main - INFO - AUC: 0.9929
2025-05-01 02:46:43,243 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:46:43,243 - main - INFO - 
==================================================
2025-05-01 02:46:43,243 - main - INFO - Running configuration 119/756:
2025-05-01 02:46:43,243 - main - INFO - Model: ResNet101
2025-05-01 02:46:43,243 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:46:43,243 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:46:43,243 - main - INFO - Loss Function: FocalLoss
2025-05-01 02:46:43,243 - main - INFO - ==================================================
2025-05-01 02:46:43,244 - training.model_ResNet101_opt_SGD_lr_0.01_id_119 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_119
2025-05-01 02:46:43,244 - training.model_ResNet101_opt_SGD_lr_0.01_id_119 - INFO - Config: {
  "id": 119,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:46:43,585 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:46:43,585 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 119,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 02:46:43,585 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:46:43,684 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:46:43,685 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:46:43,689 - training.model_ResNet101_opt_SGD_lr_0.01_id_119 - INFO - Starting model evaluation
2025-05-01 02:46:53,445 - training.model_ResNet101_opt_SGD_lr_0.01_id_119 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9228
  Recall:    0.9615
  F1 Score:  0.9418
  IoU:       0.8900
  mAP:       0.9829
  AUC:       0.9934
2025-05-01 02:46:53,446 - training.model_ResNet101_opt_SGD_lr_0.01_id_119 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_119/final_results.json
2025-05-01 02:46:53,448 - training.model_ResNet101_opt_SGD_lr_0.01_id_119 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_119/final_results.json
2025-05-01 02:46:53,448 - main - INFO - 
Summary for configuration 119:
2025-05-01 02:46:53,448 - main - INFO - Accuracy: 0.9663
2025-05-01 02:46:53,448 - main - INFO - Precision: 0.9228
2025-05-01 02:46:53,448 - main - INFO - Recall: 0.9615
2025-05-01 02:46:53,448 - main - INFO - F1 Score: 0.9418
2025-05-01 02:46:53,448 - main - INFO - IoU: 0.8900
2025-05-01 02:46:53,448 - main - INFO - mAP: 0.9829
2025-05-01 02:46:53,448 - main - INFO - AUC: 0.9934
2025-05-01 02:46:53,448 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:46:53,448 - main - INFO - 
==================================================
2025-05-01 02:46:53,448 - main - INFO - Running configuration 120/756:
2025-05-01 02:46:53,448 - main - INFO - Model: ResNet101
2025-05-01 02:46:53,448 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 02:46:53,448 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 02:46:53,448 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 02:46:53,448 - main - INFO - ==================================================
2025-05-01 02:46:53,448 - training.model_ResNet101_opt_SGD_lr_0.01_id_120 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01_id_120
2025-05-01 02:46:53,448 - training.model_ResNet101_opt_SGD_lr_0.01_id_120 - INFO - Config: {
  "id": 120,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:46:53,698 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.01
2025-05-01 02:46:53,698 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 120,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 02:46:53,698 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 02:46:53,797 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-01 02:46:53,798 - training.model_ResNet101_opt_SGD_lr_0.01 - INFO - Training completed after 985.42 seconds
2025-05-01 02:46:53,802 - training.model_ResNet101_opt_SGD_lr_0.01_id_120 - INFO - Starting model evaluation
2025-05-01 02:47:03,856 - training.model_ResNet101_opt_SGD_lr_0.01_id_120 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9293
  Recall:    0.9650
  F1 Score:  0.9468
  IoU:       0.8990
  mAP:       0.9816
  AUC:       0.9936
2025-05-01 02:47:03,858 - training.model_ResNet101_opt_SGD_lr_0.01_id_120 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_120/final_results.json
2025-05-01 02:47:03,860 - training.model_ResNet101_opt_SGD_lr_0.01_id_120 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.01_id_120/final_results.json
2025-05-01 02:47:03,860 - main - INFO - 
Summary for configuration 120:
2025-05-01 02:47:03,860 - main - INFO - Accuracy: 0.9693
2025-05-01 02:47:03,860 - main - INFO - Precision: 0.9293
2025-05-01 02:47:03,860 - main - INFO - Recall: 0.9650
2025-05-01 02:47:03,860 - main - INFO - F1 Score: 0.9468
2025-05-01 02:47:03,860 - main - INFO - IoU: 0.8990
2025-05-01 02:47:03,860 - main - INFO - mAP: 0.9816
2025-05-01 02:47:03,860 - main - INFO - AUC: 0.9936
2025-05-01 02:47:03,860 - main - INFO - Training time: 985.42 seconds
2025-05-01 02:47:03,860 - main - INFO - 
==================================================
2025-05-01 02:47:03,860 - main - INFO - Running configuration 121/756:
2025-05-01 02:47:03,860 - main - INFO - Model: ResNet101
2025-05-01 02:47:03,860 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 02:47:03,860 - main - INFO - Scheduler: StepLR
2025-05-01 02:47:03,860 - main - INFO - Loss Function: CrossEntropy
2025-05-01 02:47:03,860 - main - INFO - ==================================================
2025-05-01 02:47:03,860 - training.model_ResNet101_opt_SGD_lr_0.001_id_121 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_121
2025-05-01 02:47:03,860 - training.model_ResNet101_opt_SGD_lr_0.001_id_121 - INFO - Config: {
  "id": 121,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:47:04,134 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 02:47:04,134 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 121,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 02:47:04,134 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 02:47:04,135 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 02:47:25,011 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 02:47:52,733 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.9125
2025-05-01 02:47:52,827 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 1 completed in 48.69s - Train Loss: 0.5224, Train Acc: 0.7949, Val Loss: 0.4071, Val Acc: 0.9125
2025-05-01 02:47:53,106 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 02:47:53,107 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 02:48:14,372 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 02:48:41,822 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9125 to 0.9717
2025-05-01 02:48:41,963 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 2 completed in 48.86s - Train Loss: 0.3831, Train Acc: 0.9417, Val Loss: 0.3519, Val Acc: 0.9717
2025-05-01 02:48:42,231 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 02:48:42,232 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 02:49:03,516 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 02:49:31,314 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9717 to 0.9786
2025-05-01 02:49:31,476 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 3 completed in 49.24s - Train Loss: 0.3470, Train Acc: 0.9749, Val Loss: 0.3384, Val Acc: 0.9786
2025-05-01 02:49:31,738 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 02:49:31,739 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 02:49:52,821 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 02:50:21,172 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9786 to 0.9794
2025-05-01 02:50:21,310 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 4 completed in 49.57s - Train Loss: 0.3314, Train Acc: 0.9873, Val Loss: 0.3363, Val Acc: 0.9794
2025-05-01 02:50:21,567 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 02:50:21,568 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 02:50:43,142 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 02:51:10,673 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation loss improved from inf to 0.3327
2025-05-01 02:51:10,840 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 5 completed in 49.27s - Train Loss: 0.3256, Train Acc: 0.9903, Val Loss: 0.3327, Val Acc: 0.9777
2025-05-01 02:51:11,099 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 02:51:11,099 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 02:51:32,212 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 02:51:59,771 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9794 to 0.9820
2025-05-01 02:51:59,906 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 6 completed in 48.81s - Train Loss: 0.3237, Train Acc: 0.9916, Val Loss: 0.3316, Val Acc: 0.9820
2025-05-01 02:52:00,167 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 02:52:00,168 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 02:52:21,343 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 02:52:48,729 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9820 to 0.9871
2025-05-01 02:52:48,873 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 7 completed in 48.71s - Train Loss: 0.3226, Train Acc: 0.9944, Val Loss: 0.3273, Val Acc: 0.9871
2025-05-01 02:52:49,123 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 02:52:49,123 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 02:53:10,606 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 02:53:38,131 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3327 to 0.3282
2025-05-01 02:53:38,291 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 8 completed in 49.17s - Train Loss: 0.3205, Train Acc: 0.9944, Val Loss: 0.3282, Val Acc: 0.9854
2025-05-01 02:53:38,554 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 02:53:38,555 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 02:53:59,505 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 02:54:27,255 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3282 to 0.3280
2025-05-01 02:54:27,400 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 9 completed in 48.85s - Train Loss: 0.3190, Train Acc: 0.9955, Val Loss: 0.3280, Val Acc: 0.9854
2025-05-01 02:54:27,652 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 02:54:27,653 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 02:54:48,421 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 02:55:16,299 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 10 completed in 48.65s - Train Loss: 0.3184, Train Acc: 0.9968, Val Loss: 0.3288, Val Acc: 0.9811
2025-05-01 02:55:16,571 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 02:55:16,572 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 02:55:37,753 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 02:56:05,421 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3280 to 0.3262
2025-05-01 02:56:05,566 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 11 completed in 48.99s - Train Loss: 0.3194, Train Acc: 0.9949, Val Loss: 0.3262, Val Acc: 0.9863
2025-05-01 02:56:05,824 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 02:56:05,825 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 02:56:26,250 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 02:56:54,087 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 12 completed in 48.26s - Train Loss: 0.3172, Train Acc: 0.9976, Val Loss: 0.3271, Val Acc: 0.9863
2025-05-01 02:56:54,348 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 02:56:54,349 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 02:57:15,869 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 02:57:43,376 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 13 completed in 49.03s - Train Loss: 0.3163, Train Acc: 0.9983, Val Loss: 0.3278, Val Acc: 0.9846
2025-05-01 02:57:43,648 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 02:57:43,649 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 02:58:04,771 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 02:58:32,247 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 14 completed in 48.60s - Train Loss: 0.3165, Train Acc: 0.9979, Val Loss: 0.3277, Val Acc: 0.9846
2025-05-01 02:58:32,547 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 02:58:32,547 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 02:58:53,670 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 02:59:21,512 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 15 completed in 48.96s - Train Loss: 0.3171, Train Acc: 0.9972, Val Loss: 0.3264, Val Acc: 0.9871
2025-05-01 02:59:21,781 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 02:59:21,781 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 02:59:42,783 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 03:00:10,384 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9871 to 0.9889
2025-05-01 03:00:10,521 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 16 completed in 48.74s - Train Loss: 0.3150, Train Acc: 0.9991, Val Loss: 0.3256, Val Acc: 0.9889
2025-05-01 03:00:10,781 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 03:00:10,782 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 03:00:32,031 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 03:00:59,874 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3262 to 0.3251
2025-05-01 03:01:00,040 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 17 completed in 49.26s - Train Loss: 0.3169, Train Acc: 0.9981, Val Loss: 0.3251, Val Acc: 0.9889
2025-05-01 03:01:00,304 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 03:01:00,305 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 03:01:21,386 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 03:01:49,032 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9889 to 0.9906
2025-05-01 03:01:49,177 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 18 completed in 48.87s - Train Loss: 0.3155, Train Acc: 0.9991, Val Loss: 0.3240, Val Acc: 0.9906
2025-05-01 03:01:49,428 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 03:01:49,428 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 03:02:10,334 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 03:02:38,308 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 19 completed in 48.88s - Train Loss: 0.3159, Train Acc: 0.9987, Val Loss: 0.3270, Val Acc: 0.9863
2025-05-01 03:02:38,611 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 03:02:38,611 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 03:02:59,808 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 03:03:27,754 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3251 to 0.3245
2025-05-01 03:03:27,905 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Epoch 20 completed in 49.29s - Train Loss: 0.3159, Train Acc: 0.9983, Val Loss: 0.3245, Val Acc: 0.9880
2025-05-01 03:03:28,159 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 03:03:28,160 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:03:28,164 - training.model_ResNet101_opt_SGD_lr_0.001_id_121 - INFO - Starting model evaluation
2025-05-01 03:03:37,771 - training.model_ResNet101_opt_SGD_lr_0.001_id_121 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9317
  Recall:    0.9545
  F1 Score:  0.9430
  IoU:       0.8922
  mAP:       0.9785
  AUC:       0.9912
2025-05-01 03:03:37,772 - training.model_ResNet101_opt_SGD_lr_0.001_id_121 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_121/final_results.json
2025-05-01 03:03:37,774 - training.model_ResNet101_opt_SGD_lr_0.001_id_121 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_121/final_results.json
2025-05-01 03:03:37,774 - main - INFO - 
Summary for configuration 121:
2025-05-01 03:03:37,774 - main - INFO - Accuracy: 0.9673
2025-05-01 03:03:37,774 - main - INFO - Precision: 0.9317
2025-05-01 03:03:37,774 - main - INFO - Recall: 0.9545
2025-05-01 03:03:37,774 - main - INFO - F1 Score: 0.9430
2025-05-01 03:03:37,774 - main - INFO - IoU: 0.8922
2025-05-01 03:03:37,774 - main - INFO - mAP: 0.9785
2025-05-01 03:03:37,774 - main - INFO - AUC: 0.9912
2025-05-01 03:03:37,774 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:03:37,774 - main - INFO - 
==================================================
2025-05-01 03:03:37,774 - main - INFO - Running configuration 122/756:
2025-05-01 03:03:37,774 - main - INFO - Model: ResNet101
2025-05-01 03:03:37,774 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:03:37,774 - main - INFO - Scheduler: StepLR
2025-05-01 03:03:37,774 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:03:37,774 - main - INFO - ==================================================
2025-05-01 03:03:37,774 - training.model_ResNet101_opt_SGD_lr_0.001_id_122 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_122
2025-05-01 03:03:37,774 - training.model_ResNet101_opt_SGD_lr_0.001_id_122 - INFO - Config: {
  "id": 122,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:03:38,046 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:03:38,046 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 122,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:03:38,049 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:03:38,148 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:03:38,150 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:03:38,153 - training.model_ResNet101_opt_SGD_lr_0.001_id_122 - INFO - Starting model evaluation
2025-05-01 03:03:47,205 - training.model_ResNet101_opt_SGD_lr_0.001_id_122 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9375
  Recall:    0.9441
  F1 Score:  0.9408
  IoU:       0.8882
  mAP:       0.9873
  AUC:       0.9924
2025-05-01 03:03:47,207 - training.model_ResNet101_opt_SGD_lr_0.001_id_122 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_122/final_results.json
2025-05-01 03:03:47,208 - training.model_ResNet101_opt_SGD_lr_0.001_id_122 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_122/final_results.json
2025-05-01 03:03:47,208 - main - INFO - 
Summary for configuration 122:
2025-05-01 03:03:47,208 - main - INFO - Accuracy: 0.9663
2025-05-01 03:03:47,208 - main - INFO - Precision: 0.9375
2025-05-01 03:03:47,208 - main - INFO - Recall: 0.9441
2025-05-01 03:03:47,208 - main - INFO - F1 Score: 0.9408
2025-05-01 03:03:47,208 - main - INFO - IoU: 0.8882
2025-05-01 03:03:47,208 - main - INFO - mAP: 0.9873
2025-05-01 03:03:47,209 - main - INFO - AUC: 0.9924
2025-05-01 03:03:47,209 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:03:47,209 - main - INFO - 
==================================================
2025-05-01 03:03:47,209 - main - INFO - Running configuration 123/756:
2025-05-01 03:03:47,209 - main - INFO - Model: ResNet101
2025-05-01 03:03:47,209 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:03:47,209 - main - INFO - Scheduler: StepLR
2025-05-01 03:03:47,209 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:03:47,209 - main - INFO - ==================================================
2025-05-01 03:03:47,209 - training.model_ResNet101_opt_SGD_lr_0.001_id_123 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_123
2025-05-01 03:03:47,209 - training.model_ResNet101_opt_SGD_lr_0.001_id_123 - INFO - Config: {
  "id": 123,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:03:47,466 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:03:47,466 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 123,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:03:47,467 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:03:47,572 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:03:47,573 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:03:47,576 - training.model_ResNet101_opt_SGD_lr_0.001_id_123 - INFO - Starting model evaluation
2025-05-01 03:03:57,295 - training.model_ResNet101_opt_SGD_lr_0.001_id_123 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9381
  Recall:    0.9545
  F1 Score:  0.9463
  IoU:       0.8980
  mAP:       0.9880
  AUC:       0.9940
2025-05-01 03:03:57,296 - training.model_ResNet101_opt_SGD_lr_0.001_id_123 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_123/final_results.json
2025-05-01 03:03:57,298 - training.model_ResNet101_opt_SGD_lr_0.001_id_123 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_123/final_results.json
2025-05-01 03:03:57,298 - main - INFO - 
Summary for configuration 123:
2025-05-01 03:03:57,298 - main - INFO - Accuracy: 0.9693
2025-05-01 03:03:57,298 - main - INFO - Precision: 0.9381
2025-05-01 03:03:57,298 - main - INFO - Recall: 0.9545
2025-05-01 03:03:57,298 - main - INFO - F1 Score: 0.9463
2025-05-01 03:03:57,298 - main - INFO - IoU: 0.8980
2025-05-01 03:03:57,298 - main - INFO - mAP: 0.9880
2025-05-01 03:03:57,298 - main - INFO - AUC: 0.9940
2025-05-01 03:03:57,298 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:03:57,298 - main - INFO - 
==================================================
2025-05-01 03:03:57,298 - main - INFO - Running configuration 124/756:
2025-05-01 03:03:57,298 - main - INFO - Model: ResNet101
2025-05-01 03:03:57,298 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:03:57,298 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:03:57,298 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:03:57,298 - main - INFO - ==================================================
2025-05-01 03:03:57,298 - training.model_ResNet101_opt_SGD_lr_0.001_id_124 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_124
2025-05-01 03:03:57,298 - training.model_ResNet101_opt_SGD_lr_0.001_id_124 - INFO - Config: {
  "id": 124,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:03:57,548 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:03:57,548 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 124,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:03:57,548 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:03:57,648 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:03:57,649 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:03:57,652 - training.model_ResNet101_opt_SGD_lr_0.001_id_124 - INFO - Starting model evaluation
2025-05-01 03:04:07,337 - training.model_ResNet101_opt_SGD_lr_0.001_id_124 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9313
  Recall:    0.9476
  F1 Score:  0.9393
  IoU:       0.8856
  mAP:       0.9870
  AUC:       0.9926
2025-05-01 03:04:07,339 - training.model_ResNet101_opt_SGD_lr_0.001_id_124 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_124/final_results.json
2025-05-01 03:04:07,341 - training.model_ResNet101_opt_SGD_lr_0.001_id_124 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_124/final_results.json
2025-05-01 03:04:07,341 - main - INFO - 
Summary for configuration 124:
2025-05-01 03:04:07,341 - main - INFO - Accuracy: 0.9653
2025-05-01 03:04:07,341 - main - INFO - Precision: 0.9313
2025-05-01 03:04:07,341 - main - INFO - Recall: 0.9476
2025-05-01 03:04:07,341 - main - INFO - F1 Score: 0.9393
2025-05-01 03:04:07,341 - main - INFO - IoU: 0.8856
2025-05-01 03:04:07,341 - main - INFO - mAP: 0.9870
2025-05-01 03:04:07,341 - main - INFO - AUC: 0.9926
2025-05-01 03:04:07,341 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:04:07,341 - main - INFO - 
==================================================
2025-05-01 03:04:07,341 - main - INFO - Running configuration 125/756:
2025-05-01 03:04:07,341 - main - INFO - Model: ResNet101
2025-05-01 03:04:07,341 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:04:07,341 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:04:07,341 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:04:07,341 - main - INFO - ==================================================
2025-05-01 03:04:07,341 - training.model_ResNet101_opt_SGD_lr_0.001_id_125 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_125
2025-05-01 03:04:07,341 - training.model_ResNet101_opt_SGD_lr_0.001_id_125 - INFO - Config: {
  "id": 125,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:04:07,595 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:04:07,595 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 125,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:04:07,596 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:04:07,695 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:04:07,696 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:04:07,700 - training.model_ResNet101_opt_SGD_lr_0.001_id_125 - INFO - Starting model evaluation
2025-05-01 03:04:17,030 - training.model_ResNet101_opt_SGD_lr_0.001_id_125 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9379
  Recall:    0.9510
  F1 Score:  0.9444
  IoU:       0.8947
  mAP:       0.9793
  AUC:       0.9911
2025-05-01 03:04:17,031 - training.model_ResNet101_opt_SGD_lr_0.001_id_125 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_125/final_results.json
2025-05-01 03:04:17,033 - training.model_ResNet101_opt_SGD_lr_0.001_id_125 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_125/final_results.json
2025-05-01 03:04:17,033 - main - INFO - 
Summary for configuration 125:
2025-05-01 03:04:17,033 - main - INFO - Accuracy: 0.9683
2025-05-01 03:04:17,033 - main - INFO - Precision: 0.9379
2025-05-01 03:04:17,033 - main - INFO - Recall: 0.9510
2025-05-01 03:04:17,033 - main - INFO - F1 Score: 0.9444
2025-05-01 03:04:17,033 - main - INFO - IoU: 0.8947
2025-05-01 03:04:17,033 - main - INFO - mAP: 0.9793
2025-05-01 03:04:17,033 - main - INFO - AUC: 0.9911
2025-05-01 03:04:17,033 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:04:17,033 - main - INFO - 
==================================================
2025-05-01 03:04:17,033 - main - INFO - Running configuration 126/756:
2025-05-01 03:04:17,033 - main - INFO - Model: ResNet101
2025-05-01 03:04:17,033 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:04:17,033 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:04:17,033 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:04:17,033 - main - INFO - ==================================================
2025-05-01 03:04:17,034 - training.model_ResNet101_opt_SGD_lr_0.001_id_126 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_126
2025-05-01 03:04:17,034 - training.model_ResNet101_opt_SGD_lr_0.001_id_126 - INFO - Config: {
  "id": 126,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:04:17,306 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:04:17,306 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 126,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:04:17,306 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:04:17,406 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:04:17,407 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:04:17,411 - training.model_ResNet101_opt_SGD_lr_0.001_id_126 - INFO - Starting model evaluation
2025-05-01 03:04:27,133 - training.model_ResNet101_opt_SGD_lr_0.001_id_126 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9349
  Recall:    0.9545
  F1 Score:  0.9446
  IoU:       0.8951
  mAP:       0.9794
  AUC:       0.9916
2025-05-01 03:04:27,134 - training.model_ResNet101_opt_SGD_lr_0.001_id_126 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_126/final_results.json
2025-05-01 03:04:27,136 - training.model_ResNet101_opt_SGD_lr_0.001_id_126 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_126/final_results.json
2025-05-01 03:04:27,136 - main - INFO - 
Summary for configuration 126:
2025-05-01 03:04:27,136 - main - INFO - Accuracy: 0.9683
2025-05-01 03:04:27,136 - main - INFO - Precision: 0.9349
2025-05-01 03:04:27,136 - main - INFO - Recall: 0.9545
2025-05-01 03:04:27,136 - main - INFO - F1 Score: 0.9446
2025-05-01 03:04:27,136 - main - INFO - IoU: 0.8951
2025-05-01 03:04:27,136 - main - INFO - mAP: 0.9794
2025-05-01 03:04:27,136 - main - INFO - AUC: 0.9916
2025-05-01 03:04:27,136 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:04:27,136 - main - INFO - 
==================================================
2025-05-01 03:04:27,136 - main - INFO - Running configuration 127/756:
2025-05-01 03:04:27,136 - main - INFO - Model: ResNet101
2025-05-01 03:04:27,136 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:04:27,136 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:04:27,136 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:04:27,136 - main - INFO - ==================================================
2025-05-01 03:04:27,136 - training.model_ResNet101_opt_SGD_lr_0.001_id_127 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_127
2025-05-01 03:04:27,136 - training.model_ResNet101_opt_SGD_lr_0.001_id_127 - INFO - Config: {
  "id": 127,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:04:27,387 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:04:27,387 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 127,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:04:27,387 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:04:27,484 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:04:27,485 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:04:27,489 - training.model_ResNet101_opt_SGD_lr_0.001_id_127 - INFO - Starting model evaluation
2025-05-01 03:04:36,676 - training.model_ResNet101_opt_SGD_lr_0.001_id_127 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9320
  Recall:    0.9580
  F1 Score:  0.9448
  IoU:       0.8954
  mAP:       0.9798
  AUC:       0.9916
2025-05-01 03:04:36,678 - training.model_ResNet101_opt_SGD_lr_0.001_id_127 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_127/final_results.json
2025-05-01 03:04:36,680 - training.model_ResNet101_opt_SGD_lr_0.001_id_127 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_127/final_results.json
2025-05-01 03:04:36,680 - main - INFO - 
Summary for configuration 127:
2025-05-01 03:04:36,680 - main - INFO - Accuracy: 0.9683
2025-05-01 03:04:36,680 - main - INFO - Precision: 0.9320
2025-05-01 03:04:36,680 - main - INFO - Recall: 0.9580
2025-05-01 03:04:36,680 - main - INFO - F1 Score: 0.9448
2025-05-01 03:04:36,680 - main - INFO - IoU: 0.8954
2025-05-01 03:04:36,680 - main - INFO - mAP: 0.9798
2025-05-01 03:04:36,680 - main - INFO - AUC: 0.9916
2025-05-01 03:04:36,680 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:04:36,680 - main - INFO - 
==================================================
2025-05-01 03:04:36,680 - main - INFO - Running configuration 128/756:
2025-05-01 03:04:36,680 - main - INFO - Model: ResNet101
2025-05-01 03:04:36,680 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:04:36,680 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:04:36,680 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:04:36,680 - main - INFO - ==================================================
2025-05-01 03:04:36,680 - training.model_ResNet101_opt_SGD_lr_0.001_id_128 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_128
2025-05-01 03:04:36,680 - training.model_ResNet101_opt_SGD_lr_0.001_id_128 - INFO - Config: {
  "id": 128,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:04:36,933 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:04:36,933 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 128,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:04:36,933 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:04:37,029 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:04:37,031 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:04:37,034 - training.model_ResNet101_opt_SGD_lr_0.001_id_128 - INFO - Starting model evaluation
2025-05-01 03:04:46,312 - training.model_ResNet101_opt_SGD_lr_0.001_id_128 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9418
  Recall:    0.9615
  F1 Score:  0.9516
  IoU:       0.9076
  mAP:       0.9799
  AUC:       0.9919
2025-05-01 03:04:46,314 - training.model_ResNet101_opt_SGD_lr_0.001_id_128 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_128/final_results.json
2025-05-01 03:04:46,315 - training.model_ResNet101_opt_SGD_lr_0.001_id_128 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_128/final_results.json
2025-05-01 03:04:46,315 - main - INFO - 
Summary for configuration 128:
2025-05-01 03:04:46,315 - main - INFO - Accuracy: 0.9722
2025-05-01 03:04:46,315 - main - INFO - Precision: 0.9418
2025-05-01 03:04:46,315 - main - INFO - Recall: 0.9615
2025-05-01 03:04:46,315 - main - INFO - F1 Score: 0.9516
2025-05-01 03:04:46,315 - main - INFO - IoU: 0.9076
2025-05-01 03:04:46,315 - main - INFO - mAP: 0.9799
2025-05-01 03:04:46,315 - main - INFO - AUC: 0.9919
2025-05-01 03:04:46,315 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:04:46,315 - main - INFO - 
==================================================
2025-05-01 03:04:46,315 - main - INFO - Running configuration 129/756:
2025-05-01 03:04:46,315 - main - INFO - Model: ResNet101
2025-05-01 03:04:46,315 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:04:46,315 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:04:46,315 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:04:46,315 - main - INFO - ==================================================
2025-05-01 03:04:46,316 - training.model_ResNet101_opt_SGD_lr_0.001_id_129 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_129
2025-05-01 03:04:46,316 - training.model_ResNet101_opt_SGD_lr_0.001_id_129 - INFO - Config: {
  "id": 129,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:04:46,566 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:04:46,566 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 129,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:04:46,566 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:04:46,739 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:04:46,741 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:04:46,744 - training.model_ResNet101_opt_SGD_lr_0.001_id_129 - INFO - Starting model evaluation
2025-05-01 03:04:56,771 - training.model_ResNet101_opt_SGD_lr_0.001_id_129 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9313
  Recall:    0.9476
  F1 Score:  0.9393
  IoU:       0.8856
  mAP:       0.9812
  AUC:       0.9937
2025-05-01 03:04:56,776 - training.model_ResNet101_opt_SGD_lr_0.001_id_129 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_129/final_results.json
2025-05-01 03:04:56,777 - training.model_ResNet101_opt_SGD_lr_0.001_id_129 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_129/final_results.json
2025-05-01 03:04:56,777 - main - INFO - 
Summary for configuration 129:
2025-05-01 03:04:56,777 - main - INFO - Accuracy: 0.9653
2025-05-01 03:04:56,777 - main - INFO - Precision: 0.9313
2025-05-01 03:04:56,777 - main - INFO - Recall: 0.9476
2025-05-01 03:04:56,777 - main - INFO - F1 Score: 0.9393
2025-05-01 03:04:56,777 - main - INFO - IoU: 0.8856
2025-05-01 03:04:56,777 - main - INFO - mAP: 0.9812
2025-05-01 03:04:56,777 - main - INFO - AUC: 0.9937
2025-05-01 03:04:56,777 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:04:56,777 - main - INFO - 
==================================================
2025-05-01 03:04:56,777 - main - INFO - Running configuration 130/756:
2025-05-01 03:04:56,777 - main - INFO - Model: ResNet101
2025-05-01 03:04:56,777 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:04:56,777 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:04:56,777 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:04:56,777 - main - INFO - ==================================================
2025-05-01 03:04:56,778 - training.model_ResNet101_opt_SGD_lr_0.001_id_130 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_130
2025-05-01 03:04:56,778 - training.model_ResNet101_opt_SGD_lr_0.001_id_130 - INFO - Config: {
  "id": 130,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:04:57,039 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:04:57,039 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 130,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:04:57,039 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:04:57,138 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:04:57,139 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:04:57,143 - training.model_ResNet101_opt_SGD_lr_0.001_id_130 - INFO - Starting model evaluation
2025-05-01 03:05:06,578 - training.model_ResNet101_opt_SGD_lr_0.001_id_130 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9414
  Recall:    0.9545
  F1 Score:  0.9479
  IoU:       0.9010
  mAP:       0.9875
  AUC:       0.9929
2025-05-01 03:05:06,580 - training.model_ResNet101_opt_SGD_lr_0.001_id_130 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_130/final_results.json
2025-05-01 03:05:06,581 - training.model_ResNet101_opt_SGD_lr_0.001_id_130 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_130/final_results.json
2025-05-01 03:05:06,581 - main - INFO - 
Summary for configuration 130:
2025-05-01 03:05:06,581 - main - INFO - Accuracy: 0.9703
2025-05-01 03:05:06,581 - main - INFO - Precision: 0.9414
2025-05-01 03:05:06,581 - main - INFO - Recall: 0.9545
2025-05-01 03:05:06,581 - main - INFO - F1 Score: 0.9479
2025-05-01 03:05:06,581 - main - INFO - IoU: 0.9010
2025-05-01 03:05:06,581 - main - INFO - mAP: 0.9875
2025-05-01 03:05:06,581 - main - INFO - AUC: 0.9929
2025-05-01 03:05:06,581 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:05:06,581 - main - INFO - 
==================================================
2025-05-01 03:05:06,581 - main - INFO - Running configuration 131/756:
2025-05-01 03:05:06,581 - main - INFO - Model: ResNet101
2025-05-01 03:05:06,581 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:05:06,581 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:05:06,581 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:05:06,581 - main - INFO - ==================================================
2025-05-01 03:05:06,582 - training.model_ResNet101_opt_SGD_lr_0.001_id_131 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_131
2025-05-01 03:05:06,582 - training.model_ResNet101_opt_SGD_lr_0.001_id_131 - INFO - Config: {
  "id": 131,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:05:06,831 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:05:06,831 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 131,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:05:06,831 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:05:06,928 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:05:06,929 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:05:06,933 - training.model_ResNet101_opt_SGD_lr_0.001_id_131 - INFO - Starting model evaluation
2025-05-01 03:05:16,377 - training.model_ResNet101_opt_SGD_lr_0.001_id_131 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9317
  Recall:    0.9545
  F1 Score:  0.9430
  IoU:       0.8922
  mAP:       0.9868
  AUC:       0.9925
2025-05-01 03:05:16,379 - training.model_ResNet101_opt_SGD_lr_0.001_id_131 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_131/final_results.json
2025-05-01 03:05:16,381 - training.model_ResNet101_opt_SGD_lr_0.001_id_131 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_131/final_results.json
2025-05-01 03:05:16,381 - main - INFO - 
Summary for configuration 131:
2025-05-01 03:05:16,381 - main - INFO - Accuracy: 0.9673
2025-05-01 03:05:16,381 - main - INFO - Precision: 0.9317
2025-05-01 03:05:16,381 - main - INFO - Recall: 0.9545
2025-05-01 03:05:16,381 - main - INFO - F1 Score: 0.9430
2025-05-01 03:05:16,381 - main - INFO - IoU: 0.8922
2025-05-01 03:05:16,381 - main - INFO - mAP: 0.9868
2025-05-01 03:05:16,381 - main - INFO - AUC: 0.9925
2025-05-01 03:05:16,381 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:05:16,381 - main - INFO - 
==================================================
2025-05-01 03:05:16,381 - main - INFO - Running configuration 132/756:
2025-05-01 03:05:16,381 - main - INFO - Model: ResNet101
2025-05-01 03:05:16,381 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 03:05:16,382 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:05:16,382 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:05:16,382 - main - INFO - ==================================================
2025-05-01 03:05:16,382 - training.model_ResNet101_opt_SGD_lr_0.001_id_132 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001_id_132
2025-05-01 03:05:16,382 - training.model_ResNet101_opt_SGD_lr_0.001_id_132 - INFO - Config: {
  "id": 132,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:05:16,634 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.001
2025-05-01 03:05:16,634 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 132,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:05:16,634 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 03:05:16,730 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 03:05:16,732 - training.model_ResNet101_opt_SGD_lr_0.001 - INFO - Training completed after 983.77 seconds
2025-05-01 03:05:16,735 - training.model_ResNet101_opt_SGD_lr_0.001_id_132 - INFO - Starting model evaluation
2025-05-01 03:05:26,159 - training.model_ResNet101_opt_SGD_lr_0.001_id_132 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9247
  Recall:    0.9441
  F1 Score:  0.9343
  IoU:       0.8766
  mAP:       0.9858
  AUC:       0.9920
2025-05-01 03:05:26,161 - training.model_ResNet101_opt_SGD_lr_0.001_id_132 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_132/final_results.json
2025-05-01 03:05:26,162 - training.model_ResNet101_opt_SGD_lr_0.001_id_132 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.001_id_132/final_results.json
2025-05-01 03:05:26,162 - main - INFO - 
Summary for configuration 132:
2025-05-01 03:05:26,162 - main - INFO - Accuracy: 0.9623
2025-05-01 03:05:26,162 - main - INFO - Precision: 0.9247
2025-05-01 03:05:26,162 - main - INFO - Recall: 0.9441
2025-05-01 03:05:26,162 - main - INFO - F1 Score: 0.9343
2025-05-01 03:05:26,162 - main - INFO - IoU: 0.8766
2025-05-01 03:05:26,162 - main - INFO - mAP: 0.9858
2025-05-01 03:05:26,162 - main - INFO - AUC: 0.9920
2025-05-01 03:05:26,162 - main - INFO - Training time: 983.77 seconds
2025-05-01 03:05:26,162 - main - INFO - 
==================================================
2025-05-01 03:05:26,162 - main - INFO - Running configuration 133/756:
2025-05-01 03:05:26,162 - main - INFO - Model: ResNet101
2025-05-01 03:05:26,162 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:05:26,162 - main - INFO - Scheduler: StepLR
2025-05-01 03:05:26,163 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:05:26,163 - main - INFO - ==================================================
2025-05-01 03:05:26,163 - training.model_ResNet101_opt_SGD_lr_0.0001_id_133 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_133
2025-05-01 03:05:26,163 - training.model_ResNet101_opt_SGD_lr_0.0001_id_133 - INFO - Config: {
  "id": 133,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:05:26,428 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:05:26,428 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 133,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:05:26,428 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 03:05:26,429 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 03:05:47,695 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 03:06:15,584 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.7873
2025-05-01 03:06:15,692 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 1 completed in 49.26s - Train Loss: 0.6512, Train Acc: 0.6830, Val Loss: 0.5982, Val Acc: 0.7873
2025-05-01 03:06:15,947 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 03:06:15,948 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 03:06:37,110 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 03:07:04,763 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7873 to 0.8396
2025-05-01 03:07:04,905 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 2 completed in 48.96s - Train Loss: 0.5755, Train Acc: 0.7969, Val Loss: 0.5380, Val Acc: 0.8396
2025-05-01 03:07:05,171 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 03:07:05,171 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 03:07:26,388 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 03:07:54,272 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8396 to 0.8645
2025-05-01 03:07:54,420 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 3 completed in 49.25s - Train Loss: 0.5279, Train Acc: 0.8331, Val Loss: 0.4999, Val Acc: 0.8645
2025-05-01 03:07:54,684 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 03:07:54,684 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 03:08:15,958 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 03:08:43,458 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8645 to 0.8782
2025-05-01 03:08:43,604 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 4 completed in 48.92s - Train Loss: 0.4957, Train Acc: 0.8580, Val Loss: 0.4730, Val Acc: 0.8782
2025-05-01 03:08:43,863 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 03:08:43,863 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 03:09:04,918 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 03:09:32,726 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8782 to 0.8945
2025-05-01 03:09:32,875 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 5 completed in 49.01s - Train Loss: 0.4698, Train Acc: 0.8730, Val Loss: 0.4503, Val Acc: 0.8945
2025-05-01 03:09:33,139 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 03:09:33,140 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 03:09:54,383 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 03:10:22,035 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8945 to 0.9134
2025-05-01 03:10:22,176 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 6 completed in 49.04s - Train Loss: 0.4438, Train Acc: 0.9007, Val Loss: 0.4296, Val Acc: 0.9134
2025-05-01 03:10:22,446 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 03:10:22,446 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 03:10:43,466 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 03:11:11,271 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9134 to 0.9142
2025-05-01 03:11:11,415 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 7 completed in 48.97s - Train Loss: 0.4279, Train Acc: 0.9133, Val Loss: 0.4173, Val Acc: 0.9142
2025-05-01 03:11:11,675 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 03:11:11,675 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 03:11:32,547 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 03:12:00,535 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9142 to 0.9314
2025-05-01 03:12:00,675 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 8 completed in 49.00s - Train Loss: 0.4107, Train Acc: 0.9309, Val Loss: 0.4027, Val Acc: 0.9314
2025-05-01 03:12:00,947 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 03:12:00,948 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 03:12:21,803 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 03:12:49,595 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9314 to 0.9365
2025-05-01 03:12:49,737 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 9 completed in 48.79s - Train Loss: 0.3995, Train Acc: 0.9359, Val Loss: 0.3934, Val Acc: 0.9365
2025-05-01 03:12:49,996 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 03:12:49,997 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 03:13:11,455 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 03:13:39,018 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9365 to 0.9400
2025-05-01 03:13:39,164 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 10 completed in 49.17s - Train Loss: 0.3888, Train Acc: 0.9474, Val Loss: 0.3858, Val Acc: 0.9400
2025-05-01 03:13:39,440 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 03:13:39,441 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 03:14:00,505 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 03:14:28,303 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9400 to 0.9468
2025-05-01 03:14:28,449 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 11 completed in 49.01s - Train Loss: 0.3790, Train Acc: 0.9552, Val Loss: 0.3856, Val Acc: 0.9468
2025-05-01 03:14:28,737 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 03:14:28,737 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 03:14:50,004 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 03:15:17,284 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation loss improved from inf to 0.3873
2025-05-01 03:15:17,420 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 12 completed in 48.68s - Train Loss: 0.3785, Train Acc: 0.9547, Val Loss: 0.3873, Val Acc: 0.9391
2025-05-01 03:15:17,675 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 03:15:17,675 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 03:15:38,837 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 03:16:06,249 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.3873 to 0.3835
2025-05-01 03:16:06,405 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 13 completed in 48.73s - Train Loss: 0.3814, Train Acc: 0.9520, Val Loss: 0.3835, Val Acc: 0.9425
2025-05-01 03:16:06,680 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 03:16:06,681 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 03:16:28,235 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 03:16:56,103 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9468 to 0.9503
2025-05-01 03:16:56,240 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 14 completed in 49.56s - Train Loss: 0.3779, Train Acc: 0.9558, Val Loss: 0.3803, Val Acc: 0.9503
2025-05-01 03:16:56,493 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 03:16:56,493 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 03:17:18,035 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 03:17:45,814 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 15 completed in 49.32s - Train Loss: 0.3754, Train Acc: 0.9577, Val Loss: 0.3840, Val Acc: 0.9383
2025-05-01 03:17:46,096 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 03:17:46,097 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 03:18:07,113 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 03:18:34,884 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.3835 to 0.3796
2025-05-01 03:18:35,016 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 16 completed in 48.92s - Train Loss: 0.3778, Train Acc: 0.9565, Val Loss: 0.3796, Val Acc: 0.9460
2025-05-01 03:18:35,287 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 03:18:35,288 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 03:18:56,522 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 03:19:24,324 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 17 completed in 49.04s - Train Loss: 0.3769, Train Acc: 0.9552, Val Loss: 0.3806, Val Acc: 0.9468
2025-05-01 03:19:24,599 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 03:19:24,600 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 03:19:45,762 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 03:20:13,310 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.3796 to 0.3793
2025-05-01 03:20:13,456 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 18 completed in 48.86s - Train Loss: 0.3731, Train Acc: 0.9603, Val Loss: 0.3793, Val Acc: 0.9451
2025-05-01 03:20:13,721 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 03:20:13,721 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 03:20:35,095 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 03:21:02,362 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.3793 to 0.3769
2025-05-01 03:21:02,523 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 19 completed in 48.80s - Train Loss: 0.3729, Train Acc: 0.9599, Val Loss: 0.3769, Val Acc: 0.9468
2025-05-01 03:21:02,789 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 03:21:02,790 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 03:21:24,164 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 03:21:51,761 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.3769 to 0.3768
2025-05-01 03:21:51,902 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Epoch 20 completed in 49.11s - Train Loss: 0.3748, Train Acc: 0.9616, Val Loss: 0.3768, Val Acc: 0.9485
2025-05-01 03:21:52,155 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 03:21:52,156 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:21:52,160 - training.model_ResNet101_opt_SGD_lr_0.0001_id_133 - INFO - Starting model evaluation
2025-05-01 03:22:01,670 - training.model_ResNet101_opt_SGD_lr_0.0001_id_133 - INFO - Evaluation metrics:
  Accuracy:  0.9415
  Precision: 0.8847
  Recall:    0.9126
  F1 Score:  0.8985
  IoU:       0.8156
  mAP:       0.9641
  AUC:       0.9793
2025-05-01 03:22:01,671 - training.model_ResNet101_opt_SGD_lr_0.0001_id_133 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_133/final_results.json
2025-05-01 03:22:01,673 - training.model_ResNet101_opt_SGD_lr_0.0001_id_133 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_133/final_results.json
2025-05-01 03:22:01,673 - main - INFO - 
Summary for configuration 133:
2025-05-01 03:22:01,673 - main - INFO - Accuracy: 0.9415
2025-05-01 03:22:01,673 - main - INFO - Precision: 0.8847
2025-05-01 03:22:01,673 - main - INFO - Recall: 0.9126
2025-05-01 03:22:01,673 - main - INFO - F1 Score: 0.8985
2025-05-01 03:22:01,673 - main - INFO - IoU: 0.8156
2025-05-01 03:22:01,673 - main - INFO - mAP: 0.9641
2025-05-01 03:22:01,673 - main - INFO - AUC: 0.9793
2025-05-01 03:22:01,673 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:22:01,673 - main - INFO - 
==================================================
2025-05-01 03:22:01,673 - main - INFO - Running configuration 134/756:
2025-05-01 03:22:01,673 - main - INFO - Model: ResNet101
2025-05-01 03:22:01,673 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:22:01,673 - main - INFO - Scheduler: StepLR
2025-05-01 03:22:01,673 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:22:01,673 - main - INFO - ==================================================
2025-05-01 03:22:01,673 - training.model_ResNet101_opt_SGD_lr_0.0001_id_134 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_134
2025-05-01 03:22:01,673 - training.model_ResNet101_opt_SGD_lr_0.0001_id_134 - INFO - Config: {
  "id": 134,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:22:01,924 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:22:01,924 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 134,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:22:01,924 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:22:02,023 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:22:02,025 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:22:02,028 - training.model_ResNet101_opt_SGD_lr_0.0001_id_134 - INFO - Starting model evaluation
2025-05-01 03:22:11,274 - training.model_ResNet101_opt_SGD_lr_0.0001_id_134 - INFO - Evaluation metrics:
  Accuracy:  0.9425
  Precision: 0.8800
  Recall:    0.9231
  F1 Score:  0.9010
  IoU:       0.8199
  mAP:       0.9687
  AUC:       0.9828
2025-05-01 03:22:11,276 - training.model_ResNet101_opt_SGD_lr_0.0001_id_134 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_134/final_results.json
2025-05-01 03:22:11,277 - training.model_ResNet101_opt_SGD_lr_0.0001_id_134 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_134/final_results.json
2025-05-01 03:22:11,277 - main - INFO - 
Summary for configuration 134:
2025-05-01 03:22:11,277 - main - INFO - Accuracy: 0.9425
2025-05-01 03:22:11,277 - main - INFO - Precision: 0.8800
2025-05-01 03:22:11,277 - main - INFO - Recall: 0.9231
2025-05-01 03:22:11,277 - main - INFO - F1 Score: 0.9010
2025-05-01 03:22:11,277 - main - INFO - IoU: 0.8199
2025-05-01 03:22:11,277 - main - INFO - mAP: 0.9687
2025-05-01 03:22:11,277 - main - INFO - AUC: 0.9828
2025-05-01 03:22:11,277 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:22:11,277 - main - INFO - 
==================================================
2025-05-01 03:22:11,277 - main - INFO - Running configuration 135/756:
2025-05-01 03:22:11,277 - main - INFO - Model: ResNet101
2025-05-01 03:22:11,277 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:22:11,277 - main - INFO - Scheduler: StepLR
2025-05-01 03:22:11,277 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:22:11,277 - main - INFO - ==================================================
2025-05-01 03:22:11,278 - training.model_ResNet101_opt_SGD_lr_0.0001_id_135 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_135
2025-05-01 03:22:11,278 - training.model_ResNet101_opt_SGD_lr_0.0001_id_135 - INFO - Config: {
  "id": 135,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:22:11,528 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:22:11,528 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 135,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:22:11,528 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:22:11,627 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:22:11,628 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:22:11,632 - training.model_ResNet101_opt_SGD_lr_0.0001_id_135 - INFO - Starting model evaluation
2025-05-01 03:22:21,350 - training.model_ResNet101_opt_SGD_lr_0.0001_id_135 - INFO - Evaluation metrics:
  Accuracy:  0.9425
  Precision: 0.8826
  Recall:    0.9196
  F1 Score:  0.9007
  IoU:       0.8193
  mAP:       0.9647
  AUC:       0.9810
2025-05-01 03:22:21,352 - training.model_ResNet101_opt_SGD_lr_0.0001_id_135 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_135/final_results.json
2025-05-01 03:22:21,354 - training.model_ResNet101_opt_SGD_lr_0.0001_id_135 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_135/final_results.json
2025-05-01 03:22:21,354 - main - INFO - 
Summary for configuration 135:
2025-05-01 03:22:21,354 - main - INFO - Accuracy: 0.9425
2025-05-01 03:22:21,354 - main - INFO - Precision: 0.8826
2025-05-01 03:22:21,354 - main - INFO - Recall: 0.9196
2025-05-01 03:22:21,354 - main - INFO - F1 Score: 0.9007
2025-05-01 03:22:21,354 - main - INFO - IoU: 0.8193
2025-05-01 03:22:21,354 - main - INFO - mAP: 0.9647
2025-05-01 03:22:21,354 - main - INFO - AUC: 0.9810
2025-05-01 03:22:21,354 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:22:21,354 - main - INFO - 
==================================================
2025-05-01 03:22:21,354 - main - INFO - Running configuration 136/756:
2025-05-01 03:22:21,354 - main - INFO - Model: ResNet101
2025-05-01 03:22:21,354 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:22:21,354 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:22:21,354 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:22:21,354 - main - INFO - ==================================================
2025-05-01 03:22:21,354 - training.model_ResNet101_opt_SGD_lr_0.0001_id_136 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_136
2025-05-01 03:22:21,354 - training.model_ResNet101_opt_SGD_lr_0.0001_id_136 - INFO - Config: {
  "id": 136,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:22:21,627 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:22:21,627 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 136,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:22:21,627 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:22:21,727 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:22:21,804 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:22:21,808 - training.model_ResNet101_opt_SGD_lr_0.0001_id_136 - INFO - Starting model evaluation
2025-05-01 03:22:32,455 - training.model_ResNet101_opt_SGD_lr_0.0001_id_136 - INFO - Evaluation metrics:
  Accuracy:  0.9415
  Precision: 0.8796
  Recall:    0.9196
  F1 Score:  0.8991
  IoU:       0.8168
  mAP:       0.9671
  AUC:       0.9816
2025-05-01 03:22:32,456 - training.model_ResNet101_opt_SGD_lr_0.0001_id_136 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_136/final_results.json
2025-05-01 03:22:32,458 - training.model_ResNet101_opt_SGD_lr_0.0001_id_136 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_136/final_results.json
2025-05-01 03:22:32,458 - main - INFO - 
Summary for configuration 136:
2025-05-01 03:22:32,458 - main - INFO - Accuracy: 0.9415
2025-05-01 03:22:32,458 - main - INFO - Precision: 0.8796
2025-05-01 03:22:32,458 - main - INFO - Recall: 0.9196
2025-05-01 03:22:32,458 - main - INFO - F1 Score: 0.8991
2025-05-01 03:22:32,458 - main - INFO - IoU: 0.8168
2025-05-01 03:22:32,458 - main - INFO - mAP: 0.9671
2025-05-01 03:22:32,458 - main - INFO - AUC: 0.9816
2025-05-01 03:22:32,458 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:22:32,458 - main - INFO - 
==================================================
2025-05-01 03:22:32,458 - main - INFO - Running configuration 137/756:
2025-05-01 03:22:32,458 - main - INFO - Model: ResNet101
2025-05-01 03:22:32,458 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:22:32,458 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:22:32,458 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:22:32,458 - main - INFO - ==================================================
2025-05-01 03:22:32,458 - training.model_ResNet101_opt_SGD_lr_0.0001_id_137 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_137
2025-05-01 03:22:32,458 - training.model_ResNet101_opt_SGD_lr_0.0001_id_137 - INFO - Config: {
  "id": 137,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:22:32,708 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:22:32,708 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 137,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:22:32,708 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:22:32,807 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:22:32,808 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:22:32,812 - training.model_ResNet101_opt_SGD_lr_0.0001_id_137 - INFO - Starting model evaluation
2025-05-01 03:22:42,060 - training.model_ResNet101_opt_SGD_lr_0.0001_id_137 - INFO - Evaluation metrics:
  Accuracy:  0.9445
  Precision: 0.8885
  Recall:    0.9196
  F1 Score:  0.9038
  IoU:       0.8245
  mAP:       0.9648
  AUC:       0.9806
2025-05-01 03:22:42,061 - training.model_ResNet101_opt_SGD_lr_0.0001_id_137 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_137/final_results.json
2025-05-01 03:22:42,063 - training.model_ResNet101_opt_SGD_lr_0.0001_id_137 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_137/final_results.json
2025-05-01 03:22:42,063 - main - INFO - 
Summary for configuration 137:
2025-05-01 03:22:42,063 - main - INFO - Accuracy: 0.9445
2025-05-01 03:22:42,063 - main - INFO - Precision: 0.8885
2025-05-01 03:22:42,063 - main - INFO - Recall: 0.9196
2025-05-01 03:22:42,063 - main - INFO - F1 Score: 0.9038
2025-05-01 03:22:42,063 - main - INFO - IoU: 0.8245
2025-05-01 03:22:42,063 - main - INFO - mAP: 0.9648
2025-05-01 03:22:42,063 - main - INFO - AUC: 0.9806
2025-05-01 03:22:42,063 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:22:42,063 - main - INFO - 
==================================================
2025-05-01 03:22:42,063 - main - INFO - Running configuration 138/756:
2025-05-01 03:22:42,063 - main - INFO - Model: ResNet101
2025-05-01 03:22:42,063 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:22:42,063 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:22:42,063 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:22:42,063 - main - INFO - ==================================================
2025-05-01 03:22:42,063 - training.model_ResNet101_opt_SGD_lr_0.0001_id_138 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_138
2025-05-01 03:22:42,063 - training.model_ResNet101_opt_SGD_lr_0.0001_id_138 - INFO - Config: {
  "id": 138,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:22:42,314 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:22:42,314 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 138,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:22:42,314 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:22:42,414 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:22:42,416 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:22:42,419 - training.model_ResNet101_opt_SGD_lr_0.0001_id_138 - INFO - Starting model evaluation
2025-05-01 03:22:51,524 - training.model_ResNet101_opt_SGD_lr_0.0001_id_138 - INFO - Evaluation metrics:
  Accuracy:  0.9445
  Precision: 0.8912
  Recall:    0.9161
  F1 Score:  0.9034
  IoU:       0.8239
  mAP:       0.9672
  AUC:       0.9814
2025-05-01 03:22:51,526 - training.model_ResNet101_opt_SGD_lr_0.0001_id_138 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_138/final_results.json
2025-05-01 03:22:51,527 - training.model_ResNet101_opt_SGD_lr_0.0001_id_138 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_138/final_results.json
2025-05-01 03:22:51,527 - main - INFO - 
Summary for configuration 138:
2025-05-01 03:22:51,527 - main - INFO - Accuracy: 0.9445
2025-05-01 03:22:51,527 - main - INFO - Precision: 0.8912
2025-05-01 03:22:51,527 - main - INFO - Recall: 0.9161
2025-05-01 03:22:51,527 - main - INFO - F1 Score: 0.9034
2025-05-01 03:22:51,527 - main - INFO - IoU: 0.8239
2025-05-01 03:22:51,527 - main - INFO - mAP: 0.9672
2025-05-01 03:22:51,527 - main - INFO - AUC: 0.9814
2025-05-01 03:22:51,527 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:22:51,527 - main - INFO - 
==================================================
2025-05-01 03:22:51,527 - main - INFO - Running configuration 139/756:
2025-05-01 03:22:51,528 - main - INFO - Model: ResNet101
2025-05-01 03:22:51,528 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:22:51,528 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:22:51,528 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:22:51,528 - main - INFO - ==================================================
2025-05-01 03:22:51,528 - training.model_ResNet101_opt_SGD_lr_0.0001_id_139 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_139
2025-05-01 03:22:51,528 - training.model_ResNet101_opt_SGD_lr_0.0001_id_139 - INFO - Config: {
  "id": 139,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:22:51,781 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:22:51,781 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 139,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:22:51,781 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:22:51,878 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:22:51,879 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:22:51,883 - training.model_ResNet101_opt_SGD_lr_0.0001_id_139 - INFO - Starting model evaluation
2025-05-01 03:23:01,252 - training.model_ResNet101_opt_SGD_lr_0.0001_id_139 - INFO - Evaluation metrics:
  Accuracy:  0.9386
  Precision: 0.8784
  Recall:    0.9091
  F1 Score:  0.8935
  IoU:       0.8075
  mAP:       0.9654
  AUC:       0.9814
2025-05-01 03:23:01,254 - training.model_ResNet101_opt_SGD_lr_0.0001_id_139 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_139/final_results.json
2025-05-01 03:23:01,257 - training.model_ResNet101_opt_SGD_lr_0.0001_id_139 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_139/final_results.json
2025-05-01 03:23:01,257 - main - INFO - 
Summary for configuration 139:
2025-05-01 03:23:01,257 - main - INFO - Accuracy: 0.9386
2025-05-01 03:23:01,257 - main - INFO - Precision: 0.8784
2025-05-01 03:23:01,257 - main - INFO - Recall: 0.9091
2025-05-01 03:23:01,257 - main - INFO - F1 Score: 0.8935
2025-05-01 03:23:01,257 - main - INFO - IoU: 0.8075
2025-05-01 03:23:01,257 - main - INFO - mAP: 0.9654
2025-05-01 03:23:01,257 - main - INFO - AUC: 0.9814
2025-05-01 03:23:01,257 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:23:01,257 - main - INFO - 
==================================================
2025-05-01 03:23:01,257 - main - INFO - Running configuration 140/756:
2025-05-01 03:23:01,257 - main - INFO - Model: ResNet101
2025-05-01 03:23:01,257 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:23:01,257 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:23:01,257 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:23:01,257 - main - INFO - ==================================================
2025-05-01 03:23:01,257 - training.model_ResNet101_opt_SGD_lr_0.0001_id_140 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_140
2025-05-01 03:23:01,258 - training.model_ResNet101_opt_SGD_lr_0.0001_id_140 - INFO - Config: {
  "id": 140,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:23:01,522 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:23:01,522 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 140,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:23:01,523 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:23:01,618 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:23:01,620 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:23:01,623 - training.model_ResNet101_opt_SGD_lr_0.0001_id_140 - INFO - Starting model evaluation
2025-05-01 03:23:11,134 - training.model_ResNet101_opt_SGD_lr_0.0001_id_140 - INFO - Evaluation metrics:
  Accuracy:  0.9425
  Precision: 0.8775
  Recall:    0.9266
  F1 Score:  0.9014
  IoU:       0.8204
  mAP:       0.9669
  AUC:       0.9814
2025-05-01 03:23:11,135 - training.model_ResNet101_opt_SGD_lr_0.0001_id_140 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_140/final_results.json
2025-05-01 03:23:11,137 - training.model_ResNet101_opt_SGD_lr_0.0001_id_140 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_140/final_results.json
2025-05-01 03:23:11,137 - main - INFO - 
Summary for configuration 140:
2025-05-01 03:23:11,137 - main - INFO - Accuracy: 0.9425
2025-05-01 03:23:11,137 - main - INFO - Precision: 0.8775
2025-05-01 03:23:11,137 - main - INFO - Recall: 0.9266
2025-05-01 03:23:11,137 - main - INFO - F1 Score: 0.9014
2025-05-01 03:23:11,137 - main - INFO - IoU: 0.8204
2025-05-01 03:23:11,137 - main - INFO - mAP: 0.9669
2025-05-01 03:23:11,137 - main - INFO - AUC: 0.9814
2025-05-01 03:23:11,137 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:23:11,137 - main - INFO - 
==================================================
2025-05-01 03:23:11,137 - main - INFO - Running configuration 141/756:
2025-05-01 03:23:11,137 - main - INFO - Model: ResNet101
2025-05-01 03:23:11,137 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:23:11,137 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:23:11,137 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:23:11,137 - main - INFO - ==================================================
2025-05-01 03:23:11,137 - training.model_ResNet101_opt_SGD_lr_0.0001_id_141 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_141
2025-05-01 03:23:11,137 - training.model_ResNet101_opt_SGD_lr_0.0001_id_141 - INFO - Config: {
  "id": 141,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:23:11,387 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:23:11,387 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 141,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:23:11,387 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:23:11,485 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:23:11,486 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:23:11,489 - training.model_ResNet101_opt_SGD_lr_0.0001_id_141 - INFO - Starting model evaluation
2025-05-01 03:23:20,938 - training.model_ResNet101_opt_SGD_lr_0.0001_id_141 - INFO - Evaluation metrics:
  Accuracy:  0.9376
  Precision: 0.8704
  Recall:    0.9161
  F1 Score:  0.8927
  IoU:       0.8062
  mAP:       0.9649
  AUC:       0.9811
2025-05-01 03:23:20,940 - training.model_ResNet101_opt_SGD_lr_0.0001_id_141 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_141/final_results.json
2025-05-01 03:23:20,941 - training.model_ResNet101_opt_SGD_lr_0.0001_id_141 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_141/final_results.json
2025-05-01 03:23:20,941 - main - INFO - 
Summary for configuration 141:
2025-05-01 03:23:20,941 - main - INFO - Accuracy: 0.9376
2025-05-01 03:23:20,941 - main - INFO - Precision: 0.8704
2025-05-01 03:23:20,941 - main - INFO - Recall: 0.9161
2025-05-01 03:23:20,941 - main - INFO - F1 Score: 0.8927
2025-05-01 03:23:20,941 - main - INFO - IoU: 0.8062
2025-05-01 03:23:20,941 - main - INFO - mAP: 0.9649
2025-05-01 03:23:20,941 - main - INFO - AUC: 0.9811
2025-05-01 03:23:20,941 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:23:20,941 - main - INFO - 
==================================================
2025-05-01 03:23:20,941 - main - INFO - Running configuration 142/756:
2025-05-01 03:23:20,941 - main - INFO - Model: ResNet101
2025-05-01 03:23:20,941 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:23:20,941 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:23:20,941 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:23:20,941 - main - INFO - ==================================================
2025-05-01 03:23:20,942 - training.model_ResNet101_opt_SGD_lr_0.0001_id_142 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_142
2025-05-01 03:23:20,942 - training.model_ResNet101_opt_SGD_lr_0.0001_id_142 - INFO - Config: {
  "id": 142,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:23:21,192 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:23:21,193 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 142,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:23:21,193 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:23:21,291 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:23:21,292 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:23:21,296 - training.model_ResNet101_opt_SGD_lr_0.0001_id_142 - INFO - Starting model evaluation
2025-05-01 03:23:30,995 - training.model_ResNet101_opt_SGD_lr_0.0001_id_142 - INFO - Evaluation metrics:
  Accuracy:  0.9405
  Precision: 0.8742
  Recall:    0.9231
  F1 Score:  0.8980
  IoU:       0.8148
  mAP:       0.9643
  AUC:       0.9810
2025-05-01 03:23:30,997 - training.model_ResNet101_opt_SGD_lr_0.0001_id_142 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_142/final_results.json
2025-05-01 03:23:30,998 - training.model_ResNet101_opt_SGD_lr_0.0001_id_142 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_142/final_results.json
2025-05-01 03:23:30,998 - main - INFO - 
Summary for configuration 142:
2025-05-01 03:23:30,998 - main - INFO - Accuracy: 0.9405
2025-05-01 03:23:30,998 - main - INFO - Precision: 0.8742
2025-05-01 03:23:30,998 - main - INFO - Recall: 0.9231
2025-05-01 03:23:30,998 - main - INFO - F1 Score: 0.8980
2025-05-01 03:23:30,998 - main - INFO - IoU: 0.8148
2025-05-01 03:23:30,998 - main - INFO - mAP: 0.9643
2025-05-01 03:23:30,998 - main - INFO - AUC: 0.9810
2025-05-01 03:23:30,998 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:23:30,998 - main - INFO - 
==================================================
2025-05-01 03:23:30,998 - main - INFO - Running configuration 143/756:
2025-05-01 03:23:30,998 - main - INFO - Model: ResNet101
2025-05-01 03:23:30,998 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:23:30,998 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:23:30,998 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:23:30,998 - main - INFO - ==================================================
2025-05-01 03:23:30,999 - training.model_ResNet101_opt_SGD_lr_0.0001_id_143 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_143
2025-05-01 03:23:30,999 - training.model_ResNet101_opt_SGD_lr_0.0001_id_143 - INFO - Config: {
  "id": 143,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:23:31,249 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:23:31,250 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 143,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:23:31,250 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:23:31,346 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:23:31,347 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:23:31,351 - training.model_ResNet101_opt_SGD_lr_0.0001_id_143 - INFO - Starting model evaluation
2025-05-01 03:23:41,077 - training.model_ResNet101_opt_SGD_lr_0.0001_id_143 - INFO - Evaluation metrics:
  Accuracy:  0.9445
  Precision: 0.8912
  Recall:    0.9161
  F1 Score:  0.9034
  IoU:       0.8239
  mAP:       0.9634
  AUC:       0.9808
2025-05-01 03:23:41,079 - training.model_ResNet101_opt_SGD_lr_0.0001_id_143 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_143/final_results.json
2025-05-01 03:23:41,080 - training.model_ResNet101_opt_SGD_lr_0.0001_id_143 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_143/final_results.json
2025-05-01 03:23:41,080 - main - INFO - 
Summary for configuration 143:
2025-05-01 03:23:41,080 - main - INFO - Accuracy: 0.9445
2025-05-01 03:23:41,080 - main - INFO - Precision: 0.8912
2025-05-01 03:23:41,080 - main - INFO - Recall: 0.9161
2025-05-01 03:23:41,080 - main - INFO - F1 Score: 0.9034
2025-05-01 03:23:41,080 - main - INFO - IoU: 0.8239
2025-05-01 03:23:41,080 - main - INFO - mAP: 0.9634
2025-05-01 03:23:41,080 - main - INFO - AUC: 0.9808
2025-05-01 03:23:41,080 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:23:41,080 - main - INFO - 
==================================================
2025-05-01 03:23:41,080 - main - INFO - Running configuration 144/756:
2025-05-01 03:23:41,081 - main - INFO - Model: ResNet101
2025-05-01 03:23:41,081 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 03:23:41,081 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:23:41,081 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:23:41,081 - main - INFO - ==================================================
2025-05-01 03:23:41,081 - training.model_ResNet101_opt_SGD_lr_0.0001_id_144 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001_id_144
2025-05-01 03:23:41,081 - training.model_ResNet101_opt_SGD_lr_0.0001_id_144 - INFO - Config: {
  "id": 144,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:23:41,331 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_SGD_lr_0.0001
2025-05-01 03:23:41,331 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 144,
  "model_name": "ResNet101",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:23:41,331 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 03:23:41,428 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9503
2025-05-01 03:23:41,429 - training.model_ResNet101_opt_SGD_lr_0.0001 - INFO - Training completed after 985.47 seconds
2025-05-01 03:23:41,433 - training.model_ResNet101_opt_SGD_lr_0.0001_id_144 - INFO - Starting model evaluation
2025-05-01 03:23:51,006 - training.model_ResNet101_opt_SGD_lr_0.0001_id_144 - INFO - Evaluation metrics:
  Accuracy:  0.9415
  Precision: 0.8796
  Recall:    0.9196
  F1 Score:  0.8991
  IoU:       0.8168
  mAP:       0.9651
  AUC:       0.9808
2025-05-01 03:23:51,009 - training.model_ResNet101_opt_SGD_lr_0.0001_id_144 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_144/final_results.json
2025-05-01 03:23:51,011 - training.model_ResNet101_opt_SGD_lr_0.0001_id_144 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_SGD_lr_0.0001_id_144/final_results.json
2025-05-01 03:23:51,011 - main - INFO - 
Summary for configuration 144:
2025-05-01 03:23:51,011 - main - INFO - Accuracy: 0.9415
2025-05-01 03:23:51,011 - main - INFO - Precision: 0.8796
2025-05-01 03:23:51,011 - main - INFO - Recall: 0.9196
2025-05-01 03:23:51,012 - main - INFO - F1 Score: 0.8991
2025-05-01 03:23:51,012 - main - INFO - IoU: 0.8168
2025-05-01 03:23:51,012 - main - INFO - mAP: 0.9651
2025-05-01 03:23:51,012 - main - INFO - AUC: 0.9808
2025-05-01 03:23:51,012 - main - INFO - Training time: 985.47 seconds
2025-05-01 03:23:51,012 - main - INFO - 
==================================================
2025-05-01 03:23:51,012 - main - INFO - Running configuration 145/756:
2025-05-01 03:23:51,012 - main - INFO - Model: ResNet101
2025-05-01 03:23:51,012 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:23:51,012 - main - INFO - Scheduler: StepLR
2025-05-01 03:23:51,012 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:23:51,012 - main - INFO - ==================================================
2025-05-01 03:23:51,012 - training.model_ResNet101_opt_Adam_lr_0.01_id_145 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_145
2025-05-01 03:23:51,012 - training.model_ResNet101_opt_Adam_lr_0.01_id_145 - INFO - Config: {
  "id": 145,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:23:51,267 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:23:51,267 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 145,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:23:51,267 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 03:23:51,268 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 03:24:12,759 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 03:24:40,746 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.4957
2025-05-01 03:24:40,838 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 1 completed in 49.57s - Train Loss: 0.7439, Train Acc: 0.5133, Val Loss: 0.6992, Val Acc: 0.4957
2025-05-01 03:24:41,246 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 03:24:41,247 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 03:25:02,688 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 03:25:30,740 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation loss improved from inf to 0.7040
2025-05-01 03:25:30,882 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 2 completed in 49.64s - Train Loss: 0.6762, Train Acc: 0.5742, Val Loss: 0.7040, Val Acc: 0.4957
2025-05-01 03:25:31,277 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 03:25:31,278 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 03:25:52,888 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 03:26:20,880 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.4957 to 0.5266
2025-05-01 03:26:21,023 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 3 completed in 49.75s - Train Loss: 0.6925, Train Acc: 0.5493, Val Loss: 0.6908, Val Acc: 0.5266
2025-05-01 03:26:21,419 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 03:26:21,419 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 03:26:43,419 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 03:27:11,386 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 4 completed in 49.97s - Train Loss: 0.6860, Train Acc: 0.5573, Val Loss: 0.8031, Val Acc: 0.5043
2025-05-01 03:27:11,786 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 03:27:11,786 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 03:27:33,379 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 03:28:01,592 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 5 completed in 49.81s - Train Loss: 0.6699, Train Acc: 0.5922, Val Loss: 0.8106, Val Acc: 0.5026
2025-05-01 03:28:02,007 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 03:28:02,007 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 03:28:23,602 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 03:28:51,574 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5266 to 0.6827
2025-05-01 03:28:51,712 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 6 completed in 49.70s - Train Loss: 0.6427, Train Acc: 0.6296, Val Loss: 0.6062, Val Acc: 0.6827
2025-05-01 03:28:52,105 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 03:28:52,105 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 03:29:13,908 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 03:29:42,118 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.6827 to 0.6990
2025-05-01 03:29:42,257 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 7 completed in 50.15s - Train Loss: 0.6146, Train Acc: 0.6705, Val Loss: 0.6129, Val Acc: 0.6990
2025-05-01 03:29:42,622 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 03:29:42,622 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 03:30:04,610 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 03:30:32,660 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.6990 to 0.7084
2025-05-01 03:30:32,813 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 8 completed in 50.19s - Train Loss: 0.5991, Train Acc: 0.6845, Val Loss: 0.5918, Val Acc: 0.7084
2025-05-01 03:30:33,195 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 03:30:33,195 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 03:30:54,898 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 03:31:22,719 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.7040 to 0.5994
2025-05-01 03:31:22,865 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 9 completed in 49.67s - Train Loss: 0.5978, Train Acc: 0.6877, Val Loss: 0.5994, Val Acc: 0.6801
2025-05-01 03:31:23,236 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 03:31:23,237 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 03:31:44,941 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 03:32:13,027 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7084 to 0.7290
2025-05-01 03:32:13,164 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 10 completed in 49.93s - Train Loss: 0.5749, Train Acc: 0.7222, Val Loss: 0.5634, Val Acc: 0.7290
2025-05-01 03:32:13,529 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 03:32:13,530 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 03:32:35,302 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 03:33:03,156 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7290 to 0.7496
2025-05-01 03:33:03,298 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 11 completed in 49.77s - Train Loss: 0.5561, Train Acc: 0.7379, Val Loss: 0.5409, Val Acc: 0.7496
2025-05-01 03:33:03,694 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 03:33:03,694 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 03:33:25,506 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 03:33:53,310 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7496 to 0.7676
2025-05-01 03:33:53,442 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 12 completed in 49.75s - Train Loss: 0.5445, Train Acc: 0.7467, Val Loss: 0.5303, Val Acc: 0.7676
2025-05-01 03:33:53,834 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 03:33:53,835 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 03:34:15,207 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 03:34:43,268 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7676 to 0.7736
2025-05-01 03:34:43,427 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 13 completed in 49.59s - Train Loss: 0.5323, Train Acc: 0.7664, Val Loss: 0.5242, Val Acc: 0.7736
2025-05-01 03:34:43,831 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 03:34:43,832 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 03:35:05,262 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 03:35:33,291 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7736 to 0.7882
2025-05-01 03:35:33,455 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 14 completed in 49.62s - Train Loss: 0.5235, Train Acc: 0.7728, Val Loss: 0.5125, Val Acc: 0.7882
2025-05-01 03:35:33,840 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 03:35:33,841 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 03:35:55,539 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 03:36:23,341 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.5994 to 0.5066
2025-05-01 03:36:23,484 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 15 completed in 49.64s - Train Loss: 0.5146, Train Acc: 0.7806, Val Loss: 0.5066, Val Acc: 0.7873
2025-05-01 03:36:23,872 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 03:36:23,873 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 03:36:45,528 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 03:37:13,933 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7882 to 0.7907
2025-05-01 03:37:14,077 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 16 completed in 50.20s - Train Loss: 0.4993, Train Acc: 0.8007, Val Loss: 0.5032, Val Acc: 0.7907
2025-05-01 03:37:14,448 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 03:37:14,448 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 03:37:36,167 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 03:38:04,101 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7907 to 0.8070
2025-05-01 03:38:04,250 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 17 completed in 49.80s - Train Loss: 0.4964, Train Acc: 0.8042, Val Loss: 0.4948, Val Acc: 0.8070
2025-05-01 03:38:04,635 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 03:38:04,636 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 03:38:26,598 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 03:38:54,346 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8070 to 0.8182
2025-05-01 03:38:54,490 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 18 completed in 49.85s - Train Loss: 0.4888, Train Acc: 0.8140, Val Loss: 0.4930, Val Acc: 0.8182
2025-05-01 03:38:54,897 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 03:38:54,897 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 03:39:16,635 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 03:39:44,832 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.5066 to 0.4864
2025-05-01 03:39:44,980 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 19 completed in 50.08s - Train Loss: 0.4859, Train Acc: 0.8175, Val Loss: 0.4864, Val Acc: 0.8105
2025-05-01 03:39:45,354 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 03:39:45,355 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 03:40:06,787 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 03:40:34,753 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8182 to 0.8242
2025-05-01 03:40:34,898 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Epoch 20 completed in 49.54s - Train Loss: 0.4770, Train Acc: 0.8233, Val Loss: 0.4766, Val Acc: 0.8242
2025-05-01 03:40:35,289 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 03:40:35,290 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:40:35,294 - training.model_ResNet101_opt_Adam_lr_0.01_id_145 - INFO - Starting model evaluation
2025-05-01 03:40:44,669 - training.model_ResNet101_opt_Adam_lr_0.01_id_145 - INFO - Evaluation metrics:
  Accuracy:  0.8474
  Precision: 0.7143
  Recall:    0.7692
  F1 Score:  0.7407
  IoU:       0.5882
  mAP:       0.8263
  AUC:       0.9073
2025-05-01 03:40:44,671 - training.model_ResNet101_opt_Adam_lr_0.01_id_145 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_145/final_results.json
2025-05-01 03:40:44,672 - training.model_ResNet101_opt_Adam_lr_0.01_id_145 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_145/final_results.json
2025-05-01 03:40:44,672 - main - INFO - 
Summary for configuration 145:
2025-05-01 03:40:44,672 - main - INFO - Accuracy: 0.8474
2025-05-01 03:40:44,672 - main - INFO - Precision: 0.7143
2025-05-01 03:40:44,672 - main - INFO - Recall: 0.7692
2025-05-01 03:40:44,672 - main - INFO - F1 Score: 0.7407
2025-05-01 03:40:44,673 - main - INFO - IoU: 0.5882
2025-05-01 03:40:44,673 - main - INFO - mAP: 0.8263
2025-05-01 03:40:44,673 - main - INFO - AUC: 0.9073
2025-05-01 03:40:44,673 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:40:44,673 - main - INFO - 
==================================================
2025-05-01 03:40:44,673 - main - INFO - Running configuration 146/756:
2025-05-01 03:40:44,673 - main - INFO - Model: ResNet101
2025-05-01 03:40:44,673 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:40:44,673 - main - INFO - Scheduler: StepLR
2025-05-01 03:40:44,673 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:40:44,673 - main - INFO - ==================================================
2025-05-01 03:40:44,673 - training.model_ResNet101_opt_Adam_lr_0.01_id_146 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_146
2025-05-01 03:40:44,673 - training.model_ResNet101_opt_Adam_lr_0.01_id_146 - INFO - Config: {
  "id": 146,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:40:44,937 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:40:44,937 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 146,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:40:44,937 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:40:45,086 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:40:45,087 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:40:45,091 - training.model_ResNet101_opt_Adam_lr_0.01_id_146 - INFO - Starting model evaluation
2025-05-01 03:40:54,355 - training.model_ResNet101_opt_Adam_lr_0.01_id_146 - INFO - Evaluation metrics:
  Accuracy:  0.8533
  Precision: 0.7240
  Recall:    0.7797
  F1 Score:  0.7508
  IoU:       0.6011
  mAP:       0.8329
  AUC:       0.9086
2025-05-01 03:40:54,357 - training.model_ResNet101_opt_Adam_lr_0.01_id_146 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_146/final_results.json
2025-05-01 03:40:54,359 - training.model_ResNet101_opt_Adam_lr_0.01_id_146 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_146/final_results.json
2025-05-01 03:40:54,359 - main - INFO - 
Summary for configuration 146:
2025-05-01 03:40:54,359 - main - INFO - Accuracy: 0.8533
2025-05-01 03:40:54,359 - main - INFO - Precision: 0.7240
2025-05-01 03:40:54,359 - main - INFO - Recall: 0.7797
2025-05-01 03:40:54,359 - main - INFO - F1 Score: 0.7508
2025-05-01 03:40:54,359 - main - INFO - IoU: 0.6011
2025-05-01 03:40:54,359 - main - INFO - mAP: 0.8329
2025-05-01 03:40:54,359 - main - INFO - AUC: 0.9086
2025-05-01 03:40:54,359 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:40:54,359 - main - INFO - 
==================================================
2025-05-01 03:40:54,359 - main - INFO - Running configuration 147/756:
2025-05-01 03:40:54,359 - main - INFO - Model: ResNet101
2025-05-01 03:40:54,359 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:40:54,359 - main - INFO - Scheduler: StepLR
2025-05-01 03:40:54,359 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:40:54,359 - main - INFO - ==================================================
2025-05-01 03:40:54,359 - training.model_ResNet101_opt_Adam_lr_0.01_id_147 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_147
2025-05-01 03:40:54,359 - training.model_ResNet101_opt_Adam_lr_0.01_id_147 - INFO - Config: {
  "id": 147,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:40:54,622 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:40:54,622 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 147,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:40:54,622 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:40:54,767 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:40:54,769 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:40:54,772 - training.model_ResNet101_opt_Adam_lr_0.01_id_147 - INFO - Starting model evaluation
2025-05-01 03:41:04,306 - training.model_ResNet101_opt_Adam_lr_0.01_id_147 - INFO - Evaluation metrics:
  Accuracy:  0.8523
  Precision: 0.7261
  Recall:    0.7692
  F1 Score:  0.7470
  IoU:       0.5962
  mAP:       0.8318
  AUC:       0.9118
2025-05-01 03:41:04,308 - training.model_ResNet101_opt_Adam_lr_0.01_id_147 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_147/final_results.json
2025-05-01 03:41:04,310 - training.model_ResNet101_opt_Adam_lr_0.01_id_147 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_147/final_results.json
2025-05-01 03:41:04,310 - main - INFO - 
Summary for configuration 147:
2025-05-01 03:41:04,310 - main - INFO - Accuracy: 0.8523
2025-05-01 03:41:04,310 - main - INFO - Precision: 0.7261
2025-05-01 03:41:04,310 - main - INFO - Recall: 0.7692
2025-05-01 03:41:04,310 - main - INFO - F1 Score: 0.7470
2025-05-01 03:41:04,310 - main - INFO - IoU: 0.5962
2025-05-01 03:41:04,310 - main - INFO - mAP: 0.8318
2025-05-01 03:41:04,310 - main - INFO - AUC: 0.9118
2025-05-01 03:41:04,310 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:41:04,310 - main - INFO - 
==================================================
2025-05-01 03:41:04,310 - main - INFO - Running configuration 148/756:
2025-05-01 03:41:04,310 - main - INFO - Model: ResNet101
2025-05-01 03:41:04,310 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:41:04,310 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:41:04,310 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:41:04,310 - main - INFO - ==================================================
2025-05-01 03:41:04,310 - training.model_ResNet101_opt_Adam_lr_0.01_id_148 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_148
2025-05-01 03:41:04,310 - training.model_ResNet101_opt_Adam_lr_0.01_id_148 - INFO - Config: {
  "id": 148,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:41:04,561 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:41:04,561 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 148,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:41:04,561 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:41:04,706 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:41:04,708 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:41:04,711 - training.model_ResNet101_opt_Adam_lr_0.01_id_148 - INFO - Starting model evaluation
2025-05-01 03:41:14,319 - training.model_ResNet101_opt_Adam_lr_0.01_id_148 - INFO - Evaluation metrics:
  Accuracy:  0.8513
  Precision: 0.7282
  Recall:    0.7587
  F1 Score:  0.7432
  IoU:       0.5913
  mAP:       0.8312
  AUC:       0.9071
2025-05-01 03:41:14,324 - training.model_ResNet101_opt_Adam_lr_0.01_id_148 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_148/final_results.json
2025-05-01 03:41:14,325 - training.model_ResNet101_opt_Adam_lr_0.01_id_148 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_148/final_results.json
2025-05-01 03:41:14,325 - main - INFO - 
Summary for configuration 148:
2025-05-01 03:41:14,325 - main - INFO - Accuracy: 0.8513
2025-05-01 03:41:14,325 - main - INFO - Precision: 0.7282
2025-05-01 03:41:14,325 - main - INFO - Recall: 0.7587
2025-05-01 03:41:14,325 - main - INFO - F1 Score: 0.7432
2025-05-01 03:41:14,325 - main - INFO - IoU: 0.5913
2025-05-01 03:41:14,325 - main - INFO - mAP: 0.8312
2025-05-01 03:41:14,325 - main - INFO - AUC: 0.9071
2025-05-01 03:41:14,325 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:41:14,325 - main - INFO - 
==================================================
2025-05-01 03:41:14,325 - main - INFO - Running configuration 149/756:
2025-05-01 03:41:14,325 - main - INFO - Model: ResNet101
2025-05-01 03:41:14,325 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:41:14,325 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:41:14,325 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:41:14,325 - main - INFO - ==================================================
2025-05-01 03:41:14,326 - training.model_ResNet101_opt_Adam_lr_0.01_id_149 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_149
2025-05-01 03:41:14,326 - training.model_ResNet101_opt_Adam_lr_0.01_id_149 - INFO - Config: {
  "id": 149,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:41:14,584 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:41:14,584 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 149,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:41:14,585 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:41:14,730 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:41:14,732 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:41:14,735 - training.model_ResNet101_opt_Adam_lr_0.01_id_149 - INFO - Starting model evaluation
2025-05-01 03:41:24,312 - training.model_ResNet101_opt_Adam_lr_0.01_id_149 - INFO - Evaluation metrics:
  Accuracy:  0.8464
  Precision: 0.7079
  Recall:    0.7797
  F1 Score:  0.7421
  IoU:       0.5899
  mAP:       0.8321
  AUC:       0.9111
2025-05-01 03:41:24,313 - training.model_ResNet101_opt_Adam_lr_0.01_id_149 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_149/final_results.json
2025-05-01 03:41:24,315 - training.model_ResNet101_opt_Adam_lr_0.01_id_149 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_149/final_results.json
2025-05-01 03:41:24,315 - main - INFO - 
Summary for configuration 149:
2025-05-01 03:41:24,315 - main - INFO - Accuracy: 0.8464
2025-05-01 03:41:24,315 - main - INFO - Precision: 0.7079
2025-05-01 03:41:24,315 - main - INFO - Recall: 0.7797
2025-05-01 03:41:24,315 - main - INFO - F1 Score: 0.7421
2025-05-01 03:41:24,315 - main - INFO - IoU: 0.5899
2025-05-01 03:41:24,315 - main - INFO - mAP: 0.8321
2025-05-01 03:41:24,315 - main - INFO - AUC: 0.9111
2025-05-01 03:41:24,315 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:41:24,315 - main - INFO - 
==================================================
2025-05-01 03:41:24,315 - main - INFO - Running configuration 150/756:
2025-05-01 03:41:24,315 - main - INFO - Model: ResNet101
2025-05-01 03:41:24,315 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:41:24,315 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 03:41:24,315 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:41:24,315 - main - INFO - ==================================================
2025-05-01 03:41:24,315 - training.model_ResNet101_opt_Adam_lr_0.01_id_150 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_150
2025-05-01 03:41:24,315 - training.model_ResNet101_opt_Adam_lr_0.01_id_150 - INFO - Config: {
  "id": 150,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:41:24,643 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:41:24,643 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 150,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:41:24,643 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:41:24,787 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:41:24,788 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:41:24,794 - training.model_ResNet101_opt_Adam_lr_0.01_id_150 - INFO - Starting model evaluation
2025-05-01 03:41:34,494 - training.model_ResNet101_opt_Adam_lr_0.01_id_150 - INFO - Evaluation metrics:
  Accuracy:  0.8523
  Precision: 0.7217
  Recall:    0.7797
  F1 Score:  0.7496
  IoU:       0.5995
  mAP:       0.8295
  AUC:       0.9073
2025-05-01 03:41:34,496 - training.model_ResNet101_opt_Adam_lr_0.01_id_150 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_150/final_results.json
2025-05-01 03:41:34,497 - training.model_ResNet101_opt_Adam_lr_0.01_id_150 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_150/final_results.json
2025-05-01 03:41:34,497 - main - INFO - 
Summary for configuration 150:
2025-05-01 03:41:34,497 - main - INFO - Accuracy: 0.8523
2025-05-01 03:41:34,497 - main - INFO - Precision: 0.7217
2025-05-01 03:41:34,497 - main - INFO - Recall: 0.7797
2025-05-01 03:41:34,497 - main - INFO - F1 Score: 0.7496
2025-05-01 03:41:34,497 - main - INFO - IoU: 0.5995
2025-05-01 03:41:34,498 - main - INFO - mAP: 0.8295
2025-05-01 03:41:34,498 - main - INFO - AUC: 0.9073
2025-05-01 03:41:34,498 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:41:34,498 - main - INFO - 
==================================================
2025-05-01 03:41:34,498 - main - INFO - Running configuration 151/756:
2025-05-01 03:41:34,498 - main - INFO - Model: ResNet101
2025-05-01 03:41:34,498 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:41:34,498 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:41:34,498 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:41:34,498 - main - INFO - ==================================================
2025-05-01 03:41:34,498 - training.model_ResNet101_opt_Adam_lr_0.01_id_151 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_151
2025-05-01 03:41:34,498 - training.model_ResNet101_opt_Adam_lr_0.01_id_151 - INFO - Config: {
  "id": 151,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:41:34,748 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:41:34,748 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 151,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:41:34,748 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:41:34,888 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:41:34,889 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:41:34,893 - training.model_ResNet101_opt_Adam_lr_0.01_id_151 - INFO - Starting model evaluation
2025-05-01 03:41:44,237 - training.model_ResNet101_opt_Adam_lr_0.01_id_151 - INFO - Evaluation metrics:
  Accuracy:  0.8494
  Precision: 0.7161
  Recall:    0.7762
  F1 Score:  0.7450
  IoU:       0.5936
  mAP:       0.8297
  AUC:       0.9056
2025-05-01 03:41:44,239 - training.model_ResNet101_opt_Adam_lr_0.01_id_151 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_151/final_results.json
2025-05-01 03:41:44,240 - training.model_ResNet101_opt_Adam_lr_0.01_id_151 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_151/final_results.json
2025-05-01 03:41:44,240 - main - INFO - 
Summary for configuration 151:
2025-05-01 03:41:44,240 - main - INFO - Accuracy: 0.8494
2025-05-01 03:41:44,240 - main - INFO - Precision: 0.7161
2025-05-01 03:41:44,240 - main - INFO - Recall: 0.7762
2025-05-01 03:41:44,240 - main - INFO - F1 Score: 0.7450
2025-05-01 03:41:44,240 - main - INFO - IoU: 0.5936
2025-05-01 03:41:44,240 - main - INFO - mAP: 0.8297
2025-05-01 03:41:44,240 - main - INFO - AUC: 0.9056
2025-05-01 03:41:44,240 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:41:44,240 - main - INFO - 
==================================================
2025-05-01 03:41:44,241 - main - INFO - Running configuration 152/756:
2025-05-01 03:41:44,241 - main - INFO - Model: ResNet101
2025-05-01 03:41:44,241 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:41:44,241 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:41:44,241 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:41:44,241 - main - INFO - ==================================================
2025-05-01 03:41:44,241 - training.model_ResNet101_opt_Adam_lr_0.01_id_152 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_152
2025-05-01 03:41:44,241 - training.model_ResNet101_opt_Adam_lr_0.01_id_152 - INFO - Config: {
  "id": 152,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:41:44,490 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:41:44,490 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 152,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:41:44,491 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:41:44,630 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:41:44,631 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:41:44,635 - training.model_ResNet101_opt_Adam_lr_0.01_id_152 - INFO - Starting model evaluation
2025-05-01 03:41:54,011 - training.model_ResNet101_opt_Adam_lr_0.01_id_152 - INFO - Evaluation metrics:
  Accuracy:  0.8484
  Precision: 0.7166
  Recall:    0.7692
  F1 Score:  0.7420
  IoU:       0.5898
  mAP:       0.8313
  AUC:       0.9084
2025-05-01 03:41:54,014 - training.model_ResNet101_opt_Adam_lr_0.01_id_152 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_152/final_results.json
2025-05-01 03:41:54,016 - training.model_ResNet101_opt_Adam_lr_0.01_id_152 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_152/final_results.json
2025-05-01 03:41:54,016 - main - INFO - 
Summary for configuration 152:
2025-05-01 03:41:54,016 - main - INFO - Accuracy: 0.8484
2025-05-01 03:41:54,016 - main - INFO - Precision: 0.7166
2025-05-01 03:41:54,016 - main - INFO - Recall: 0.7692
2025-05-01 03:41:54,016 - main - INFO - F1 Score: 0.7420
2025-05-01 03:41:54,016 - main - INFO - IoU: 0.5898
2025-05-01 03:41:54,016 - main - INFO - mAP: 0.8313
2025-05-01 03:41:54,016 - main - INFO - AUC: 0.9084
2025-05-01 03:41:54,016 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:41:54,016 - main - INFO - 
==================================================
2025-05-01 03:41:54,016 - main - INFO - Running configuration 153/756:
2025-05-01 03:41:54,016 - main - INFO - Model: ResNet101
2025-05-01 03:41:54,016 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:41:54,016 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 03:41:54,016 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:41:54,016 - main - INFO - ==================================================
2025-05-01 03:41:54,016 - training.model_ResNet101_opt_Adam_lr_0.01_id_153 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_153
2025-05-01 03:41:54,016 - training.model_ResNet101_opt_Adam_lr_0.01_id_153 - INFO - Config: {
  "id": 153,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:41:54,279 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:41:54,279 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 153,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:41:54,279 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:41:54,419 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:41:54,421 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:41:54,424 - training.model_ResNet101_opt_Adam_lr_0.01_id_153 - INFO - Starting model evaluation
2025-05-01 03:42:04,058 - training.model_ResNet101_opt_Adam_lr_0.01_id_153 - INFO - Evaluation metrics:
  Accuracy:  0.8454
  Precision: 0.7138
  Recall:    0.7587
  F1 Score:  0.7356
  IoU:       0.5818
  mAP:       0.8279
  AUC:       0.9084
2025-05-01 03:42:04,059 - training.model_ResNet101_opt_Adam_lr_0.01_id_153 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_153/final_results.json
2025-05-01 03:42:04,061 - training.model_ResNet101_opt_Adam_lr_0.01_id_153 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_153/final_results.json
2025-05-01 03:42:04,061 - main - INFO - 
Summary for configuration 153:
2025-05-01 03:42:04,061 - main - INFO - Accuracy: 0.8454
2025-05-01 03:42:04,061 - main - INFO - Precision: 0.7138
2025-05-01 03:42:04,061 - main - INFO - Recall: 0.7587
2025-05-01 03:42:04,061 - main - INFO - F1 Score: 0.7356
2025-05-01 03:42:04,061 - main - INFO - IoU: 0.5818
2025-05-01 03:42:04,061 - main - INFO - mAP: 0.8279
2025-05-01 03:42:04,061 - main - INFO - AUC: 0.9084
2025-05-01 03:42:04,061 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:42:04,061 - main - INFO - 
==================================================
2025-05-01 03:42:04,061 - main - INFO - Running configuration 154/756:
2025-05-01 03:42:04,061 - main - INFO - Model: ResNet101
2025-05-01 03:42:04,061 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:42:04,061 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:42:04,061 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:42:04,061 - main - INFO - ==================================================
2025-05-01 03:42:04,061 - training.model_ResNet101_opt_Adam_lr_0.01_id_154 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_154
2025-05-01 03:42:04,062 - training.model_ResNet101_opt_Adam_lr_0.01_id_154 - INFO - Config: {
  "id": 154,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:42:04,325 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:42:04,326 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 154,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:42:04,326 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:42:04,467 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:42:04,469 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:42:04,472 - training.model_ResNet101_opt_Adam_lr_0.01_id_154 - INFO - Starting model evaluation
2025-05-01 03:42:14,105 - training.model_ResNet101_opt_Adam_lr_0.01_id_154 - INFO - Evaluation metrics:
  Accuracy:  0.8484
  Precision: 0.7209
  Recall:    0.7587
  F1 Score:  0.7394
  IoU:       0.5865
  mAP:       0.8268
  AUC:       0.9079
2025-05-01 03:42:14,107 - training.model_ResNet101_opt_Adam_lr_0.01_id_154 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_154/final_results.json
2025-05-01 03:42:14,109 - training.model_ResNet101_opt_Adam_lr_0.01_id_154 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_154/final_results.json
2025-05-01 03:42:14,109 - main - INFO - 
Summary for configuration 154:
2025-05-01 03:42:14,109 - main - INFO - Accuracy: 0.8484
2025-05-01 03:42:14,109 - main - INFO - Precision: 0.7209
2025-05-01 03:42:14,109 - main - INFO - Recall: 0.7587
2025-05-01 03:42:14,109 - main - INFO - F1 Score: 0.7394
2025-05-01 03:42:14,109 - main - INFO - IoU: 0.5865
2025-05-01 03:42:14,109 - main - INFO - mAP: 0.8268
2025-05-01 03:42:14,109 - main - INFO - AUC: 0.9079
2025-05-01 03:42:14,109 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:42:14,109 - main - INFO - 
==================================================
2025-05-01 03:42:14,109 - main - INFO - Running configuration 155/756:
2025-05-01 03:42:14,109 - main - INFO - Model: ResNet101
2025-05-01 03:42:14,109 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:42:14,109 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:42:14,109 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:42:14,109 - main - INFO - ==================================================
2025-05-01 03:42:14,109 - training.model_ResNet101_opt_Adam_lr_0.01_id_155 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_155
2025-05-01 03:42:14,109 - training.model_ResNet101_opt_Adam_lr_0.01_id_155 - INFO - Config: {
  "id": 155,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:42:14,365 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:42:14,365 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 155,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 03:42:14,365 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:42:14,506 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:42:14,507 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:42:14,511 - training.model_ResNet101_opt_Adam_lr_0.01_id_155 - INFO - Starting model evaluation
2025-05-01 03:42:24,445 - training.model_ResNet101_opt_Adam_lr_0.01_id_155 - INFO - Evaluation metrics:
  Accuracy:  0.8434
  Precision: 0.7078
  Recall:    0.7622
  F1 Score:  0.7340
  IoU:       0.5798
  mAP:       0.8230
  AUC:       0.9027
2025-05-01 03:42:24,447 - training.model_ResNet101_opt_Adam_lr_0.01_id_155 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_155/final_results.json
2025-05-01 03:42:24,449 - training.model_ResNet101_opt_Adam_lr_0.01_id_155 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_155/final_results.json
2025-05-01 03:42:24,449 - main - INFO - 
Summary for configuration 155:
2025-05-01 03:42:24,449 - main - INFO - Accuracy: 0.8434
2025-05-01 03:42:24,449 - main - INFO - Precision: 0.7078
2025-05-01 03:42:24,449 - main - INFO - Recall: 0.7622
2025-05-01 03:42:24,449 - main - INFO - F1 Score: 0.7340
2025-05-01 03:42:24,449 - main - INFO - IoU: 0.5798
2025-05-01 03:42:24,449 - main - INFO - mAP: 0.8230
2025-05-01 03:42:24,449 - main - INFO - AUC: 0.9027
2025-05-01 03:42:24,449 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:42:24,449 - main - INFO - 
==================================================
2025-05-01 03:42:24,449 - main - INFO - Running configuration 156/756:
2025-05-01 03:42:24,449 - main - INFO - Model: ResNet101
2025-05-01 03:42:24,449 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 03:42:24,449 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 03:42:24,449 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 03:42:24,449 - main - INFO - ==================================================
2025-05-01 03:42:24,450 - training.model_ResNet101_opt_Adam_lr_0.01_id_156 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01_id_156
2025-05-01 03:42:24,450 - training.model_ResNet101_opt_Adam_lr_0.01_id_156 - INFO - Config: {
  "id": 156,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:42:24,714 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.01
2025-05-01 03:42:24,714 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 156,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 03:42:24,714 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 03:42:24,853 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8242
2025-05-01 03:42:24,855 - training.model_ResNet101_opt_Adam_lr_0.01 - INFO - Training completed after 1003.63 seconds
2025-05-01 03:42:24,858 - training.model_ResNet101_opt_Adam_lr_0.01_id_156 - INFO - Starting model evaluation
2025-05-01 03:42:34,295 - training.model_ResNet101_opt_Adam_lr_0.01_id_156 - INFO - Evaluation metrics:
  Accuracy:  0.8513
  Precision: 0.7237
  Recall:    0.7692
  F1 Score:  0.7458
  IoU:       0.5946
  mAP:       0.8313
  AUC:       0.9066
2025-05-01 03:42:34,297 - training.model_ResNet101_opt_Adam_lr_0.01_id_156 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_156/final_results.json
2025-05-01 03:42:34,298 - training.model_ResNet101_opt_Adam_lr_0.01_id_156 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.01_id_156/final_results.json
2025-05-01 03:42:34,298 - main - INFO - 
Summary for configuration 156:
2025-05-01 03:42:34,298 - main - INFO - Accuracy: 0.8513
2025-05-01 03:42:34,298 - main - INFO - Precision: 0.7237
2025-05-01 03:42:34,298 - main - INFO - Recall: 0.7692
2025-05-01 03:42:34,298 - main - INFO - F1 Score: 0.7458
2025-05-01 03:42:34,298 - main - INFO - IoU: 0.5946
2025-05-01 03:42:34,298 - main - INFO - mAP: 0.8313
2025-05-01 03:42:34,298 - main - INFO - AUC: 0.9066
2025-05-01 03:42:34,298 - main - INFO - Training time: 1003.63 seconds
2025-05-01 03:42:34,298 - main - INFO - 
==================================================
2025-05-01 03:42:34,298 - main - INFO - Running configuration 157/756:
2025-05-01 03:42:34,298 - main - INFO - Model: ResNet101
2025-05-01 03:42:34,298 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 03:42:34,298 - main - INFO - Scheduler: StepLR
2025-05-01 03:42:34,299 - main - INFO - Loss Function: CrossEntropy
2025-05-01 03:42:34,299 - main - INFO - ==================================================
2025-05-01 03:42:34,299 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_157
2025-05-01 03:42:34,299 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Config: {
  "id": 157,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:42:34,549 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 03:42:34,550 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 157,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 03:42:34,550 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 03:42:34,551 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 03:42:55,379 - training.model_ResNet101_opt_Adam_lr_0.001 - ERROR - Training interrupted: [enforce fail at inline_container.cc:626] . unexpected pos 38637440 vs 38637332
2025-05-01 03:42:55,379 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 03:42:55,418 - main - ERROR - Error running configuration 157: [enforce fail at inline_container.cc:626] . unexpected pos 38637376 vs 38637268
2025-05-01 03:42:55,418 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 944, in save
    _save(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 1216, in _save
    zip_file.write_record(name, storage, num_bytes)
RuntimeError: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/1122: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1193, in train_model_with_resume
    manager.save_checkpoint(
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 981, in save_checkpoint
    torch.save(checkpoint, self.checkpoint_path)
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 784, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:626] . unexpected pos 38637440 vs 38637332

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 944, in save
    _save(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 1216, in _save
    zip_file.write_record(name, storage, num_bytes)
RuntimeError: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/1122: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1279, in train_model_with_resume
    manager.save_checkpoint(model, optimizer, scheduler, epoch)
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 981, in save_checkpoint
    torch.save(checkpoint, self.checkpoint_path)
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 784, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:626] . unexpected pos 38637376 vs 38637268
2025-05-01 03:42:55,427 - main - INFO - 
==================================================
2025-05-01 03:42:55,427 - main - INFO - Running configuration 158/756:
2025-05-01 03:42:55,427 - main - INFO - Model: ResNet101
2025-05-01 03:42:55,427 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 03:42:55,427 - main - INFO - Scheduler: StepLR
2025-05-01 03:42:55,427 - main - INFO - Loss Function: FocalLoss
2025-05-01 03:42:55,427 - main - INFO - ==================================================
2025-05-01 05:12:52,212 - main - INFO - Starting training with device: cuda
2025-05-01 05:12:52,281 - main - INFO - Class distribution:
2025-05-01 05:12:52,281 - main - INFO - Training set: Counter({1: 2336, 0: 2326})
2025-05-01 05:12:52,281 - main - INFO - Validation set: Counter({0: 588, 1: 578})
2025-05-01 05:12:52,281 - main - INFO - Test set: Counter({0: 723, 1: 286})
2025-05-01 05:12:53,446 - main - INFO - Experiment status: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156] completed, 0 in progress, 600 pending
2025-05-01 05:12:53,446 - main - INFO - Configuration 1 already completed, skipping
2025-05-01 05:12:53,446 - main - INFO - Configuration 2 already completed, skipping
2025-05-01 05:12:53,446 - main - INFO - Configuration 3 already completed, skipping
2025-05-01 05:12:53,447 - main - INFO - Configuration 4 already completed, skipping
2025-05-01 05:12:53,447 - main - INFO - Configuration 5 already completed, skipping
2025-05-01 05:12:53,447 - main - INFO - Configuration 6 already completed, skipping
2025-05-01 05:12:53,447 - main - INFO - Configuration 7 already completed, skipping
2025-05-01 05:12:53,447 - main - INFO - Configuration 8 already completed, skipping
2025-05-01 05:12:53,448 - main - INFO - Configuration 9 already completed, skipping
2025-05-01 05:12:53,448 - main - INFO - Configuration 10 already completed, skipping
2025-05-01 05:12:53,448 - main - INFO - Configuration 11 already completed, skipping
2025-05-01 05:12:53,448 - main - INFO - Configuration 12 already completed, skipping
2025-05-01 05:12:53,449 - main - INFO - Configuration 13 already completed, skipping
2025-05-01 05:12:53,449 - main - INFO - Configuration 14 already completed, skipping
2025-05-01 05:12:53,449 - main - INFO - Configuration 15 already completed, skipping
2025-05-01 05:12:53,449 - main - INFO - Configuration 16 already completed, skipping
2025-05-01 05:12:53,450 - main - INFO - Configuration 17 already completed, skipping
2025-05-01 05:12:53,450 - main - INFO - Configuration 18 already completed, skipping
2025-05-01 05:12:53,450 - main - INFO - Configuration 19 already completed, skipping
2025-05-01 05:12:53,450 - main - INFO - Configuration 20 already completed, skipping
2025-05-01 05:12:53,450 - main - INFO - Configuration 21 already completed, skipping
2025-05-01 05:12:53,451 - main - INFO - Configuration 22 already completed, skipping
2025-05-01 05:12:53,451 - main - INFO - Configuration 23 already completed, skipping
2025-05-01 05:12:53,451 - main - INFO - Configuration 24 already completed, skipping
2025-05-01 05:12:53,451 - main - INFO - Configuration 25 already completed, skipping
2025-05-01 05:12:53,452 - main - INFO - Configuration 26 already completed, skipping
2025-05-01 05:12:53,452 - main - INFO - Configuration 27 already completed, skipping
2025-05-01 05:12:53,452 - main - INFO - Configuration 28 already completed, skipping
2025-05-01 05:12:53,452 - main - INFO - Configuration 29 already completed, skipping
2025-05-01 05:12:53,452 - main - INFO - Configuration 30 already completed, skipping
2025-05-01 05:12:53,453 - main - INFO - Configuration 31 already completed, skipping
2025-05-01 05:12:53,453 - main - INFO - Configuration 32 already completed, skipping
2025-05-01 05:12:53,453 - main - INFO - Configuration 33 already completed, skipping
2025-05-01 05:12:53,453 - main - INFO - Configuration 34 already completed, skipping
2025-05-01 05:12:53,454 - main - INFO - Configuration 35 already completed, skipping
2025-05-01 05:12:53,454 - main - INFO - Configuration 36 already completed, skipping
2025-05-01 05:12:53,454 - main - INFO - Configuration 37 already completed, skipping
2025-05-01 05:12:53,454 - main - INFO - Configuration 38 already completed, skipping
2025-05-01 05:12:53,454 - main - INFO - Configuration 39 already completed, skipping
2025-05-01 05:12:53,455 - main - INFO - Configuration 40 already completed, skipping
2025-05-01 05:12:53,455 - main - INFO - Configuration 41 already completed, skipping
2025-05-01 05:12:53,455 - main - INFO - Configuration 42 already completed, skipping
2025-05-01 05:12:53,455 - main - INFO - Configuration 43 already completed, skipping
2025-05-01 05:12:53,456 - main - INFO - Configuration 44 already completed, skipping
2025-05-01 05:12:53,456 - main - INFO - Configuration 45 already completed, skipping
2025-05-01 05:12:53,456 - main - INFO - Configuration 46 already completed, skipping
2025-05-01 05:12:53,456 - main - INFO - Configuration 47 already completed, skipping
2025-05-01 05:12:53,456 - main - INFO - Configuration 48 already completed, skipping
2025-05-01 05:12:53,457 - main - INFO - Configuration 49 already completed, skipping
2025-05-01 05:12:53,457 - main - INFO - Configuration 50 already completed, skipping
2025-05-01 05:12:53,457 - main - INFO - Configuration 51 already completed, skipping
2025-05-01 05:12:53,457 - main - INFO - Configuration 52 already completed, skipping
2025-05-01 05:12:53,458 - main - INFO - Configuration 53 already completed, skipping
2025-05-01 05:12:53,458 - main - INFO - Configuration 54 already completed, skipping
2025-05-01 05:12:53,458 - main - INFO - Configuration 55 already completed, skipping
2025-05-01 05:12:53,458 - main - INFO - Configuration 56 already completed, skipping
2025-05-01 05:12:53,459 - main - INFO - Configuration 57 already completed, skipping
2025-05-01 05:12:53,459 - main - INFO - Configuration 58 already completed, skipping
2025-05-01 05:12:53,459 - main - INFO - Configuration 59 already completed, skipping
2025-05-01 05:12:53,460 - main - INFO - Configuration 60 already completed, skipping
2025-05-01 05:12:53,460 - main - INFO - Configuration 61 already completed, skipping
2025-05-01 05:12:53,460 - main - INFO - Configuration 62 already completed, skipping
2025-05-01 05:12:53,460 - main - INFO - Configuration 63 already completed, skipping
2025-05-01 05:12:53,460 - main - INFO - Configuration 64 already completed, skipping
2025-05-01 05:12:53,461 - main - INFO - Configuration 65 already completed, skipping
2025-05-01 05:12:53,461 - main - INFO - Configuration 66 already completed, skipping
2025-05-01 05:12:53,461 - main - INFO - Configuration 67 already completed, skipping
2025-05-01 05:12:53,461 - main - INFO - Configuration 68 already completed, skipping
2025-05-01 05:12:53,462 - main - INFO - Configuration 69 already completed, skipping
2025-05-01 05:12:53,462 - main - INFO - Configuration 70 already completed, skipping
2025-05-01 05:12:53,462 - main - INFO - Configuration 71 already completed, skipping
2025-05-01 05:12:53,462 - main - INFO - Configuration 72 already completed, skipping
2025-05-01 05:12:53,463 - main - INFO - Configuration 73 already completed, skipping
2025-05-01 05:12:53,463 - main - INFO - Configuration 74 already completed, skipping
2025-05-01 05:12:53,463 - main - INFO - Configuration 75 already completed, skipping
2025-05-01 05:12:53,463 - main - INFO - Configuration 76 already completed, skipping
2025-05-01 05:12:53,463 - main - INFO - Configuration 77 already completed, skipping
2025-05-01 05:12:53,464 - main - INFO - Configuration 78 already completed, skipping
2025-05-01 05:12:53,464 - main - INFO - Configuration 79 already completed, skipping
2025-05-01 05:12:53,464 - main - INFO - Configuration 80 already completed, skipping
2025-05-01 05:12:53,464 - main - INFO - Configuration 81 already completed, skipping
2025-05-01 05:12:53,464 - main - INFO - Configuration 82 already completed, skipping
2025-05-01 05:12:53,465 - main - INFO - Configuration 83 already completed, skipping
2025-05-01 05:12:53,465 - main - INFO - Configuration 84 already completed, skipping
2025-05-01 05:12:53,465 - main - INFO - Configuration 85 already completed, skipping
2025-05-01 05:12:53,465 - main - INFO - Configuration 86 already completed, skipping
2025-05-01 05:12:53,466 - main - INFO - Configuration 87 already completed, skipping
2025-05-01 05:12:53,466 - main - INFO - Configuration 88 already completed, skipping
2025-05-01 05:12:53,466 - main - INFO - Configuration 89 already completed, skipping
2025-05-01 05:12:53,466 - main - INFO - Configuration 90 already completed, skipping
2025-05-01 05:12:53,467 - main - INFO - Configuration 91 already completed, skipping
2025-05-01 05:12:53,467 - main - INFO - Configuration 92 already completed, skipping
2025-05-01 05:12:53,467 - main - INFO - Configuration 93 already completed, skipping
2025-05-01 05:12:53,467 - main - INFO - Configuration 94 already completed, skipping
2025-05-01 05:12:53,467 - main - INFO - Configuration 95 already completed, skipping
2025-05-01 05:12:53,468 - main - INFO - Configuration 96 already completed, skipping
2025-05-01 05:12:53,468 - main - INFO - Configuration 97 already completed, skipping
2025-05-01 05:12:53,468 - main - INFO - Configuration 98 already completed, skipping
2025-05-01 05:12:53,468 - main - INFO - Configuration 99 already completed, skipping
2025-05-01 05:12:53,469 - main - INFO - Configuration 100 already completed, skipping
2025-05-01 05:12:53,469 - main - INFO - Configuration 101 already completed, skipping
2025-05-01 05:12:53,469 - main - INFO - Configuration 102 already completed, skipping
2025-05-01 05:12:53,469 - main - INFO - Configuration 103 already completed, skipping
2025-05-01 05:12:53,470 - main - INFO - Configuration 104 already completed, skipping
2025-05-01 05:12:53,470 - main - INFO - Configuration 105 already completed, skipping
2025-05-01 05:12:53,470 - main - INFO - Configuration 106 already completed, skipping
2025-05-01 05:12:53,471 - main - INFO - Configuration 107 already completed, skipping
2025-05-01 05:12:53,471 - main - INFO - Configuration 108 already completed, skipping
2025-05-01 05:12:53,471 - main - INFO - Configuration 109 already completed, skipping
2025-05-01 05:12:53,471 - main - INFO - Configuration 110 already completed, skipping
2025-05-01 05:12:53,471 - main - INFO - Configuration 111 already completed, skipping
2025-05-01 05:12:53,472 - main - INFO - Configuration 112 already completed, skipping
2025-05-01 05:12:53,472 - main - INFO - Configuration 113 already completed, skipping
2025-05-01 05:12:53,472 - main - INFO - Configuration 114 already completed, skipping
2025-05-01 05:12:53,472 - main - INFO - Configuration 115 already completed, skipping
2025-05-01 05:12:53,473 - main - INFO - Configuration 116 already completed, skipping
2025-05-01 05:12:53,473 - main - INFO - Configuration 117 already completed, skipping
2025-05-01 05:12:53,473 - main - INFO - Configuration 118 already completed, skipping
2025-05-01 05:12:53,473 - main - INFO - Configuration 119 already completed, skipping
2025-05-01 05:12:53,473 - main - INFO - Configuration 120 already completed, skipping
2025-05-01 05:12:53,474 - main - INFO - Configuration 121 already completed, skipping
2025-05-01 05:12:53,474 - main - INFO - Configuration 122 already completed, skipping
2025-05-01 05:12:53,474 - main - INFO - Configuration 123 already completed, skipping
2025-05-01 05:12:53,474 - main - INFO - Configuration 124 already completed, skipping
2025-05-01 05:12:53,475 - main - INFO - Configuration 125 already completed, skipping
2025-05-01 05:12:53,475 - main - INFO - Configuration 126 already completed, skipping
2025-05-01 05:12:53,475 - main - INFO - Configuration 127 already completed, skipping
2025-05-01 05:12:53,475 - main - INFO - Configuration 128 already completed, skipping
2025-05-01 05:12:53,476 - main - INFO - Configuration 129 already completed, skipping
2025-05-01 05:12:53,476 - main - INFO - Configuration 130 already completed, skipping
2025-05-01 05:12:53,476 - main - INFO - Configuration 131 already completed, skipping
2025-05-01 05:12:53,476 - main - INFO - Configuration 132 already completed, skipping
2025-05-01 05:12:53,476 - main - INFO - Configuration 133 already completed, skipping
2025-05-01 05:12:53,477 - main - INFO - Configuration 134 already completed, skipping
2025-05-01 05:12:53,477 - main - INFO - Configuration 135 already completed, skipping
2025-05-01 05:12:53,477 - main - INFO - Configuration 136 already completed, skipping
2025-05-01 05:12:53,477 - main - INFO - Configuration 137 already completed, skipping
2025-05-01 05:12:53,478 - main - INFO - Configuration 138 already completed, skipping
2025-05-01 05:12:53,478 - main - INFO - Configuration 139 already completed, skipping
2025-05-01 05:12:53,478 - main - INFO - Configuration 140 already completed, skipping
2025-05-01 05:12:53,478 - main - INFO - Configuration 141 already completed, skipping
2025-05-01 05:12:53,478 - main - INFO - Configuration 142 already completed, skipping
2025-05-01 05:12:53,479 - main - INFO - Configuration 143 already completed, skipping
2025-05-01 05:12:53,479 - main - INFO - Configuration 144 already completed, skipping
2025-05-01 05:12:53,479 - main - INFO - Configuration 145 already completed, skipping
2025-05-01 05:12:53,479 - main - INFO - Configuration 146 already completed, skipping
2025-05-01 05:12:53,480 - main - INFO - Configuration 147 already completed, skipping
2025-05-01 05:12:53,480 - main - INFO - Configuration 148 already completed, skipping
2025-05-01 05:12:53,480 - main - INFO - Configuration 149 already completed, skipping
2025-05-01 05:12:53,480 - main - INFO - Configuration 150 already completed, skipping
2025-05-01 05:12:53,481 - main - INFO - Configuration 151 already completed, skipping
2025-05-01 05:12:53,481 - main - INFO - Configuration 152 already completed, skipping
2025-05-01 05:12:53,481 - main - INFO - Configuration 153 already completed, skipping
2025-05-01 05:12:53,481 - main - INFO - Configuration 154 already completed, skipping
2025-05-01 05:12:53,482 - main - INFO - Configuration 155 already completed, skipping
2025-05-01 05:12:53,482 - main - INFO - Configuration 156 already completed, skipping
2025-05-01 05:12:53,482 - main - INFO - 
==================================================
2025-05-01 05:12:53,482 - main - INFO - Running configuration 157/756:
2025-05-01 05:12:53,482 - main - INFO - Model: ResNet101
2025-05-01 05:12:53,482 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:12:53,482 - main - INFO - Scheduler: StepLR
2025-05-01 05:12:53,482 - main - INFO - Loss Function: CrossEntropy
2025-05-01 05:12:53,482 - main - INFO - ==================================================
2025-05-01 05:12:53,482 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_157
2025-05-01 05:12:53,482 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Config: {
  "id": 157,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:12:53,799 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:12:53,799 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 157,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:12:53,799 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:12:53,799 - training.model_ResNet101_opt_Adam_lr_0.001 - ERROR - Failed to load checkpoint: PytorchStreamReader failed reading zip archive: failed finding central directory
2025-05-01 05:12:53,800 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 05:13:19,943 - training.model_ResNet101_opt_Adam_lr_0.001 - ERROR - Training interrupted: [enforce fail at inline_container.cc:626] . unexpected pos 161743104 vs 161742996
2025-05-01 05:13:19,943 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 05:13:20,057 - main - ERROR - Error running configuration 157: [enforce fail at inline_container.cc:626] . unexpected pos 161743040 vs 161742932
2025-05-01 05:13:20,057 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 944, in save
    _save(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 1216, in _save
    zip_file.write_record(name, storage, num_bytes)
RuntimeError: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/1482: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1193, in train_model_with_resume
    manager.save_checkpoint(
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 981, in save_checkpoint
    torch.save(checkpoint, self.checkpoint_path)
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 784, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:626] . unexpected pos 161743104 vs 161742996

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 944, in save
    _save(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 1216, in _save
    zip_file.write_record(name, storage, num_bytes)
RuntimeError: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/1482: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1279, in train_model_with_resume
    manager.save_checkpoint(model, optimizer, scheduler, epoch)
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 981, in save_checkpoint
    torch.save(checkpoint, self.checkpoint_path)
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/serialization.py", line 784, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:626] . unexpected pos 161743040 vs 161742932
2025-05-01 05:13:20,095 - main - INFO - 
==================================================
2025-05-01 05:13:20,095 - main - INFO - Running configuration 158/756:
2025-05-01 05:13:20,095 - main - INFO - Model: ResNet101
2025-05-01 05:13:20,095 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:13:20,095 - main - INFO - Scheduler: StepLR
2025-05-01 05:13:20,095 - main - INFO - Loss Function: FocalLoss
2025-05-01 05:13:20,095 - main - INFO - ==================================================
2025-05-01 05:21:02,865 - main - INFO - Starting training with device: cuda
2025-05-01 05:21:02,947 - main - INFO - Class distribution:
2025-05-01 05:21:02,947 - main - INFO - Training set: Counter({1: 2336, 0: 2326})
2025-05-01 05:21:02,947 - main - INFO - Validation set: Counter({0: 588, 1: 578})
2025-05-01 05:21:02,947 - main - INFO - Test set: Counter({0: 723, 1: 286})
2025-05-01 05:21:04,153 - main - INFO - Experiment status: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156] completed, 0 in progress, 600 pending
2025-05-01 05:21:04,153 - main - INFO - Configuration 1 already completed, skipping
2025-05-01 05:21:04,153 - main - INFO - Configuration 2 already completed, skipping
2025-05-01 05:21:04,153 - main - INFO - Configuration 3 already completed, skipping
2025-05-01 05:21:04,153 - main - INFO - Configuration 4 already completed, skipping
2025-05-01 05:21:04,154 - main - INFO - Configuration 5 already completed, skipping
2025-05-01 05:21:04,154 - main - INFO - Configuration 6 already completed, skipping
2025-05-01 05:21:04,154 - main - INFO - Configuration 7 already completed, skipping
2025-05-01 05:21:04,154 - main - INFO - Configuration 8 already completed, skipping
2025-05-01 05:21:04,155 - main - INFO - Configuration 9 already completed, skipping
2025-05-01 05:21:04,155 - main - INFO - Configuration 10 already completed, skipping
2025-05-01 05:21:04,155 - main - INFO - Configuration 11 already completed, skipping
2025-05-01 05:21:04,155 - main - INFO - Configuration 12 already completed, skipping
2025-05-01 05:21:04,156 - main - INFO - Configuration 13 already completed, skipping
2025-05-01 05:21:04,156 - main - INFO - Configuration 14 already completed, skipping
2025-05-01 05:21:04,156 - main - INFO - Configuration 15 already completed, skipping
2025-05-01 05:21:04,156 - main - INFO - Configuration 16 already completed, skipping
2025-05-01 05:21:04,156 - main - INFO - Configuration 17 already completed, skipping
2025-05-01 05:21:04,157 - main - INFO - Configuration 18 already completed, skipping
2025-05-01 05:21:04,157 - main - INFO - Configuration 19 already completed, skipping
2025-05-01 05:21:04,157 - main - INFO - Configuration 20 already completed, skipping
2025-05-01 05:21:04,157 - main - INFO - Configuration 21 already completed, skipping
2025-05-01 05:21:04,158 - main - INFO - Configuration 22 already completed, skipping
2025-05-01 05:21:04,158 - main - INFO - Configuration 23 already completed, skipping
2025-05-01 05:21:04,158 - main - INFO - Configuration 24 already completed, skipping
2025-05-01 05:21:04,158 - main - INFO - Configuration 25 already completed, skipping
2025-05-01 05:21:04,158 - main - INFO - Configuration 26 already completed, skipping
2025-05-01 05:21:04,159 - main - INFO - Configuration 27 already completed, skipping
2025-05-01 05:21:04,159 - main - INFO - Configuration 28 already completed, skipping
2025-05-01 05:21:04,159 - main - INFO - Configuration 29 already completed, skipping
2025-05-01 05:21:04,159 - main - INFO - Configuration 30 already completed, skipping
2025-05-01 05:21:04,160 - main - INFO - Configuration 31 already completed, skipping
2025-05-01 05:21:04,160 - main - INFO - Configuration 32 already completed, skipping
2025-05-01 05:21:04,160 - main - INFO - Configuration 33 already completed, skipping
2025-05-01 05:21:04,160 - main - INFO - Configuration 34 already completed, skipping
2025-05-01 05:21:04,160 - main - INFO - Configuration 35 already completed, skipping
2025-05-01 05:21:04,161 - main - INFO - Configuration 36 already completed, skipping
2025-05-01 05:21:04,161 - main - INFO - Configuration 37 already completed, skipping
2025-05-01 05:21:04,161 - main - INFO - Configuration 38 already completed, skipping
2025-05-01 05:21:04,161 - main - INFO - Configuration 39 already completed, skipping
2025-05-01 05:21:04,162 - main - INFO - Configuration 40 already completed, skipping
2025-05-01 05:21:04,162 - main - INFO - Configuration 41 already completed, skipping
2025-05-01 05:21:04,162 - main - INFO - Configuration 42 already completed, skipping
2025-05-01 05:21:04,162 - main - INFO - Configuration 43 already completed, skipping
2025-05-01 05:21:04,162 - main - INFO - Configuration 44 already completed, skipping
2025-05-01 05:21:04,163 - main - INFO - Configuration 45 already completed, skipping
2025-05-01 05:21:04,163 - main - INFO - Configuration 46 already completed, skipping
2025-05-01 05:21:04,163 - main - INFO - Configuration 47 already completed, skipping
2025-05-01 05:21:04,163 - main - INFO - Configuration 48 already completed, skipping
2025-05-01 05:21:04,164 - main - INFO - Configuration 49 already completed, skipping
2025-05-01 05:21:04,164 - main - INFO - Configuration 50 already completed, skipping
2025-05-01 05:21:04,164 - main - INFO - Configuration 51 already completed, skipping
2025-05-01 05:21:04,164 - main - INFO - Configuration 52 already completed, skipping
2025-05-01 05:21:04,165 - main - INFO - Configuration 53 already completed, skipping
2025-05-01 05:21:04,165 - main - INFO - Configuration 54 already completed, skipping
2025-05-01 05:21:04,165 - main - INFO - Configuration 55 already completed, skipping
2025-05-01 05:21:04,165 - main - INFO - Configuration 56 already completed, skipping
2025-05-01 05:21:04,166 - main - INFO - Configuration 57 already completed, skipping
2025-05-01 05:21:04,166 - main - INFO - Configuration 58 already completed, skipping
2025-05-01 05:21:04,166 - main - INFO - Configuration 59 already completed, skipping
2025-05-01 05:21:04,167 - main - INFO - Configuration 60 already completed, skipping
2025-05-01 05:21:04,167 - main - INFO - Configuration 61 already completed, skipping
2025-05-01 05:21:04,167 - main - INFO - Configuration 62 already completed, skipping
2025-05-01 05:21:04,167 - main - INFO - Configuration 63 already completed, skipping
2025-05-01 05:21:04,168 - main - INFO - Configuration 64 already completed, skipping
2025-05-01 05:21:04,168 - main - INFO - Configuration 65 already completed, skipping
2025-05-01 05:21:04,168 - main - INFO - Configuration 66 already completed, skipping
2025-05-01 05:21:04,168 - main - INFO - Configuration 67 already completed, skipping
2025-05-01 05:21:04,168 - main - INFO - Configuration 68 already completed, skipping
2025-05-01 05:21:04,169 - main - INFO - Configuration 69 already completed, skipping
2025-05-01 05:21:04,169 - main - INFO - Configuration 70 already completed, skipping
2025-05-01 05:21:04,169 - main - INFO - Configuration 71 already completed, skipping
2025-05-01 05:21:04,169 - main - INFO - Configuration 72 already completed, skipping
2025-05-01 05:21:04,170 - main - INFO - Configuration 73 already completed, skipping
2025-05-01 05:21:04,170 - main - INFO - Configuration 74 already completed, skipping
2025-05-01 05:21:04,170 - main - INFO - Configuration 75 already completed, skipping
2025-05-01 05:21:04,170 - main - INFO - Configuration 76 already completed, skipping
2025-05-01 05:21:04,170 - main - INFO - Configuration 77 already completed, skipping
2025-05-01 05:21:04,171 - main - INFO - Configuration 78 already completed, skipping
2025-05-01 05:21:04,171 - main - INFO - Configuration 79 already completed, skipping
2025-05-01 05:21:04,171 - main - INFO - Configuration 80 already completed, skipping
2025-05-01 05:21:04,171 - main - INFO - Configuration 81 already completed, skipping
2025-05-01 05:21:04,171 - main - INFO - Configuration 82 already completed, skipping
2025-05-01 05:21:04,172 - main - INFO - Configuration 83 already completed, skipping
2025-05-01 05:21:04,172 - main - INFO - Configuration 84 already completed, skipping
2025-05-01 05:21:04,172 - main - INFO - Configuration 85 already completed, skipping
2025-05-01 05:21:04,172 - main - INFO - Configuration 86 already completed, skipping
2025-05-01 05:21:04,173 - main - INFO - Configuration 87 already completed, skipping
2025-05-01 05:21:04,173 - main - INFO - Configuration 88 already completed, skipping
2025-05-01 05:21:04,173 - main - INFO - Configuration 89 already completed, skipping
2025-05-01 05:21:04,173 - main - INFO - Configuration 90 already completed, skipping
2025-05-01 05:21:04,174 - main - INFO - Configuration 91 already completed, skipping
2025-05-01 05:21:04,174 - main - INFO - Configuration 92 already completed, skipping
2025-05-01 05:21:04,174 - main - INFO - Configuration 93 already completed, skipping
2025-05-01 05:21:04,174 - main - INFO - Configuration 94 already completed, skipping
2025-05-01 05:21:04,175 - main - INFO - Configuration 95 already completed, skipping
2025-05-01 05:21:04,175 - main - INFO - Configuration 96 already completed, skipping
2025-05-01 05:21:04,175 - main - INFO - Configuration 97 already completed, skipping
2025-05-01 05:21:04,175 - main - INFO - Configuration 98 already completed, skipping
2025-05-01 05:21:04,175 - main - INFO - Configuration 99 already completed, skipping
2025-05-01 05:21:04,176 - main - INFO - Configuration 100 already completed, skipping
2025-05-01 05:21:04,176 - main - INFO - Configuration 101 already completed, skipping
2025-05-01 05:21:04,176 - main - INFO - Configuration 102 already completed, skipping
2025-05-01 05:21:04,176 - main - INFO - Configuration 103 already completed, skipping
2025-05-01 05:21:04,177 - main - INFO - Configuration 104 already completed, skipping
2025-05-01 05:21:04,177 - main - INFO - Configuration 105 already completed, skipping
2025-05-01 05:21:04,177 - main - INFO - Configuration 106 already completed, skipping
2025-05-01 05:21:04,178 - main - INFO - Configuration 107 already completed, skipping
2025-05-01 05:21:04,178 - main - INFO - Configuration 108 already completed, skipping
2025-05-01 05:21:04,178 - main - INFO - Configuration 109 already completed, skipping
2025-05-01 05:21:04,178 - main - INFO - Configuration 110 already completed, skipping
2025-05-01 05:21:04,179 - main - INFO - Configuration 111 already completed, skipping
2025-05-01 05:21:04,179 - main - INFO - Configuration 112 already completed, skipping
2025-05-01 05:21:04,179 - main - INFO - Configuration 113 already completed, skipping
2025-05-01 05:21:04,179 - main - INFO - Configuration 114 already completed, skipping
2025-05-01 05:21:04,179 - main - INFO - Configuration 115 already completed, skipping
2025-05-01 05:21:04,180 - main - INFO - Configuration 116 already completed, skipping
2025-05-01 05:21:04,180 - main - INFO - Configuration 117 already completed, skipping
2025-05-01 05:21:04,180 - main - INFO - Configuration 118 already completed, skipping
2025-05-01 05:21:04,180 - main - INFO - Configuration 119 already completed, skipping
2025-05-01 05:21:04,181 - main - INFO - Configuration 120 already completed, skipping
2025-05-01 05:21:04,181 - main - INFO - Configuration 121 already completed, skipping
2025-05-01 05:21:04,181 - main - INFO - Configuration 122 already completed, skipping
2025-05-01 05:21:04,181 - main - INFO - Configuration 123 already completed, skipping
2025-05-01 05:21:04,182 - main - INFO - Configuration 124 already completed, skipping
2025-05-01 05:21:04,182 - main - INFO - Configuration 125 already completed, skipping
2025-05-01 05:21:04,182 - main - INFO - Configuration 126 already completed, skipping
2025-05-01 05:21:04,182 - main - INFO - Configuration 127 already completed, skipping
2025-05-01 05:21:04,182 - main - INFO - Configuration 128 already completed, skipping
2025-05-01 05:21:04,183 - main - INFO - Configuration 129 already completed, skipping
2025-05-01 05:21:04,183 - main - INFO - Configuration 130 already completed, skipping
2025-05-01 05:21:04,183 - main - INFO - Configuration 131 already completed, skipping
2025-05-01 05:21:04,183 - main - INFO - Configuration 132 already completed, skipping
2025-05-01 05:21:04,184 - main - INFO - Configuration 133 already completed, skipping
2025-05-01 05:21:04,184 - main - INFO - Configuration 134 already completed, skipping
2025-05-01 05:21:04,184 - main - INFO - Configuration 135 already completed, skipping
2025-05-01 05:21:04,184 - main - INFO - Configuration 136 already completed, skipping
2025-05-01 05:21:04,185 - main - INFO - Configuration 137 already completed, skipping
2025-05-01 05:21:04,185 - main - INFO - Configuration 138 already completed, skipping
2025-05-01 05:21:04,185 - main - INFO - Configuration 139 already completed, skipping
2025-05-01 05:21:04,185 - main - INFO - Configuration 140 already completed, skipping
2025-05-01 05:21:04,185 - main - INFO - Configuration 141 already completed, skipping
2025-05-01 05:21:04,186 - main - INFO - Configuration 142 already completed, skipping
2025-05-01 05:21:04,186 - main - INFO - Configuration 143 already completed, skipping
2025-05-01 05:21:04,186 - main - INFO - Configuration 144 already completed, skipping
2025-05-01 05:21:04,186 - main - INFO - Configuration 145 already completed, skipping
2025-05-01 05:21:04,187 - main - INFO - Configuration 146 already completed, skipping
2025-05-01 05:21:04,187 - main - INFO - Configuration 147 already completed, skipping
2025-05-01 05:21:04,187 - main - INFO - Configuration 148 already completed, skipping
2025-05-01 05:21:04,187 - main - INFO - Configuration 149 already completed, skipping
2025-05-01 05:21:04,188 - main - INFO - Configuration 150 already completed, skipping
2025-05-01 05:21:04,188 - main - INFO - Configuration 151 already completed, skipping
2025-05-01 05:21:04,188 - main - INFO - Configuration 152 already completed, skipping
2025-05-01 05:21:04,188 - main - INFO - Configuration 153 already completed, skipping
2025-05-01 05:21:04,189 - main - INFO - Configuration 154 already completed, skipping
2025-05-01 05:21:04,189 - main - INFO - Configuration 155 already completed, skipping
2025-05-01 05:21:04,189 - main - INFO - Configuration 156 already completed, skipping
2025-05-01 05:21:04,189 - main - INFO - 
==================================================
2025-05-01 05:21:04,189 - main - INFO - Running configuration 157/756:
2025-05-01 05:21:04,189 - main - INFO - Model: ResNet101
2025-05-01 05:21:04,189 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:21:04,189 - main - INFO - Scheduler: StepLR
2025-05-01 05:21:04,189 - main - INFO - Loss Function: CrossEntropy
2025-05-01 05:21:04,189 - main - INFO - ==================================================
2025-05-01 05:21:04,190 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_157
2025-05-01 05:21:04,190 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Config: {
  "id": 157,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:21:04,518 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:21:04,518 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 157,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:21:04,518 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:21:04,518 - training.model_ResNet101_opt_Adam_lr_0.001 - ERROR - Failed to load checkpoint: PytorchStreamReader failed reading zip archive: failed finding central directory
2025-05-01 05:21:04,519 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 05:21:31,720 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 05:22:07,893 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.6981
2025-05-01 05:22:07,992 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 1 completed in 63.47s - Train Loss: 0.6400, Train Acc: 0.6285, Val Loss: 0.5902, Val Acc: 0.6981
2025-05-01 05:22:08,384 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 05:22:08,385 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 05:22:36,105 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 05:23:11,694 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation loss improved from inf to 0.7242
2025-05-01 05:23:11,852 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 2 completed in 63.47s - Train Loss: 0.5860, Train Acc: 0.7115, Val Loss: 0.7242, Val Acc: 0.5815
2025-05-01 05:23:12,247 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 05:23:12,248 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 05:23:38,279 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 05:24:14,436 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.7242 to 0.6801
2025-05-01 05:24:14,585 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 3 completed in 62.34s - Train Loss: 0.6036, Train Acc: 0.6903, Val Loss: 0.6801, Val Acc: 0.6106
2025-05-01 05:24:14,982 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 05:24:14,982 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 05:24:41,371 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 05:25:17,543 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.6981 to 0.7461
2025-05-01 05:25:17,700 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 4 completed in 62.72s - Train Loss: 0.5963, Train Acc: 0.6961, Val Loss: 0.5477, Val Acc: 0.7461
2025-05-01 05:25:18,096 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 05:25:18,097 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 05:25:44,593 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 05:26:20,741 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.7461 to 0.7684
2025-05-01 05:26:20,886 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 5 completed in 62.79s - Train Loss: 0.5589, Train Acc: 0.7385, Val Loss: 0.5241, Val Acc: 0.7684
2025-05-01 05:26:21,279 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 05:26:21,280 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 05:26:47,858 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 05:27:23,785 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 6 completed in 62.50s - Train Loss: 0.5361, Train Acc: 0.7664, Val Loss: 0.7507, Val Acc: 0.5386
2025-05-01 05:27:24,208 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 05:27:24,209 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 05:27:50,656 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 05:28:25,942 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.7684 to 0.8045
2025-05-01 05:28:26,086 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 7 completed in 61.88s - Train Loss: 0.5119, Train Acc: 0.7876, Val Loss: 0.5001, Val Acc: 0.8045
2025-05-01 05:28:26,484 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 05:28:26,484 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 05:28:52,197 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 05:29:28,205 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.6801 to 0.5027
2025-05-01 05:29:28,346 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 8 completed in 61.86s - Train Loss: 0.5006, Train Acc: 0.7997, Val Loss: 0.5027, Val Acc: 0.7967
2025-05-01 05:29:28,738 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 05:29:28,739 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 05:29:55,289 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 05:30:30,727 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 9 completed in 61.99s - Train Loss: 0.4826, Train Acc: 0.8220, Val Loss: 0.5213, Val Acc: 0.7762
2025-05-01 05:30:31,139 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 05:30:31,140 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 05:30:57,530 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 05:31:33,299 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8045 to 0.8053
2025-05-01 05:31:33,447 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 10 completed in 62.31s - Train Loss: 0.4861, Train Acc: 0.8172, Val Loss: 0.4928, Val Acc: 0.8053
2025-05-01 05:31:33,845 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 05:31:33,845 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 05:32:00,222 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 05:32:36,201 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8053 to 0.8816
2025-05-01 05:32:36,360 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 11 completed in 62.51s - Train Loss: 0.4417, Train Acc: 0.8666, Val Loss: 0.4314, Val Acc: 0.8816
2025-05-01 05:32:36,763 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 05:32:36,764 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 05:33:04,011 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 05:33:41,305 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8816 to 0.8911
2025-05-01 05:33:41,453 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 12 completed in 64.69s - Train Loss: 0.4127, Train Acc: 0.8994, Val Loss: 0.4203, Val Acc: 0.8911
2025-05-01 05:33:41,849 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 05:33:41,849 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 05:34:09,174 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 05:34:47,037 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8911 to 0.9099
2025-05-01 05:34:47,186 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 13 completed in 65.34s - Train Loss: 0.4012, Train Acc: 0.9129, Val Loss: 0.4054, Val Acc: 0.9099
2025-05-01 05:34:47,590 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 05:34:47,590 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 05:35:15,204 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 05:35:52,658 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9099 to 0.9194
2025-05-01 05:35:52,802 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 14 completed in 65.21s - Train Loss: 0.3926, Train Acc: 0.9213, Val Loss: 0.3932, Val Acc: 0.9194
2025-05-01 05:35:53,192 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 05:35:53,193 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 05:36:20,072 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 05:36:56,662 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.5027 to 0.4008
2025-05-01 05:36:56,806 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 15 completed in 63.61s - Train Loss: 0.3829, Train Acc: 0.9326, Val Loss: 0.4008, Val Acc: 0.9117
2025-05-01 05:36:57,218 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 05:36:57,219 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 05:37:24,199 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 05:38:00,762 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9194 to 0.9322
2025-05-01 05:38:00,911 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 16 completed in 63.69s - Train Loss: 0.3751, Train Acc: 0.9417, Val Loss: 0.3815, Val Acc: 0.9322
2025-05-01 05:38:01,318 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 05:38:01,319 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 05:38:28,640 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 05:39:04,649 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4008 to 0.3868
2025-05-01 05:39:04,805 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 17 completed in 63.49s - Train Loss: 0.3743, Train Acc: 0.9417, Val Loss: 0.3868, Val Acc: 0.9237
2025-05-01 05:39:05,199 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 05:39:05,200 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 05:39:31,998 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 05:40:08,779 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9322 to 0.9408
2025-05-01 05:40:08,917 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 18 completed in 63.72s - Train Loss: 0.3671, Train Acc: 0.9472, Val Loss: 0.3720, Val Acc: 0.9408
2025-05-01 05:40:09,320 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 05:40:09,320 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 05:40:36,643 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 05:41:13,035 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3868 to 0.3777
2025-05-01 05:41:13,185 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 19 completed in 63.86s - Train Loss: 0.3682, Train Acc: 0.9444, Val Loss: 0.3777, Val Acc: 0.9383
2025-05-01 05:41:13,596 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 05:41:13,597 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 05:41:40,649 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 05:42:17,203 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9408 to 0.9468
2025-05-01 05:42:17,363 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Epoch 20 completed in 63.77s - Train Loss: 0.3592, Train Acc: 0.9556, Val Loss: 0.3662, Val Acc: 0.9468
2025-05-01 05:42:17,768 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 05:42:17,769 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:42:17,772 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Starting model evaluation
2025-05-01 05:42:30,147 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Evaluation metrics:
  Accuracy:  0.9138
  Precision: 0.8284
  Recall:    0.8776
  F1 Score:  0.8523
  IoU:       0.7426
  mAP:       0.8997
  AUC:       0.9578
2025-05-01 05:42:30,149 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_157/final_results.json
2025-05-01 05:42:30,151 - training.model_ResNet101_opt_Adam_lr_0.001_id_157 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_157/final_results.json
2025-05-01 05:42:30,151 - main - INFO - 
Summary for configuration 157:
2025-05-01 05:42:30,151 - main - INFO - Accuracy: 0.9138
2025-05-01 05:42:30,151 - main - INFO - Precision: 0.8284
2025-05-01 05:42:30,151 - main - INFO - Recall: 0.8776
2025-05-01 05:42:30,151 - main - INFO - F1 Score: 0.8523
2025-05-01 05:42:30,151 - main - INFO - IoU: 0.7426
2025-05-01 05:42:30,151 - main - INFO - mAP: 0.8997
2025-05-01 05:42:30,151 - main - INFO - AUC: 0.9578
2025-05-01 05:42:30,151 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:42:30,151 - main - INFO - 
==================================================
2025-05-01 05:42:30,151 - main - INFO - Running configuration 158/756:
2025-05-01 05:42:30,151 - main - INFO - Model: ResNet101
2025-05-01 05:42:30,151 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:42:30,151 - main - INFO - Scheduler: StepLR
2025-05-01 05:42:30,151 - main - INFO - Loss Function: FocalLoss
2025-05-01 05:42:30,151 - main - INFO - ==================================================
2025-05-01 05:42:30,153 - training.model_ResNet101_opt_Adam_lr_0.001_id_158 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_158
2025-05-01 05:42:30,153 - training.model_ResNet101_opt_Adam_lr_0.001_id_158 - INFO - Config: {
  "id": 158,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 05:42:30,464 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:42:30,464 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 158,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 05:42:30,464 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:42:30,626 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:42:30,628 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:42:30,631 - training.model_ResNet101_opt_Adam_lr_0.001_id_158 - INFO - Starting model evaluation
2025-05-01 05:42:40,025 - training.model_ResNet101_opt_Adam_lr_0.001_id_158 - INFO - Evaluation metrics:
  Accuracy:  0.9049
  Precision: 0.8167
  Recall:    0.8566
  F1 Score:  0.8362
  IoU:       0.7185
  mAP:       0.9022
  AUC:       0.9592
2025-05-01 05:42:40,027 - training.model_ResNet101_opt_Adam_lr_0.001_id_158 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_158/final_results.json
2025-05-01 05:42:40,028 - training.model_ResNet101_opt_Adam_lr_0.001_id_158 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_158/final_results.json
2025-05-01 05:42:40,028 - main - INFO - 
Summary for configuration 158:
2025-05-01 05:42:40,028 - main - INFO - Accuracy: 0.9049
2025-05-01 05:42:40,028 - main - INFO - Precision: 0.8167
2025-05-01 05:42:40,028 - main - INFO - Recall: 0.8566
2025-05-01 05:42:40,028 - main - INFO - F1 Score: 0.8362
2025-05-01 05:42:40,028 - main - INFO - IoU: 0.7185
2025-05-01 05:42:40,028 - main - INFO - mAP: 0.9022
2025-05-01 05:42:40,028 - main - INFO - AUC: 0.9592
2025-05-01 05:42:40,028 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:42:40,028 - main - INFO - 
==================================================
2025-05-01 05:42:40,028 - main - INFO - Running configuration 159/756:
2025-05-01 05:42:40,028 - main - INFO - Model: ResNet101
2025-05-01 05:42:40,028 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:42:40,028 - main - INFO - Scheduler: StepLR
2025-05-01 05:42:40,028 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 05:42:40,028 - main - INFO - ==================================================
2025-05-01 05:42:40,029 - training.model_ResNet101_opt_Adam_lr_0.001_id_159 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_159
2025-05-01 05:42:40,029 - training.model_ResNet101_opt_Adam_lr_0.001_id_159 - INFO - Config: {
  "id": 159,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 05:42:40,362 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:42:40,362 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 159,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 05:42:40,362 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:42:40,517 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:42:40,519 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:42:40,522 - training.model_ResNet101_opt_Adam_lr_0.001_id_159 - INFO - Starting model evaluation
2025-05-01 05:42:50,823 - training.model_ResNet101_opt_Adam_lr_0.001_id_159 - INFO - Evaluation metrics:
  Accuracy:  0.9148
  Precision: 0.8226
  Recall:    0.8916
  F1 Score:  0.8557
  IoU:       0.7478
  mAP:       0.9095
  AUC:       0.9609
2025-05-01 05:42:50,825 - training.model_ResNet101_opt_Adam_lr_0.001_id_159 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_159/final_results.json
2025-05-01 05:42:50,827 - training.model_ResNet101_opt_Adam_lr_0.001_id_159 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_159/final_results.json
2025-05-01 05:42:50,827 - main - INFO - 
Summary for configuration 159:
2025-05-01 05:42:50,827 - main - INFO - Accuracy: 0.9148
2025-05-01 05:42:50,827 - main - INFO - Precision: 0.8226
2025-05-01 05:42:50,827 - main - INFO - Recall: 0.8916
2025-05-01 05:42:50,827 - main - INFO - F1 Score: 0.8557
2025-05-01 05:42:50,827 - main - INFO - IoU: 0.7478
2025-05-01 05:42:50,827 - main - INFO - mAP: 0.9095
2025-05-01 05:42:50,827 - main - INFO - AUC: 0.9609
2025-05-01 05:42:50,827 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:42:50,827 - main - INFO - 
==================================================
2025-05-01 05:42:50,827 - main - INFO - Running configuration 160/756:
2025-05-01 05:42:50,827 - main - INFO - Model: ResNet101
2025-05-01 05:42:50,827 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:42:50,827 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 05:42:50,827 - main - INFO - Loss Function: CrossEntropy
2025-05-01 05:42:50,827 - main - INFO - ==================================================
2025-05-01 05:42:50,827 - training.model_ResNet101_opt_Adam_lr_0.001_id_160 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_160
2025-05-01 05:42:50,827 - training.model_ResNet101_opt_Adam_lr_0.001_id_160 - INFO - Config: {
  "id": 160,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:42:51,078 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:42:51,078 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 160,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:42:51,078 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:42:51,234 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:42:51,235 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:42:51,239 - training.model_ResNet101_opt_Adam_lr_0.001_id_160 - INFO - Starting model evaluation
2025-05-01 05:43:00,428 - training.model_ResNet101_opt_Adam_lr_0.001_id_160 - INFO - Evaluation metrics:
  Accuracy:  0.9078
  Precision: 0.8206
  Recall:    0.8636
  F1 Score:  0.8416
  IoU:       0.7265
  mAP:       0.8956
  AUC:       0.9602
2025-05-01 05:43:00,429 - training.model_ResNet101_opt_Adam_lr_0.001_id_160 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_160/final_results.json
2025-05-01 05:43:00,431 - training.model_ResNet101_opt_Adam_lr_0.001_id_160 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_160/final_results.json
2025-05-01 05:43:00,431 - main - INFO - 
Summary for configuration 160:
2025-05-01 05:43:00,431 - main - INFO - Accuracy: 0.9078
2025-05-01 05:43:00,431 - main - INFO - Precision: 0.8206
2025-05-01 05:43:00,431 - main - INFO - Recall: 0.8636
2025-05-01 05:43:00,431 - main - INFO - F1 Score: 0.8416
2025-05-01 05:43:00,431 - main - INFO - IoU: 0.7265
2025-05-01 05:43:00,431 - main - INFO - mAP: 0.8956
2025-05-01 05:43:00,431 - main - INFO - AUC: 0.9602
2025-05-01 05:43:00,431 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:43:00,431 - main - INFO - 
==================================================
2025-05-01 05:43:00,431 - main - INFO - Running configuration 161/756:
2025-05-01 05:43:00,431 - main - INFO - Model: ResNet101
2025-05-01 05:43:00,431 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:43:00,431 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 05:43:00,431 - main - INFO - Loss Function: FocalLoss
2025-05-01 05:43:00,431 - main - INFO - ==================================================
2025-05-01 05:43:00,431 - training.model_ResNet101_opt_Adam_lr_0.001_id_161 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_161
2025-05-01 05:43:00,431 - training.model_ResNet101_opt_Adam_lr_0.001_id_161 - INFO - Config: {
  "id": 161,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 05:43:00,683 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:43:00,683 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 161,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 05:43:00,683 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:43:00,843 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:43:00,844 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:43:00,847 - training.model_ResNet101_opt_Adam_lr_0.001_id_161 - INFO - Starting model evaluation
2025-05-01 05:43:10,077 - training.model_ResNet101_opt_Adam_lr_0.001_id_161 - INFO - Evaluation metrics:
  Accuracy:  0.9088
  Precision: 0.8170
  Recall:    0.8741
  F1 Score:  0.8446
  IoU:       0.7310
  mAP:       0.9071
  AUC:       0.9600
2025-05-01 05:43:10,079 - training.model_ResNet101_opt_Adam_lr_0.001_id_161 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_161/final_results.json
2025-05-01 05:43:10,080 - training.model_ResNet101_opt_Adam_lr_0.001_id_161 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_161/final_results.json
2025-05-01 05:43:10,080 - main - INFO - 
Summary for configuration 161:
2025-05-01 05:43:10,080 - main - INFO - Accuracy: 0.9088
2025-05-01 05:43:10,080 - main - INFO - Precision: 0.8170
2025-05-01 05:43:10,080 - main - INFO - Recall: 0.8741
2025-05-01 05:43:10,080 - main - INFO - F1 Score: 0.8446
2025-05-01 05:43:10,080 - main - INFO - IoU: 0.7310
2025-05-01 05:43:10,080 - main - INFO - mAP: 0.9071
2025-05-01 05:43:10,080 - main - INFO - AUC: 0.9600
2025-05-01 05:43:10,080 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:43:10,080 - main - INFO - 
==================================================
2025-05-01 05:43:10,080 - main - INFO - Running configuration 162/756:
2025-05-01 05:43:10,080 - main - INFO - Model: ResNet101
2025-05-01 05:43:10,080 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:43:10,080 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 05:43:10,080 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 05:43:10,080 - main - INFO - ==================================================
2025-05-01 05:43:10,080 - training.model_ResNet101_opt_Adam_lr_0.001_id_162 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_162
2025-05-01 05:43:10,081 - training.model_ResNet101_opt_Adam_lr_0.001_id_162 - INFO - Config: {
  "id": 162,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 05:43:10,337 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:43:10,337 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 162,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 05:43:10,337 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:43:10,494 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:43:10,496 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:43:10,499 - training.model_ResNet101_opt_Adam_lr_0.001_id_162 - INFO - Starting model evaluation
2025-05-01 05:43:20,077 - training.model_ResNet101_opt_Adam_lr_0.001_id_162 - INFO - Evaluation metrics:
  Accuracy:  0.9128
  Precision: 0.8235
  Recall:    0.8811
  F1 Score:  0.8514
  IoU:       0.7412
  mAP:       0.9059
  AUC:       0.9602
2025-05-01 05:43:20,079 - training.model_ResNet101_opt_Adam_lr_0.001_id_162 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_162/final_results.json
2025-05-01 05:43:20,080 - training.model_ResNet101_opt_Adam_lr_0.001_id_162 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_162/final_results.json
2025-05-01 05:43:20,080 - main - INFO - 
Summary for configuration 162:
2025-05-01 05:43:20,080 - main - INFO - Accuracy: 0.9128
2025-05-01 05:43:20,080 - main - INFO - Precision: 0.8235
2025-05-01 05:43:20,080 - main - INFO - Recall: 0.8811
2025-05-01 05:43:20,080 - main - INFO - F1 Score: 0.8514
2025-05-01 05:43:20,080 - main - INFO - IoU: 0.7412
2025-05-01 05:43:20,080 - main - INFO - mAP: 0.9059
2025-05-01 05:43:20,080 - main - INFO - AUC: 0.9602
2025-05-01 05:43:20,080 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:43:20,081 - main - INFO - 
==================================================
2025-05-01 05:43:20,081 - main - INFO - Running configuration 163/756:
2025-05-01 05:43:20,081 - main - INFO - Model: ResNet101
2025-05-01 05:43:20,081 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:43:20,081 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 05:43:20,081 - main - INFO - Loss Function: CrossEntropy
2025-05-01 05:43:20,081 - main - INFO - ==================================================
2025-05-01 05:43:20,081 - training.model_ResNet101_opt_Adam_lr_0.001_id_163 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_163
2025-05-01 05:43:20,081 - training.model_ResNet101_opt_Adam_lr_0.001_id_163 - INFO - Config: {
  "id": 163,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:43:20,332 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:43:20,332 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 163,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:43:20,332 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:43:20,485 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:43:20,486 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:43:20,489 - training.model_ResNet101_opt_Adam_lr_0.001_id_163 - INFO - Starting model evaluation
2025-05-01 05:43:30,099 - training.model_ResNet101_opt_Adam_lr_0.001_id_163 - INFO - Evaluation metrics:
  Accuracy:  0.9128
  Precision: 0.8300
  Recall:    0.8706
  F1 Score:  0.8498
  IoU:       0.7389
  mAP:       0.9044
  AUC:       0.9600
2025-05-01 05:43:30,100 - training.model_ResNet101_opt_Adam_lr_0.001_id_163 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_163/final_results.json
2025-05-01 05:43:30,102 - training.model_ResNet101_opt_Adam_lr_0.001_id_163 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_163/final_results.json
2025-05-01 05:43:30,102 - main - INFO - 
Summary for configuration 163:
2025-05-01 05:43:30,102 - main - INFO - Accuracy: 0.9128
2025-05-01 05:43:30,102 - main - INFO - Precision: 0.8300
2025-05-01 05:43:30,102 - main - INFO - Recall: 0.8706
2025-05-01 05:43:30,102 - main - INFO - F1 Score: 0.8498
2025-05-01 05:43:30,102 - main - INFO - IoU: 0.7389
2025-05-01 05:43:30,102 - main - INFO - mAP: 0.9044
2025-05-01 05:43:30,102 - main - INFO - AUC: 0.9600
2025-05-01 05:43:30,102 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:43:30,102 - main - INFO - 
==================================================
2025-05-01 05:43:30,102 - main - INFO - Running configuration 164/756:
2025-05-01 05:43:30,102 - main - INFO - Model: ResNet101
2025-05-01 05:43:30,102 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:43:30,102 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 05:43:30,102 - main - INFO - Loss Function: FocalLoss
2025-05-01 05:43:30,102 - main - INFO - ==================================================
2025-05-01 05:43:30,102 - training.model_ResNet101_opt_Adam_lr_0.001_id_164 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_164
2025-05-01 05:43:30,102 - training.model_ResNet101_opt_Adam_lr_0.001_id_164 - INFO - Config: {
  "id": 164,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 05:43:30,354 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:43:30,354 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 164,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 05:43:30,354 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:43:30,505 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:43:30,506 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:43:30,509 - training.model_ResNet101_opt_Adam_lr_0.001_id_164 - INFO - Starting model evaluation
2025-05-01 05:43:40,323 - training.model_ResNet101_opt_Adam_lr_0.001_id_164 - INFO - Evaluation metrics:
  Accuracy:  0.9177
  Precision: 0.8328
  Recall:    0.8881
  F1 Score:  0.8596
  IoU:       0.7537
  mAP:       0.8965
  AUC:       0.9601
2025-05-01 05:43:40,325 - training.model_ResNet101_opt_Adam_lr_0.001_id_164 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_164/final_results.json
2025-05-01 05:43:40,327 - training.model_ResNet101_opt_Adam_lr_0.001_id_164 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_164/final_results.json
2025-05-01 05:43:40,327 - main - INFO - 
Summary for configuration 164:
2025-05-01 05:43:40,327 - main - INFO - Accuracy: 0.9177
2025-05-01 05:43:40,327 - main - INFO - Precision: 0.8328
2025-05-01 05:43:40,327 - main - INFO - Recall: 0.8881
2025-05-01 05:43:40,327 - main - INFO - F1 Score: 0.8596
2025-05-01 05:43:40,327 - main - INFO - IoU: 0.7537
2025-05-01 05:43:40,327 - main - INFO - mAP: 0.8965
2025-05-01 05:43:40,327 - main - INFO - AUC: 0.9601
2025-05-01 05:43:40,327 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:43:40,327 - main - INFO - 
==================================================
2025-05-01 05:43:40,327 - main - INFO - Running configuration 165/756:
2025-05-01 05:43:40,327 - main - INFO - Model: ResNet101
2025-05-01 05:43:40,327 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:43:40,327 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 05:43:40,327 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 05:43:40,327 - main - INFO - ==================================================
2025-05-01 05:43:40,327 - training.model_ResNet101_opt_Adam_lr_0.001_id_165 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_165
2025-05-01 05:43:40,327 - training.model_ResNet101_opt_Adam_lr_0.001_id_165 - INFO - Config: {
  "id": 165,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 05:43:40,590 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:43:40,590 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 165,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 05:43:40,590 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:43:40,741 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:43:40,743 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:43:40,746 - training.model_ResNet101_opt_Adam_lr_0.001_id_165 - INFO - Starting model evaluation
2025-05-01 05:43:50,298 - training.model_ResNet101_opt_Adam_lr_0.001_id_165 - INFO - Evaluation metrics:
  Accuracy:  0.9118
  Precision: 0.8230
  Recall:    0.8776
  F1 Score:  0.8494
  IoU:       0.7382
  mAP:       0.8991
  AUC:       0.9593
2025-05-01 05:43:50,299 - training.model_ResNet101_opt_Adam_lr_0.001_id_165 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_165/final_results.json
2025-05-01 05:43:50,301 - training.model_ResNet101_opt_Adam_lr_0.001_id_165 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_165/final_results.json
2025-05-01 05:43:50,301 - main - INFO - 
Summary for configuration 165:
2025-05-01 05:43:50,301 - main - INFO - Accuracy: 0.9118
2025-05-01 05:43:50,301 - main - INFO - Precision: 0.8230
2025-05-01 05:43:50,301 - main - INFO - Recall: 0.8776
2025-05-01 05:43:50,301 - main - INFO - F1 Score: 0.8494
2025-05-01 05:43:50,301 - main - INFO - IoU: 0.7382
2025-05-01 05:43:50,301 - main - INFO - mAP: 0.8991
2025-05-01 05:43:50,301 - main - INFO - AUC: 0.9593
2025-05-01 05:43:50,301 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:43:50,301 - main - INFO - 
==================================================
2025-05-01 05:43:50,301 - main - INFO - Running configuration 166/756:
2025-05-01 05:43:50,301 - main - INFO - Model: ResNet101
2025-05-01 05:43:50,301 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:43:50,301 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 05:43:50,301 - main - INFO - Loss Function: CrossEntropy
2025-05-01 05:43:50,301 - main - INFO - ==================================================
2025-05-01 05:43:50,301 - training.model_ResNet101_opt_Adam_lr_0.001_id_166 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_166
2025-05-01 05:43:50,301 - training.model_ResNet101_opt_Adam_lr_0.001_id_166 - INFO - Config: {
  "id": 166,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:43:50,555 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:43:50,556 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 166,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:43:50,556 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:43:50,709 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:43:50,710 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:43:50,714 - training.model_ResNet101_opt_Adam_lr_0.001_id_166 - INFO - Starting model evaluation
2025-05-01 05:44:00,086 - training.model_ResNet101_opt_Adam_lr_0.001_id_166 - INFO - Evaluation metrics:
  Accuracy:  0.9118
  Precision: 0.8294
  Recall:    0.8671
  F1 Score:  0.8479
  IoU:       0.7359
  mAP:       0.9039
  AUC:       0.9613
2025-05-01 05:44:00,088 - training.model_ResNet101_opt_Adam_lr_0.001_id_166 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_166/final_results.json
2025-05-01 05:44:00,090 - training.model_ResNet101_opt_Adam_lr_0.001_id_166 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_166/final_results.json
2025-05-01 05:44:00,090 - main - INFO - 
Summary for configuration 166:
2025-05-01 05:44:00,090 - main - INFO - Accuracy: 0.9118
2025-05-01 05:44:00,090 - main - INFO - Precision: 0.8294
2025-05-01 05:44:00,090 - main - INFO - Recall: 0.8671
2025-05-01 05:44:00,090 - main - INFO - F1 Score: 0.8479
2025-05-01 05:44:00,090 - main - INFO - IoU: 0.7359
2025-05-01 05:44:00,090 - main - INFO - mAP: 0.9039
2025-05-01 05:44:00,090 - main - INFO - AUC: 0.9613
2025-05-01 05:44:00,090 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:44:00,090 - main - INFO - 
==================================================
2025-05-01 05:44:00,090 - main - INFO - Running configuration 167/756:
2025-05-01 05:44:00,090 - main - INFO - Model: ResNet101
2025-05-01 05:44:00,090 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:44:00,090 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 05:44:00,090 - main - INFO - Loss Function: FocalLoss
2025-05-01 05:44:00,090 - main - INFO - ==================================================
2025-05-01 05:44:00,090 - training.model_ResNet101_opt_Adam_lr_0.001_id_167 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_167
2025-05-01 05:44:00,090 - training.model_ResNet101_opt_Adam_lr_0.001_id_167 - INFO - Config: {
  "id": 167,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 05:44:00,361 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:44:00,361 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 167,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 05:44:00,361 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:44:00,513 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:44:00,515 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:44:00,518 - training.model_ResNet101_opt_Adam_lr_0.001_id_167 - INFO - Starting model evaluation
2025-05-01 05:44:10,548 - training.model_ResNet101_opt_Adam_lr_0.001_id_167 - INFO - Evaluation metrics:
  Accuracy:  0.9098
  Precision: 0.8218
  Recall:    0.8706
  F1 Score:  0.8455
  IoU:       0.7324
  mAP:       0.9055
  AUC:       0.9611
2025-05-01 05:44:10,550 - training.model_ResNet101_opt_Adam_lr_0.001_id_167 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_167/final_results.json
2025-05-01 05:44:10,551 - training.model_ResNet101_opt_Adam_lr_0.001_id_167 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_167/final_results.json
2025-05-01 05:44:10,551 - main - INFO - 
Summary for configuration 167:
2025-05-01 05:44:10,551 - main - INFO - Accuracy: 0.9098
2025-05-01 05:44:10,551 - main - INFO - Precision: 0.8218
2025-05-01 05:44:10,551 - main - INFO - Recall: 0.8706
2025-05-01 05:44:10,551 - main - INFO - F1 Score: 0.8455
2025-05-01 05:44:10,551 - main - INFO - IoU: 0.7324
2025-05-01 05:44:10,551 - main - INFO - mAP: 0.9055
2025-05-01 05:44:10,551 - main - INFO - AUC: 0.9611
2025-05-01 05:44:10,551 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:44:10,551 - main - INFO - 
==================================================
2025-05-01 05:44:10,551 - main - INFO - Running configuration 168/756:
2025-05-01 05:44:10,551 - main - INFO - Model: ResNet101
2025-05-01 05:44:10,551 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 05:44:10,551 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 05:44:10,551 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 05:44:10,551 - main - INFO - ==================================================
2025-05-01 05:44:10,551 - training.model_ResNet101_opt_Adam_lr_0.001_id_168 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001_id_168
2025-05-01 05:44:10,552 - training.model_ResNet101_opt_Adam_lr_0.001_id_168 - INFO - Config: {
  "id": 168,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 05:44:10,804 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.001
2025-05-01 05:44:10,804 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 168,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 05:44:10,804 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 05:44:11,031 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9468
2025-05-01 05:44:11,032 - training.model_ResNet101_opt_Adam_lr_0.001 - INFO - Training completed after 1272.84 seconds
2025-05-01 05:44:11,036 - training.model_ResNet101_opt_Adam_lr_0.001_id_168 - INFO - Starting model evaluation
2025-05-01 05:44:20,706 - training.model_ResNet101_opt_Adam_lr_0.001_id_168 - INFO - Evaluation metrics:
  Accuracy:  0.9118
  Precision: 0.8251
  Recall:    0.8741
  F1 Score:  0.8489
  IoU:       0.7375
  mAP:       0.9038
  AUC:       0.9602
2025-05-01 05:44:20,707 - training.model_ResNet101_opt_Adam_lr_0.001_id_168 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_168/final_results.json
2025-05-01 05:44:20,709 - training.model_ResNet101_opt_Adam_lr_0.001_id_168 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.001_id_168/final_results.json
2025-05-01 05:44:20,709 - main - INFO - 
Summary for configuration 168:
2025-05-01 05:44:20,709 - main - INFO - Accuracy: 0.9118
2025-05-01 05:44:20,709 - main - INFO - Precision: 0.8251
2025-05-01 05:44:20,709 - main - INFO - Recall: 0.8741
2025-05-01 05:44:20,709 - main - INFO - F1 Score: 0.8489
2025-05-01 05:44:20,709 - main - INFO - IoU: 0.7375
2025-05-01 05:44:20,709 - main - INFO - mAP: 0.9038
2025-05-01 05:44:20,709 - main - INFO - AUC: 0.9602
2025-05-01 05:44:20,709 - main - INFO - Training time: 1272.84 seconds
2025-05-01 05:44:20,709 - main - INFO - 
==================================================
2025-05-01 05:44:20,709 - main - INFO - Running configuration 169/756:
2025-05-01 05:44:20,709 - main - INFO - Model: ResNet101
2025-05-01 05:44:20,709 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 05:44:20,709 - main - INFO - Scheduler: StepLR
2025-05-01 05:44:20,709 - main - INFO - Loss Function: CrossEntropy
2025-05-01 05:44:20,709 - main - INFO - ==================================================
2025-05-01 05:44:20,709 - training.model_ResNet101_opt_Adam_lr_0.0001_id_169 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_169
2025-05-01 05:44:20,709 - training.model_ResNet101_opt_Adam_lr_0.0001_id_169 - INFO - Config: {
  "id": 169,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:44:20,960 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 05:44:20,960 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 169,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 05:44:20,960 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 05:44:20,961 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 05:44:42,208 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 05:45:10,490 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9211
2025-05-01 05:45:10,583 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 1 completed in 49.62s - Train Loss: 0.4323, Train Acc: 0.8790, Val Loss: 0.3905, Val Acc: 0.9211
2025-05-01 05:45:10,987 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 05:45:10,988 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 05:45:33,082 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 05:46:01,169 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from inf to 0.3966
2025-05-01 05:46:01,314 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 2 completed in 50.33s - Train Loss: 0.3794, Train Acc: 0.9320, Val Loss: 0.3966, Val Acc: 0.9117
2025-05-01 05:46:01,711 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 05:46:01,712 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 05:46:23,845 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 05:46:51,891 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9211 to 0.9520
2025-05-01 05:46:52,032 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 3 completed in 50.32s - Train Loss: 0.3663, Train Acc: 0.9466, Val Loss: 0.3614, Val Acc: 0.9520
2025-05-01 05:46:52,431 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 05:46:52,432 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 05:47:14,515 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 05:47:44,796 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3966 to 0.3707
2025-05-01 05:47:44,940 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 4 completed in 52.51s - Train Loss: 0.3661, Train Acc: 0.9472, Val Loss: 0.3707, Val Acc: 0.9383
2025-05-01 05:47:45,331 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 05:47:45,332 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 05:48:08,514 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 05:48:37,879 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9520 to 0.9580
2025-05-01 05:48:38,029 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 5 completed in 52.70s - Train Loss: 0.3565, Train Acc: 0.9567, Val Loss: 0.3578, Val Acc: 0.9580
2025-05-01 05:48:38,433 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 05:48:38,434 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 05:49:02,034 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 05:49:34,000 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3707 to 0.3700
2025-05-01 05:49:34,140 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 6 completed in 55.71s - Train Loss: 0.3543, Train Acc: 0.9590, Val Loss: 0.3700, Val Acc: 0.9417
2025-05-01 05:49:34,542 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 05:49:34,542 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 05:49:58,996 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 05:50:31,516 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9580 to 0.9623
2025-05-01 05:50:31,667 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 7 completed in 57.12s - Train Loss: 0.3475, Train Acc: 0.9653, Val Loss: 0.3531, Val Acc: 0.9623
2025-05-01 05:50:32,078 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 05:50:32,079 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 05:50:56,980 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 05:51:27,174 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3700 to 0.3599
2025-05-01 05:51:27,316 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 8 completed in 55.24s - Train Loss: 0.3453, Train Acc: 0.9672, Val Loss: 0.3599, Val Acc: 0.9511
2025-05-01 05:51:27,714 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 05:51:27,715 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 05:51:50,387 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 05:52:19,870 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9623 to 0.9683
2025-05-01 05:52:20,013 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 9 completed in 52.30s - Train Loss: 0.3550, Train Acc: 0.9580, Val Loss: 0.3466, Val Acc: 0.9683
2025-05-01 05:52:20,412 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 05:52:20,412 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 05:52:42,838 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 05:53:12,668 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3599 to 0.3535
2025-05-01 05:53:12,814 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 10 completed in 52.40s - Train Loss: 0.3446, Train Acc: 0.9676, Val Loss: 0.3535, Val Acc: 0.9588
2025-05-01 05:53:13,220 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 05:53:13,221 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 05:53:35,118 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 05:54:05,364 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9683 to 0.9768
2025-05-01 05:54:05,519 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 11 completed in 52.30s - Train Loss: 0.3369, Train Acc: 0.9768, Val Loss: 0.3361, Val Acc: 0.9768
2025-05-01 05:54:05,949 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 05:54:05,949 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 05:54:30,724 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 05:55:02,870 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3535 to 0.3411
2025-05-01 05:55:03,034 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 12 completed in 57.08s - Train Loss: 0.3287, Train Acc: 0.9858, Val Loss: 0.3411, Val Acc: 0.9717
2025-05-01 05:55:03,440 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 05:55:03,441 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 05:55:29,964 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 05:56:02,481 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3411 to 0.3391
2025-05-01 05:56:02,636 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 13 completed in 59.20s - Train Loss: 0.3285, Train Acc: 0.9854, Val Loss: 0.3391, Val Acc: 0.9743
2025-05-01 05:56:03,046 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 05:56:03,047 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 05:56:27,328 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 05:56:56,930 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9768 to 0.9803
2025-05-01 05:56:57,076 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 14 completed in 54.03s - Train Loss: 0.3268, Train Acc: 0.9867, Val Loss: 0.3339, Val Acc: 0.9803
2025-05-01 05:56:57,478 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 05:56:57,479 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 05:57:20,847 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 05:57:49,915 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9803 to 0.9837
2025-05-01 05:57:50,055 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 15 completed in 52.58s - Train Loss: 0.3251, Train Acc: 0.9880, Val Loss: 0.3310, Val Acc: 0.9837
2025-05-01 05:57:50,457 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 05:57:50,458 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 05:58:13,070 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 05:58:42,438 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3391 to 0.3312
2025-05-01 05:58:42,581 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 16 completed in 52.12s - Train Loss: 0.3241, Train Acc: 0.9895, Val Loss: 0.3312, Val Acc: 0.9828
2025-05-01 05:58:42,980 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 05:58:42,981 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 05:59:06,080 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 05:59:38,020 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 17 completed in 55.04s - Train Loss: 0.3225, Train Acc: 0.9908, Val Loss: 0.3316, Val Acc: 0.9820
2025-05-01 05:59:38,448 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 05:59:38,449 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 06:00:04,718 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 06:00:36,718 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 18 completed in 58.27s - Train Loss: 0.3222, Train Acc: 0.9916, Val Loss: 0.3354, Val Acc: 0.9768
2025-05-01 06:00:37,129 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 06:00:37,130 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 06:01:01,780 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 06:01:30,571 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3312 to 0.3300
2025-05-01 06:01:30,726 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 19 completed in 53.60s - Train Loss: 0.3218, Train Acc: 0.9916, Val Loss: 0.3300, Val Acc: 0.9828
2025-05-01 06:01:31,137 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 06:01:31,138 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 06:01:54,641 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 06:02:26,242 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9837 to 0.9846
2025-05-01 06:02:26,394 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Epoch 20 completed in 55.26s - Train Loss: 0.3200, Train Acc: 0.9936, Val Loss: 0.3287, Val Acc: 0.9846
2025-05-01 06:02:26,796 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 06:02:26,797 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:02:26,801 - training.model_ResNet101_opt_Adam_lr_0.0001_id_169 - INFO - Starting model evaluation
2025-05-01 06:02:37,237 - training.model_ResNet101_opt_Adam_lr_0.0001_id_169 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9109
  Recall:    0.9650
  F1 Score:  0.9372
  IoU:       0.8818
  mAP:       0.9829
  AUC:       0.9921
2025-05-01 06:02:37,239 - training.model_ResNet101_opt_Adam_lr_0.0001_id_169 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_169/final_results.json
2025-05-01 06:02:37,240 - training.model_ResNet101_opt_Adam_lr_0.0001_id_169 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_169/final_results.json
2025-05-01 06:02:37,240 - main - INFO - 
Summary for configuration 169:
2025-05-01 06:02:37,240 - main - INFO - Accuracy: 0.9633
2025-05-01 06:02:37,240 - main - INFO - Precision: 0.9109
2025-05-01 06:02:37,240 - main - INFO - Recall: 0.9650
2025-05-01 06:02:37,240 - main - INFO - F1 Score: 0.9372
2025-05-01 06:02:37,240 - main - INFO - IoU: 0.8818
2025-05-01 06:02:37,241 - main - INFO - mAP: 0.9829
2025-05-01 06:02:37,241 - main - INFO - AUC: 0.9921
2025-05-01 06:02:37,241 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:02:37,241 - main - INFO - 
==================================================
2025-05-01 06:02:37,241 - main - INFO - Running configuration 170/756:
2025-05-01 06:02:37,241 - main - INFO - Model: ResNet101
2025-05-01 06:02:37,241 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:02:37,241 - main - INFO - Scheduler: StepLR
2025-05-01 06:02:37,241 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:02:37,241 - main - INFO - ==================================================
2025-05-01 06:02:37,241 - training.model_ResNet101_opt_Adam_lr_0.0001_id_170 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_170
2025-05-01 06:02:37,241 - training.model_ResNet101_opt_Adam_lr_0.0001_id_170 - INFO - Config: {
  "id": 170,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:02:37,505 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:02:37,505 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 170,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:02:37,505 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:02:37,675 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:02:37,677 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:02:37,680 - training.model_ResNet101_opt_Adam_lr_0.0001_id_170 - INFO - Starting model evaluation
2025-05-01 06:02:48,516 - training.model_ResNet101_opt_Adam_lr_0.0001_id_170 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9016
  Recall:    0.9615
  F1 Score:  0.9306
  IoU:       0.8703
  mAP:       0.9646
  AUC:       0.9865
2025-05-01 06:02:48,518 - training.model_ResNet101_opt_Adam_lr_0.0001_id_170 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_170/final_results.json
2025-05-01 06:02:48,519 - training.model_ResNet101_opt_Adam_lr_0.0001_id_170 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_170/final_results.json
2025-05-01 06:02:48,520 - main - INFO - 
Summary for configuration 170:
2025-05-01 06:02:48,520 - main - INFO - Accuracy: 0.9594
2025-05-01 06:02:48,520 - main - INFO - Precision: 0.9016
2025-05-01 06:02:48,520 - main - INFO - Recall: 0.9615
2025-05-01 06:02:48,520 - main - INFO - F1 Score: 0.9306
2025-05-01 06:02:48,520 - main - INFO - IoU: 0.8703
2025-05-01 06:02:48,520 - main - INFO - mAP: 0.9646
2025-05-01 06:02:48,520 - main - INFO - AUC: 0.9865
2025-05-01 06:02:48,520 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:02:48,520 - main - INFO - 
==================================================
2025-05-01 06:02:48,520 - main - INFO - Running configuration 171/756:
2025-05-01 06:02:48,520 - main - INFO - Model: ResNet101
2025-05-01 06:02:48,520 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:02:48,520 - main - INFO - Scheduler: StepLR
2025-05-01 06:02:48,520 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:02:48,520 - main - INFO - ==================================================
2025-05-01 06:02:48,520 - training.model_ResNet101_opt_Adam_lr_0.0001_id_171 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_171
2025-05-01 06:02:48,520 - training.model_ResNet101_opt_Adam_lr_0.0001_id_171 - INFO - Config: {
  "id": 171,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:02:48,785 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:02:48,785 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 171,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:02:48,785 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:02:48,954 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:02:49,034 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:02:49,038 - training.model_ResNet101_opt_Adam_lr_0.0001_id_171 - INFO - Starting model evaluation
2025-05-01 06:03:00,263 - training.model_ResNet101_opt_Adam_lr_0.0001_id_171 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9106
  Recall:    0.9615
  F1 Score:  0.9354
  IoU:       0.8786
  mAP:       0.9787
  AUC:       0.9895
2025-05-01 06:03:00,264 - training.model_ResNet101_opt_Adam_lr_0.0001_id_171 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_171/final_results.json
2025-05-01 06:03:00,266 - training.model_ResNet101_opt_Adam_lr_0.0001_id_171 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_171/final_results.json
2025-05-01 06:03:00,266 - main - INFO - 
Summary for configuration 171:
2025-05-01 06:03:00,266 - main - INFO - Accuracy: 0.9623
2025-05-01 06:03:00,266 - main - INFO - Precision: 0.9106
2025-05-01 06:03:00,266 - main - INFO - Recall: 0.9615
2025-05-01 06:03:00,266 - main - INFO - F1 Score: 0.9354
2025-05-01 06:03:00,266 - main - INFO - IoU: 0.8786
2025-05-01 06:03:00,266 - main - INFO - mAP: 0.9787
2025-05-01 06:03:00,266 - main - INFO - AUC: 0.9895
2025-05-01 06:03:00,266 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:03:00,266 - main - INFO - 
==================================================
2025-05-01 06:03:00,266 - main - INFO - Running configuration 172/756:
2025-05-01 06:03:00,266 - main - INFO - Model: ResNet101
2025-05-01 06:03:00,266 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:03:00,266 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:03:00,266 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:03:00,266 - main - INFO - ==================================================
2025-05-01 06:03:00,266 - training.model_ResNet101_opt_Adam_lr_0.0001_id_172 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_172
2025-05-01 06:03:00,266 - training.model_ResNet101_opt_Adam_lr_0.0001_id_172 - INFO - Config: {
  "id": 172,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:03:00,533 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:03:00,533 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 172,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:03:00,533 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:03:00,702 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:03:00,704 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:03:00,708 - training.model_ResNet101_opt_Adam_lr_0.0001_id_172 - INFO - Starting model evaluation
2025-05-01 06:03:11,628 - training.model_ResNet101_opt_Adam_lr_0.0001_id_172 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.8958
  Recall:    0.9615
  F1 Score:  0.9275
  IoU:       0.8648
  mAP:       0.9554
  AUC:       0.9850
2025-05-01 06:03:11,630 - training.model_ResNet101_opt_Adam_lr_0.0001_id_172 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_172/final_results.json
2025-05-01 06:03:11,631 - training.model_ResNet101_opt_Adam_lr_0.0001_id_172 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_172/final_results.json
2025-05-01 06:03:11,631 - main - INFO - 
Summary for configuration 172:
2025-05-01 06:03:11,632 - main - INFO - Accuracy: 0.9574
2025-05-01 06:03:11,632 - main - INFO - Precision: 0.8958
2025-05-01 06:03:11,632 - main - INFO - Recall: 0.9615
2025-05-01 06:03:11,632 - main - INFO - F1 Score: 0.9275
2025-05-01 06:03:11,632 - main - INFO - IoU: 0.8648
2025-05-01 06:03:11,632 - main - INFO - mAP: 0.9554
2025-05-01 06:03:11,632 - main - INFO - AUC: 0.9850
2025-05-01 06:03:11,632 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:03:11,632 - main - INFO - 
==================================================
2025-05-01 06:03:11,632 - main - INFO - Running configuration 173/756:
2025-05-01 06:03:11,632 - main - INFO - Model: ResNet101
2025-05-01 06:03:11,632 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:03:11,632 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:03:11,632 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:03:11,632 - main - INFO - ==================================================
2025-05-01 06:03:11,632 - training.model_ResNet101_opt_Adam_lr_0.0001_id_173 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_173
2025-05-01 06:03:11,632 - training.model_ResNet101_opt_Adam_lr_0.0001_id_173 - INFO - Config: {
  "id": 173,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:03:11,899 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:03:11,899 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 173,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:03:11,899 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:03:12,068 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:03:12,069 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:03:12,072 - training.model_ResNet101_opt_Adam_lr_0.0001_id_173 - INFO - Starting model evaluation
2025-05-01 06:03:22,539 - training.model_ResNet101_opt_Adam_lr_0.0001_id_173 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9133
  Recall:    0.9580
  F1 Score:  0.9352
  IoU:       0.8782
  mAP:       0.9685
  AUC:       0.9882
2025-05-01 06:03:22,540 - training.model_ResNet101_opt_Adam_lr_0.0001_id_173 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_173/final_results.json
2025-05-01 06:03:22,542 - training.model_ResNet101_opt_Adam_lr_0.0001_id_173 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_173/final_results.json
2025-05-01 06:03:22,542 - main - INFO - 
Summary for configuration 173:
2025-05-01 06:03:22,542 - main - INFO - Accuracy: 0.9623
2025-05-01 06:03:22,542 - main - INFO - Precision: 0.9133
2025-05-01 06:03:22,542 - main - INFO - Recall: 0.9580
2025-05-01 06:03:22,542 - main - INFO - F1 Score: 0.9352
2025-05-01 06:03:22,542 - main - INFO - IoU: 0.8782
2025-05-01 06:03:22,542 - main - INFO - mAP: 0.9685
2025-05-01 06:03:22,542 - main - INFO - AUC: 0.9882
2025-05-01 06:03:22,542 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:03:22,542 - main - INFO - 
==================================================
2025-05-01 06:03:22,542 - main - INFO - Running configuration 174/756:
2025-05-01 06:03:22,542 - main - INFO - Model: ResNet101
2025-05-01 06:03:22,542 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:03:22,542 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:03:22,542 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:03:22,542 - main - INFO - ==================================================
2025-05-01 06:03:22,543 - training.model_ResNet101_opt_Adam_lr_0.0001_id_174 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_174
2025-05-01 06:03:22,543 - training.model_ResNet101_opt_Adam_lr_0.0001_id_174 - INFO - Config: {
  "id": 174,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:03:22,812 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:03:22,812 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 174,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:03:22,812 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:03:22,981 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:03:22,982 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:03:22,986 - training.model_ResNet101_opt_Adam_lr_0.0001_id_174 - INFO - Starting model evaluation
2025-05-01 06:03:33,403 - training.model_ResNet101_opt_Adam_lr_0.0001_id_174 - INFO - Evaluation metrics:
  Accuracy:  0.9584
  Precision: 0.8961
  Recall:    0.9650
  F1 Score:  0.9293
  IoU:       0.8679
  mAP:       0.9595
  AUC:       0.9859
2025-05-01 06:03:33,405 - training.model_ResNet101_opt_Adam_lr_0.0001_id_174 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_174/final_results.json
2025-05-01 06:03:33,406 - training.model_ResNet101_opt_Adam_lr_0.0001_id_174 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_174/final_results.json
2025-05-01 06:03:33,406 - main - INFO - 
Summary for configuration 174:
2025-05-01 06:03:33,406 - main - INFO - Accuracy: 0.9584
2025-05-01 06:03:33,406 - main - INFO - Precision: 0.8961
2025-05-01 06:03:33,406 - main - INFO - Recall: 0.9650
2025-05-01 06:03:33,406 - main - INFO - F1 Score: 0.9293
2025-05-01 06:03:33,406 - main - INFO - IoU: 0.8679
2025-05-01 06:03:33,406 - main - INFO - mAP: 0.9595
2025-05-01 06:03:33,406 - main - INFO - AUC: 0.9859
2025-05-01 06:03:33,406 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:03:33,406 - main - INFO - 
==================================================
2025-05-01 06:03:33,406 - main - INFO - Running configuration 175/756:
2025-05-01 06:03:33,406 - main - INFO - Model: ResNet101
2025-05-01 06:03:33,406 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:03:33,406 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:03:33,406 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:03:33,407 - main - INFO - ==================================================
2025-05-01 06:03:33,407 - training.model_ResNet101_opt_Adam_lr_0.0001_id_175 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_175
2025-05-01 06:03:33,407 - training.model_ResNet101_opt_Adam_lr_0.0001_id_175 - INFO - Config: {
  "id": 175,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:03:33,669 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:03:33,669 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 175,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:03:33,669 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:03:33,830 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:03:33,831 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:03:33,835 - training.model_ResNet101_opt_Adam_lr_0.0001_id_175 - INFO - Starting model evaluation
2025-05-01 06:03:44,595 - training.model_ResNet101_opt_Adam_lr_0.0001_id_175 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9142
  Recall:    0.9685
  F1 Score:  0.9406
  IoU:       0.8878
  mAP:       0.9746
  AUC:       0.9893
2025-05-01 06:03:44,596 - training.model_ResNet101_opt_Adam_lr_0.0001_id_175 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_175/final_results.json
2025-05-01 06:03:44,598 - training.model_ResNet101_opt_Adam_lr_0.0001_id_175 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_175/final_results.json
2025-05-01 06:03:44,598 - main - INFO - 
Summary for configuration 175:
2025-05-01 06:03:44,598 - main - INFO - Accuracy: 0.9653
2025-05-01 06:03:44,598 - main - INFO - Precision: 0.9142
2025-05-01 06:03:44,598 - main - INFO - Recall: 0.9685
2025-05-01 06:03:44,598 - main - INFO - F1 Score: 0.9406
2025-05-01 06:03:44,598 - main - INFO - IoU: 0.8878
2025-05-01 06:03:44,598 - main - INFO - mAP: 0.9746
2025-05-01 06:03:44,598 - main - INFO - AUC: 0.9893
2025-05-01 06:03:44,598 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:03:44,598 - main - INFO - 
==================================================
2025-05-01 06:03:44,598 - main - INFO - Running configuration 176/756:
2025-05-01 06:03:44,598 - main - INFO - Model: ResNet101
2025-05-01 06:03:44,598 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:03:44,598 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:03:44,598 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:03:44,598 - main - INFO - ==================================================
2025-05-01 06:03:44,598 - training.model_ResNet101_opt_Adam_lr_0.0001_id_176 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_176
2025-05-01 06:03:44,598 - training.model_ResNet101_opt_Adam_lr_0.0001_id_176 - INFO - Config: {
  "id": 176,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:03:44,864 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:03:44,864 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 176,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:03:44,864 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:03:45,026 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:03:45,027 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:03:45,030 - training.model_ResNet101_opt_Adam_lr_0.0001_id_176 - INFO - Starting model evaluation
2025-05-01 06:03:55,133 - training.model_ResNet101_opt_Adam_lr_0.0001_id_176 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9112
  Recall:    0.9685
  F1 Score:  0.9390
  IoU:       0.8850
  mAP:       0.9654
  AUC:       0.9895
2025-05-01 06:03:55,135 - training.model_ResNet101_opt_Adam_lr_0.0001_id_176 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_176/final_results.json
2025-05-01 06:03:55,136 - training.model_ResNet101_opt_Adam_lr_0.0001_id_176 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_176/final_results.json
2025-05-01 06:03:55,136 - main - INFO - 
Summary for configuration 176:
2025-05-01 06:03:55,136 - main - INFO - Accuracy: 0.9643
2025-05-01 06:03:55,136 - main - INFO - Precision: 0.9112
2025-05-01 06:03:55,136 - main - INFO - Recall: 0.9685
2025-05-01 06:03:55,136 - main - INFO - F1 Score: 0.9390
2025-05-01 06:03:55,136 - main - INFO - IoU: 0.8850
2025-05-01 06:03:55,136 - main - INFO - mAP: 0.9654
2025-05-01 06:03:55,136 - main - INFO - AUC: 0.9895
2025-05-01 06:03:55,136 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:03:55,136 - main - INFO - 
==================================================
2025-05-01 06:03:55,136 - main - INFO - Running configuration 177/756:
2025-05-01 06:03:55,136 - main - INFO - Model: ResNet101
2025-05-01 06:03:55,136 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:03:55,136 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:03:55,136 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:03:55,136 - main - INFO - ==================================================
2025-05-01 06:03:55,137 - training.model_ResNet101_opt_Adam_lr_0.0001_id_177 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_177
2025-05-01 06:03:55,137 - training.model_ResNet101_opt_Adam_lr_0.0001_id_177 - INFO - Config: {
  "id": 177,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:03:55,393 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:03:55,396 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 177,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:03:55,396 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:03:55,552 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:03:55,553 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:03:55,557 - training.model_ResNet101_opt_Adam_lr_0.0001_id_177 - INFO - Starting model evaluation
2025-05-01 06:04:05,553 - training.model_ResNet101_opt_Adam_lr_0.0001_id_177 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.8968
  Recall:    0.9720
  F1 Score:  0.9329
  IoU:       0.8742
  mAP:       0.9636
  AUC:       0.9897
2025-05-01 06:04:05,555 - training.model_ResNet101_opt_Adam_lr_0.0001_id_177 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_177/final_results.json
2025-05-01 06:04:05,556 - training.model_ResNet101_opt_Adam_lr_0.0001_id_177 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_177/final_results.json
2025-05-01 06:04:05,556 - main - INFO - 
Summary for configuration 177:
2025-05-01 06:04:05,556 - main - INFO - Accuracy: 0.9604
2025-05-01 06:04:05,556 - main - INFO - Precision: 0.8968
2025-05-01 06:04:05,556 - main - INFO - Recall: 0.9720
2025-05-01 06:04:05,556 - main - INFO - F1 Score: 0.9329
2025-05-01 06:04:05,556 - main - INFO - IoU: 0.8742
2025-05-01 06:04:05,556 - main - INFO - mAP: 0.9636
2025-05-01 06:04:05,556 - main - INFO - AUC: 0.9897
2025-05-01 06:04:05,556 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:04:05,556 - main - INFO - 
==================================================
2025-05-01 06:04:05,556 - main - INFO - Running configuration 178/756:
2025-05-01 06:04:05,556 - main - INFO - Model: ResNet101
2025-05-01 06:04:05,557 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:04:05,557 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:04:05,557 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:04:05,557 - main - INFO - ==================================================
2025-05-01 06:04:05,557 - training.model_ResNet101_opt_Adam_lr_0.0001_id_178 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_178
2025-05-01 06:04:05,557 - training.model_ResNet101_opt_Adam_lr_0.0001_id_178 - INFO - Config: {
  "id": 178,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:04:05,824 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:04:05,824 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 178,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:04:05,824 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:04:05,978 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:04:05,979 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:04:05,983 - training.model_ResNet101_opt_Adam_lr_0.0001_id_178 - INFO - Starting model evaluation
2025-05-01 06:04:15,762 - training.model_ResNet101_opt_Adam_lr_0.0001_id_178 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9136
  Recall:    0.9615
  F1 Score:  0.9370
  IoU:       0.8814
  mAP:       0.9624
  AUC:       0.9878
2025-05-01 06:04:15,764 - training.model_ResNet101_opt_Adam_lr_0.0001_id_178 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_178/final_results.json
2025-05-01 06:04:15,766 - training.model_ResNet101_opt_Adam_lr_0.0001_id_178 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_178/final_results.json
2025-05-01 06:04:15,766 - main - INFO - 
Summary for configuration 178:
2025-05-01 06:04:15,766 - main - INFO - Accuracy: 0.9633
2025-05-01 06:04:15,766 - main - INFO - Precision: 0.9136
2025-05-01 06:04:15,766 - main - INFO - Recall: 0.9615
2025-05-01 06:04:15,766 - main - INFO - F1 Score: 0.9370
2025-05-01 06:04:15,766 - main - INFO - IoU: 0.8814
2025-05-01 06:04:15,766 - main - INFO - mAP: 0.9624
2025-05-01 06:04:15,766 - main - INFO - AUC: 0.9878
2025-05-01 06:04:15,766 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:04:15,766 - main - INFO - 
==================================================
2025-05-01 06:04:15,766 - main - INFO - Running configuration 179/756:
2025-05-01 06:04:15,766 - main - INFO - Model: ResNet101
2025-05-01 06:04:15,766 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:04:15,766 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:04:15,766 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:04:15,766 - main - INFO - ==================================================
2025-05-01 06:04:15,766 - training.model_ResNet101_opt_Adam_lr_0.0001_id_179 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_179
2025-05-01 06:04:15,766 - training.model_ResNet101_opt_Adam_lr_0.0001_id_179 - INFO - Config: {
  "id": 179,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:04:16,033 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:04:16,033 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 179,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:04:16,033 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:04:16,190 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:04:16,191 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:04:16,195 - training.model_ResNet101_opt_Adam_lr_0.0001_id_179 - INFO - Starting model evaluation
2025-05-01 06:04:25,933 - training.model_ResNet101_opt_Adam_lr_0.0001_id_179 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9023
  Recall:    0.9685
  F1 Score:  0.9342
  IoU:       0.8766
  mAP:       0.9619
  AUC:       0.9893
2025-05-01 06:04:25,935 - training.model_ResNet101_opt_Adam_lr_0.0001_id_179 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_179/final_results.json
2025-05-01 06:04:25,937 - training.model_ResNet101_opt_Adam_lr_0.0001_id_179 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_179/final_results.json
2025-05-01 06:04:25,937 - main - INFO - 
Summary for configuration 179:
2025-05-01 06:04:25,937 - main - INFO - Accuracy: 0.9613
2025-05-01 06:04:25,937 - main - INFO - Precision: 0.9023
2025-05-01 06:04:25,937 - main - INFO - Recall: 0.9685
2025-05-01 06:04:25,937 - main - INFO - F1 Score: 0.9342
2025-05-01 06:04:25,937 - main - INFO - IoU: 0.8766
2025-05-01 06:04:25,937 - main - INFO - mAP: 0.9619
2025-05-01 06:04:25,937 - main - INFO - AUC: 0.9893
2025-05-01 06:04:25,937 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:04:25,937 - main - INFO - 
==================================================
2025-05-01 06:04:25,937 - main - INFO - Running configuration 180/756:
2025-05-01 06:04:25,937 - main - INFO - Model: ResNet101
2025-05-01 06:04:25,937 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 06:04:25,937 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:04:25,937 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:04:25,937 - main - INFO - ==================================================
2025-05-01 06:04:25,937 - training.model_ResNet101_opt_Adam_lr_0.0001_id_180 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001_id_180
2025-05-01 06:04:25,937 - training.model_ResNet101_opt_Adam_lr_0.0001_id_180 - INFO - Config: {
  "id": 180,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:04:26,215 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_Adam_lr_0.0001
2025-05-01 06:04:26,215 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 180,
  "model_name": "ResNet101",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:04:26,215 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 06:04:26,454 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-01 06:04:26,456 - training.model_ResNet101_opt_Adam_lr_0.0001 - INFO - Training completed after 1085.43 seconds
2025-05-01 06:04:26,459 - training.model_ResNet101_opt_Adam_lr_0.0001_id_180 - INFO - Starting model evaluation
2025-05-01 06:04:36,430 - training.model_ResNet101_opt_Adam_lr_0.0001_id_180 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.8964
  Recall:    0.9685
  F1 Score:  0.9311
  IoU:       0.8711
  mAP:       0.9605
  AUC:       0.9860
2025-05-01 06:04:36,432 - training.model_ResNet101_opt_Adam_lr_0.0001_id_180 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_180/final_results.json
2025-05-01 06:04:36,433 - training.model_ResNet101_opt_Adam_lr_0.0001_id_180 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_Adam_lr_0.0001_id_180/final_results.json
2025-05-01 06:04:36,433 - main - INFO - 
Summary for configuration 180:
2025-05-01 06:04:36,433 - main - INFO - Accuracy: 0.9594
2025-05-01 06:04:36,433 - main - INFO - Precision: 0.8964
2025-05-01 06:04:36,433 - main - INFO - Recall: 0.9685
2025-05-01 06:04:36,433 - main - INFO - F1 Score: 0.9311
2025-05-01 06:04:36,433 - main - INFO - IoU: 0.8711
2025-05-01 06:04:36,433 - main - INFO - mAP: 0.9605
2025-05-01 06:04:36,433 - main - INFO - AUC: 0.9860
2025-05-01 06:04:36,433 - main - INFO - Training time: 1085.43 seconds
2025-05-01 06:04:36,433 - main - INFO - 
==================================================
2025-05-01 06:04:36,433 - main - INFO - Running configuration 181/756:
2025-05-01 06:04:36,433 - main - INFO - Model: ResNet101
2025-05-01 06:04:36,433 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:04:36,433 - main - INFO - Scheduler: StepLR
2025-05-01 06:04:36,433 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:04:36,433 - main - INFO - ==================================================
2025-05-01 06:04:36,434 - training.model_ResNet101_opt_AdamW_lr_0.01_id_181 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_181
2025-05-01 06:04:36,434 - training.model_ResNet101_opt_AdamW_lr_0.01_id_181 - INFO - Config: {
  "id": 181,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:04:36,699 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:04:36,699 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 181,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:04:36,699 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 06:04:36,700 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 06:04:58,859 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 06:05:28,781 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.5043
2025-05-01 06:05:28,876 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 1 completed in 52.18s - Train Loss: 0.8149, Train Acc: 0.4987, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:05:29,280 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 06:05:29,280 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 06:05:51,479 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 06:06:22,138 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Validation loss improved from inf to 0.8096
2025-05-01 06:06:22,273 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 2 completed in 52.99s - Train Loss: 0.8143, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:06:22,660 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 06:06:22,660 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 06:06:45,616 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 06:07:17,677 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 3 completed in 55.02s - Train Loss: 0.8137, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:07:18,085 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 06:07:18,086 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 06:07:43,538 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 06:08:15,712 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8096 to 0.8096
2025-05-01 06:08:15,857 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 4 completed in 57.77s - Train Loss: 0.8130, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:08:16,278 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 06:08:16,278 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 06:08:40,735 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 06:09:11,057 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 5 completed in 54.78s - Train Loss: 0.8111, Train Acc: 0.5017, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:09:11,459 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 06:09:11,460 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 06:09:34,591 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 06:10:04,654 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 6 completed in 53.19s - Train Loss: 0.8143, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:10:05,108 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 06:10:05,108 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 06:10:27,237 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 06:10:57,024 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 7 completed in 51.92s - Train Loss: 0.8150, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:10:57,465 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 06:10:57,466 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 06:11:19,939 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 06:11:52,874 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 8 completed in 55.41s - Train Loss: 0.8156, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:11:53,299 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 06:11:53,299 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 06:12:18,193 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 06:12:51,300 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 9 completed in 58.00s - Train Loss: 0.8156, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:12:51,718 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 06:12:51,719 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 9 epochs
2025-05-01 06:12:51,719 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 494.60 seconds
2025-05-01 06:12:51,723 - training.model_ResNet101_opt_AdamW_lr_0.01_id_181 - INFO - Starting model evaluation
2025-05-01 06:13:02,378 - training.model_ResNet101_opt_AdamW_lr_0.01_id_181 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2834
  AUC:       0.4882
2025-05-01 06:13:02,379 - training.model_ResNet101_opt_AdamW_lr_0.01_id_181 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_181/final_results.json
2025-05-01 06:13:02,380 - training.model_ResNet101_opt_AdamW_lr_0.01_id_181 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_181/final_results.json
2025-05-01 06:13:02,380 - main - INFO - 
Summary for configuration 181:
2025-05-01 06:13:02,380 - main - INFO - Accuracy: 0.7166
2025-05-01 06:13:02,380 - main - INFO - Precision: 0.0000
2025-05-01 06:13:02,380 - main - INFO - Recall: 0.0000
2025-05-01 06:13:02,380 - main - INFO - F1 Score: 0.0000
2025-05-01 06:13:02,380 - main - INFO - IoU: 0.0000
2025-05-01 06:13:02,380 - main - INFO - mAP: 0.2834
2025-05-01 06:13:02,380 - main - INFO - AUC: 0.4882
2025-05-01 06:13:02,380 - main - INFO - Training time: 494.60 seconds
2025-05-01 06:13:02,380 - main - INFO - 
==================================================
2025-05-01 06:13:02,380 - main - INFO - Running configuration 182/756:
2025-05-01 06:13:02,380 - main - INFO - Model: ResNet101
2025-05-01 06:13:02,380 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:13:02,380 - main - INFO - Scheduler: StepLR
2025-05-01 06:13:02,380 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:13:02,380 - main - INFO - ==================================================
2025-05-01 06:13:02,380 - training.model_ResNet101_opt_AdamW_lr_0.01_id_182 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_182
2025-05-01 06:13:02,380 - training.model_ResNet101_opt_AdamW_lr_0.01_id_182 - INFO - Config: {
  "id": 182,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:13:02,642 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:13:02,642 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 182,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:13:02,642 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:13:02,813 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 9, best validation accuracy: 0.5043
2025-05-01 06:13:02,814 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 06:13:29,911 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 06:14:11,613 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8096 to 0.0899
2025-05-01 06:14:11,769 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 10 completed in 68.95s - Train Loss: 0.0906, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-01 06:14:12,259 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 06:14:12,261 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 06:14:43,916 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 06:15:24,706 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 11 completed in 72.45s - Train Loss: 0.0909, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-01 06:15:25,188 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 06:15:25,188 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 06:15:56,432 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 06:16:38,075 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 12 completed in 72.89s - Train Loss: 0.0909, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-01 06:16:38,571 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 06:16:38,572 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 06:17:08,935 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 06:17:49,045 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 13 completed in 70.47s - Train Loss: 0.0907, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-01 06:17:49,467 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 06:17:49,468 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 06:18:19,640 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 06:18:58,765 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 14 completed in 69.30s - Train Loss: 0.0909, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-01 06:18:59,167 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 06:18:59,168 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 06:19:29,478 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 06:20:08,479 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 15 completed in 69.31s - Train Loss: 0.0906, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-01 06:20:08,901 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 06:20:08,902 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 15 epochs
2025-05-01 06:20:08,902 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 920.26 seconds
2025-05-01 06:20:08,906 - training.model_ResNet101_opt_AdamW_lr_0.01_id_182 - INFO - Starting model evaluation
2025-05-01 06:20:20,381 - training.model_ResNet101_opt_AdamW_lr_0.01_id_182 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2827
  AUC:       0.4893
2025-05-01 06:20:20,383 - training.model_ResNet101_opt_AdamW_lr_0.01_id_182 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_182/final_results.json
2025-05-01 06:20:20,384 - training.model_ResNet101_opt_AdamW_lr_0.01_id_182 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_182/final_results.json
2025-05-01 06:20:20,384 - main - INFO - 
Summary for configuration 182:
2025-05-01 06:20:20,384 - main - INFO - Accuracy: 0.7166
2025-05-01 06:20:20,384 - main - INFO - Precision: 0.0000
2025-05-01 06:20:20,384 - main - INFO - Recall: 0.0000
2025-05-01 06:20:20,384 - main - INFO - F1 Score: 0.0000
2025-05-01 06:20:20,384 - main - INFO - IoU: 0.0000
2025-05-01 06:20:20,384 - main - INFO - mAP: 0.2827
2025-05-01 06:20:20,384 - main - INFO - AUC: 0.4893
2025-05-01 06:20:20,384 - main - INFO - Training time: 920.26 seconds
2025-05-01 06:20:20,384 - main - INFO - 
==================================================
2025-05-01 06:20:20,384 - main - INFO - Running configuration 183/756:
2025-05-01 06:20:20,384 - main - INFO - Model: ResNet101
2025-05-01 06:20:20,384 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:20:20,384 - main - INFO - Scheduler: StepLR
2025-05-01 06:20:20,384 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:20:20,384 - main - INFO - ==================================================
2025-05-01 06:20:20,384 - training.model_ResNet101_opt_AdamW_lr_0.01_id_183 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_183
2025-05-01 06:20:20,384 - training.model_ResNet101_opt_AdamW_lr_0.01_id_183 - INFO - Config: {
  "id": 183,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:20:21,207 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:20:21,207 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 183,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:20:21,207 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:20:22,414 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 15, best validation accuracy: 0.5043
2025-05-01 06:20:22,416 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 06:20:52,575 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 06:21:30,683 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 16 completed in 68.27s - Train Loss: 0.8146, Train Acc: 0.4989, Val Loss: 0.8103, Val Acc: 0.5043
2025-05-01 06:21:31,092 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 06:21:31,093 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 16 epochs
2025-05-01 06:21:31,093 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 988.53 seconds
2025-05-01 06:21:31,097 - training.model_ResNet101_opt_AdamW_lr_0.01_id_183 - INFO - Starting model evaluation
2025-05-01 06:21:42,921 - training.model_ResNet101_opt_AdamW_lr_0.01_id_183 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2836
  AUC:       0.4907
2025-05-01 06:21:42,923 - training.model_ResNet101_opt_AdamW_lr_0.01_id_183 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_183/final_results.json
2025-05-01 06:21:42,924 - training.model_ResNet101_opt_AdamW_lr_0.01_id_183 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_183/final_results.json
2025-05-01 06:21:42,924 - main - INFO - 
Summary for configuration 183:
2025-05-01 06:21:42,924 - main - INFO - Accuracy: 0.7166
2025-05-01 06:21:42,924 - main - INFO - Precision: 0.0000
2025-05-01 06:21:42,924 - main - INFO - Recall: 0.0000
2025-05-01 06:21:42,924 - main - INFO - F1 Score: 0.0000
2025-05-01 06:21:42,924 - main - INFO - IoU: 0.0000
2025-05-01 06:21:42,924 - main - INFO - mAP: 0.2836
2025-05-01 06:21:42,924 - main - INFO - AUC: 0.4907
2025-05-01 06:21:42,924 - main - INFO - Training time: 988.53 seconds
2025-05-01 06:21:42,924 - main - INFO - 
==================================================
2025-05-01 06:21:42,924 - main - INFO - Running configuration 184/756:
2025-05-01 06:21:42,925 - main - INFO - Model: ResNet101
2025-05-01 06:21:42,925 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:21:42,925 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:21:42,925 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:21:42,925 - main - INFO - ==================================================
2025-05-01 06:21:42,925 - training.model_ResNet101_opt_AdamW_lr_0.01_id_184 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_184
2025-05-01 06:21:42,925 - training.model_ResNet101_opt_AdamW_lr_0.01_id_184 - INFO - Config: {
  "id": 184,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:21:43,634 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:21:43,634 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 184,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:21:43,634 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:21:44,858 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 16, best validation accuracy: 0.5043
2025-05-01 06:21:44,859 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 06:22:14,467 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 06:22:51,928 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Epoch 17 completed in 67.07s - Train Loss: 0.8130, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-01 06:22:52,348 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 06:22:52,349 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 17 epochs
2025-05-01 06:22:52,349 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1055.60 seconds
2025-05-01 06:22:52,353 - training.model_ResNet101_opt_AdamW_lr_0.01_id_184 - INFO - Starting model evaluation
2025-05-01 06:23:03,571 - training.model_ResNet101_opt_AdamW_lr_0.01_id_184 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2827
  AUC:       0.4920
2025-05-01 06:23:03,572 - training.model_ResNet101_opt_AdamW_lr_0.01_id_184 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_184/final_results.json
2025-05-01 06:23:03,573 - training.model_ResNet101_opt_AdamW_lr_0.01_id_184 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_184/final_results.json
2025-05-01 06:23:03,573 - main - INFO - 
Summary for configuration 184:
2025-05-01 06:23:03,573 - main - INFO - Accuracy: 0.7166
2025-05-01 06:23:03,573 - main - INFO - Precision: 0.0000
2025-05-01 06:23:03,573 - main - INFO - Recall: 0.0000
2025-05-01 06:23:03,573 - main - INFO - F1 Score: 0.0000
2025-05-01 06:23:03,573 - main - INFO - IoU: 0.0000
2025-05-01 06:23:03,573 - main - INFO - mAP: 0.2827
2025-05-01 06:23:03,573 - main - INFO - AUC: 0.4920
2025-05-01 06:23:03,573 - main - INFO - Training time: 1055.60 seconds
2025-05-01 06:23:03,573 - main - INFO - 
==================================================
2025-05-01 06:23:03,573 - main - INFO - Running configuration 185/756:
2025-05-01 06:23:03,573 - main - INFO - Model: ResNet101
2025-05-01 06:23:03,573 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:23:03,573 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:23:03,573 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:23:03,573 - main - INFO - ==================================================
2025-05-01 06:23:03,573 - training.model_ResNet101_opt_AdamW_lr_0.01_id_185 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_185
2025-05-01 06:23:03,573 - training.model_ResNet101_opt_AdamW_lr_0.01_id_185 - INFO - Config: {
  "id": 185,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:23:04,249 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:23:04,249 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 185,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:23:04,249 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:23:05,606 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 17, best validation accuracy: 0.5043
2025-05-01 06:23:05,607 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 06:23:05,834 - training.model_ResNet101_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.40 GiB is allocated by PyTorch, and 208.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:05,834 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 06:23:06,242 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 06:23:06,243 - main - ERROR - Error running configuration 185: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.40 GiB is allocated by PyTorch, and 208.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:06,243 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 276, in _forward_impl
    x = self.layer4(x)
        ^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 154, in forward
    out = self.conv3(out)
          ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.40 GiB is allocated by PyTorch, and 208.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:06,245 - main - INFO - 
==================================================
2025-05-01 06:23:06,245 - main - INFO - Running configuration 186/756:
2025-05-01 06:23:06,245 - main - INFO - Model: ResNet101
2025-05-01 06:23:06,245 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:23:06,245 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:23:06,245 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:23:06,245 - main - INFO - ==================================================
2025-05-01 06:23:06,245 - training.model_ResNet101_opt_AdamW_lr_0.01_id_186 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_186
2025-05-01 06:23:06,245 - training.model_ResNet101_opt_AdamW_lr_0.01_id_186 - INFO - Config: {
  "id": 186,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:23:07,235 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:23:07,235 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 186,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:23:07,235 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:23:08,497 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 18, best validation accuracy: 0.5043
2025-05-01 06:23:08,498 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 06:23:08,659 - training.model_ResNet101_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 290.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:08,659 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 06:23:09,097 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 06:23:09,097 - main - ERROR - Error running configuration 186: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 290.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:09,097 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 275, in _forward_impl
    x = self.layer3(x)
        ^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 150, in forward
    out = self.conv2(out)
          ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 290.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:09,098 - main - INFO - 
==================================================
2025-05-01 06:23:09,099 - main - INFO - Running configuration 187/756:
2025-05-01 06:23:09,099 - main - INFO - Model: ResNet101
2025-05-01 06:23:09,099 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:23:09,099 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:23:09,099 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:23:09,099 - main - INFO - ==================================================
2025-05-01 06:23:09,099 - training.model_ResNet101_opt_AdamW_lr_0.01_id_187 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_187
2025-05-01 06:23:09,099 - training.model_ResNet101_opt_AdamW_lr_0.01_id_187 - INFO - Config: {
  "id": 187,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:23:09,815 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:23:09,815 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 187,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:23:09,815 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:23:11,182 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 19, best validation accuracy: 0.5043
2025-05-01 06:23:11,183 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 06:23:11,371 - training.model_ResNet101_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.39 GiB is allocated by PyTorch, and 215.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:11,371 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 06:23:11,781 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 06:23:11,781 - main - ERROR - Error running configuration 187: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.39 GiB is allocated by PyTorch, and 215.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:11,781 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 275, in _forward_impl
    x = self.layer3(x)
        ^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/resnet.py", line 154, in forward
    out = self.conv3(out)
          ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.39 GiB is allocated by PyTorch, and 215.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 06:23:11,782 - main - INFO - 
==================================================
2025-05-01 06:23:11,783 - main - INFO - Running configuration 188/756:
2025-05-01 06:23:11,783 - main - INFO - Model: ResNet101
2025-05-01 06:23:11,783 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:23:11,783 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:23:11,783 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:23:11,783 - main - INFO - ==================================================
2025-05-01 06:23:11,783 - training.model_ResNet101_opt_AdamW_lr_0.01_id_188 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_188
2025-05-01 06:23:11,783 - training.model_ResNet101_opt_AdamW_lr_0.01_id_188 - INFO - Config: {
  "id": 188,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:23:12,521 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:23:12,521 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 188,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:23:12,521 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:23:13,845 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-01 06:23:13,847 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1056.18 seconds
2025-05-01 06:23:13,851 - training.model_ResNet101_opt_AdamW_lr_0.01_id_188 - INFO - Starting model evaluation
2025-05-01 06:23:25,484 - training.model_ResNet101_opt_AdamW_lr_0.01_id_188 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2836
  AUC:       0.4921
2025-05-01 06:23:25,485 - training.model_ResNet101_opt_AdamW_lr_0.01_id_188 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_188/final_results.json
2025-05-01 06:23:25,486 - training.model_ResNet101_opt_AdamW_lr_0.01_id_188 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_188/final_results.json
2025-05-01 06:23:25,486 - main - INFO - 
Summary for configuration 188:
2025-05-01 06:23:25,486 - main - INFO - Accuracy: 0.7166
2025-05-01 06:23:25,486 - main - INFO - Precision: 0.0000
2025-05-01 06:23:25,486 - main - INFO - Recall: 0.0000
2025-05-01 06:23:25,486 - main - INFO - F1 Score: 0.0000
2025-05-01 06:23:25,486 - main - INFO - IoU: 0.0000
2025-05-01 06:23:25,486 - main - INFO - mAP: 0.2836
2025-05-01 06:23:25,486 - main - INFO - AUC: 0.4921
2025-05-01 06:23:25,486 - main - INFO - Training time: 1056.18 seconds
2025-05-01 06:23:25,486 - main - INFO - 
==================================================
2025-05-01 06:23:25,486 - main - INFO - Running configuration 189/756:
2025-05-01 06:23:25,486 - main - INFO - Model: ResNet101
2025-05-01 06:23:25,486 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:23:25,486 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:23:25,486 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:23:25,486 - main - INFO - ==================================================
2025-05-01 06:23:25,486 - training.model_ResNet101_opt_AdamW_lr_0.01_id_189 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_189
2025-05-01 06:23:25,486 - training.model_ResNet101_opt_AdamW_lr_0.01_id_189 - INFO - Config: {
  "id": 189,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:23:26,175 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:23:26,175 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 189,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:23:26,175 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:23:27,389 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-01 06:23:27,391 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1056.18 seconds
2025-05-01 06:23:27,394 - training.model_ResNet101_opt_AdamW_lr_0.01_id_189 - INFO - Starting model evaluation
2025-05-01 06:23:39,375 - training.model_ResNet101_opt_AdamW_lr_0.01_id_189 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2827
  AUC:       0.4920
2025-05-01 06:23:39,376 - training.model_ResNet101_opt_AdamW_lr_0.01_id_189 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_189/final_results.json
2025-05-01 06:23:39,377 - training.model_ResNet101_opt_AdamW_lr_0.01_id_189 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_189/final_results.json
2025-05-01 06:23:39,377 - main - INFO - 
Summary for configuration 189:
2025-05-01 06:23:39,377 - main - INFO - Accuracy: 0.7166
2025-05-01 06:23:39,377 - main - INFO - Precision: 0.0000
2025-05-01 06:23:39,377 - main - INFO - Recall: 0.0000
2025-05-01 06:23:39,377 - main - INFO - F1 Score: 0.0000
2025-05-01 06:23:39,377 - main - INFO - IoU: 0.0000
2025-05-01 06:23:39,377 - main - INFO - mAP: 0.2827
2025-05-01 06:23:39,378 - main - INFO - AUC: 0.4920
2025-05-01 06:23:39,378 - main - INFO - Training time: 1056.18 seconds
2025-05-01 06:23:39,378 - main - INFO - 
==================================================
2025-05-01 06:23:39,378 - main - INFO - Running configuration 190/756:
2025-05-01 06:23:39,378 - main - INFO - Model: ResNet101
2025-05-01 06:23:39,378 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:23:39,378 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:23:39,378 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:23:39,378 - main - INFO - ==================================================
2025-05-01 06:23:39,378 - training.model_ResNet101_opt_AdamW_lr_0.01_id_190 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_190
2025-05-01 06:23:39,378 - training.model_ResNet101_opt_AdamW_lr_0.01_id_190 - INFO - Config: {
  "id": 190,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:23:39,997 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:23:39,997 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 190,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:23:39,997 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:23:41,207 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-01 06:23:41,283 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1056.18 seconds
2025-05-01 06:23:41,287 - training.model_ResNet101_opt_AdamW_lr_0.01_id_190 - INFO - Starting model evaluation
2025-05-01 06:23:52,709 - training.model_ResNet101_opt_AdamW_lr_0.01_id_190 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2827
  AUC:       0.4913
2025-05-01 06:23:52,710 - training.model_ResNet101_opt_AdamW_lr_0.01_id_190 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_190/final_results.json
2025-05-01 06:23:52,711 - training.model_ResNet101_opt_AdamW_lr_0.01_id_190 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_190/final_results.json
2025-05-01 06:23:52,711 - main - INFO - 
Summary for configuration 190:
2025-05-01 06:23:52,711 - main - INFO - Accuracy: 0.7166
2025-05-01 06:23:52,711 - main - INFO - Precision: 0.0000
2025-05-01 06:23:52,711 - main - INFO - Recall: 0.0000
2025-05-01 06:23:52,711 - main - INFO - F1 Score: 0.0000
2025-05-01 06:23:52,711 - main - INFO - IoU: 0.0000
2025-05-01 06:23:52,711 - main - INFO - mAP: 0.2827
2025-05-01 06:23:52,711 - main - INFO - AUC: 0.4913
2025-05-01 06:23:52,711 - main - INFO - Training time: 1056.18 seconds
2025-05-01 06:23:52,711 - main - INFO - 
==================================================
2025-05-01 06:23:52,711 - main - INFO - Running configuration 191/756:
2025-05-01 06:23:52,711 - main - INFO - Model: ResNet101
2025-05-01 06:23:52,711 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:23:52,711 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:23:52,711 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:23:52,711 - main - INFO - ==================================================
2025-05-01 06:23:52,711 - training.model_ResNet101_opt_AdamW_lr_0.01_id_191 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_191
2025-05-01 06:23:52,711 - training.model_ResNet101_opt_AdamW_lr_0.01_id_191 - INFO - Config: {
  "id": 191,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:23:53,448 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:23:53,449 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 191,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:23:53,449 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:23:54,686 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-01 06:23:54,688 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1056.18 seconds
2025-05-01 06:23:54,692 - training.model_ResNet101_opt_AdamW_lr_0.01_id_191 - INFO - Starting model evaluation
2025-05-01 06:24:05,990 - training.model_ResNet101_opt_AdamW_lr_0.01_id_191 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2827
  AUC:       0.4900
2025-05-01 06:24:05,991 - training.model_ResNet101_opt_AdamW_lr_0.01_id_191 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_191/final_results.json
2025-05-01 06:24:05,993 - training.model_ResNet101_opt_AdamW_lr_0.01_id_191 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_191/final_results.json
2025-05-01 06:24:05,993 - main - INFO - 
Summary for configuration 191:
2025-05-01 06:24:05,993 - main - INFO - Accuracy: 0.7166
2025-05-01 06:24:05,993 - main - INFO - Precision: 0.0000
2025-05-01 06:24:05,993 - main - INFO - Recall: 0.0000
2025-05-01 06:24:05,993 - main - INFO - F1 Score: 0.0000
2025-05-01 06:24:05,993 - main - INFO - IoU: 0.0000
2025-05-01 06:24:05,993 - main - INFO - mAP: 0.2827
2025-05-01 06:24:05,993 - main - INFO - AUC: 0.4900
2025-05-01 06:24:05,993 - main - INFO - Training time: 1056.18 seconds
2025-05-01 06:24:05,993 - main - INFO - 
==================================================
2025-05-01 06:24:05,993 - main - INFO - Running configuration 192/756:
2025-05-01 06:24:05,993 - main - INFO - Model: ResNet101
2025-05-01 06:24:05,993 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 06:24:05,993 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:24:05,993 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:24:05,993 - main - INFO - ==================================================
2025-05-01 06:24:05,993 - training.model_ResNet101_opt_AdamW_lr_0.01_id_192 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_192
2025-05-01 06:24:05,993 - training.model_ResNet101_opt_AdamW_lr_0.01_id_192 - INFO - Config: {
  "id": 192,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:24:06,761 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-01 06:24:06,761 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 192,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:24:06,761 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 06:24:07,970 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-01 06:24:07,972 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1056.18 seconds
2025-05-01 06:24:07,975 - training.model_ResNet101_opt_AdamW_lr_0.01_id_192 - INFO - Starting model evaluation
2025-05-01 06:24:19,400 - training.model_ResNet101_opt_AdamW_lr_0.01_id_192 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2830
  AUC:       0.4900
2025-05-01 06:24:19,401 - training.model_ResNet101_opt_AdamW_lr_0.01_id_192 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_192/final_results.json
2025-05-01 06:24:19,402 - training.model_ResNet101_opt_AdamW_lr_0.01_id_192 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_192/final_results.json
2025-05-01 06:24:19,402 - main - INFO - 
Summary for configuration 192:
2025-05-01 06:24:19,402 - main - INFO - Accuracy: 0.7166
2025-05-01 06:24:19,402 - main - INFO - Precision: 0.0000
2025-05-01 06:24:19,402 - main - INFO - Recall: 0.0000
2025-05-01 06:24:19,402 - main - INFO - F1 Score: 0.0000
2025-05-01 06:24:19,402 - main - INFO - IoU: 0.0000
2025-05-01 06:24:19,402 - main - INFO - mAP: 0.2830
2025-05-01 06:24:19,402 - main - INFO - AUC: 0.4900
2025-05-01 06:24:19,402 - main - INFO - Training time: 1056.18 seconds
2025-05-01 06:24:19,402 - main - INFO - 
==================================================
2025-05-01 06:24:19,402 - main - INFO - Running configuration 193/756:
2025-05-01 06:24:19,402 - main - INFO - Model: ResNet101
2025-05-01 06:24:19,402 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:24:19,402 - main - INFO - Scheduler: StepLR
2025-05-01 06:24:19,402 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:24:19,402 - main - INFO - ==================================================
2025-05-01 06:24:19,402 - training.model_ResNet101_opt_AdamW_lr_0.001_id_193 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_193
2025-05-01 06:24:19,402 - training.model_ResNet101_opt_AdamW_lr_0.001_id_193 - INFO - Config: {
  "id": 193,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:24:20,063 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:24:20,063 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 193,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:24:20,063 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 06:24:20,064 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 06:24:49,616 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 06:25:28,933 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.6655
2025-05-01 06:25:29,030 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 1 completed in 68.97s - Train Loss: 0.6835, Train Acc: 0.5682, Val Loss: 0.6245, Val Acc: 0.6655
2025-05-01 06:25:29,452 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 06:25:29,452 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 06:25:58,926 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 06:26:36,240 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation loss improved from inf to 0.7169
2025-05-01 06:26:36,382 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 2 completed in 66.93s - Train Loss: 0.6583, Train Acc: 0.6130, Val Loss: 0.7169, Val Acc: 0.5463
2025-05-01 06:26:36,772 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 06:26:36,773 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 06:27:06,210 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 06:27:43,954 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.6655 to 0.7058
2025-05-01 06:27:44,104 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 3 completed in 67.33s - Train Loss: 0.6121, Train Acc: 0.6744, Val Loss: 0.5826, Val Acc: 0.7058
2025-05-01 06:27:44,495 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 06:27:44,495 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 06:28:13,097 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 06:28:52,407 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.7058 to 0.7376
2025-05-01 06:28:52,564 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 4 completed in 68.07s - Train Loss: 0.5745, Train Acc: 0.7194, Val Loss: 0.5593, Val Acc: 0.7376
2025-05-01 06:28:52,954 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 06:28:52,955 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 06:29:22,471 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 06:30:00,523 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.7376 to 0.7864
2025-05-01 06:30:00,674 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 5 completed in 67.72s - Train Loss: 0.5527, Train Acc: 0.7439, Val Loss: 0.5228, Val Acc: 0.7864
2025-05-01 06:30:01,064 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 06:30:01,065 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 06:30:29,644 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 06:31:08,091 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.7169 to 0.5358
2025-05-01 06:31:08,239 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 6 completed in 67.17s - Train Loss: 0.5315, Train Acc: 0.7707, Val Loss: 0.5358, Val Acc: 0.7616
2025-05-01 06:31:08,638 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 06:31:08,639 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 06:31:37,435 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 06:32:15,304 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 7 completed in 66.66s - Train Loss: 0.5388, Train Acc: 0.7608, Val Loss: 0.5724, Val Acc: 0.7410
2025-05-01 06:32:15,714 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 06:32:15,714 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 06:32:45,934 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 06:33:24,006 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.5358 to 0.5156
2025-05-01 06:33:24,153 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 8 completed in 68.44s - Train Loss: 0.5193, Train Acc: 0.7846, Val Loss: 0.5156, Val Acc: 0.7822
2025-05-01 06:33:24,568 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 06:33:24,568 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 06:33:54,178 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 06:34:31,803 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.7864 to 0.8045
2025-05-01 06:34:31,949 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 9 completed in 67.38s - Train Loss: 0.4975, Train Acc: 0.8069, Val Loss: 0.4931, Val Acc: 0.8045
2025-05-01 06:34:32,337 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 06:34:32,337 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 06:35:02,343 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 06:35:39,852 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8045 to 0.8482
2025-05-01 06:35:39,994 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 10 completed in 67.66s - Train Loss: 0.4879, Train Acc: 0.8132, Val Loss: 0.4565, Val Acc: 0.8482
2025-05-01 06:35:40,378 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 06:35:40,378 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 06:36:09,942 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 06:36:48,135 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8482 to 0.8782
2025-05-01 06:36:48,280 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 11 completed in 67.90s - Train Loss: 0.4423, Train Acc: 0.8662, Val Loss: 0.4291, Val Acc: 0.8782
2025-05-01 06:36:48,677 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 06:36:48,678 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 06:37:18,658 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 06:37:56,820 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8782 to 0.8859
2025-05-01 06:37:56,969 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 12 completed in 68.29s - Train Loss: 0.4233, Train Acc: 0.8857, Val Loss: 0.4226, Val Acc: 0.8859
2025-05-01 06:37:57,359 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 06:37:57,360 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 06:38:27,030 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 06:39:05,399 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8859 to 0.8979
2025-05-01 06:39:05,541 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 13 completed in 68.18s - Train Loss: 0.4148, Train Acc: 0.8988, Val Loss: 0.4139, Val Acc: 0.8979
2025-05-01 06:39:05,939 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 06:39:05,939 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 06:39:35,764 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 06:40:14,209 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8979 to 0.9091
2025-05-01 06:40:14,363 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 14 completed in 68.42s - Train Loss: 0.4098, Train Acc: 0.9013, Val Loss: 0.4045, Val Acc: 0.9091
2025-05-01 06:40:14,756 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 06:40:14,756 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 06:40:44,922 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 06:41:22,604 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9091 to 0.9168
2025-05-01 06:41:22,757 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 15 completed in 68.00s - Train Loss: 0.4006, Train Acc: 0.9138, Val Loss: 0.3984, Val Acc: 0.9168
2025-05-01 06:41:23,157 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 06:41:23,157 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 06:41:53,261 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 06:42:31,856 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9168 to 0.9220
2025-05-01 06:42:32,017 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 16 completed in 68.86s - Train Loss: 0.3958, Train Acc: 0.9151, Val Loss: 0.3909, Val Acc: 0.9220
2025-05-01 06:42:32,419 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 06:42:32,419 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 06:43:01,340 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 06:43:40,840 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.5156 to 0.3951
2025-05-01 06:43:40,999 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 17 completed in 68.58s - Train Loss: 0.3911, Train Acc: 0.9217, Val Loss: 0.3951, Val Acc: 0.9168
2025-05-01 06:43:41,396 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 06:43:41,396 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 06:44:10,992 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 06:44:49,146 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3951 to 0.3853
2025-05-01 06:44:49,287 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 18 completed in 67.89s - Train Loss: 0.3875, Train Acc: 0.9247, Val Loss: 0.3853, Val Acc: 0.9202
2025-05-01 06:44:49,687 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 06:44:49,688 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 06:45:19,500 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 06:45:57,787 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9220 to 0.9348
2025-05-01 06:45:57,924 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 19 completed in 68.24s - Train Loss: 0.3841, Train Acc: 0.9288, Val Loss: 0.3791, Val Acc: 0.9348
2025-05-01 06:45:58,310 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 06:45:58,311 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 06:46:28,515 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 06:47:06,760 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3853 to 0.3779
2025-05-01 06:47:06,900 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Epoch 20 completed in 68.59s - Train Loss: 0.3764, Train Acc: 0.9354, Val Loss: 0.3779, Val Acc: 0.9331
2025-05-01 06:47:07,383 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 06:47:07,384 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:47:07,388 - training.model_ResNet101_opt_AdamW_lr_0.001_id_193 - INFO - Starting model evaluation
2025-05-01 06:47:18,652 - training.model_ResNet101_opt_AdamW_lr_0.001_id_193 - INFO - Evaluation metrics:
  Accuracy:  0.8801
  Precision: 0.7508
  Recall:    0.8636
  F1 Score:  0.8033
  IoU:       0.6712
  mAP:       0.8938
  AUC:       0.9419
2025-05-01 06:47:18,654 - training.model_ResNet101_opt_AdamW_lr_0.001_id_193 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_193/final_results.json
2025-05-01 06:47:18,655 - training.model_ResNet101_opt_AdamW_lr_0.001_id_193 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_193/final_results.json
2025-05-01 06:47:18,655 - main - INFO - 
Summary for configuration 193:
2025-05-01 06:47:18,655 - main - INFO - Accuracy: 0.8801
2025-05-01 06:47:18,655 - main - INFO - Precision: 0.7508
2025-05-01 06:47:18,655 - main - INFO - Recall: 0.8636
2025-05-01 06:47:18,655 - main - INFO - F1 Score: 0.8033
2025-05-01 06:47:18,655 - main - INFO - IoU: 0.6712
2025-05-01 06:47:18,655 - main - INFO - mAP: 0.8938
2025-05-01 06:47:18,655 - main - INFO - AUC: 0.9419
2025-05-01 06:47:18,655 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:47:18,655 - main - INFO - 
==================================================
2025-05-01 06:47:18,655 - main - INFO - Running configuration 194/756:
2025-05-01 06:47:18,655 - main - INFO - Model: ResNet101
2025-05-01 06:47:18,655 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:47:18,656 - main - INFO - Scheduler: StepLR
2025-05-01 06:47:18,656 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:47:18,656 - main - INFO - ==================================================
2025-05-01 06:47:18,656 - training.model_ResNet101_opt_AdamW_lr_0.001_id_194 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_194
2025-05-01 06:47:18,656 - training.model_ResNet101_opt_AdamW_lr_0.001_id_194 - INFO - Config: {
  "id": 194,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:47:19,331 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:47:19,331 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 194,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:47:19,331 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:47:20,523 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:47:20,525 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:47:20,529 - training.model_ResNet101_opt_AdamW_lr_0.001_id_194 - INFO - Starting model evaluation
2025-05-01 06:47:32,055 - training.model_ResNet101_opt_AdamW_lr_0.001_id_194 - INFO - Evaluation metrics:
  Accuracy:  0.8821
  Precision: 0.7538
  Recall:    0.8671
  F1 Score:  0.8065
  IoU:       0.6757
  mAP:       0.8884
  AUC:       0.9381
2025-05-01 06:47:32,057 - training.model_ResNet101_opt_AdamW_lr_0.001_id_194 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_194/final_results.json
2025-05-01 06:47:32,058 - training.model_ResNet101_opt_AdamW_lr_0.001_id_194 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_194/final_results.json
2025-05-01 06:47:32,058 - main - INFO - 
Summary for configuration 194:
2025-05-01 06:47:32,058 - main - INFO - Accuracy: 0.8821
2025-05-01 06:47:32,059 - main - INFO - Precision: 0.7538
2025-05-01 06:47:32,059 - main - INFO - Recall: 0.8671
2025-05-01 06:47:32,059 - main - INFO - F1 Score: 0.8065
2025-05-01 06:47:32,059 - main - INFO - IoU: 0.6757
2025-05-01 06:47:32,059 - main - INFO - mAP: 0.8884
2025-05-01 06:47:32,059 - main - INFO - AUC: 0.9381
2025-05-01 06:47:32,059 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:47:32,059 - main - INFO - 
==================================================
2025-05-01 06:47:32,059 - main - INFO - Running configuration 195/756:
2025-05-01 06:47:32,059 - main - INFO - Model: ResNet101
2025-05-01 06:47:32,059 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:47:32,059 - main - INFO - Scheduler: StepLR
2025-05-01 06:47:32,059 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:47:32,059 - main - INFO - ==================================================
2025-05-01 06:47:32,059 - training.model_ResNet101_opt_AdamW_lr_0.001_id_195 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_195
2025-05-01 06:47:32,059 - training.model_ResNet101_opt_AdamW_lr_0.001_id_195 - INFO - Config: {
  "id": 195,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:47:32,812 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:47:32,812 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 195,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:47:32,812 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:47:34,061 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:47:34,062 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:47:34,066 - training.model_ResNet101_opt_AdamW_lr_0.001_id_195 - INFO - Starting model evaluation
2025-05-01 06:47:45,610 - training.model_ResNet101_opt_AdamW_lr_0.001_id_195 - INFO - Evaluation metrics:
  Accuracy:  0.8741
  Precision: 0.7387
  Recall:    0.8601
  F1 Score:  0.7948
  IoU:       0.6595
  mAP:       0.8850
  AUC:       0.9355
2025-05-01 06:47:45,612 - training.model_ResNet101_opt_AdamW_lr_0.001_id_195 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_195/final_results.json
2025-05-01 06:47:45,614 - training.model_ResNet101_opt_AdamW_lr_0.001_id_195 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_195/final_results.json
2025-05-01 06:47:45,614 - main - INFO - 
Summary for configuration 195:
2025-05-01 06:47:45,614 - main - INFO - Accuracy: 0.8741
2025-05-01 06:47:45,614 - main - INFO - Precision: 0.7387
2025-05-01 06:47:45,614 - main - INFO - Recall: 0.8601
2025-05-01 06:47:45,614 - main - INFO - F1 Score: 0.7948
2025-05-01 06:47:45,614 - main - INFO - IoU: 0.6595
2025-05-01 06:47:45,614 - main - INFO - mAP: 0.8850
2025-05-01 06:47:45,614 - main - INFO - AUC: 0.9355
2025-05-01 06:47:45,614 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:47:45,614 - main - INFO - 
==================================================
2025-05-01 06:47:45,614 - main - INFO - Running configuration 196/756:
2025-05-01 06:47:45,614 - main - INFO - Model: ResNet101
2025-05-01 06:47:45,614 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:47:45,614 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:47:45,614 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:47:45,614 - main - INFO - ==================================================
2025-05-01 06:47:45,614 - training.model_ResNet101_opt_AdamW_lr_0.001_id_196 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_196
2025-05-01 06:47:45,614 - training.model_ResNet101_opt_AdamW_lr_0.001_id_196 - INFO - Config: {
  "id": 196,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:47:46,305 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:47:46,305 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 196,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:47:46,305 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:47:47,507 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:47:47,509 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:47:47,512 - training.model_ResNet101_opt_AdamW_lr_0.001_id_196 - INFO - Starting model evaluation
2025-05-01 06:47:58,908 - training.model_ResNet101_opt_AdamW_lr_0.001_id_196 - INFO - Evaluation metrics:
  Accuracy:  0.8801
  Precision: 0.7523
  Recall:    0.8601
  F1 Score:  0.8026
  IoU:       0.6703
  mAP:       0.8884
  AUC:       0.9378
2025-05-01 06:47:58,910 - training.model_ResNet101_opt_AdamW_lr_0.001_id_196 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_196/final_results.json
2025-05-01 06:47:58,911 - training.model_ResNet101_opt_AdamW_lr_0.001_id_196 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_196/final_results.json
2025-05-01 06:47:58,912 - main - INFO - 
Summary for configuration 196:
2025-05-01 06:47:58,912 - main - INFO - Accuracy: 0.8801
2025-05-01 06:47:58,912 - main - INFO - Precision: 0.7523
2025-05-01 06:47:58,912 - main - INFO - Recall: 0.8601
2025-05-01 06:47:58,912 - main - INFO - F1 Score: 0.8026
2025-05-01 06:47:58,912 - main - INFO - IoU: 0.6703
2025-05-01 06:47:58,912 - main - INFO - mAP: 0.8884
2025-05-01 06:47:58,912 - main - INFO - AUC: 0.9378
2025-05-01 06:47:58,912 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:47:58,912 - main - INFO - 
==================================================
2025-05-01 06:47:58,912 - main - INFO - Running configuration 197/756:
2025-05-01 06:47:58,912 - main - INFO - Model: ResNet101
2025-05-01 06:47:58,912 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:47:58,912 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:47:58,912 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:47:58,912 - main - INFO - ==================================================
2025-05-01 06:47:58,912 - training.model_ResNet101_opt_AdamW_lr_0.001_id_197 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_197
2025-05-01 06:47:58,912 - training.model_ResNet101_opt_AdamW_lr_0.001_id_197 - INFO - Config: {
  "id": 197,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:47:59,597 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:47:59,597 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 197,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:47:59,597 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:48:00,889 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:48:00,890 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:48:00,894 - training.model_ResNet101_opt_AdamW_lr_0.001_id_197 - INFO - Starting model evaluation
2025-05-01 06:48:12,400 - training.model_ResNet101_opt_AdamW_lr_0.001_id_197 - INFO - Evaluation metrics:
  Accuracy:  0.8801
  Precision: 0.7477
  Recall:    0.8706
  F1 Score:  0.8045
  IoU:       0.6730
  mAP:       0.8859
  AUC:       0.9395
2025-05-01 06:48:12,401 - training.model_ResNet101_opt_AdamW_lr_0.001_id_197 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_197/final_results.json
2025-05-01 06:48:12,403 - training.model_ResNet101_opt_AdamW_lr_0.001_id_197 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_197/final_results.json
2025-05-01 06:48:12,403 - main - INFO - 
Summary for configuration 197:
2025-05-01 06:48:12,403 - main - INFO - Accuracy: 0.8801
2025-05-01 06:48:12,403 - main - INFO - Precision: 0.7477
2025-05-01 06:48:12,403 - main - INFO - Recall: 0.8706
2025-05-01 06:48:12,403 - main - INFO - F1 Score: 0.8045
2025-05-01 06:48:12,403 - main - INFO - IoU: 0.6730
2025-05-01 06:48:12,403 - main - INFO - mAP: 0.8859
2025-05-01 06:48:12,403 - main - INFO - AUC: 0.9395
2025-05-01 06:48:12,403 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:48:12,403 - main - INFO - 
==================================================
2025-05-01 06:48:12,403 - main - INFO - Running configuration 198/756:
2025-05-01 06:48:12,403 - main - INFO - Model: ResNet101
2025-05-01 06:48:12,403 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:48:12,403 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 06:48:12,403 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:48:12,403 - main - INFO - ==================================================
2025-05-01 06:48:12,403 - training.model_ResNet101_opt_AdamW_lr_0.001_id_198 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_198
2025-05-01 06:48:12,403 - training.model_ResNet101_opt_AdamW_lr_0.001_id_198 - INFO - Config: {
  "id": 198,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:48:13,093 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:48:13,093 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 198,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:48:13,093 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:48:14,381 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:48:14,383 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:48:14,386 - training.model_ResNet101_opt_AdamW_lr_0.001_id_198 - INFO - Starting model evaluation
2025-05-01 06:48:26,060 - training.model_ResNet101_opt_AdamW_lr_0.001_id_198 - INFO - Evaluation metrics:
  Accuracy:  0.8821
  Precision: 0.7569
  Recall:    0.8601
  F1 Score:  0.8052
  IoU:       0.6740
  mAP:       0.8892
  AUC:       0.9381
2025-05-01 06:48:26,062 - training.model_ResNet101_opt_AdamW_lr_0.001_id_198 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_198/final_results.json
2025-05-01 06:48:26,063 - training.model_ResNet101_opt_AdamW_lr_0.001_id_198 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_198/final_results.json
2025-05-01 06:48:26,063 - main - INFO - 
Summary for configuration 198:
2025-05-01 06:48:26,063 - main - INFO - Accuracy: 0.8821
2025-05-01 06:48:26,064 - main - INFO - Precision: 0.7569
2025-05-01 06:48:26,064 - main - INFO - Recall: 0.8601
2025-05-01 06:48:26,064 - main - INFO - F1 Score: 0.8052
2025-05-01 06:48:26,064 - main - INFO - IoU: 0.6740
2025-05-01 06:48:26,064 - main - INFO - mAP: 0.8892
2025-05-01 06:48:26,064 - main - INFO - AUC: 0.9381
2025-05-01 06:48:26,064 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:48:26,064 - main - INFO - 
==================================================
2025-05-01 06:48:26,064 - main - INFO - Running configuration 199/756:
2025-05-01 06:48:26,064 - main - INFO - Model: ResNet101
2025-05-01 06:48:26,064 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:48:26,064 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:48:26,064 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:48:26,064 - main - INFO - ==================================================
2025-05-01 06:48:26,064 - training.model_ResNet101_opt_AdamW_lr_0.001_id_199 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_199
2025-05-01 06:48:26,064 - training.model_ResNet101_opt_AdamW_lr_0.001_id_199 - INFO - Config: {
  "id": 199,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:48:26,797 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:48:26,797 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 199,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:48:26,797 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:48:28,000 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:48:28,001 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:48:28,005 - training.model_ResNet101_opt_AdamW_lr_0.001_id_199 - INFO - Starting model evaluation
2025-05-01 06:48:39,486 - training.model_ResNet101_opt_AdamW_lr_0.001_id_199 - INFO - Evaluation metrics:
  Accuracy:  0.8811
  Precision: 0.7530
  Recall:    0.8636
  F1 Score:  0.8046
  IoU:       0.6730
  mAP:       0.8880
  AUC:       0.9396
2025-05-01 06:48:39,488 - training.model_ResNet101_opt_AdamW_lr_0.001_id_199 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_199/final_results.json
2025-05-01 06:48:39,489 - training.model_ResNet101_opt_AdamW_lr_0.001_id_199 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_199/final_results.json
2025-05-01 06:48:39,489 - main - INFO - 
Summary for configuration 199:
2025-05-01 06:48:39,489 - main - INFO - Accuracy: 0.8811
2025-05-01 06:48:39,489 - main - INFO - Precision: 0.7530
2025-05-01 06:48:39,489 - main - INFO - Recall: 0.8636
2025-05-01 06:48:39,489 - main - INFO - F1 Score: 0.8046
2025-05-01 06:48:39,489 - main - INFO - IoU: 0.6730
2025-05-01 06:48:39,489 - main - INFO - mAP: 0.8880
2025-05-01 06:48:39,489 - main - INFO - AUC: 0.9396
2025-05-01 06:48:39,489 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:48:39,489 - main - INFO - 
==================================================
2025-05-01 06:48:39,489 - main - INFO - Running configuration 200/756:
2025-05-01 06:48:39,489 - main - INFO - Model: ResNet101
2025-05-01 06:48:39,489 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:48:39,489 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:48:39,490 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:48:39,490 - main - INFO - ==================================================
2025-05-01 06:48:39,490 - training.model_ResNet101_opt_AdamW_lr_0.001_id_200 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_200
2025-05-01 06:48:39,490 - training.model_ResNet101_opt_AdamW_lr_0.001_id_200 - INFO - Config: {
  "id": 200,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:48:40,204 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:48:40,204 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 200,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:48:40,204 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:48:41,410 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:48:41,411 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:48:41,415 - training.model_ResNet101_opt_AdamW_lr_0.001_id_200 - INFO - Starting model evaluation
2025-05-01 06:48:52,733 - training.model_ResNet101_opt_AdamW_lr_0.001_id_200 - INFO - Evaluation metrics:
  Accuracy:  0.8771
  Precision: 0.7440
  Recall:    0.8636
  F1 Score:  0.7994
  IoU:       0.6658
  mAP:       0.8876
  AUC:       0.9369
2025-05-01 06:48:52,735 - training.model_ResNet101_opt_AdamW_lr_0.001_id_200 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_200/final_results.json
2025-05-01 06:48:52,737 - training.model_ResNet101_opt_AdamW_lr_0.001_id_200 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_200/final_results.json
2025-05-01 06:48:52,737 - main - INFO - 
Summary for configuration 200:
2025-05-01 06:48:52,737 - main - INFO - Accuracy: 0.8771
2025-05-01 06:48:52,737 - main - INFO - Precision: 0.7440
2025-05-01 06:48:52,737 - main - INFO - Recall: 0.8636
2025-05-01 06:48:52,737 - main - INFO - F1 Score: 0.7994
2025-05-01 06:48:52,737 - main - INFO - IoU: 0.6658
2025-05-01 06:48:52,737 - main - INFO - mAP: 0.8876
2025-05-01 06:48:52,737 - main - INFO - AUC: 0.9369
2025-05-01 06:48:52,737 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:48:52,737 - main - INFO - 
==================================================
2025-05-01 06:48:52,737 - main - INFO - Running configuration 201/756:
2025-05-01 06:48:52,737 - main - INFO - Model: ResNet101
2025-05-01 06:48:52,737 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:48:52,737 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 06:48:52,737 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:48:52,737 - main - INFO - ==================================================
2025-05-01 06:48:52,737 - training.model_ResNet101_opt_AdamW_lr_0.001_id_201 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_201
2025-05-01 06:48:52,737 - training.model_ResNet101_opt_AdamW_lr_0.001_id_201 - INFO - Config: {
  "id": 201,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:48:53,395 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:48:53,395 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 201,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:48:53,395 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:48:54,706 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:48:54,709 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:48:54,713 - training.model_ResNet101_opt_AdamW_lr_0.001_id_201 - INFO - Starting model evaluation
2025-05-01 06:49:06,452 - training.model_ResNet101_opt_AdamW_lr_0.001_id_201 - INFO - Evaluation metrics:
  Accuracy:  0.8890
  Precision: 0.7753
  Recall:    0.8566
  F1 Score:  0.8140
  IoU:       0.6863
  mAP:       0.8875
  AUC:       0.9379
2025-05-01 06:49:06,454 - training.model_ResNet101_opt_AdamW_lr_0.001_id_201 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_201/final_results.json
2025-05-01 06:49:06,456 - training.model_ResNet101_opt_AdamW_lr_0.001_id_201 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_201/final_results.json
2025-05-01 06:49:06,456 - main - INFO - 
Summary for configuration 201:
2025-05-01 06:49:06,456 - main - INFO - Accuracy: 0.8890
2025-05-01 06:49:06,456 - main - INFO - Precision: 0.7753
2025-05-01 06:49:06,456 - main - INFO - Recall: 0.8566
2025-05-01 06:49:06,456 - main - INFO - F1 Score: 0.8140
2025-05-01 06:49:06,456 - main - INFO - IoU: 0.6863
2025-05-01 06:49:06,456 - main - INFO - mAP: 0.8875
2025-05-01 06:49:06,456 - main - INFO - AUC: 0.9379
2025-05-01 06:49:06,456 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:49:06,456 - main - INFO - 
==================================================
2025-05-01 06:49:06,456 - main - INFO - Running configuration 202/756:
2025-05-01 06:49:06,456 - main - INFO - Model: ResNet101
2025-05-01 06:49:06,456 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:49:06,456 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:49:06,456 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:49:06,456 - main - INFO - ==================================================
2025-05-01 06:49:06,456 - training.model_ResNet101_opt_AdamW_lr_0.001_id_202 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_202
2025-05-01 06:49:06,456 - training.model_ResNet101_opt_AdamW_lr_0.001_id_202 - INFO - Config: {
  "id": 202,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:49:07,173 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:49:07,173 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 202,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:49:07,173 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:49:08,454 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:49:08,456 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:49:08,459 - training.model_ResNet101_opt_AdamW_lr_0.001_id_202 - INFO - Starting model evaluation
2025-05-01 06:49:19,957 - training.model_ResNet101_opt_AdamW_lr_0.001_id_202 - INFO - Evaluation metrics:
  Accuracy:  0.8811
  Precision: 0.7515
  Recall:    0.8671
  F1 Score:  0.8052
  IoU:       0.6739
  mAP:       0.8930
  AUC:       0.9435
2025-05-01 06:49:19,958 - training.model_ResNet101_opt_AdamW_lr_0.001_id_202 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_202/final_results.json
2025-05-01 06:49:19,960 - training.model_ResNet101_opt_AdamW_lr_0.001_id_202 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_202/final_results.json
2025-05-01 06:49:19,960 - main - INFO - 
Summary for configuration 202:
2025-05-01 06:49:19,960 - main - INFO - Accuracy: 0.8811
2025-05-01 06:49:19,960 - main - INFO - Precision: 0.7515
2025-05-01 06:49:19,960 - main - INFO - Recall: 0.8671
2025-05-01 06:49:19,960 - main - INFO - F1 Score: 0.8052
2025-05-01 06:49:19,960 - main - INFO - IoU: 0.6739
2025-05-01 06:49:19,960 - main - INFO - mAP: 0.8930
2025-05-01 06:49:19,960 - main - INFO - AUC: 0.9435
2025-05-01 06:49:19,960 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:49:19,960 - main - INFO - 
==================================================
2025-05-01 06:49:19,960 - main - INFO - Running configuration 203/756:
2025-05-01 06:49:19,960 - main - INFO - Model: ResNet101
2025-05-01 06:49:19,960 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:49:19,960 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:49:19,960 - main - INFO - Loss Function: FocalLoss
2025-05-01 06:49:19,960 - main - INFO - ==================================================
2025-05-01 06:49:19,960 - training.model_ResNet101_opt_AdamW_lr_0.001_id_203 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_203
2025-05-01 06:49:19,960 - training.model_ResNet101_opt_AdamW_lr_0.001_id_203 - INFO - Config: {
  "id": 203,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:49:20,750 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:49:20,750 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 203,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 06:49:20,750 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:49:21,954 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:49:21,956 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:49:21,960 - training.model_ResNet101_opt_AdamW_lr_0.001_id_203 - INFO - Starting model evaluation
2025-05-01 06:49:33,403 - training.model_ResNet101_opt_AdamW_lr_0.001_id_203 - INFO - Evaluation metrics:
  Accuracy:  0.8811
  Precision: 0.7530
  Recall:    0.8636
  F1 Score:  0.8046
  IoU:       0.6730
  mAP:       0.8895
  AUC:       0.9401
2025-05-01 06:49:33,405 - training.model_ResNet101_opt_AdamW_lr_0.001_id_203 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_203/final_results.json
2025-05-01 06:49:33,406 - training.model_ResNet101_opt_AdamW_lr_0.001_id_203 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_203/final_results.json
2025-05-01 06:49:33,406 - main - INFO - 
Summary for configuration 203:
2025-05-01 06:49:33,406 - main - INFO - Accuracy: 0.8811
2025-05-01 06:49:33,406 - main - INFO - Precision: 0.7530
2025-05-01 06:49:33,406 - main - INFO - Recall: 0.8636
2025-05-01 06:49:33,406 - main - INFO - F1 Score: 0.8046
2025-05-01 06:49:33,406 - main - INFO - IoU: 0.6730
2025-05-01 06:49:33,406 - main - INFO - mAP: 0.8895
2025-05-01 06:49:33,406 - main - INFO - AUC: 0.9401
2025-05-01 06:49:33,406 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:49:33,406 - main - INFO - 
==================================================
2025-05-01 06:49:33,406 - main - INFO - Running configuration 204/756:
2025-05-01 06:49:33,406 - main - INFO - Model: ResNet101
2025-05-01 06:49:33,406 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 06:49:33,406 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 06:49:33,406 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 06:49:33,406 - main - INFO - ==================================================
2025-05-01 06:49:33,407 - training.model_ResNet101_opt_AdamW_lr_0.001_id_204 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001_id_204
2025-05-01 06:49:33,407 - training.model_ResNet101_opt_AdamW_lr_0.001_id_204 - INFO - Config: {
  "id": 204,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:49:34,129 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.001
2025-05-01 06:49:34,129 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 204,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 06:49:34,130 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 06:49:35,339 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-01 06:49:35,340 - training.model_ResNet101_opt_AdamW_lr_0.001 - INFO - Training completed after 1366.84 seconds
2025-05-01 06:49:35,344 - training.model_ResNet101_opt_AdamW_lr_0.001_id_204 - INFO - Starting model evaluation
2025-05-01 06:49:46,527 - training.model_ResNet101_opt_AdamW_lr_0.001_id_204 - INFO - Evaluation metrics:
  Accuracy:  0.8751
  Precision: 0.7424
  Recall:    0.8566
  F1 Score:  0.7955
  IoU:       0.6604
  mAP:       0.8888
  AUC:       0.9375
2025-05-01 06:49:46,528 - training.model_ResNet101_opt_AdamW_lr_0.001_id_204 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_204/final_results.json
2025-05-01 06:49:46,530 - training.model_ResNet101_opt_AdamW_lr_0.001_id_204 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.001_id_204/final_results.json
2025-05-01 06:49:46,530 - main - INFO - 
Summary for configuration 204:
2025-05-01 06:49:46,530 - main - INFO - Accuracy: 0.8751
2025-05-01 06:49:46,530 - main - INFO - Precision: 0.7424
2025-05-01 06:49:46,530 - main - INFO - Recall: 0.8566
2025-05-01 06:49:46,530 - main - INFO - F1 Score: 0.7955
2025-05-01 06:49:46,530 - main - INFO - IoU: 0.6604
2025-05-01 06:49:46,530 - main - INFO - mAP: 0.8888
2025-05-01 06:49:46,530 - main - INFO - AUC: 0.9375
2025-05-01 06:49:46,530 - main - INFO - Training time: 1366.84 seconds
2025-05-01 06:49:46,530 - main - INFO - 
==================================================
2025-05-01 06:49:46,530 - main - INFO - Running configuration 205/756:
2025-05-01 06:49:46,530 - main - INFO - Model: ResNet101
2025-05-01 06:49:46,530 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 06:49:46,530 - main - INFO - Scheduler: StepLR
2025-05-01 06:49:46,530 - main - INFO - Loss Function: CrossEntropy
2025-05-01 06:49:46,530 - main - INFO - ==================================================
2025-05-01 06:49:46,530 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_205 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_205
2025-05-01 06:49:46,530 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_205 - INFO - Config: {
  "id": 205,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:49:47,266 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 06:49:47,266 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 205,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 06:49:47,266 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 06:49:47,267 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 06:50:16,641 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 06:50:54,407 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.8310
2025-05-01 06:50:54,504 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 1 completed in 67.24s - Train Loss: 0.4355, Train Acc: 0.8764, Val Loss: 0.4749, Val Acc: 0.8310
2025-05-01 06:50:54,932 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 06:50:54,933 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 06:51:23,533 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 06:52:01,144 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.8310 to 0.9048
2025-05-01 06:52:01,287 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 2 completed in 66.35s - Train Loss: 0.3936, Train Acc: 0.9170, Val Loss: 0.4049, Val Acc: 0.9048
2025-05-01 06:52:01,679 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 06:52:01,679 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 06:52:30,780 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 06:53:08,815 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9048 to 0.9485
2025-05-01 06:53:08,960 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 3 completed in 67.28s - Train Loss: 0.3717, Train Acc: 0.9382, Val Loss: 0.3632, Val Acc: 0.9485
2025-05-01 06:53:09,356 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 06:53:09,356 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 06:53:39,588 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 06:54:16,328 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9485 to 0.9631
2025-05-01 06:54:16,475 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 4 completed in 67.12s - Train Loss: 0.3552, Train Acc: 0.9588, Val Loss: 0.3495, Val Acc: 0.9631
2025-05-01 06:54:16,867 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 06:54:16,867 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 06:54:45,876 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 06:55:23,805 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from inf to 0.3616
2025-05-01 06:55:23,948 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 5 completed in 67.08s - Train Loss: 0.3475, Train Acc: 0.9648, Val Loss: 0.3616, Val Acc: 0.9485
2025-05-01 06:55:24,354 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 06:55:24,355 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 06:55:53,127 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 06:56:31,046 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 6 completed in 66.69s - Train Loss: 0.3551, Train Acc: 0.9584, Val Loss: 0.3792, Val Acc: 0.9288
2025-05-01 06:56:31,454 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 06:56:31,454 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 06:57:00,542 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 06:57:37,973 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 7 completed in 66.52s - Train Loss: 0.3527, Train Acc: 0.9582, Val Loss: 0.3752, Val Acc: 0.9374
2025-05-01 06:57:38,396 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 06:57:38,397 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 06:58:08,632 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 06:58:46,633 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3616 to 0.3593
2025-05-01 06:58:46,784 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 8 completed in 68.39s - Train Loss: 0.3528, Train Acc: 0.9603, Val Loss: 0.3593, Val Acc: 0.9503
2025-05-01 06:58:47,172 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 06:58:47,173 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 06:59:17,482 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 06:59:55,217 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3593 to 0.3485
2025-05-01 06:59:55,381 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 9 completed in 68.21s - Train Loss: 0.3438, Train Acc: 0.9713, Val Loss: 0.3485, Val Acc: 0.9614
2025-05-01 06:59:55,777 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 06:59:55,778 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 07:00:25,605 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 07:01:02,888 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 10 completed in 67.11s - Train Loss: 0.3494, Train Acc: 0.9646, Val Loss: 0.3486, Val Acc: 0.9631
2025-05-01 07:01:03,314 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 07:01:03,314 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 07:01:33,534 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 07:02:10,784 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9631 to 0.9760
2025-05-01 07:02:10,947 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 11 completed in 67.63s - Train Loss: 0.3357, Train Acc: 0.9770, Val Loss: 0.3363, Val Acc: 0.9760
2025-05-01 07:02:11,340 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 07:02:11,342 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 07:02:40,347 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 07:03:17,518 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3485 to 0.3369
2025-05-01 07:03:17,667 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 12 completed in 66.33s - Train Loss: 0.3290, Train Acc: 0.9833, Val Loss: 0.3369, Val Acc: 0.9751
2025-05-01 07:03:18,075 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 07:03:18,076 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 07:03:47,535 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 07:04:25,974 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9760 to 0.9794
2025-05-01 07:04:26,116 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 13 completed in 68.04s - Train Loss: 0.3239, Train Acc: 0.9901, Val Loss: 0.3350, Val Acc: 0.9794
2025-05-01 07:04:26,503 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 07:04:26,504 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 07:04:56,223 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 07:05:33,737 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3369 to 0.3331
2025-05-01 07:05:33,885 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 14 completed in 67.38s - Train Loss: 0.3228, Train Acc: 0.9906, Val Loss: 0.3331, Val Acc: 0.9786
2025-05-01 07:05:34,303 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 07:05:34,304 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 07:06:03,207 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 07:06:41,963 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 15 completed in 67.66s - Train Loss: 0.3210, Train Acc: 0.9927, Val Loss: 0.3335, Val Acc: 0.9794
2025-05-01 07:06:42,393 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 07:06:42,394 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 07:07:11,513 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 07:07:49,985 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9794 to 0.9811
2025-05-01 07:07:50,122 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 16 completed in 67.73s - Train Loss: 0.3212, Train Acc: 0.9925, Val Loss: 0.3317, Val Acc: 0.9811
2025-05-01 07:07:50,520 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 07:07:50,521 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 07:08:20,027 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 07:08:58,460 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3331 to 0.3325
2025-05-01 07:08:58,604 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 17 completed in 68.08s - Train Loss: 0.3202, Train Acc: 0.9931, Val Loss: 0.3325, Val Acc: 0.9803
2025-05-01 07:08:58,991 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 07:08:58,992 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 07:09:28,878 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 07:10:06,428 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 18 completed in 67.44s - Train Loss: 0.3212, Train Acc: 0.9925, Val Loss: 0.3331, Val Acc: 0.9786
2025-05-01 07:10:06,854 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 07:10:06,855 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 07:10:36,291 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 07:11:14,205 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 19 completed in 67.35s - Train Loss: 0.3185, Train Acc: 0.9944, Val Loss: 0.3330, Val Acc: 0.9794
2025-05-01 07:11:14,613 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 07:11:14,613 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 07:11:43,260 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 07:12:19,891 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9811 to 0.9837
2025-05-01 07:12:20,031 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Epoch 20 completed in 65.42s - Train Loss: 0.3186, Train Acc: 0.9953, Val Loss: 0.3289, Val Acc: 0.9837
2025-05-01 07:12:20,427 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 07:12:20,428 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:12:20,432 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_205 - INFO - Starting model evaluation
2025-05-01 07:12:32,134 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_205 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9303
  Recall:    0.9336
  F1 Score:  0.9319
  IoU:       0.8725
  mAP:       0.9687
  AUC:       0.9853
2025-05-01 07:12:32,136 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_205 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_205/final_results.json
2025-05-01 07:12:32,137 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_205 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_205/final_results.json
2025-05-01 07:12:32,137 - main - INFO - 
Summary for configuration 205:
2025-05-01 07:12:32,137 - main - INFO - Accuracy: 0.9613
2025-05-01 07:12:32,137 - main - INFO - Precision: 0.9303
2025-05-01 07:12:32,137 - main - INFO - Recall: 0.9336
2025-05-01 07:12:32,137 - main - INFO - F1 Score: 0.9319
2025-05-01 07:12:32,137 - main - INFO - IoU: 0.8725
2025-05-01 07:12:32,137 - main - INFO - mAP: 0.9687
2025-05-01 07:12:32,137 - main - INFO - AUC: 0.9853
2025-05-01 07:12:32,137 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:12:32,137 - main - INFO - 
==================================================
2025-05-01 07:12:32,137 - main - INFO - Running configuration 206/756:
2025-05-01 07:12:32,137 - main - INFO - Model: ResNet101
2025-05-01 07:12:32,137 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:12:32,137 - main - INFO - Scheduler: StepLR
2025-05-01 07:12:32,137 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:12:32,137 - main - INFO - ==================================================
2025-05-01 07:12:32,138 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_206 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_206
2025-05-01 07:12:32,138 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_206 - INFO - Config: {
  "id": 206,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:12:32,898 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:12:32,898 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 206,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:12:32,898 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:12:34,094 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:12:34,096 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:12:34,099 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_206 - INFO - Starting model evaluation
2025-05-01 07:12:46,040 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_206 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.9147
  Recall:    0.9371
  F1 Score:  0.9257
  IoU:       0.8617
  mAP:       0.9577
  AUC:       0.9813
2025-05-01 07:12:46,041 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_206 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_206/final_results.json
2025-05-01 07:12:46,043 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_206 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_206/final_results.json
2025-05-01 07:12:46,043 - main - INFO - 
Summary for configuration 206:
2025-05-01 07:12:46,043 - main - INFO - Accuracy: 0.9574
2025-05-01 07:12:46,043 - main - INFO - Precision: 0.9147
2025-05-01 07:12:46,043 - main - INFO - Recall: 0.9371
2025-05-01 07:12:46,043 - main - INFO - F1 Score: 0.9257
2025-05-01 07:12:46,043 - main - INFO - IoU: 0.8617
2025-05-01 07:12:46,043 - main - INFO - mAP: 0.9577
2025-05-01 07:12:46,043 - main - INFO - AUC: 0.9813
2025-05-01 07:12:46,043 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:12:46,043 - main - INFO - 
==================================================
2025-05-01 07:12:46,043 - main - INFO - Running configuration 207/756:
2025-05-01 07:12:46,043 - main - INFO - Model: ResNet101
2025-05-01 07:12:46,043 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:12:46,043 - main - INFO - Scheduler: StepLR
2025-05-01 07:12:46,043 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:12:46,043 - main - INFO - ==================================================
2025-05-01 07:12:46,043 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_207 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_207
2025-05-01 07:12:46,043 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_207 - INFO - Config: {
  "id": 207,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:12:46,678 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:12:46,678 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 207,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:12:46,678 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:12:47,970 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:12:47,971 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:12:47,975 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_207 - INFO - Starting model evaluation
2025-05-01 07:12:59,578 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_207 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9308
  Recall:    0.9406
  F1 Score:  0.9357
  IoU:       0.8791
  mAP:       0.9611
  AUC:       0.9822
2025-05-01 07:12:59,580 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_207 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_207/final_results.json
2025-05-01 07:12:59,581 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_207 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_207/final_results.json
2025-05-01 07:12:59,581 - main - INFO - 
Summary for configuration 207:
2025-05-01 07:12:59,581 - main - INFO - Accuracy: 0.9633
2025-05-01 07:12:59,581 - main - INFO - Precision: 0.9308
2025-05-01 07:12:59,581 - main - INFO - Recall: 0.9406
2025-05-01 07:12:59,581 - main - INFO - F1 Score: 0.9357
2025-05-01 07:12:59,581 - main - INFO - IoU: 0.8791
2025-05-01 07:12:59,581 - main - INFO - mAP: 0.9611
2025-05-01 07:12:59,581 - main - INFO - AUC: 0.9822
2025-05-01 07:12:59,581 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:12:59,581 - main - INFO - 
==================================================
2025-05-01 07:12:59,581 - main - INFO - Running configuration 208/756:
2025-05-01 07:12:59,581 - main - INFO - Model: ResNet101
2025-05-01 07:12:59,581 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:12:59,582 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:12:59,582 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:12:59,582 - main - INFO - ==================================================
2025-05-01 07:12:59,582 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_208 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_208
2025-05-01 07:12:59,582 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_208 - INFO - Config: {
  "id": 208,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:13:00,313 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:13:00,313 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 208,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:13:00,313 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:13:01,526 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:13:01,527 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:13:01,531 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_208 - INFO - Starting model evaluation
2025-05-01 07:13:12,902 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_208 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9308
  Recall:    0.9406
  F1 Score:  0.9357
  IoU:       0.8791
  mAP:       0.9649
  AUC:       0.9847
2025-05-01 07:13:12,904 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_208 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_208/final_results.json
2025-05-01 07:13:12,906 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_208 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_208/final_results.json
2025-05-01 07:13:12,906 - main - INFO - 
Summary for configuration 208:
2025-05-01 07:13:12,906 - main - INFO - Accuracy: 0.9633
2025-05-01 07:13:12,906 - main - INFO - Precision: 0.9308
2025-05-01 07:13:12,906 - main - INFO - Recall: 0.9406
2025-05-01 07:13:12,906 - main - INFO - F1 Score: 0.9357
2025-05-01 07:13:12,906 - main - INFO - IoU: 0.8791
2025-05-01 07:13:12,906 - main - INFO - mAP: 0.9649
2025-05-01 07:13:12,906 - main - INFO - AUC: 0.9847
2025-05-01 07:13:12,906 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:13:12,906 - main - INFO - 
==================================================
2025-05-01 07:13:12,906 - main - INFO - Running configuration 209/756:
2025-05-01 07:13:12,906 - main - INFO - Model: ResNet101
2025-05-01 07:13:12,906 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:13:12,906 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:13:12,906 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:13:12,906 - main - INFO - ==================================================
2025-05-01 07:13:12,906 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_209 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_209
2025-05-01 07:13:12,906 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_209 - INFO - Config: {
  "id": 209,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:13:13,593 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:13:13,594 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 209,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:13:13,594 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:13:14,806 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:13:14,807 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:13:14,811 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_209 - INFO - Starting model evaluation
2025-05-01 07:13:26,243 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_209 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9343
  Recall:    0.9441
  F1 Score:  0.9391
  IoU:       0.8852
  mAP:       0.9634
  AUC:       0.9849
2025-05-01 07:13:26,245 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_209 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_209/final_results.json
2025-05-01 07:13:26,246 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_209 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_209/final_results.json
2025-05-01 07:13:26,246 - main - INFO - 
Summary for configuration 209:
2025-05-01 07:13:26,246 - main - INFO - Accuracy: 0.9653
2025-05-01 07:13:26,246 - main - INFO - Precision: 0.9343
2025-05-01 07:13:26,246 - main - INFO - Recall: 0.9441
2025-05-01 07:13:26,246 - main - INFO - F1 Score: 0.9391
2025-05-01 07:13:26,246 - main - INFO - IoU: 0.8852
2025-05-01 07:13:26,246 - main - INFO - mAP: 0.9634
2025-05-01 07:13:26,246 - main - INFO - AUC: 0.9849
2025-05-01 07:13:26,246 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:13:26,247 - main - INFO - 
==================================================
2025-05-01 07:13:26,247 - main - INFO - Running configuration 210/756:
2025-05-01 07:13:26,247 - main - INFO - Model: ResNet101
2025-05-01 07:13:26,247 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:13:26,247 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:13:26,247 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:13:26,247 - main - INFO - ==================================================
2025-05-01 07:13:26,247 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_210 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_210
2025-05-01 07:13:26,247 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_210 - INFO - Config: {
  "id": 210,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:13:26,883 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:13:26,883 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 210,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:13:26,883 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:13:28,060 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:13:28,062 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:13:28,066 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_210 - INFO - Starting model evaluation
2025-05-01 07:13:39,405 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_210 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9210
  Recall:    0.9371
  F1 Score:  0.9289
  IoU:       0.8673
  mAP:       0.9574
  AUC:       0.9830
2025-05-01 07:13:39,409 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_210 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_210/final_results.json
2025-05-01 07:13:39,411 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_210 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_210/final_results.json
2025-05-01 07:13:39,411 - main - INFO - 
Summary for configuration 210:
2025-05-01 07:13:39,411 - main - INFO - Accuracy: 0.9594
2025-05-01 07:13:39,411 - main - INFO - Precision: 0.9210
2025-05-01 07:13:39,411 - main - INFO - Recall: 0.9371
2025-05-01 07:13:39,411 - main - INFO - F1 Score: 0.9289
2025-05-01 07:13:39,411 - main - INFO - IoU: 0.8673
2025-05-01 07:13:39,411 - main - INFO - mAP: 0.9574
2025-05-01 07:13:39,411 - main - INFO - AUC: 0.9830
2025-05-01 07:13:39,411 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:13:39,411 - main - INFO - 
==================================================
2025-05-01 07:13:39,411 - main - INFO - Running configuration 211/756:
2025-05-01 07:13:39,411 - main - INFO - Model: ResNet101
2025-05-01 07:13:39,411 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:13:39,411 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:13:39,411 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:13:39,411 - main - INFO - ==================================================
2025-05-01 07:13:39,411 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_211 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_211
2025-05-01 07:13:39,411 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_211 - INFO - Config: {
  "id": 211,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:13:40,154 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:13:40,154 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 211,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:13:40,154 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:13:41,383 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:13:41,384 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:13:41,388 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_211 - INFO - Starting model evaluation
2025-05-01 07:13:53,004 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_211 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9373
  Recall:    0.9406
  F1 Score:  0.9389
  IoU:       0.8849
  mAP:       0.9718
  AUC:       0.9855
2025-05-01 07:13:53,006 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_211 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_211/final_results.json
2025-05-01 07:13:53,007 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_211 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_211/final_results.json
2025-05-01 07:13:53,007 - main - INFO - 
Summary for configuration 211:
2025-05-01 07:13:53,007 - main - INFO - Accuracy: 0.9653
2025-05-01 07:13:53,007 - main - INFO - Precision: 0.9373
2025-05-01 07:13:53,007 - main - INFO - Recall: 0.9406
2025-05-01 07:13:53,007 - main - INFO - F1 Score: 0.9389
2025-05-01 07:13:53,007 - main - INFO - IoU: 0.8849
2025-05-01 07:13:53,007 - main - INFO - mAP: 0.9718
2025-05-01 07:13:53,007 - main - INFO - AUC: 0.9855
2025-05-01 07:13:53,007 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:13:53,007 - main - INFO - 
==================================================
2025-05-01 07:13:53,007 - main - INFO - Running configuration 212/756:
2025-05-01 07:13:53,007 - main - INFO - Model: ResNet101
2025-05-01 07:13:53,007 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:13:53,007 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:13:53,007 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:13:53,007 - main - INFO - ==================================================
2025-05-01 07:13:53,008 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_212 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_212
2025-05-01 07:13:53,008 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_212 - INFO - Config: {
  "id": 212,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:13:53,699 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:13:53,699 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 212,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:13:53,699 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:13:54,948 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:13:54,949 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:13:54,954 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_212 - INFO - Starting model evaluation
2025-05-01 07:14:06,830 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_212 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9281
  Recall:    0.9476
  F1 Score:  0.9377
  IoU:       0.8827
  mAP:       0.9698
  AUC:       0.9865
2025-05-01 07:14:06,832 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_212 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_212/final_results.json
2025-05-01 07:14:06,833 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_212 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_212/final_results.json
2025-05-01 07:14:06,833 - main - INFO - 
Summary for configuration 212:
2025-05-01 07:14:06,833 - main - INFO - Accuracy: 0.9643
2025-05-01 07:14:06,833 - main - INFO - Precision: 0.9281
2025-05-01 07:14:06,833 - main - INFO - Recall: 0.9476
2025-05-01 07:14:06,833 - main - INFO - F1 Score: 0.9377
2025-05-01 07:14:06,833 - main - INFO - IoU: 0.8827
2025-05-01 07:14:06,833 - main - INFO - mAP: 0.9698
2025-05-01 07:14:06,833 - main - INFO - AUC: 0.9865
2025-05-01 07:14:06,833 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:14:06,833 - main - INFO - 
==================================================
2025-05-01 07:14:06,833 - main - INFO - Running configuration 213/756:
2025-05-01 07:14:06,833 - main - INFO - Model: ResNet101
2025-05-01 07:14:06,833 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:14:06,833 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:14:06,833 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:14:06,833 - main - INFO - ==================================================
2025-05-01 07:14:06,834 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_213 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_213
2025-05-01 07:14:06,834 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_213 - INFO - Config: {
  "id": 213,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:14:07,506 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:14:07,506 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 213,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:14:07,506 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:14:08,711 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:14:08,712 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:14:08,716 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_213 - INFO - Starting model evaluation
2025-05-01 07:14:20,308 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_213 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9313
  Recall:    0.9476
  F1 Score:  0.9393
  IoU:       0.8856
  mAP:       0.9700
  AUC:       0.9870
2025-05-01 07:14:20,310 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_213 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_213/final_results.json
2025-05-01 07:14:20,312 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_213 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_213/final_results.json
2025-05-01 07:14:20,312 - main - INFO - 
Summary for configuration 213:
2025-05-01 07:14:20,312 - main - INFO - Accuracy: 0.9653
2025-05-01 07:14:20,312 - main - INFO - Precision: 0.9313
2025-05-01 07:14:20,312 - main - INFO - Recall: 0.9476
2025-05-01 07:14:20,312 - main - INFO - F1 Score: 0.9393
2025-05-01 07:14:20,312 - main - INFO - IoU: 0.8856
2025-05-01 07:14:20,312 - main - INFO - mAP: 0.9700
2025-05-01 07:14:20,312 - main - INFO - AUC: 0.9870
2025-05-01 07:14:20,312 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:14:20,312 - main - INFO - 
==================================================
2025-05-01 07:14:20,312 - main - INFO - Running configuration 214/756:
2025-05-01 07:14:20,312 - main - INFO - Model: ResNet101
2025-05-01 07:14:20,312 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:14:20,312 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:14:20,312 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:14:20,312 - main - INFO - ==================================================
2025-05-01 07:14:20,312 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_214 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_214
2025-05-01 07:14:20,312 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_214 - INFO - Config: {
  "id": 214,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:14:20,989 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:14:20,989 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 214,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:14:20,989 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:14:22,191 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:14:22,193 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:14:22,196 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_214 - INFO - Starting model evaluation
2025-05-01 07:14:33,623 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_214 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9379
  Recall:    0.9510
  F1 Score:  0.9444
  IoU:       0.8947
  mAP:       0.9601
  AUC:       0.9841
2025-05-01 07:14:33,625 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_214 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_214/final_results.json
2025-05-01 07:14:33,627 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_214 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_214/final_results.json
2025-05-01 07:14:33,627 - main - INFO - 
Summary for configuration 214:
2025-05-01 07:14:33,627 - main - INFO - Accuracy: 0.9683
2025-05-01 07:14:33,627 - main - INFO - Precision: 0.9379
2025-05-01 07:14:33,627 - main - INFO - Recall: 0.9510
2025-05-01 07:14:33,627 - main - INFO - F1 Score: 0.9444
2025-05-01 07:14:33,627 - main - INFO - IoU: 0.8947
2025-05-01 07:14:33,627 - main - INFO - mAP: 0.9601
2025-05-01 07:14:33,627 - main - INFO - AUC: 0.9841
2025-05-01 07:14:33,627 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:14:33,627 - main - INFO - 
==================================================
2025-05-01 07:14:33,627 - main - INFO - Running configuration 215/756:
2025-05-01 07:14:33,627 - main - INFO - Model: ResNet101
2025-05-01 07:14:33,627 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:14:33,627 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:14:33,627 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:14:33,627 - main - INFO - ==================================================
2025-05-01 07:14:33,627 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_215 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_215
2025-05-01 07:14:33,627 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_215 - INFO - Config: {
  "id": 215,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:14:34,295 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:14:34,295 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 215,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:14:34,295 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:14:35,607 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:14:35,608 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:14:35,612 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_215 - INFO - Starting model evaluation
2025-05-01 07:14:46,908 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_215 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9343
  Recall:    0.9441
  F1 Score:  0.9391
  IoU:       0.8852
  mAP:       0.9613
  AUC:       0.9830
2025-05-01 07:14:46,909 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_215 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_215/final_results.json
2025-05-01 07:14:46,911 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_215 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_215/final_results.json
2025-05-01 07:14:46,911 - main - INFO - 
Summary for configuration 215:
2025-05-01 07:14:46,911 - main - INFO - Accuracy: 0.9653
2025-05-01 07:14:46,911 - main - INFO - Precision: 0.9343
2025-05-01 07:14:46,911 - main - INFO - Recall: 0.9441
2025-05-01 07:14:46,911 - main - INFO - F1 Score: 0.9391
2025-05-01 07:14:46,911 - main - INFO - IoU: 0.8852
2025-05-01 07:14:46,911 - main - INFO - mAP: 0.9613
2025-05-01 07:14:46,911 - main - INFO - AUC: 0.9830
2025-05-01 07:14:46,911 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:14:46,911 - main - INFO - 
==================================================
2025-05-01 07:14:46,911 - main - INFO - Running configuration 216/756:
2025-05-01 07:14:46,911 - main - INFO - Model: ResNet101
2025-05-01 07:14:46,911 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 07:14:46,911 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:14:46,911 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:14:46,911 - main - INFO - ==================================================
2025-05-01 07:14:46,911 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_216 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001_id_216
2025-05-01 07:14:46,911 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_216 - INFO - Config: {
  "id": 216,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:14:47,504 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.0001
2025-05-01 07:14:47,504 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 216,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:14:47,504 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 07:14:48,751 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9837
2025-05-01 07:14:48,752 - training.model_ResNet101_opt_AdamW_lr_0.0001 - INFO - Training completed after 1352.76 seconds
2025-05-01 07:14:48,756 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_216 - INFO - Starting model evaluation
2025-05-01 07:15:00,072 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_216 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9368
  Recall:    0.9336
  F1 Score:  0.9352
  IoU:       0.8783
  mAP:       0.9668
  AUC:       0.9846
2025-05-01 07:15:00,074 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_216 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_216/final_results.json
2025-05-01 07:15:00,076 - training.model_ResNet101_opt_AdamW_lr_0.0001_id_216 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.0001_id_216/final_results.json
2025-05-01 07:15:00,076 - main - INFO - 
Summary for configuration 216:
2025-05-01 07:15:00,076 - main - INFO - Accuracy: 0.9633
2025-05-01 07:15:00,076 - main - INFO - Precision: 0.9368
2025-05-01 07:15:00,076 - main - INFO - Recall: 0.9336
2025-05-01 07:15:00,076 - main - INFO - F1 Score: 0.9352
2025-05-01 07:15:00,076 - main - INFO - IoU: 0.8783
2025-05-01 07:15:00,076 - main - INFO - mAP: 0.9668
2025-05-01 07:15:00,076 - main - INFO - AUC: 0.9846
2025-05-01 07:15:00,076 - main - INFO - Training time: 1352.76 seconds
2025-05-01 07:15:00,076 - main - INFO - 
==================================================
2025-05-01 07:15:00,076 - main - INFO - Running configuration 217/756:
2025-05-01 07:15:00,076 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:15:00,076 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:15:00,076 - main - INFO - Scheduler: StepLR
2025-05-01 07:15:00,076 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:15:00,076 - main - INFO - ==================================================
2025-05-01 07:15:00,076 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_217 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_217
2025-05-01 07:15:00,076 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_217 - INFO - Config: {
  "id": 217,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:15:00,461 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:15:00,461 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 217,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:15:00,461 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 07:15:00,462 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 07:15:25,036 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 07:15:57,713 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.9305
2025-05-01 07:15:57,742 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 1 completed in 57.28s - Train Loss: 0.4648, Train Acc: 0.8511, Val Loss: 0.3838, Val Acc: 0.9305
2025-05-01 07:15:57,809 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 07:15:57,809 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 07:16:22,201 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 07:16:54,534 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9305 to 0.9545
2025-05-01 07:16:54,572 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 2 completed in 56.76s - Train Loss: 0.3655, Train Acc: 0.9500, Val Loss: 0.3588, Val Acc: 0.9545
2025-05-01 07:16:54,636 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 07:16:54,636 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 07:17:18,690 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 07:17:50,847 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9545 to 0.9708
2025-05-01 07:17:50,899 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 3 completed in 56.26s - Train Loss: 0.3487, Train Acc: 0.9655, Val Loss: 0.3424, Val Acc: 0.9708
2025-05-01 07:17:50,964 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 07:17:50,965 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 07:18:15,540 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 07:18:47,774 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation loss improved from inf to 0.3418
2025-05-01 07:18:47,825 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 4 completed in 56.86s - Train Loss: 0.3465, Train Acc: 0.9689, Val Loss: 0.3418, Val Acc: 0.9700
2025-05-01 07:18:47,889 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 07:18:47,889 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 07:19:11,096 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 07:19:44,572 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9708 to 0.9726
2025-05-01 07:19:44,611 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 5 completed in 56.72s - Train Loss: 0.3403, Train Acc: 0.9723, Val Loss: 0.3406, Val Acc: 0.9726
2025-05-01 07:19:44,675 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 07:19:44,676 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 07:20:09,501 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 07:20:41,138 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9726 to 0.9734
2025-05-01 07:20:41,175 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 6 completed in 56.50s - Train Loss: 0.3370, Train Acc: 0.9760, Val Loss: 0.3414, Val Acc: 0.9734
2025-05-01 07:20:41,236 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 07:20:41,237 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 07:21:05,259 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 07:21:37,887 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9734 to 0.9811
2025-05-01 07:21:37,934 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 7 completed in 56.70s - Train Loss: 0.3338, Train Acc: 0.9796, Val Loss: 0.3332, Val Acc: 0.9811
2025-05-01 07:21:37,999 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 07:21:37,999 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 07:22:01,232 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 07:22:33,852 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3418 to 0.3349
2025-05-01 07:22:33,894 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 8 completed in 55.89s - Train Loss: 0.3300, Train Acc: 0.9833, Val Loss: 0.3349, Val Acc: 0.9760
2025-05-01 07:22:33,958 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 07:22:33,958 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 07:22:57,948 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 07:23:30,563 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 9 completed in 56.60s - Train Loss: 0.3303, Train Acc: 0.9828, Val Loss: 0.3352, Val Acc: 0.9777
2025-05-01 07:23:30,638 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 07:23:30,639 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 07:23:54,117 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 07:24:26,687 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9811 to 0.9828
2025-05-01 07:24:26,731 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 10 completed in 56.09s - Train Loss: 0.3277, Train Acc: 0.9863, Val Loss: 0.3300, Val Acc: 0.9828
2025-05-01 07:24:26,797 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 07:24:26,797 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 07:24:51,005 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 07:25:23,068 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9828 to 0.9863
2025-05-01 07:25:23,105 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 11 completed in 56.31s - Train Loss: 0.3229, Train Acc: 0.9912, Val Loss: 0.3262, Val Acc: 0.9863
2025-05-01 07:25:23,166 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 07:25:23,167 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 07:25:47,494 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 07:26:20,961 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3349 to 0.3255
2025-05-01 07:26:21,004 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 12 completed in 57.84s - Train Loss: 0.3233, Train Acc: 0.9903, Val Loss: 0.3255, Val Acc: 0.9863
2025-05-01 07:26:21,071 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 07:26:21,071 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 07:26:45,143 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 07:27:18,028 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 13 completed in 56.96s - Train Loss: 0.3223, Train Acc: 0.9916, Val Loss: 0.3273, Val Acc: 0.9846
2025-05-01 07:27:18,097 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 07:27:18,097 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 07:27:42,546 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 07:28:14,561 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9863 to 0.9871
2025-05-01 07:28:14,600 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 14 completed in 56.50s - Train Loss: 0.3219, Train Acc: 0.9914, Val Loss: 0.3259, Val Acc: 0.9871
2025-05-01 07:28:14,663 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 07:28:14,664 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 07:28:38,978 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 07:29:10,650 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 15 completed in 55.99s - Train Loss: 0.3200, Train Acc: 0.9942, Val Loss: 0.3269, Val Acc: 0.9837
2025-05-01 07:29:10,720 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 07:29:10,721 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 07:29:35,305 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 07:30:07,416 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3255 to 0.3254
2025-05-01 07:30:07,462 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 16 completed in 56.74s - Train Loss: 0.3200, Train Acc: 0.9936, Val Loss: 0.3254, Val Acc: 0.9854
2025-05-01 07:30:07,524 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 07:30:07,524 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 07:30:31,657 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 07:31:02,357 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9871 to 0.9880
2025-05-01 07:31:02,414 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 17 completed in 54.89s - Train Loss: 0.3199, Train Acc: 0.9936, Val Loss: 0.3241, Val Acc: 0.9880
2025-05-01 07:31:02,482 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 07:31:02,483 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 07:31:26,711 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 07:31:59,964 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 18 completed in 57.48s - Train Loss: 0.3195, Train Acc: 0.9944, Val Loss: 0.3263, Val Acc: 0.9863
2025-05-01 07:32:00,044 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 07:32:00,044 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 07:32:24,098 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 07:32:56,540 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9880 to 0.9897
2025-05-01 07:32:56,585 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 19 completed in 56.54s - Train Loss: 0.3174, Train Acc: 0.9968, Val Loss: 0.3243, Val Acc: 0.9897
2025-05-01 07:32:56,651 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 07:32:56,651 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 07:33:21,226 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 07:33:54,057 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9897 to 0.9906
2025-05-01 07:33:54,097 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Epoch 20 completed in 57.45s - Train Loss: 0.3190, Train Acc: 0.9949, Val Loss: 0.3231, Val Acc: 0.9906
2025-05-01 07:33:54,159 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 07:33:54,159 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:33:54,163 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_217 - INFO - Starting model evaluation
2025-05-01 07:34:05,209 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_217 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9360
  Recall:    0.9720
  F1 Score:  0.9537
  IoU:       0.9115
  mAP:       0.9881
  AUC:       0.9916
2025-05-01 07:34:05,210 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_217 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_217/final_results.json
2025-05-01 07:34:05,212 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_217 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_217/final_results.json
2025-05-01 07:34:05,212 - main - INFO - 
Summary for configuration 217:
2025-05-01 07:34:05,212 - main - INFO - Accuracy: 0.9732
2025-05-01 07:34:05,212 - main - INFO - Precision: 0.9360
2025-05-01 07:34:05,212 - main - INFO - Recall: 0.9720
2025-05-01 07:34:05,212 - main - INFO - F1 Score: 0.9537
2025-05-01 07:34:05,212 - main - INFO - IoU: 0.9115
2025-05-01 07:34:05,212 - main - INFO - mAP: 0.9881
2025-05-01 07:34:05,212 - main - INFO - AUC: 0.9916
2025-05-01 07:34:05,212 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:34:05,212 - main - INFO - 
==================================================
2025-05-01 07:34:05,212 - main - INFO - Running configuration 218/756:
2025-05-01 07:34:05,212 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:34:05,212 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:34:05,212 - main - INFO - Scheduler: StepLR
2025-05-01 07:34:05,212 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:34:05,212 - main - INFO - ==================================================
2025-05-01 07:34:05,212 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_218 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_218
2025-05-01 07:34:05,212 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_218 - INFO - Config: {
  "id": 218,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:34:05,540 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:34:05,540 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 218,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:34:05,540 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:34:06,012 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:34:06,013 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:34:06,017 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_218 - INFO - Starting model evaluation
2025-05-01 07:34:16,967 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_218 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9324
  Recall:    0.9650
  F1 Score:  0.9485
  IoU:       0.9020
  mAP:       0.9892
  AUC:       0.9921
2025-05-01 07:34:16,969 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_218 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_218/final_results.json
2025-05-01 07:34:16,971 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_218 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_218/final_results.json
2025-05-01 07:34:16,971 - main - INFO - 
Summary for configuration 218:
2025-05-01 07:34:16,971 - main - INFO - Accuracy: 0.9703
2025-05-01 07:34:16,971 - main - INFO - Precision: 0.9324
2025-05-01 07:34:16,971 - main - INFO - Recall: 0.9650
2025-05-01 07:34:16,971 - main - INFO - F1 Score: 0.9485
2025-05-01 07:34:16,971 - main - INFO - IoU: 0.9020
2025-05-01 07:34:16,971 - main - INFO - mAP: 0.9892
2025-05-01 07:34:16,971 - main - INFO - AUC: 0.9921
2025-05-01 07:34:16,971 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:34:16,971 - main - INFO - 
==================================================
2025-05-01 07:34:16,971 - main - INFO - Running configuration 219/756:
2025-05-01 07:34:16,971 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:34:16,971 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:34:16,971 - main - INFO - Scheduler: StepLR
2025-05-01 07:34:16,971 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:34:16,971 - main - INFO - ==================================================
2025-05-01 07:34:16,971 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_219 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_219
2025-05-01 07:34:16,971 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_219 - INFO - Config: {
  "id": 219,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:34:17,260 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:34:17,260 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 219,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:34:17,260 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:34:17,880 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:34:17,882 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:34:17,886 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_219 - INFO - Starting model evaluation
2025-05-01 07:34:28,762 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_219 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9262
  Recall:    0.9650
  F1 Score:  0.9452
  IoU:       0.8961
  mAP:       0.9864
  AUC:       0.9910
2025-05-01 07:34:28,764 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_219 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_219/final_results.json
2025-05-01 07:34:28,765 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_219 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_219/final_results.json
2025-05-01 07:34:28,765 - main - INFO - 
Summary for configuration 219:
2025-05-01 07:34:28,765 - main - INFO - Accuracy: 0.9683
2025-05-01 07:34:28,765 - main - INFO - Precision: 0.9262
2025-05-01 07:34:28,765 - main - INFO - Recall: 0.9650
2025-05-01 07:34:28,765 - main - INFO - F1 Score: 0.9452
2025-05-01 07:34:28,765 - main - INFO - IoU: 0.8961
2025-05-01 07:34:28,765 - main - INFO - mAP: 0.9864
2025-05-01 07:34:28,765 - main - INFO - AUC: 0.9910
2025-05-01 07:34:28,765 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:34:28,765 - main - INFO - 
==================================================
2025-05-01 07:34:28,765 - main - INFO - Running configuration 220/756:
2025-05-01 07:34:28,765 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:34:28,765 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:34:28,765 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:34:28,765 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:34:28,765 - main - INFO - ==================================================
2025-05-01 07:34:28,766 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_220 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_220
2025-05-01 07:34:28,766 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_220 - INFO - Config: {
  "id": 220,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:34:29,173 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:34:29,173 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 220,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:34:29,173 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:34:29,774 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:34:29,775 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:34:29,779 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_220 - INFO - Starting model evaluation
2025-05-01 07:34:40,942 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_220 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9293
  Recall:    0.9650
  F1 Score:  0.9468
  IoU:       0.8990
  mAP:       0.9882
  AUC:       0.9922
2025-05-01 07:34:40,944 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_220 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_220/final_results.json
2025-05-01 07:34:40,946 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_220 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_220/final_results.json
2025-05-01 07:34:40,946 - main - INFO - 
Summary for configuration 220:
2025-05-01 07:34:40,946 - main - INFO - Accuracy: 0.9693
2025-05-01 07:34:40,946 - main - INFO - Precision: 0.9293
2025-05-01 07:34:40,946 - main - INFO - Recall: 0.9650
2025-05-01 07:34:40,946 - main - INFO - F1 Score: 0.9468
2025-05-01 07:34:40,946 - main - INFO - IoU: 0.8990
2025-05-01 07:34:40,946 - main - INFO - mAP: 0.9882
2025-05-01 07:34:40,946 - main - INFO - AUC: 0.9922
2025-05-01 07:34:40,946 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:34:40,946 - main - INFO - 
==================================================
2025-05-01 07:34:40,946 - main - INFO - Running configuration 221/756:
2025-05-01 07:34:40,946 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:34:40,946 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:34:40,946 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:34:40,946 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:34:40,946 - main - INFO - ==================================================
2025-05-01 07:34:40,946 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_221 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_221
2025-05-01 07:34:40,946 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_221 - INFO - Config: {
  "id": 221,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:34:41,255 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:34:41,255 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 221,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:34:41,255 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:34:41,857 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:34:41,859 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:34:41,862 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_221 - INFO - Starting model evaluation
2025-05-01 07:34:52,789 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_221 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9386
  Recall:    0.9615
  F1 Score:  0.9499
  IoU:       0.9046
  mAP:       0.9875
  AUC:       0.9921
2025-05-01 07:34:52,791 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_221 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_221/final_results.json
2025-05-01 07:34:52,792 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_221 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_221/final_results.json
2025-05-01 07:34:52,792 - main - INFO - 
Summary for configuration 221:
2025-05-01 07:34:52,792 - main - INFO - Accuracy: 0.9713
2025-05-01 07:34:52,792 - main - INFO - Precision: 0.9386
2025-05-01 07:34:52,792 - main - INFO - Recall: 0.9615
2025-05-01 07:34:52,792 - main - INFO - F1 Score: 0.9499
2025-05-01 07:34:52,792 - main - INFO - IoU: 0.9046
2025-05-01 07:34:52,792 - main - INFO - mAP: 0.9875
2025-05-01 07:34:52,792 - main - INFO - AUC: 0.9921
2025-05-01 07:34:52,792 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:34:52,792 - main - INFO - 
==================================================
2025-05-01 07:34:52,792 - main - INFO - Running configuration 222/756:
2025-05-01 07:34:52,792 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:34:52,792 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:34:52,792 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:34:52,792 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:34:52,792 - main - INFO - ==================================================
2025-05-01 07:34:52,793 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_222 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_222
2025-05-01 07:34:52,793 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_222 - INFO - Config: {
  "id": 222,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:34:53,079 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:34:53,079 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 222,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:34:53,079 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:34:53,630 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:34:53,631 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:34:53,635 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_222 - INFO - Starting model evaluation
2025-05-01 07:35:04,491 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_222 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9329
  Recall:    0.9720
  F1 Score:  0.9521
  IoU:       0.9085
  mAP:       0.9868
  AUC:       0.9917
2025-05-01 07:35:04,493 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_222 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_222/final_results.json
2025-05-01 07:35:04,495 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_222 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_222/final_results.json
2025-05-01 07:35:04,495 - main - INFO - 
Summary for configuration 222:
2025-05-01 07:35:04,495 - main - INFO - Accuracy: 0.9722
2025-05-01 07:35:04,495 - main - INFO - Precision: 0.9329
2025-05-01 07:35:04,495 - main - INFO - Recall: 0.9720
2025-05-01 07:35:04,495 - main - INFO - F1 Score: 0.9521
2025-05-01 07:35:04,495 - main - INFO - IoU: 0.9085
2025-05-01 07:35:04,495 - main - INFO - mAP: 0.9868
2025-05-01 07:35:04,495 - main - INFO - AUC: 0.9917
2025-05-01 07:35:04,495 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:35:04,495 - main - INFO - 
==================================================
2025-05-01 07:35:04,495 - main - INFO - Running configuration 223/756:
2025-05-01 07:35:04,495 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:35:04,495 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:35:04,495 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:35:04,495 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:35:04,495 - main - INFO - ==================================================
2025-05-01 07:35:04,495 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_223 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_223
2025-05-01 07:35:04,495 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_223 - INFO - Config: {
  "id": 223,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:35:04,780 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:35:04,780 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 223,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:35:04,780 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:35:05,399 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:35:05,400 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:35:05,404 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_223 - INFO - Starting model evaluation
2025-05-01 07:35:16,501 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_223 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9233
  Recall:    0.9685
  F1 Score:  0.9454
  IoU:       0.8964
  mAP:       0.9870
  AUC:       0.9918
2025-05-01 07:35:16,503 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_223 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_223/final_results.json
2025-05-01 07:35:16,504 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_223 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_223/final_results.json
2025-05-01 07:35:16,504 - main - INFO - 
Summary for configuration 223:
2025-05-01 07:35:16,504 - main - INFO - Accuracy: 0.9683
2025-05-01 07:35:16,504 - main - INFO - Precision: 0.9233
2025-05-01 07:35:16,504 - main - INFO - Recall: 0.9685
2025-05-01 07:35:16,504 - main - INFO - F1 Score: 0.9454
2025-05-01 07:35:16,504 - main - INFO - IoU: 0.8964
2025-05-01 07:35:16,504 - main - INFO - mAP: 0.9870
2025-05-01 07:35:16,504 - main - INFO - AUC: 0.9918
2025-05-01 07:35:16,504 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:35:16,504 - main - INFO - 
==================================================
2025-05-01 07:35:16,504 - main - INFO - Running configuration 224/756:
2025-05-01 07:35:16,504 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:35:16,504 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:35:16,504 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:35:16,504 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:35:16,504 - main - INFO - ==================================================
2025-05-01 07:35:16,505 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_224 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_224
2025-05-01 07:35:16,505 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_224 - INFO - Config: {
  "id": 224,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:35:16,854 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:35:16,855 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 224,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:35:16,855 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:35:17,328 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:35:17,329 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:35:17,333 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_224 - INFO - Starting model evaluation
2025-05-01 07:35:28,295 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_224 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9320
  Recall:    0.9580
  F1 Score:  0.9448
  IoU:       0.8954
  mAP:       0.9873
  AUC:       0.9919
2025-05-01 07:35:28,296 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_224 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_224/final_results.json
2025-05-01 07:35:28,298 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_224 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_224/final_results.json
2025-05-01 07:35:28,298 - main - INFO - 
Summary for configuration 224:
2025-05-01 07:35:28,298 - main - INFO - Accuracy: 0.9683
2025-05-01 07:35:28,298 - main - INFO - Precision: 0.9320
2025-05-01 07:35:28,298 - main - INFO - Recall: 0.9580
2025-05-01 07:35:28,298 - main - INFO - F1 Score: 0.9448
2025-05-01 07:35:28,298 - main - INFO - IoU: 0.8954
2025-05-01 07:35:28,298 - main - INFO - mAP: 0.9873
2025-05-01 07:35:28,298 - main - INFO - AUC: 0.9919
2025-05-01 07:35:28,298 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:35:28,298 - main - INFO - 
==================================================
2025-05-01 07:35:28,298 - main - INFO - Running configuration 225/756:
2025-05-01 07:35:28,298 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:35:28,298 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:35:28,298 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:35:28,298 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:35:28,298 - main - INFO - ==================================================
2025-05-01 07:35:28,298 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_225 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_225
2025-05-01 07:35:28,298 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_225 - INFO - Config: {
  "id": 225,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:35:28,780 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:35:28,780 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 225,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:35:28,781 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:35:29,385 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:35:29,386 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:35:29,390 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_225 - INFO - Starting model evaluation
2025-05-01 07:35:40,467 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_225 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9231
  Recall:    0.9650
  F1 Score:  0.9436
  IoU:       0.8932
  mAP:       0.9874
  AUC:       0.9916
2025-05-01 07:35:40,469 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_225 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_225/final_results.json
2025-05-01 07:35:40,470 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_225 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_225/final_results.json
2025-05-01 07:35:40,470 - main - INFO - 
Summary for configuration 225:
2025-05-01 07:35:40,470 - main - INFO - Accuracy: 0.9673
2025-05-01 07:35:40,470 - main - INFO - Precision: 0.9231
2025-05-01 07:35:40,470 - main - INFO - Recall: 0.9650
2025-05-01 07:35:40,470 - main - INFO - F1 Score: 0.9436
2025-05-01 07:35:40,470 - main - INFO - IoU: 0.8932
2025-05-01 07:35:40,470 - main - INFO - mAP: 0.9874
2025-05-01 07:35:40,470 - main - INFO - AUC: 0.9916
2025-05-01 07:35:40,470 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:35:40,470 - main - INFO - 
==================================================
2025-05-01 07:35:40,470 - main - INFO - Running configuration 226/756:
2025-05-01 07:35:40,470 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:35:40,470 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:35:40,470 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:35:40,470 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:35:40,470 - main - INFO - ==================================================
2025-05-01 07:35:40,471 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_226 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_226
2025-05-01 07:35:40,471 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_226 - INFO - Config: {
  "id": 226,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:35:40,885 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:35:40,885 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 226,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:35:40,886 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:35:41,475 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:35:41,477 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:35:41,481 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_226 - INFO - Starting model evaluation
2025-05-01 07:35:52,627 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_226 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9324
  Recall:    0.9650
  F1 Score:  0.9485
  IoU:       0.9020
  mAP:       0.9869
  AUC:       0.9912
2025-05-01 07:35:52,629 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_226 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_226/final_results.json
2025-05-01 07:35:52,630 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_226 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_226/final_results.json
2025-05-01 07:35:52,630 - main - INFO - 
Summary for configuration 226:
2025-05-01 07:35:52,630 - main - INFO - Accuracy: 0.9703
2025-05-01 07:35:52,630 - main - INFO - Precision: 0.9324
2025-05-01 07:35:52,630 - main - INFO - Recall: 0.9650
2025-05-01 07:35:52,630 - main - INFO - F1 Score: 0.9485
2025-05-01 07:35:52,630 - main - INFO - IoU: 0.9020
2025-05-01 07:35:52,630 - main - INFO - mAP: 0.9869
2025-05-01 07:35:52,630 - main - INFO - AUC: 0.9912
2025-05-01 07:35:52,630 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:35:52,630 - main - INFO - 
==================================================
2025-05-01 07:35:52,630 - main - INFO - Running configuration 227/756:
2025-05-01 07:35:52,631 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:35:52,631 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:35:52,631 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:35:52,631 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:35:52,631 - main - INFO - ==================================================
2025-05-01 07:35:52,631 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_227 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_227
2025-05-01 07:35:52,631 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_227 - INFO - Config: {
  "id": 227,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:35:53,033 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:35:53,033 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 227,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:35:53,033 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:35:53,511 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:35:53,512 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:35:53,516 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_227 - INFO - Starting model evaluation
2025-05-01 07:36:04,545 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_227 - INFO - Evaluation metrics:
  Accuracy:  0.9732
  Precision: 0.9390
  Recall:    0.9685
  F1 Score:  0.9535
  IoU:       0.9112
  mAP:       0.9894
  AUC:       0.9922
2025-05-01 07:36:04,548 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_227 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_227/final_results.json
2025-05-01 07:36:04,550 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_227 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_227/final_results.json
2025-05-01 07:36:04,550 - main - INFO - 
Summary for configuration 227:
2025-05-01 07:36:04,550 - main - INFO - Accuracy: 0.9732
2025-05-01 07:36:04,550 - main - INFO - Precision: 0.9390
2025-05-01 07:36:04,550 - main - INFO - Recall: 0.9685
2025-05-01 07:36:04,550 - main - INFO - F1 Score: 0.9535
2025-05-01 07:36:04,550 - main - INFO - IoU: 0.9112
2025-05-01 07:36:04,550 - main - INFO - mAP: 0.9894
2025-05-01 07:36:04,550 - main - INFO - AUC: 0.9922
2025-05-01 07:36:04,550 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:36:04,551 - main - INFO - 
==================================================
2025-05-01 07:36:04,551 - main - INFO - Running configuration 228/756:
2025-05-01 07:36:04,551 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:36:04,551 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 07:36:04,551 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:36:04,551 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:36:04,551 - main - INFO - ==================================================
2025-05-01 07:36:04,551 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_228 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01_id_228
2025-05-01 07:36:04,551 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_228 - INFO - Config: {
  "id": 228,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:36:04,937 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.01
2025-05-01 07:36:04,937 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 228,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:36:04,937 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 07:36:05,537 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9906
2025-05-01 07:36:05,539 - training.model_EfficientNet-B2_opt_SGD_lr_0.01 - INFO - Training completed after 1133.63 seconds
2025-05-01 07:36:05,542 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_228 - INFO - Starting model evaluation
2025-05-01 07:36:16,627 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_228 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9322
  Recall:    0.9615
  F1 Score:  0.9466
  IoU:       0.8987
  mAP:       0.9868
  AUC:       0.9917
2025-05-01 07:36:16,629 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_228 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_228/final_results.json
2025-05-01 07:36:16,630 - training.model_EfficientNet-B2_opt_SGD_lr_0.01_id_228 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.01_id_228/final_results.json
2025-05-01 07:36:16,630 - main - INFO - 
Summary for configuration 228:
2025-05-01 07:36:16,631 - main - INFO - Accuracy: 0.9693
2025-05-01 07:36:16,631 - main - INFO - Precision: 0.9322
2025-05-01 07:36:16,631 - main - INFO - Recall: 0.9615
2025-05-01 07:36:16,631 - main - INFO - F1 Score: 0.9466
2025-05-01 07:36:16,631 - main - INFO - IoU: 0.8987
2025-05-01 07:36:16,631 - main - INFO - mAP: 0.9868
2025-05-01 07:36:16,631 - main - INFO - AUC: 0.9917
2025-05-01 07:36:16,631 - main - INFO - Training time: 1133.63 seconds
2025-05-01 07:36:16,631 - main - INFO - 
==================================================
2025-05-01 07:36:16,631 - main - INFO - Running configuration 229/756:
2025-05-01 07:36:16,631 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:36:16,631 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:36:16,631 - main - INFO - Scheduler: StepLR
2025-05-01 07:36:16,631 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:36:16,631 - main - INFO - ==================================================
2025-05-01 07:36:16,631 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_229 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_229
2025-05-01 07:36:16,631 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_229 - INFO - Config: {
  "id": 229,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:36:17,037 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:36:17,037 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 229,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:36:17,037 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 07:36:17,038 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 07:36:40,382 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 07:37:12,385 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.7950
2025-05-01 07:37:12,414 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 1 completed in 55.38s - Train Loss: 0.6226, Train Acc: 0.7014, Val Loss: 0.5468, Val Acc: 0.7950
2025-05-01 07:37:12,479 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 07:37:12,480 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 07:37:37,532 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 07:38:09,524 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.7950 to 0.8533
2025-05-01 07:38:09,573 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 2 completed in 57.09s - Train Loss: 0.5242, Train Acc: 0.8155, Val Loss: 0.4831, Val Acc: 0.8533
2025-05-01 07:38:09,637 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 07:38:09,638 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 07:38:34,387 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 07:39:07,692 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.8533 to 0.8902
2025-05-01 07:39:07,729 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 3 completed in 58.09s - Train Loss: 0.4752, Train Acc: 0.8576, Val Loss: 0.4457, Val Acc: 0.8902
2025-05-01 07:39:07,792 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 07:39:07,793 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 07:39:32,491 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 07:40:05,154 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.8902 to 0.9108
2025-05-01 07:40:05,191 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 4 completed in 57.40s - Train Loss: 0.4407, Train Acc: 0.8904, Val Loss: 0.4215, Val Acc: 0.9108
2025-05-01 07:40:05,250 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 07:40:05,250 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 07:40:28,766 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 07:41:00,330 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9108 to 0.9202
2025-05-01 07:41:00,371 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 5 completed in 55.12s - Train Loss: 0.4182, Train Acc: 0.9088, Val Loss: 0.4011, Val Acc: 0.9202
2025-05-01 07:41:00,435 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 07:41:00,436 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 07:41:24,960 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 07:41:57,090 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9202 to 0.9365
2025-05-01 07:41:57,127 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 6 completed in 56.69s - Train Loss: 0.4003, Train Acc: 0.9266, Val Loss: 0.3878, Val Acc: 0.9365
2025-05-01 07:41:57,194 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 07:41:57,195 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 07:42:21,218 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 07:42:55,491 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9365 to 0.9451
2025-05-01 07:42:55,531 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 7 completed in 58.34s - Train Loss: 0.3856, Train Acc: 0.9395, Val Loss: 0.3730, Val Acc: 0.9451
2025-05-01 07:42:55,596 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 07:42:55,597 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 07:43:21,334 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 07:43:56,738 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9451 to 0.9563
2025-05-01 07:43:56,776 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 8 completed in 61.18s - Train Loss: 0.3755, Train Acc: 0.9479, Val Loss: 0.3664, Val Acc: 0.9563
2025-05-01 07:43:56,843 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 07:43:56,844 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 07:44:23,367 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 07:44:57,785 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9563 to 0.9623
2025-05-01 07:44:57,824 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 9 completed in 60.98s - Train Loss: 0.3660, Train Acc: 0.9560, Val Loss: 0.3596, Val Acc: 0.9623
2025-05-01 07:44:57,892 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 07:44:57,893 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 07:45:22,921 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 07:45:58,438 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9623 to 0.9674
2025-05-01 07:45:58,477 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 10 completed in 60.58s - Train Loss: 0.3589, Train Acc: 0.9665, Val Loss: 0.3522, Val Acc: 0.9674
2025-05-01 07:45:58,541 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 07:45:58,542 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 07:46:23,819 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 07:46:57,840 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9674 to 0.9691
2025-05-01 07:46:57,887 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 11 completed in 59.35s - Train Loss: 0.3549, Train Acc: 0.9685, Val Loss: 0.3520, Val Acc: 0.9691
2025-05-01 07:46:57,948 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 07:46:57,948 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 07:47:22,288 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 07:47:54,251 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation loss improved from inf to 0.3549
2025-05-01 07:47:54,290 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 12 completed in 56.34s - Train Loss: 0.3568, Train Acc: 0.9655, Val Loss: 0.3549, Val Acc: 0.9631
2025-05-01 07:47:54,356 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 07:47:54,357 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 07:48:18,524 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 07:48:51,210 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3549 to 0.3499
2025-05-01 07:48:51,247 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 13 completed in 56.89s - Train Loss: 0.3540, Train Acc: 0.9678, Val Loss: 0.3499, Val Acc: 0.9683
2025-05-01 07:48:51,310 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 07:48:51,311 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 07:49:15,703 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 07:49:48,223 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 14 completed in 56.91s - Train Loss: 0.3542, Train Acc: 0.9695, Val Loss: 0.3541, Val Acc: 0.9666
2025-05-01 07:49:48,294 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 07:49:48,295 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 07:50:12,526 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 07:50:45,153 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 15 completed in 56.86s - Train Loss: 0.3520, Train Acc: 0.9670, Val Loss: 0.3516, Val Acc: 0.9657
2025-05-01 07:50:45,237 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 07:50:45,238 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 07:51:09,115 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 07:51:41,912 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 16 completed in 56.67s - Train Loss: 0.3553, Train Acc: 0.9653, Val Loss: 0.3510, Val Acc: 0.9683
2025-05-01 07:51:41,981 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 07:51:41,982 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 07:52:06,784 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 07:52:38,261 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3499 to 0.3498
2025-05-01 07:52:38,299 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 17 completed in 56.32s - Train Loss: 0.3545, Train Acc: 0.9672, Val Loss: 0.3498, Val Acc: 0.9683
2025-05-01 07:52:38,372 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 07:52:38,372 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 07:53:02,963 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 07:53:34,994 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9691 to 0.9708
2025-05-01 07:53:35,040 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 18 completed in 56.67s - Train Loss: 0.3510, Train Acc: 0.9689, Val Loss: 0.3479, Val Acc: 0.9708
2025-05-01 07:53:35,107 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 07:53:35,107 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 07:53:59,718 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 07:54:31,672 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 19 completed in 56.56s - Train Loss: 0.3545, Train Acc: 0.9655, Val Loss: 0.3508, Val Acc: 0.9666
2025-05-01 07:54:31,744 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 07:54:31,744 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 07:54:55,820 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 07:55:27,587 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Epoch 20 completed in 55.84s - Train Loss: 0.3544, Train Acc: 0.9674, Val Loss: 0.3499, Val Acc: 0.9683
2025-05-01 07:55:27,665 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 07:55:27,665 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:55:27,669 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_229 - INFO - Starting model evaluation
2025-05-01 07:55:38,408 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_229 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.9020
  Recall:    0.9336
  F1 Score:  0.9175
  IoU:       0.8476
  mAP:       0.9746
  AUC:       0.9892
2025-05-01 07:55:38,409 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_229 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_229/final_results.json
2025-05-01 07:55:38,411 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_229 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_229/final_results.json
2025-05-01 07:55:38,411 - main - INFO - 
Summary for configuration 229:
2025-05-01 07:55:38,411 - main - INFO - Accuracy: 0.9524
2025-05-01 07:55:38,411 - main - INFO - Precision: 0.9020
2025-05-01 07:55:38,411 - main - INFO - Recall: 0.9336
2025-05-01 07:55:38,411 - main - INFO - F1 Score: 0.9175
2025-05-01 07:55:38,411 - main - INFO - IoU: 0.8476
2025-05-01 07:55:38,411 - main - INFO - mAP: 0.9746
2025-05-01 07:55:38,411 - main - INFO - AUC: 0.9892
2025-05-01 07:55:38,411 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:55:38,411 - main - INFO - 
==================================================
2025-05-01 07:55:38,411 - main - INFO - Running configuration 230/756:
2025-05-01 07:55:38,411 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:55:38,411 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:55:38,411 - main - INFO - Scheduler: StepLR
2025-05-01 07:55:38,411 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:55:38,411 - main - INFO - ==================================================
2025-05-01 07:55:38,411 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_230 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_230
2025-05-01 07:55:38,411 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_230 - INFO - Config: {
  "id": 230,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:55:38,686 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:55:38,686 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 230,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:55:38,686 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:55:39,158 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:55:39,159 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:55:39,163 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_230 - INFO - Starting model evaluation
2025-05-01 07:55:50,188 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_230 - INFO - Evaluation metrics:
  Accuracy:  0.9534
  Precision: 0.8997
  Recall:    0.9406
  F1 Score:  0.9197
  IoU:       0.8513
  mAP:       0.9774
  AUC:       0.9901
2025-05-01 07:55:50,190 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_230 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_230/final_results.json
2025-05-01 07:55:50,191 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_230 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_230/final_results.json
2025-05-01 07:55:50,191 - main - INFO - 
Summary for configuration 230:
2025-05-01 07:55:50,191 - main - INFO - Accuracy: 0.9534
2025-05-01 07:55:50,191 - main - INFO - Precision: 0.8997
2025-05-01 07:55:50,191 - main - INFO - Recall: 0.9406
2025-05-01 07:55:50,191 - main - INFO - F1 Score: 0.9197
2025-05-01 07:55:50,191 - main - INFO - IoU: 0.8513
2025-05-01 07:55:50,192 - main - INFO - mAP: 0.9774
2025-05-01 07:55:50,192 - main - INFO - AUC: 0.9901
2025-05-01 07:55:50,192 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:55:50,192 - main - INFO - 
==================================================
2025-05-01 07:55:50,192 - main - INFO - Running configuration 231/756:
2025-05-01 07:55:50,192 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:55:50,192 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:55:50,192 - main - INFO - Scheduler: StepLR
2025-05-01 07:55:50,192 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:55:50,192 - main - INFO - ==================================================
2025-05-01 07:55:50,192 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_231 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_231
2025-05-01 07:55:50,192 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_231 - INFO - Config: {
  "id": 231,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:55:50,619 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:55:50,619 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 231,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:55:50,619 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:55:51,204 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:55:51,205 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:55:51,209 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_231 - INFO - Starting model evaluation
2025-05-01 07:56:02,293 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_231 - INFO - Evaluation metrics:
  Accuracy:  0.9475
  Precision: 0.8949
  Recall:    0.9231
  F1 Score:  0.9088
  IoU:       0.8328
  mAP:       0.9771
  AUC:       0.9891
2025-05-01 07:56:02,295 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_231 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_231/final_results.json
2025-05-01 07:56:02,296 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_231 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_231/final_results.json
2025-05-01 07:56:02,296 - main - INFO - 
Summary for configuration 231:
2025-05-01 07:56:02,296 - main - INFO - Accuracy: 0.9475
2025-05-01 07:56:02,296 - main - INFO - Precision: 0.8949
2025-05-01 07:56:02,296 - main - INFO - Recall: 0.9231
2025-05-01 07:56:02,296 - main - INFO - F1 Score: 0.9088
2025-05-01 07:56:02,296 - main - INFO - IoU: 0.8328
2025-05-01 07:56:02,296 - main - INFO - mAP: 0.9771
2025-05-01 07:56:02,296 - main - INFO - AUC: 0.9891
2025-05-01 07:56:02,296 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:56:02,296 - main - INFO - 
==================================================
2025-05-01 07:56:02,296 - main - INFO - Running configuration 232/756:
2025-05-01 07:56:02,296 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:56:02,296 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:56:02,296 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:56:02,296 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:56:02,296 - main - INFO - ==================================================
2025-05-01 07:56:02,296 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_232 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_232
2025-05-01 07:56:02,296 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_232 - INFO - Config: {
  "id": 232,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:56:02,667 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:56:02,668 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 232,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:56:02,668 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:56:03,272 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:56:03,273 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:56:03,277 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_232 - INFO - Starting model evaluation
2025-05-01 07:56:14,017 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_232 - INFO - Evaluation metrics:
  Accuracy:  0.9504
  Precision: 0.9014
  Recall:    0.9266
  F1 Score:  0.9138
  IoU:       0.8413
  mAP:       0.9752
  AUC:       0.9889
2025-05-01 07:56:14,019 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_232 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_232/final_results.json
2025-05-01 07:56:14,020 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_232 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_232/final_results.json
2025-05-01 07:56:14,020 - main - INFO - 
Summary for configuration 232:
2025-05-01 07:56:14,020 - main - INFO - Accuracy: 0.9504
2025-05-01 07:56:14,020 - main - INFO - Precision: 0.9014
2025-05-01 07:56:14,021 - main - INFO - Recall: 0.9266
2025-05-01 07:56:14,021 - main - INFO - F1 Score: 0.9138
2025-05-01 07:56:14,021 - main - INFO - IoU: 0.8413
2025-05-01 07:56:14,021 - main - INFO - mAP: 0.9752
2025-05-01 07:56:14,021 - main - INFO - AUC: 0.9889
2025-05-01 07:56:14,021 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:56:14,021 - main - INFO - 
==================================================
2025-05-01 07:56:14,021 - main - INFO - Running configuration 233/756:
2025-05-01 07:56:14,021 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:56:14,021 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:56:14,021 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:56:14,021 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:56:14,021 - main - INFO - ==================================================
2025-05-01 07:56:14,021 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_233 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_233
2025-05-01 07:56:14,021 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_233 - INFO - Config: {
  "id": 233,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:56:14,454 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:56:14,454 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 233,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:56:14,454 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:56:15,063 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:56:15,065 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:56:15,069 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_233 - INFO - Starting model evaluation
2025-05-01 07:56:26,266 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_233 - INFO - Evaluation metrics:
  Accuracy:  0.9495
  Precision: 0.8983
  Recall:    0.9266
  F1 Score:  0.9122
  IoU:       0.8386
  mAP:       0.9743
  AUC:       0.9892
2025-05-01 07:56:26,267 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_233 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_233/final_results.json
2025-05-01 07:56:26,269 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_233 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_233/final_results.json
2025-05-01 07:56:26,269 - main - INFO - 
Summary for configuration 233:
2025-05-01 07:56:26,269 - main - INFO - Accuracy: 0.9495
2025-05-01 07:56:26,269 - main - INFO - Precision: 0.8983
2025-05-01 07:56:26,269 - main - INFO - Recall: 0.9266
2025-05-01 07:56:26,269 - main - INFO - F1 Score: 0.9122
2025-05-01 07:56:26,269 - main - INFO - IoU: 0.8386
2025-05-01 07:56:26,269 - main - INFO - mAP: 0.9743
2025-05-01 07:56:26,269 - main - INFO - AUC: 0.9892
2025-05-01 07:56:26,269 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:56:26,269 - main - INFO - 
==================================================
2025-05-01 07:56:26,269 - main - INFO - Running configuration 234/756:
2025-05-01 07:56:26,269 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:56:26,269 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:56:26,269 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 07:56:26,269 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:56:26,269 - main - INFO - ==================================================
2025-05-01 07:56:26,269 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_234 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_234
2025-05-01 07:56:26,269 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_234 - INFO - Config: {
  "id": 234,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:56:26,531 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:56:26,532 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 234,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:56:26,532 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:56:27,156 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:56:27,157 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:56:27,161 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_234 - INFO - Starting model evaluation
2025-05-01 07:56:38,115 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_234 - INFO - Evaluation metrics:
  Accuracy:  0.9495
  Precision: 0.9038
  Recall:    0.9196
  F1 Score:  0.9116
  IoU:       0.8376
  mAP:       0.9763
  AUC:       0.9893
2025-05-01 07:56:38,117 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_234 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_234/final_results.json
2025-05-01 07:56:38,118 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_234 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_234/final_results.json
2025-05-01 07:56:38,118 - main - INFO - 
Summary for configuration 234:
2025-05-01 07:56:38,118 - main - INFO - Accuracy: 0.9495
2025-05-01 07:56:38,118 - main - INFO - Precision: 0.9038
2025-05-01 07:56:38,118 - main - INFO - Recall: 0.9196
2025-05-01 07:56:38,118 - main - INFO - F1 Score: 0.9116
2025-05-01 07:56:38,118 - main - INFO - IoU: 0.8376
2025-05-01 07:56:38,118 - main - INFO - mAP: 0.9763
2025-05-01 07:56:38,118 - main - INFO - AUC: 0.9893
2025-05-01 07:56:38,118 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:56:38,118 - main - INFO - 
==================================================
2025-05-01 07:56:38,118 - main - INFO - Running configuration 235/756:
2025-05-01 07:56:38,118 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:56:38,118 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:56:38,118 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:56:38,118 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:56:38,118 - main - INFO - ==================================================
2025-05-01 07:56:38,119 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_235 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_235
2025-05-01 07:56:38,119 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_235 - INFO - Config: {
  "id": 235,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:56:38,548 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:56:38,548 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 235,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:56:38,548 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:56:39,013 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:56:39,014 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:56:39,018 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_235 - INFO - Starting model evaluation
2025-05-01 07:56:50,015 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_235 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9110
  Recall:    0.9301
  F1 Score:  0.9204
  IoU:       0.8526
  mAP:       0.9764
  AUC:       0.9892
2025-05-01 07:56:50,017 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_235 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_235/final_results.json
2025-05-01 07:56:50,018 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_235 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_235/final_results.json
2025-05-01 07:56:50,018 - main - INFO - 
Summary for configuration 235:
2025-05-01 07:56:50,018 - main - INFO - Accuracy: 0.9544
2025-05-01 07:56:50,018 - main - INFO - Precision: 0.9110
2025-05-01 07:56:50,019 - main - INFO - Recall: 0.9301
2025-05-01 07:56:50,019 - main - INFO - F1 Score: 0.9204
2025-05-01 07:56:50,019 - main - INFO - IoU: 0.8526
2025-05-01 07:56:50,019 - main - INFO - mAP: 0.9764
2025-05-01 07:56:50,019 - main - INFO - AUC: 0.9892
2025-05-01 07:56:50,019 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:56:50,019 - main - INFO - 
==================================================
2025-05-01 07:56:50,019 - main - INFO - Running configuration 236/756:
2025-05-01 07:56:50,019 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:56:50,019 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:56:50,019 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:56:50,019 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:56:50,019 - main - INFO - ==================================================
2025-05-01 07:56:50,019 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_236 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_236
2025-05-01 07:56:50,019 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_236 - INFO - Config: {
  "id": 236,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:56:50,285 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:56:50,286 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 236,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:56:50,286 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:56:50,889 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:56:50,890 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:56:50,894 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_236 - INFO - Starting model evaluation
2025-05-01 07:57:02,117 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_236 - INFO - Evaluation metrics:
  Accuracy:  0.9534
  Precision: 0.9078
  Recall:    0.9301
  F1 Score:  0.9188
  IoU:       0.8498
  mAP:       0.9732
  AUC:       0.9887
2025-05-01 07:57:02,118 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_236 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_236/final_results.json
2025-05-01 07:57:02,120 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_236 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_236/final_results.json
2025-05-01 07:57:02,120 - main - INFO - 
Summary for configuration 236:
2025-05-01 07:57:02,120 - main - INFO - Accuracy: 0.9534
2025-05-01 07:57:02,120 - main - INFO - Precision: 0.9078
2025-05-01 07:57:02,120 - main - INFO - Recall: 0.9301
2025-05-01 07:57:02,120 - main - INFO - F1 Score: 0.9188
2025-05-01 07:57:02,120 - main - INFO - IoU: 0.8498
2025-05-01 07:57:02,120 - main - INFO - mAP: 0.9732
2025-05-01 07:57:02,120 - main - INFO - AUC: 0.9887
2025-05-01 07:57:02,120 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:57:02,120 - main - INFO - 
==================================================
2025-05-01 07:57:02,120 - main - INFO - Running configuration 237/756:
2025-05-01 07:57:02,120 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:57:02,120 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:57:02,120 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 07:57:02,120 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:57:02,120 - main - INFO - ==================================================
2025-05-01 07:57:02,120 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_237 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_237
2025-05-01 07:57:02,120 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_237 - INFO - Config: {
  "id": 237,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:57:02,555 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:57:02,555 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 237,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:57:02,555 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:57:03,147 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:57:03,149 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:57:03,152 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_237 - INFO - Starting model evaluation
2025-05-01 07:57:13,790 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_237 - INFO - Evaluation metrics:
  Accuracy:  0.9514
  Precision: 0.9044
  Recall:    0.9266
  F1 Score:  0.9154
  IoU:       0.8439
  mAP:       0.9765
  AUC:       0.9889
2025-05-01 07:57:13,791 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_237 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_237/final_results.json
2025-05-01 07:57:13,793 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_237 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_237/final_results.json
2025-05-01 07:57:13,793 - main - INFO - 
Summary for configuration 237:
2025-05-01 07:57:13,793 - main - INFO - Accuracy: 0.9514
2025-05-01 07:57:13,793 - main - INFO - Precision: 0.9044
2025-05-01 07:57:13,793 - main - INFO - Recall: 0.9266
2025-05-01 07:57:13,793 - main - INFO - F1 Score: 0.9154
2025-05-01 07:57:13,793 - main - INFO - IoU: 0.8439
2025-05-01 07:57:13,793 - main - INFO - mAP: 0.9765
2025-05-01 07:57:13,793 - main - INFO - AUC: 0.9889
2025-05-01 07:57:13,793 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:57:13,793 - main - INFO - 
==================================================
2025-05-01 07:57:13,793 - main - INFO - Running configuration 238/756:
2025-05-01 07:57:13,793 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:57:13,793 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:57:13,793 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:57:13,793 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:57:13,793 - main - INFO - ==================================================
2025-05-01 07:57:13,793 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_238 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_238
2025-05-01 07:57:13,793 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_238 - INFO - Config: {
  "id": 238,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:57:14,207 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:57:14,208 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 238,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:57:14,208 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:57:14,804 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:57:14,805 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:57:14,809 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_238 - INFO - Starting model evaluation
2025-05-01 07:57:25,716 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_238 - INFO - Evaluation metrics:
  Accuracy:  0.9514
  Precision: 0.9017
  Recall:    0.9301
  F1 Score:  0.9157
  IoU:       0.8444
  mAP:       0.9756
  AUC:       0.9894
2025-05-01 07:57:25,718 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_238 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_238/final_results.json
2025-05-01 07:57:25,719 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_238 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_238/final_results.json
2025-05-01 07:57:25,719 - main - INFO - 
Summary for configuration 238:
2025-05-01 07:57:25,719 - main - INFO - Accuracy: 0.9514
2025-05-01 07:57:25,719 - main - INFO - Precision: 0.9017
2025-05-01 07:57:25,719 - main - INFO - Recall: 0.9301
2025-05-01 07:57:25,719 - main - INFO - F1 Score: 0.9157
2025-05-01 07:57:25,719 - main - INFO - IoU: 0.8444
2025-05-01 07:57:25,720 - main - INFO - mAP: 0.9756
2025-05-01 07:57:25,720 - main - INFO - AUC: 0.9894
2025-05-01 07:57:25,720 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:57:25,720 - main - INFO - 
==================================================
2025-05-01 07:57:25,720 - main - INFO - Running configuration 239/756:
2025-05-01 07:57:25,720 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:57:25,720 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:57:25,720 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:57:25,720 - main - INFO - Loss Function: FocalLoss
2025-05-01 07:57:25,720 - main - INFO - ==================================================
2025-05-01 07:57:25,720 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_239 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_239
2025-05-01 07:57:25,720 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_239 - INFO - Config: {
  "id": 239,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:57:26,010 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:57:26,010 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 239,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 07:57:26,010 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:57:26,476 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:57:26,478 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:57:26,481 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_239 - INFO - Starting model evaluation
2025-05-01 07:57:37,726 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_239 - INFO - Evaluation metrics:
  Accuracy:  0.9534
  Precision: 0.9107
  Recall:    0.9266
  F1 Score:  0.9185
  IoU:       0.8494
  mAP:       0.9773
  AUC:       0.9895
2025-05-01 07:57:37,728 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_239 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_239/final_results.json
2025-05-01 07:57:37,729 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_239 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_239/final_results.json
2025-05-01 07:57:37,729 - main - INFO - 
Summary for configuration 239:
2025-05-01 07:57:37,729 - main - INFO - Accuracy: 0.9534
2025-05-01 07:57:37,729 - main - INFO - Precision: 0.9107
2025-05-01 07:57:37,729 - main - INFO - Recall: 0.9266
2025-05-01 07:57:37,729 - main - INFO - F1 Score: 0.9185
2025-05-01 07:57:37,729 - main - INFO - IoU: 0.8494
2025-05-01 07:57:37,729 - main - INFO - mAP: 0.9773
2025-05-01 07:57:37,729 - main - INFO - AUC: 0.9895
2025-05-01 07:57:37,729 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:57:37,729 - main - INFO - 
==================================================
2025-05-01 07:57:37,729 - main - INFO - Running configuration 240/756:
2025-05-01 07:57:37,729 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:57:37,729 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 07:57:37,729 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 07:57:37,729 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 07:57:37,729 - main - INFO - ==================================================
2025-05-01 07:57:37,730 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_240 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001_id_240
2025-05-01 07:57:37,730 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_240 - INFO - Config: {
  "id": 240,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:57:38,074 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.001
2025-05-01 07:57:38,075 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 240,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 07:57:38,075 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 07:57:38,679 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-01 07:57:38,681 - training.model_EfficientNet-B2_opt_SGD_lr_0.001 - INFO - Training completed after 1150.55 seconds
2025-05-01 07:57:38,684 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_240 - INFO - Starting model evaluation
2025-05-01 07:57:49,291 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_240 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.9020
  Recall:    0.9336
  F1 Score:  0.9175
  IoU:       0.8476
  mAP:       0.9752
  AUC:       0.9892
2025-05-01 07:57:49,292 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_240 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_240/final_results.json
2025-05-01 07:57:49,294 - training.model_EfficientNet-B2_opt_SGD_lr_0.001_id_240 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.001_id_240/final_results.json
2025-05-01 07:57:49,294 - main - INFO - 
Summary for configuration 240:
2025-05-01 07:57:49,294 - main - INFO - Accuracy: 0.9524
2025-05-01 07:57:49,294 - main - INFO - Precision: 0.9020
2025-05-01 07:57:49,294 - main - INFO - Recall: 0.9336
2025-05-01 07:57:49,294 - main - INFO - F1 Score: 0.9175
2025-05-01 07:57:49,294 - main - INFO - IoU: 0.8476
2025-05-01 07:57:49,294 - main - INFO - mAP: 0.9752
2025-05-01 07:57:49,294 - main - INFO - AUC: 0.9892
2025-05-01 07:57:49,294 - main - INFO - Training time: 1150.55 seconds
2025-05-01 07:57:49,294 - main - INFO - 
==================================================
2025-05-01 07:57:49,294 - main - INFO - Running configuration 241/756:
2025-05-01 07:57:49,294 - main - INFO - Model: EfficientNet-B2
2025-05-01 07:57:49,294 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 07:57:49,294 - main - INFO - Scheduler: StepLR
2025-05-01 07:57:49,294 - main - INFO - Loss Function: CrossEntropy
2025-05-01 07:57:49,294 - main - INFO - ==================================================
2025-05-01 07:57:49,294 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241
2025-05-01 07:57:49,294 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241 - INFO - Config: {
  "id": 241,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:57:49,675 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 07:57:49,678 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 241,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 07:57:49,678 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 07:57:49,680 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 07:58:14,778 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 07:58:47,281 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.6304
2025-05-01 07:58:47,321 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 1 completed in 57.64s - Train Loss: 0.6863, Train Acc: 0.5523, Val Loss: 0.6718, Val Acc: 0.6304
2025-05-01 07:58:47,392 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 07:58:47,393 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 07:59:11,573 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 07:59:45,667 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.6304 to 0.6981
2025-05-01 07:59:45,711 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 2 completed in 58.32s - Train Loss: 0.6633, Train Acc: 0.6555, Val Loss: 0.6466, Val Acc: 0.6981
2025-05-01 07:59:45,774 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 07:59:45,775 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 08:00:09,975 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 08:00:43,257 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.6981 to 0.7350
2025-05-01 08:00:43,297 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 3 completed in 57.52s - Train Loss: 0.6430, Train Acc: 0.7048, Val Loss: 0.6270, Val Acc: 0.7350
2025-05-01 08:00:43,363 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 08:00:43,364 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 08:01:08,372 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 08:01:41,006 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7350 to 0.7530
2025-05-01 08:01:41,044 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 4 completed in 57.68s - Train Loss: 0.6265, Train Acc: 0.7392, Val Loss: 0.6112, Val Acc: 0.7530
2025-05-01 08:01:41,112 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 08:01:41,112 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 08:02:05,289 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 08:02:38,136 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7530 to 0.7642
2025-05-01 08:02:38,172 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 5 completed in 57.06s - Train Loss: 0.6099, Train Acc: 0.7570, Val Loss: 0.5975, Val Acc: 0.7642
2025-05-01 08:02:38,238 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 08:02:38,239 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 08:03:02,551 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 08:03:34,178 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7642 to 0.7710
2025-05-01 08:03:34,226 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 6 completed in 55.99s - Train Loss: 0.5969, Train Acc: 0.7735, Val Loss: 0.5834, Val Acc: 0.7710
2025-05-01 08:03:34,293 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 08:03:34,294 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 08:03:59,045 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 08:04:31,013 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation loss improved from inf to 0.5722
2025-05-01 08:04:31,064 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 7 completed in 56.77s - Train Loss: 0.5857, Train Acc: 0.7763, Val Loss: 0.5722, Val Acc: 0.7702
2025-05-01 08:04:31,129 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 08:04:31,130 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 08:04:55,182 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 08:05:26,907 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7710 to 0.7796
2025-05-01 08:05:26,948 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 8 completed in 55.82s - Train Loss: 0.5737, Train Acc: 0.7870, Val Loss: 0.5608, Val Acc: 0.7796
2025-05-01 08:05:27,014 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 08:05:27,015 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 08:05:51,485 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 08:06:23,366 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7796 to 0.7804
2025-05-01 08:06:23,406 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 9 completed in 56.39s - Train Loss: 0.5632, Train Acc: 0.7898, Val Loss: 0.5527, Val Acc: 0.7804
2025-05-01 08:06:23,470 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 08:06:23,471 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 08:06:47,629 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 08:07:18,969 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7804 to 0.7899
2025-05-01 08:07:19,007 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 10 completed in 55.54s - Train Loss: 0.5534, Train Acc: 0.7997, Val Loss: 0.5440, Val Acc: 0.7899
2025-05-01 08:07:19,072 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 08:07:19,072 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 08:07:42,575 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 08:08:14,334 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7899 to 0.8027
2025-05-01 08:08:14,373 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 11 completed in 55.30s - Train Loss: 0.5487, Train Acc: 0.8001, Val Loss: 0.5444, Val Acc: 0.8027
2025-05-01 08:08:14,436 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 08:08:14,436 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 08:08:37,843 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 08:09:11,005 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5722 to 0.5406
2025-05-01 08:09:11,043 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 12 completed in 56.61s - Train Loss: 0.5475, Train Acc: 0.8069, Val Loss: 0.5406, Val Acc: 0.8019
2025-05-01 08:09:11,115 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 08:09:11,116 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 08:09:36,012 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 08:10:07,960 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5406 to 0.5405
2025-05-01 08:10:07,998 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 13 completed in 56.88s - Train Loss: 0.5493, Train Acc: 0.7994, Val Loss: 0.5405, Val Acc: 0.7899
2025-05-01 08:10:08,066 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 08:10:08,066 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 08:10:31,471 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 08:11:02,831 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 14 completed in 54.77s - Train Loss: 0.5473, Train Acc: 0.8052, Val Loss: 0.5425, Val Acc: 0.8010
2025-05-01 08:11:02,898 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 08:11:02,899 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 08:11:27,274 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 08:12:00,390 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8027 to 0.8045
2025-05-01 08:12:00,429 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 15 completed in 57.53s - Train Loss: 0.5450, Train Acc: 0.8102, Val Loss: 0.5413, Val Acc: 0.8045
2025-05-01 08:12:00,496 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 08:12:00,497 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 08:12:24,746 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 08:12:56,881 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5405 to 0.5373
2025-05-01 08:12:56,930 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 16 completed in 56.43s - Train Loss: 0.5439, Train Acc: 0.8078, Val Loss: 0.5373, Val Acc: 0.8027
2025-05-01 08:12:56,995 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 08:12:56,995 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 08:13:20,237 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 08:13:52,953 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 17 completed in 55.96s - Train Loss: 0.5454, Train Acc: 0.8042, Val Loss: 0.5378, Val Acc: 0.8045
2025-05-01 08:13:53,037 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 08:13:53,037 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 08:14:17,012 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 08:14:49,264 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8045 to 0.8087
2025-05-01 08:14:49,384 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 18 completed in 56.35s - Train Loss: 0.5440, Train Acc: 0.8065, Val Loss: 0.5358, Val Acc: 0.8087
2025-05-01 08:14:49,449 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 08:14:49,449 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 08:15:13,874 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 08:15:45,613 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8087 to 0.8105
2025-05-01 08:15:45,649 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 19 completed in 56.20s - Train Loss: 0.5413, Train Acc: 0.8097, Val Loss: 0.5355, Val Acc: 0.8105
2025-05-01 08:15:45,715 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 08:15:45,715 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 08:16:09,003 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 08:16:41,497 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5373 to 0.5363
2025-05-01 08:16:41,544 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Epoch 20 completed in 55.83s - Train Loss: 0.5422, Train Acc: 0.8057, Val Loss: 0.5363, Val Acc: 0.8002
2025-05-01 08:16:41,612 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 08:16:41,613 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:16:41,617 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241 - INFO - Starting model evaluation
2025-05-01 08:16:52,725 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241 - INFO - Evaluation metrics:
  Accuracy:  0.8295
  Precision: 0.6792
  Recall:    0.7552
  F1 Score:  0.7152
  IoU:       0.5567
  mAP:       0.8456
  AUC:       0.9092
2025-05-01 08:16:52,727 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241/final_results.json
2025-05-01 08:16:52,728 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_241/final_results.json
2025-05-01 08:16:52,728 - main - INFO - 
Summary for configuration 241:
2025-05-01 08:16:52,728 - main - INFO - Accuracy: 0.8295
2025-05-01 08:16:52,728 - main - INFO - Precision: 0.6792
2025-05-01 08:16:52,728 - main - INFO - Recall: 0.7552
2025-05-01 08:16:52,728 - main - INFO - F1 Score: 0.7152
2025-05-01 08:16:52,728 - main - INFO - IoU: 0.5567
2025-05-01 08:16:52,728 - main - INFO - mAP: 0.8456
2025-05-01 08:16:52,728 - main - INFO - AUC: 0.9092
2025-05-01 08:16:52,728 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:16:52,728 - main - INFO - 
==================================================
2025-05-01 08:16:52,728 - main - INFO - Running configuration 242/756:
2025-05-01 08:16:52,728 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:16:52,728 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:16:52,728 - main - INFO - Scheduler: StepLR
2025-05-01 08:16:52,728 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:16:52,728 - main - INFO - ==================================================
2025-05-01 08:16:52,728 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242
2025-05-01 08:16:52,728 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242 - INFO - Config: {
  "id": 242,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:16:53,131 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:16:53,131 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 242,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:16:53,131 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:16:53,609 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:16:53,610 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:16:53,616 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242 - INFO - Starting model evaluation
2025-05-01 08:17:04,505 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242 - INFO - Evaluation metrics:
  Accuracy:  0.8246
  Precision: 0.6647
  Recall:    0.7692
  F1 Score:  0.7131
  IoU:       0.5542
  mAP:       0.8373
  AUC:       0.9022
2025-05-01 08:17:04,508 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242/final_results.json
2025-05-01 08:17:04,510 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_242/final_results.json
2025-05-01 08:17:04,510 - main - INFO - 
Summary for configuration 242:
2025-05-01 08:17:04,510 - main - INFO - Accuracy: 0.8246
2025-05-01 08:17:04,510 - main - INFO - Precision: 0.6647
2025-05-01 08:17:04,510 - main - INFO - Recall: 0.7692
2025-05-01 08:17:04,510 - main - INFO - F1 Score: 0.7131
2025-05-01 08:17:04,510 - main - INFO - IoU: 0.5542
2025-05-01 08:17:04,510 - main - INFO - mAP: 0.8373
2025-05-01 08:17:04,510 - main - INFO - AUC: 0.9022
2025-05-01 08:17:04,510 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:17:04,510 - main - INFO - 
==================================================
2025-05-01 08:17:04,510 - main - INFO - Running configuration 243/756:
2025-05-01 08:17:04,510 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:17:04,510 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:17:04,510 - main - INFO - Scheduler: StepLR
2025-05-01 08:17:04,510 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:17:04,510 - main - INFO - ==================================================
2025-05-01 08:17:04,510 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243
2025-05-01 08:17:04,511 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243 - INFO - Config: {
  "id": 243,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:17:04,864 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:17:04,864 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 243,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:17:04,864 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:17:05,335 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:17:05,337 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:17:05,340 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243 - INFO - Starting model evaluation
2025-05-01 08:17:16,119 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243 - INFO - Evaluation metrics:
  Accuracy:  0.8295
  Precision: 0.6781
  Recall:    0.7587
  F1 Score:  0.7162
  IoU:       0.5578
  mAP:       0.8376
  AUC:       0.9054
2025-05-01 08:17:16,121 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243/final_results.json
2025-05-01 08:17:16,123 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_243/final_results.json
2025-05-01 08:17:16,123 - main - INFO - 
Summary for configuration 243:
2025-05-01 08:17:16,123 - main - INFO - Accuracy: 0.8295
2025-05-01 08:17:16,123 - main - INFO - Precision: 0.6781
2025-05-01 08:17:16,123 - main - INFO - Recall: 0.7587
2025-05-01 08:17:16,123 - main - INFO - F1 Score: 0.7162
2025-05-01 08:17:16,123 - main - INFO - IoU: 0.5578
2025-05-01 08:17:16,123 - main - INFO - mAP: 0.8376
2025-05-01 08:17:16,123 - main - INFO - AUC: 0.9054
2025-05-01 08:17:16,123 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:17:16,123 - main - INFO - 
==================================================
2025-05-01 08:17:16,123 - main - INFO - Running configuration 244/756:
2025-05-01 08:17:16,123 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:17:16,123 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:17:16,123 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 08:17:16,123 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:17:16,123 - main - INFO - ==================================================
2025-05-01 08:17:16,123 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244
2025-05-01 08:17:16,123 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244 - INFO - Config: {
  "id": 244,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:17:16,428 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:17:16,428 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 244,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:17:16,428 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:17:17,031 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:17:17,033 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:17:17,036 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244 - INFO - Starting model evaluation
2025-05-01 08:17:27,865 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244 - INFO - Evaluation metrics:
  Accuracy:  0.8295
  Precision: 0.6770
  Recall:    0.7622
  F1 Score:  0.7171
  IoU:       0.5590
  mAP:       0.8431
  AUC:       0.9075
2025-05-01 08:17:27,866 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244/final_results.json
2025-05-01 08:17:27,868 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_244/final_results.json
2025-05-01 08:17:27,868 - main - INFO - 
Summary for configuration 244:
2025-05-01 08:17:27,868 - main - INFO - Accuracy: 0.8295
2025-05-01 08:17:27,868 - main - INFO - Precision: 0.6770
2025-05-01 08:17:27,868 - main - INFO - Recall: 0.7622
2025-05-01 08:17:27,868 - main - INFO - F1 Score: 0.7171
2025-05-01 08:17:27,868 - main - INFO - IoU: 0.5590
2025-05-01 08:17:27,868 - main - INFO - mAP: 0.8431
2025-05-01 08:17:27,868 - main - INFO - AUC: 0.9075
2025-05-01 08:17:27,868 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:17:27,868 - main - INFO - 
==================================================
2025-05-01 08:17:27,868 - main - INFO - Running configuration 245/756:
2025-05-01 08:17:27,868 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:17:27,868 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:17:27,868 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 08:17:27,868 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:17:27,868 - main - INFO - ==================================================
2025-05-01 08:17:27,868 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245
2025-05-01 08:17:27,868 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245 - INFO - Config: {
  "id": 245,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:17:28,270 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:17:28,270 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 245,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:17:28,270 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:17:28,891 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:17:28,893 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:17:28,897 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245 - INFO - Starting model evaluation
2025-05-01 08:17:39,924 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245 - INFO - Evaluation metrics:
  Accuracy:  0.8216
  Precision: 0.6596
  Recall:    0.7657
  F1 Score:  0.7087
  IoU:       0.5489
  mAP:       0.8372
  AUC:       0.9032
2025-05-01 08:17:39,925 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245/final_results.json
2025-05-01 08:17:39,927 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_245/final_results.json
2025-05-01 08:17:39,927 - main - INFO - 
Summary for configuration 245:
2025-05-01 08:17:39,927 - main - INFO - Accuracy: 0.8216
2025-05-01 08:17:39,927 - main - INFO - Precision: 0.6596
2025-05-01 08:17:39,927 - main - INFO - Recall: 0.7657
2025-05-01 08:17:39,927 - main - INFO - F1 Score: 0.7087
2025-05-01 08:17:39,927 - main - INFO - IoU: 0.5489
2025-05-01 08:17:39,927 - main - INFO - mAP: 0.8372
2025-05-01 08:17:39,927 - main - INFO - AUC: 0.9032
2025-05-01 08:17:39,927 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:17:39,927 - main - INFO - 
==================================================
2025-05-01 08:17:39,927 - main - INFO - Running configuration 246/756:
2025-05-01 08:17:39,927 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:17:39,927 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:17:39,927 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 08:17:39,927 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:17:39,927 - main - INFO - ==================================================
2025-05-01 08:17:39,927 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246
2025-05-01 08:17:39,927 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246 - INFO - Config: {
  "id": 246,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:17:40,304 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:17:40,304 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 246,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:17:40,304 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:17:40,783 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:17:40,785 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:17:40,790 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246 - INFO - Starting model evaluation
2025-05-01 08:17:52,080 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246 - INFO - Evaluation metrics:
  Accuracy:  0.8355
  Precision: 0.6887
  Recall:    0.7657
  F1 Score:  0.7252
  IoU:       0.5688
  mAP:       0.8432
  AUC:       0.9053
2025-05-01 08:17:52,081 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246/final_results.json
2025-05-01 08:17:52,083 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_246/final_results.json
2025-05-01 08:17:52,083 - main - INFO - 
Summary for configuration 246:
2025-05-01 08:17:52,083 - main - INFO - Accuracy: 0.8355
2025-05-01 08:17:52,083 - main - INFO - Precision: 0.6887
2025-05-01 08:17:52,083 - main - INFO - Recall: 0.7657
2025-05-01 08:17:52,083 - main - INFO - F1 Score: 0.7252
2025-05-01 08:17:52,083 - main - INFO - IoU: 0.5688
2025-05-01 08:17:52,083 - main - INFO - mAP: 0.8432
2025-05-01 08:17:52,083 - main - INFO - AUC: 0.9053
2025-05-01 08:17:52,083 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:17:52,083 - main - INFO - 
==================================================
2025-05-01 08:17:52,083 - main - INFO - Running configuration 247/756:
2025-05-01 08:17:52,083 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:17:52,083 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:17:52,083 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 08:17:52,083 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:17:52,083 - main - INFO - ==================================================
2025-05-01 08:17:52,083 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247
2025-05-01 08:17:52,083 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247 - INFO - Config: {
  "id": 247,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:17:52,372 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:17:52,372 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 247,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:17:52,372 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:17:52,839 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:17:52,840 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:17:52,844 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247 - INFO - Starting model evaluation
2025-05-01 08:18:03,841 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247 - INFO - Evaluation metrics:
  Accuracy:  0.8236
  Precision: 0.6677
  Recall:    0.7517
  F1 Score:  0.7072
  IoU:       0.5471
  mAP:       0.8417
  AUC:       0.9057
2025-05-01 08:18:03,843 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247/final_results.json
2025-05-01 08:18:03,844 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_247/final_results.json
2025-05-01 08:18:03,844 - main - INFO - 
Summary for configuration 247:
2025-05-01 08:18:03,845 - main - INFO - Accuracy: 0.8236
2025-05-01 08:18:03,845 - main - INFO - Precision: 0.6677
2025-05-01 08:18:03,845 - main - INFO - Recall: 0.7517
2025-05-01 08:18:03,845 - main - INFO - F1 Score: 0.7072
2025-05-01 08:18:03,845 - main - INFO - IoU: 0.5471
2025-05-01 08:18:03,845 - main - INFO - mAP: 0.8417
2025-05-01 08:18:03,845 - main - INFO - AUC: 0.9057
2025-05-01 08:18:03,845 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:18:03,845 - main - INFO - 
==================================================
2025-05-01 08:18:03,845 - main - INFO - Running configuration 248/756:
2025-05-01 08:18:03,845 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:18:03,845 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:18:03,845 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 08:18:03,845 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:18:03,845 - main - INFO - ==================================================
2025-05-01 08:18:03,845 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248
2025-05-01 08:18:03,845 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248 - INFO - Config: {
  "id": 248,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:18:04,114 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:18:04,114 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 248,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:18:04,114 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:18:04,595 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:18:04,596 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:18:04,600 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248 - INFO - Starting model evaluation
2025-05-01 08:18:15,514 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248 - INFO - Evaluation metrics:
  Accuracy:  0.8176
  Precision: 0.6584
  Recall:    0.7413
  F1 Score:  0.6974
  IoU:       0.5354
  mAP:       0.8405
  AUC:       0.9048
2025-05-01 08:18:15,515 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248/final_results.json
2025-05-01 08:18:15,517 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_248/final_results.json
2025-05-01 08:18:15,517 - main - INFO - 
Summary for configuration 248:
2025-05-01 08:18:15,517 - main - INFO - Accuracy: 0.8176
2025-05-01 08:18:15,517 - main - INFO - Precision: 0.6584
2025-05-01 08:18:15,517 - main - INFO - Recall: 0.7413
2025-05-01 08:18:15,517 - main - INFO - F1 Score: 0.6974
2025-05-01 08:18:15,517 - main - INFO - IoU: 0.5354
2025-05-01 08:18:15,517 - main - INFO - mAP: 0.8405
2025-05-01 08:18:15,517 - main - INFO - AUC: 0.9048
2025-05-01 08:18:15,517 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:18:15,517 - main - INFO - 
==================================================
2025-05-01 08:18:15,517 - main - INFO - Running configuration 249/756:
2025-05-01 08:18:15,517 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:18:15,517 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:18:15,517 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 08:18:15,517 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:18:15,517 - main - INFO - ==================================================
2025-05-01 08:18:15,517 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249
2025-05-01 08:18:15,517 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249 - INFO - Config: {
  "id": 249,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:18:15,806 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:18:15,806 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 249,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:18:15,806 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:18:16,413 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:18:16,414 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:18:16,418 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249 - INFO - Starting model evaluation
2025-05-01 08:18:27,393 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249 - INFO - Evaluation metrics:
  Accuracy:  0.8335
  Precision: 0.6832
  Recall:    0.7692
  F1 Score:  0.7237
  IoU:       0.5670
  mAP:       0.8431
  AUC:       0.9075
2025-05-01 08:18:27,395 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249/final_results.json
2025-05-01 08:18:27,402 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_249/final_results.json
2025-05-01 08:18:27,402 - main - INFO - 
Summary for configuration 249:
2025-05-01 08:18:27,402 - main - INFO - Accuracy: 0.8335
2025-05-01 08:18:27,402 - main - INFO - Precision: 0.6832
2025-05-01 08:18:27,402 - main - INFO - Recall: 0.7692
2025-05-01 08:18:27,402 - main - INFO - F1 Score: 0.7237
2025-05-01 08:18:27,402 - main - INFO - IoU: 0.5670
2025-05-01 08:18:27,402 - main - INFO - mAP: 0.8431
2025-05-01 08:18:27,402 - main - INFO - AUC: 0.9075
2025-05-01 08:18:27,402 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:18:27,402 - main - INFO - 
==================================================
2025-05-01 08:18:27,402 - main - INFO - Running configuration 250/756:
2025-05-01 08:18:27,402 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:18:27,402 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:18:27,402 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 08:18:27,402 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:18:27,402 - main - INFO - ==================================================
2025-05-01 08:18:27,402 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250
2025-05-01 08:18:27,402 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250 - INFO - Config: {
  "id": 250,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:18:27,806 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:18:27,809 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 250,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:18:27,809 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:18:28,280 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:18:28,281 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:18:28,285 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250 - INFO - Starting model evaluation
2025-05-01 08:18:39,039 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250 - INFO - Evaluation metrics:
  Accuracy:  0.8305
  Precision: 0.6780
  Recall:    0.7657
  F1 Score:  0.7192
  IoU:       0.5615
  mAP:       0.8477
  AUC:       0.9087
2025-05-01 08:18:39,040 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250/final_results.json
2025-05-01 08:18:39,042 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_250/final_results.json
2025-05-01 08:18:39,042 - main - INFO - 
Summary for configuration 250:
2025-05-01 08:18:39,042 - main - INFO - Accuracy: 0.8305
2025-05-01 08:18:39,042 - main - INFO - Precision: 0.6780
2025-05-01 08:18:39,042 - main - INFO - Recall: 0.7657
2025-05-01 08:18:39,042 - main - INFO - F1 Score: 0.7192
2025-05-01 08:18:39,042 - main - INFO - IoU: 0.5615
2025-05-01 08:18:39,042 - main - INFO - mAP: 0.8477
2025-05-01 08:18:39,042 - main - INFO - AUC: 0.9087
2025-05-01 08:18:39,042 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:18:39,042 - main - INFO - 
==================================================
2025-05-01 08:18:39,042 - main - INFO - Running configuration 251/756:
2025-05-01 08:18:39,042 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:18:39,042 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:18:39,042 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 08:18:39,042 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:18:39,042 - main - INFO - ==================================================
2025-05-01 08:18:39,042 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251
2025-05-01 08:18:39,042 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251 - INFO - Config: {
  "id": 251,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:18:39,363 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:18:39,363 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 251,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:18:39,363 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:18:39,970 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:18:39,971 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:18:39,975 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251 - INFO - Starting model evaluation
2025-05-01 08:18:50,929 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251 - INFO - Evaluation metrics:
  Accuracy:  0.8365
  Precision: 0.6921
  Recall:    0.7622
  F1 Score:  0.7255
  IoU:       0.5692
  mAP:       0.8460
  AUC:       0.9091
2025-05-01 08:18:50,930 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251/final_results.json
2025-05-01 08:18:50,932 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_251/final_results.json
2025-05-01 08:18:50,932 - main - INFO - 
Summary for configuration 251:
2025-05-01 08:18:50,932 - main - INFO - Accuracy: 0.8365
2025-05-01 08:18:50,932 - main - INFO - Precision: 0.6921
2025-05-01 08:18:50,932 - main - INFO - Recall: 0.7622
2025-05-01 08:18:50,932 - main - INFO - F1 Score: 0.7255
2025-05-01 08:18:50,932 - main - INFO - IoU: 0.5692
2025-05-01 08:18:50,932 - main - INFO - mAP: 0.8460
2025-05-01 08:18:50,932 - main - INFO - AUC: 0.9091
2025-05-01 08:18:50,932 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:18:50,932 - main - INFO - 
==================================================
2025-05-01 08:18:50,932 - main - INFO - Running configuration 252/756:
2025-05-01 08:18:50,932 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:18:50,932 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 08:18:50,932 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 08:18:50,932 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:18:50,932 - main - INFO - ==================================================
2025-05-01 08:18:50,932 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252
2025-05-01 08:18:50,932 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252 - INFO - Config: {
  "id": 252,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:18:51,443 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_SGD_lr_0.0001
2025-05-01 08:18:51,443 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 252,
  "model_name": "EfficientNet-B2",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:18:51,443 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 08:18:52,033 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8105
2025-05-01 08:18:52,034 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001 - INFO - Training completed after 1131.86 seconds
2025-05-01 08:18:52,038 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252 - INFO - Starting model evaluation
2025-05-01 08:19:03,171 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252 - INFO - Evaluation metrics:
  Accuracy:  0.8216
  Precision: 0.6656
  Recall:    0.7448
  F1 Score:  0.7030
  IoU:       0.5420
  mAP:       0.8397
  AUC:       0.9054
2025-05-01 08:19:03,173 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252/final_results.json
2025-05-01 08:19:03,175 - training.model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_SGD_lr_0.0001_id_252/final_results.json
2025-05-01 08:19:03,175 - main - INFO - 
Summary for configuration 252:
2025-05-01 08:19:03,175 - main - INFO - Accuracy: 0.8216
2025-05-01 08:19:03,175 - main - INFO - Precision: 0.6656
2025-05-01 08:19:03,175 - main - INFO - Recall: 0.7448
2025-05-01 08:19:03,175 - main - INFO - F1 Score: 0.7030
2025-05-01 08:19:03,175 - main - INFO - IoU: 0.5420
2025-05-01 08:19:03,175 - main - INFO - mAP: 0.8397
2025-05-01 08:19:03,175 - main - INFO - AUC: 0.9054
2025-05-01 08:19:03,175 - main - INFO - Training time: 1131.86 seconds
2025-05-01 08:19:03,175 - main - INFO - 
==================================================
2025-05-01 08:19:03,175 - main - INFO - Running configuration 253/756:
2025-05-01 08:19:03,175 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:19:03,175 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:19:03,175 - main - INFO - Scheduler: StepLR
2025-05-01 08:19:03,175 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:19:03,175 - main - INFO - ==================================================
2025-05-01 08:19:03,175 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_253 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_253
2025-05-01 08:19:03,175 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_253 - INFO - Config: {
  "id": 253,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:19:03,519 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:19:03,519 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 253,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:19:03,519 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 08:19:03,521 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 08:19:27,896 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 08:20:00,018 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.5214
2025-05-01 08:20:00,047 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 1 completed in 56.53s - Train Loss: 0.7906, Train Acc: 0.5079, Val Loss: 0.7832, Val Acc: 0.5214
2025-05-01 08:20:00,161 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 08:20:00,162 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 08:20:23,956 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 08:20:55,720 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation loss improved from inf to 0.7692
2025-05-01 08:20:55,759 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 2 completed in 55.60s - Train Loss: 0.7821, Train Acc: 0.5039, Val Loss: 0.7692, Val Acc: 0.4957
2025-05-01 08:20:55,863 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 08:20:55,864 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 08:21:19,822 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 08:21:51,811 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 3 completed in 55.95s - Train Loss: 0.8126, Train Acc: 0.4910, Val Loss: 0.8156, Val Acc: 0.4983
2025-05-01 08:21:51,912 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 08:21:51,912 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 08:22:16,752 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 08:22:49,040 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.7692 to 0.7563
2025-05-01 08:22:49,088 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 4 completed in 57.18s - Train Loss: 0.8118, Train Acc: 0.4985, Val Loss: 0.7563, Val Acc: 0.5197
2025-05-01 08:22:49,189 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 08:22:49,190 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 08:23:13,432 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 08:23:45,743 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 5 completed in 56.55s - Train Loss: 0.7791, Train Acc: 0.5245, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-01 08:23:45,853 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 08:23:45,854 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 08:24:09,900 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 08:24:41,638 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 6 completed in 55.78s - Train Loss: 0.7852, Train Acc: 0.5234, Val Loss: 0.8054, Val Acc: 0.4983
2025-05-01 08:24:41,741 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 08:24:41,742 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 08:25:05,694 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 08:25:38,447 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 7 completed in 56.70s - Train Loss: 0.7488, Train Acc: 0.5086, Val Loss: 0.7977, Val Acc: 0.4803
2025-05-01 08:25:38,549 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 08:25:38,549 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 08:26:02,034 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 08:26:33,862 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5214 to 0.5763
2025-05-01 08:26:33,923 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 8 completed in 55.37s - Train Loss: 0.7080, Train Acc: 0.5363, Val Loss: 0.7117, Val Acc: 0.5763
2025-05-01 08:26:34,022 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 08:26:34,022 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 08:26:58,066 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 08:27:30,003 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5763 to 0.6312
2025-05-01 08:27:30,043 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 9 completed in 56.02s - Train Loss: 0.6773, Train Acc: 0.5847, Val Loss: 0.6720, Val Acc: 0.6312
2025-05-01 08:27:30,142 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 08:27:30,143 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 08:27:54,588 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 08:28:26,752 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.6312 to 0.6681
2025-05-01 08:28:26,792 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 10 completed in 56.65s - Train Loss: 0.6529, Train Acc: 0.6143, Val Loss: 0.6232, Val Acc: 0.6681
2025-05-01 08:28:26,891 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 08:28:26,892 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 08:28:50,637 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 08:29:22,529 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.6681 to 0.6750
2025-05-01 08:29:22,593 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 11 completed in 55.70s - Train Loss: 0.5987, Train Acc: 0.6860, Val Loss: 0.6035, Val Acc: 0.6750
2025-05-01 08:29:22,698 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 08:29:22,698 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 08:29:46,544 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 08:30:19,259 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.6750 to 0.7187
2025-05-01 08:30:19,300 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 12 completed in 56.60s - Train Loss: 0.5764, Train Acc: 0.7117, Val Loss: 0.5698, Val Acc: 0.7187
2025-05-01 08:30:19,397 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 08:30:19,398 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 08:30:44,152 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 08:31:16,444 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7187 to 0.7376
2025-05-01 08:31:16,495 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 13 completed in 57.10s - Train Loss: 0.5630, Train Acc: 0.7282, Val Loss: 0.5573, Val Acc: 0.7376
2025-05-01 08:31:16,593 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 08:31:16,594 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 08:31:41,007 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 08:32:13,506 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.7563 to 0.5618
2025-05-01 08:32:13,548 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 14 completed in 56.95s - Train Loss: 0.5488, Train Acc: 0.7477, Val Loss: 0.5618, Val Acc: 0.7247
2025-05-01 08:32:13,653 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 08:32:13,653 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 08:32:37,325 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 08:33:09,998 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7376 to 0.7607
2025-05-01 08:33:10,046 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 15 completed in 56.39s - Train Loss: 0.5472, Train Acc: 0.7443, Val Loss: 0.5390, Val Acc: 0.7607
2025-05-01 08:33:10,155 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 08:33:10,155 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 08:33:33,642 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 08:34:06,044 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7607 to 0.7642
2025-05-01 08:34:06,084 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 16 completed in 55.93s - Train Loss: 0.5343, Train Acc: 0.7630, Val Loss: 0.5374, Val Acc: 0.7642
2025-05-01 08:34:06,185 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 08:34:06,186 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 08:34:30,083 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 08:35:02,128 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.5618 to 0.5320
2025-05-01 08:35:02,165 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 17 completed in 55.98s - Train Loss: 0.5222, Train Acc: 0.7765, Val Loss: 0.5320, Val Acc: 0.7616
2025-05-01 08:35:02,265 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 08:35:02,265 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 08:35:26,603 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 08:36:00,252 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 18 completed in 57.99s - Train Loss: 0.5112, Train Acc: 0.7917, Val Loss: 0.5442, Val Acc: 0.7384
2025-05-01 08:36:00,357 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 08:36:00,357 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 08:36:24,744 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 08:36:57,720 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7642 to 0.7864
2025-05-01 08:36:57,759 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 19 completed in 57.40s - Train Loss: 0.4968, Train Acc: 0.8069, Val Loss: 0.5124, Val Acc: 0.7864
2025-05-01 08:36:57,854 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 08:36:57,855 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 08:37:22,350 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 08:37:55,513 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7864 to 0.8113
2025-05-01 08:37:55,551 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Epoch 20 completed in 57.70s - Train Loss: 0.4948, Train Acc: 0.8087, Val Loss: 0.4978, Val Acc: 0.8113
2025-05-01 08:37:55,645 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 08:37:55,646 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:37:55,650 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_253 - INFO - Starting model evaluation
2025-05-01 08:38:07,142 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_253 - INFO - Evaluation metrics:
  Accuracy:  0.8256
  Precision: 0.6637
  Recall:    0.7797
  F1 Score:  0.7170
  IoU:       0.5589
  mAP:       0.8253
  AUC:       0.8979
2025-05-01 08:38:07,144 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_253 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_253/final_results.json
2025-05-01 08:38:07,145 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_253 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_253/final_results.json
2025-05-01 08:38:07,145 - main - INFO - 
Summary for configuration 253:
2025-05-01 08:38:07,145 - main - INFO - Accuracy: 0.8256
2025-05-01 08:38:07,145 - main - INFO - Precision: 0.6637
2025-05-01 08:38:07,145 - main - INFO - Recall: 0.7797
2025-05-01 08:38:07,145 - main - INFO - F1 Score: 0.7170
2025-05-01 08:38:07,145 - main - INFO - IoU: 0.5589
2025-05-01 08:38:07,145 - main - INFO - mAP: 0.8253
2025-05-01 08:38:07,145 - main - INFO - AUC: 0.8979
2025-05-01 08:38:07,145 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:38:07,145 - main - INFO - 
==================================================
2025-05-01 08:38:07,145 - main - INFO - Running configuration 254/756:
2025-05-01 08:38:07,145 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:38:07,145 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:38:07,145 - main - INFO - Scheduler: StepLR
2025-05-01 08:38:07,145 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:38:07,145 - main - INFO - ==================================================
2025-05-01 08:38:07,146 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_254 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_254
2025-05-01 08:38:07,146 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_254 - INFO - Config: {
  "id": 254,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:38:07,464 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:38:07,465 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 254,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:38:07,465 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:38:08,401 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:38:08,402 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:38:08,406 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_254 - INFO - Starting model evaluation
2025-05-01 08:38:19,390 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_254 - INFO - Evaluation metrics:
  Accuracy:  0.8325
  Precision: 0.6757
  Recall:    0.7867
  F1 Score:  0.7270
  IoU:       0.5711
  mAP:       0.8251
  AUC:       0.9013
2025-05-01 08:38:19,391 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_254 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_254/final_results.json
2025-05-01 08:38:19,393 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_254 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_254/final_results.json
2025-05-01 08:38:19,393 - main - INFO - 
Summary for configuration 254:
2025-05-01 08:38:19,393 - main - INFO - Accuracy: 0.8325
2025-05-01 08:38:19,393 - main - INFO - Precision: 0.6757
2025-05-01 08:38:19,393 - main - INFO - Recall: 0.7867
2025-05-01 08:38:19,393 - main - INFO - F1 Score: 0.7270
2025-05-01 08:38:19,393 - main - INFO - IoU: 0.5711
2025-05-01 08:38:19,393 - main - INFO - mAP: 0.8251
2025-05-01 08:38:19,393 - main - INFO - AUC: 0.9013
2025-05-01 08:38:19,393 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:38:19,393 - main - INFO - 
==================================================
2025-05-01 08:38:19,393 - main - INFO - Running configuration 255/756:
2025-05-01 08:38:19,393 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:38:19,393 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:38:19,393 - main - INFO - Scheduler: StepLR
2025-05-01 08:38:19,393 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:38:19,393 - main - INFO - ==================================================
2025-05-01 08:38:19,393 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_255 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_255
2025-05-01 08:38:19,393 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_255 - INFO - Config: {
  "id": 255,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:38:19,919 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:38:19,919 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 255,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:38:19,919 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:38:20,832 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:38:20,833 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:38:20,837 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_255 - INFO - Starting model evaluation
2025-05-01 08:38:31,732 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_255 - INFO - Evaluation metrics:
  Accuracy:  0.8196
  Precision: 0.6529
  Recall:    0.7762
  F1 Score:  0.7093
  IoU:       0.5495
  mAP:       0.8215
  AUC:       0.8942
2025-05-01 08:38:31,733 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_255 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_255/final_results.json
2025-05-01 08:38:31,735 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_255 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_255/final_results.json
2025-05-01 08:38:31,735 - main - INFO - 
Summary for configuration 255:
2025-05-01 08:38:31,735 - main - INFO - Accuracy: 0.8196
2025-05-01 08:38:31,735 - main - INFO - Precision: 0.6529
2025-05-01 08:38:31,735 - main - INFO - Recall: 0.7762
2025-05-01 08:38:31,735 - main - INFO - F1 Score: 0.7093
2025-05-01 08:38:31,735 - main - INFO - IoU: 0.5495
2025-05-01 08:38:31,735 - main - INFO - mAP: 0.8215
2025-05-01 08:38:31,735 - main - INFO - AUC: 0.8942
2025-05-01 08:38:31,735 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:38:31,735 - main - INFO - 
==================================================
2025-05-01 08:38:31,735 - main - INFO - Running configuration 256/756:
2025-05-01 08:38:31,735 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:38:31,735 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:38:31,735 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 08:38:31,735 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:38:31,735 - main - INFO - ==================================================
2025-05-01 08:38:31,735 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_256 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_256
2025-05-01 08:38:31,735 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_256 - INFO - Config: {
  "id": 256,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:38:32,105 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:38:32,105 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 256,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:38:32,105 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:38:32,897 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:38:32,899 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:38:32,902 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_256 - INFO - Starting model evaluation
2025-05-01 08:38:43,904 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_256 - INFO - Evaluation metrics:
  Accuracy:  0.8266
  Precision: 0.6697
  Recall:    0.7657
  F1 Score:  0.7145
  IoU:       0.5558
  mAP:       0.8227
  AUC:       0.8965
2025-05-01 08:38:43,906 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_256 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_256/final_results.json
2025-05-01 08:38:43,908 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_256 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_256/final_results.json
2025-05-01 08:38:43,908 - main - INFO - 
Summary for configuration 256:
2025-05-01 08:38:43,908 - main - INFO - Accuracy: 0.8266
2025-05-01 08:38:43,908 - main - INFO - Precision: 0.6697
2025-05-01 08:38:43,908 - main - INFO - Recall: 0.7657
2025-05-01 08:38:43,908 - main - INFO - F1 Score: 0.7145
2025-05-01 08:38:43,908 - main - INFO - IoU: 0.5558
2025-05-01 08:38:43,908 - main - INFO - mAP: 0.8227
2025-05-01 08:38:43,908 - main - INFO - AUC: 0.8965
2025-05-01 08:38:43,908 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:38:43,908 - main - INFO - 
==================================================
2025-05-01 08:38:43,908 - main - INFO - Running configuration 257/756:
2025-05-01 08:38:43,908 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:38:43,908 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:38:43,908 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 08:38:43,908 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:38:43,908 - main - INFO - ==================================================
2025-05-01 08:38:43,908 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_257 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_257
2025-05-01 08:38:43,908 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_257 - INFO - Config: {
  "id": 257,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:38:44,182 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:38:44,182 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 257,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:38:44,182 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:38:45,091 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:38:45,093 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:38:45,096 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_257 - INFO - Starting model evaluation
2025-05-01 08:38:56,320 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_257 - INFO - Evaluation metrics:
  Accuracy:  0.8236
  Precision: 0.6627
  Recall:    0.7692
  F1 Score:  0.7120
  IoU:       0.5528
  mAP:       0.8235
  AUC:       0.8977
2025-05-01 08:38:56,322 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_257 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_257/final_results.json
2025-05-01 08:38:56,323 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_257 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_257/final_results.json
2025-05-01 08:38:56,323 - main - INFO - 
Summary for configuration 257:
2025-05-01 08:38:56,323 - main - INFO - Accuracy: 0.8236
2025-05-01 08:38:56,323 - main - INFO - Precision: 0.6627
2025-05-01 08:38:56,323 - main - INFO - Recall: 0.7692
2025-05-01 08:38:56,323 - main - INFO - F1 Score: 0.7120
2025-05-01 08:38:56,323 - main - INFO - IoU: 0.5528
2025-05-01 08:38:56,323 - main - INFO - mAP: 0.8235
2025-05-01 08:38:56,323 - main - INFO - AUC: 0.8977
2025-05-01 08:38:56,323 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:38:56,323 - main - INFO - 
==================================================
2025-05-01 08:38:56,323 - main - INFO - Running configuration 258/756:
2025-05-01 08:38:56,324 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:38:56,324 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:38:56,324 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 08:38:56,324 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:38:56,324 - main - INFO - ==================================================
2025-05-01 08:38:56,324 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_258 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_258
2025-05-01 08:38:56,324 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_258 - INFO - Config: {
  "id": 258,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:38:56,603 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:38:56,603 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 258,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:38:56,603 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:38:57,396 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:38:57,398 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:38:57,402 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_258 - INFO - Starting model evaluation
2025-05-01 08:39:08,647 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_258 - INFO - Evaluation metrics:
  Accuracy:  0.8315
  Precision: 0.6768
  Recall:    0.7762
  F1 Score:  0.7231
  IoU:       0.5663
  mAP:       0.8232
  AUC:       0.8992
2025-05-01 08:39:08,648 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_258 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_258/final_results.json
2025-05-01 08:39:08,650 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_258 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_258/final_results.json
2025-05-01 08:39:08,650 - main - INFO - 
Summary for configuration 258:
2025-05-01 08:39:08,650 - main - INFO - Accuracy: 0.8315
2025-05-01 08:39:08,650 - main - INFO - Precision: 0.6768
2025-05-01 08:39:08,650 - main - INFO - Recall: 0.7762
2025-05-01 08:39:08,650 - main - INFO - F1 Score: 0.7231
2025-05-01 08:39:08,650 - main - INFO - IoU: 0.5663
2025-05-01 08:39:08,650 - main - INFO - mAP: 0.8232
2025-05-01 08:39:08,650 - main - INFO - AUC: 0.8992
2025-05-01 08:39:08,650 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:39:08,650 - main - INFO - 
==================================================
2025-05-01 08:39:08,650 - main - INFO - Running configuration 259/756:
2025-05-01 08:39:08,650 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:39:08,650 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:39:08,650 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 08:39:08,650 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:39:08,650 - main - INFO - ==================================================
2025-05-01 08:39:08,650 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_259 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_259
2025-05-01 08:39:08,651 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_259 - INFO - Config: {
  "id": 259,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:39:09,034 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:39:09,034 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 259,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:39:09,034 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:39:09,824 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:39:09,826 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:39:09,829 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_259 - INFO - Starting model evaluation
2025-05-01 08:39:20,876 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_259 - INFO - Evaluation metrics:
  Accuracy:  0.8315
  Precision: 0.6779
  Recall:    0.7727
  F1 Score:  0.7222
  IoU:       0.5652
  mAP:       0.8286
  AUC:       0.9000
2025-05-01 08:39:20,878 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_259 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_259/final_results.json
2025-05-01 08:39:20,880 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_259 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_259/final_results.json
2025-05-01 08:39:20,880 - main - INFO - 
Summary for configuration 259:
2025-05-01 08:39:20,880 - main - INFO - Accuracy: 0.8315
2025-05-01 08:39:20,880 - main - INFO - Precision: 0.6779
2025-05-01 08:39:20,880 - main - INFO - Recall: 0.7727
2025-05-01 08:39:20,880 - main - INFO - F1 Score: 0.7222
2025-05-01 08:39:20,880 - main - INFO - IoU: 0.5652
2025-05-01 08:39:20,880 - main - INFO - mAP: 0.8286
2025-05-01 08:39:20,880 - main - INFO - AUC: 0.9000
2025-05-01 08:39:20,880 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:39:20,880 - main - INFO - 
==================================================
2025-05-01 08:39:20,880 - main - INFO - Running configuration 260/756:
2025-05-01 08:39:20,880 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:39:20,880 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:39:20,880 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 08:39:20,880 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:39:20,880 - main - INFO - ==================================================
2025-05-01 08:39:20,880 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_260 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_260
2025-05-01 08:39:20,880 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_260 - INFO - Config: {
  "id": 260,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:39:21,283 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:39:21,283 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 260,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:39:21,283 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:39:22,199 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:39:22,200 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:39:22,204 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_260 - INFO - Starting model evaluation
2025-05-01 08:39:33,277 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_260 - INFO - Evaluation metrics:
  Accuracy:  0.8285
  Precision: 0.6697
  Recall:    0.7797
  F1 Score:  0.7205
  IoU:       0.5631
  mAP:       0.8258
  AUC:       0.8983
2025-05-01 08:39:33,279 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_260 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_260/final_results.json
2025-05-01 08:39:33,281 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_260 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_260/final_results.json
2025-05-01 08:39:33,281 - main - INFO - 
Summary for configuration 260:
2025-05-01 08:39:33,281 - main - INFO - Accuracy: 0.8285
2025-05-01 08:39:33,281 - main - INFO - Precision: 0.6697
2025-05-01 08:39:33,281 - main - INFO - Recall: 0.7797
2025-05-01 08:39:33,281 - main - INFO - F1 Score: 0.7205
2025-05-01 08:39:33,281 - main - INFO - IoU: 0.5631
2025-05-01 08:39:33,281 - main - INFO - mAP: 0.8258
2025-05-01 08:39:33,281 - main - INFO - AUC: 0.8983
2025-05-01 08:39:33,281 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:39:33,281 - main - INFO - 
==================================================
2025-05-01 08:39:33,281 - main - INFO - Running configuration 261/756:
2025-05-01 08:39:33,281 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:39:33,281 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:39:33,281 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 08:39:33,281 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:39:33,281 - main - INFO - ==================================================
2025-05-01 08:39:33,281 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_261 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_261
2025-05-01 08:39:33,281 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_261 - INFO - Config: {
  "id": 261,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:39:33,566 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:39:33,567 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 261,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:39:33,567 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:39:34,419 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:39:34,420 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:39:34,424 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_261 - INFO - Starting model evaluation
2025-05-01 08:39:45,708 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_261 - INFO - Evaluation metrics:
  Accuracy:  0.8295
  Precision: 0.6717
  Recall:    0.7797
  F1 Score:  0.7217
  IoU:       0.5646
  mAP:       0.8249
  AUC:       0.8974
2025-05-01 08:39:45,709 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_261 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_261/final_results.json
2025-05-01 08:39:45,711 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_261 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_261/final_results.json
2025-05-01 08:39:45,711 - main - INFO - 
Summary for configuration 261:
2025-05-01 08:39:45,711 - main - INFO - Accuracy: 0.8295
2025-05-01 08:39:45,711 - main - INFO - Precision: 0.6717
2025-05-01 08:39:45,711 - main - INFO - Recall: 0.7797
2025-05-01 08:39:45,711 - main - INFO - F1 Score: 0.7217
2025-05-01 08:39:45,711 - main - INFO - IoU: 0.5646
2025-05-01 08:39:45,711 - main - INFO - mAP: 0.8249
2025-05-01 08:39:45,711 - main - INFO - AUC: 0.8974
2025-05-01 08:39:45,711 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:39:45,711 - main - INFO - 
==================================================
2025-05-01 08:39:45,711 - main - INFO - Running configuration 262/756:
2025-05-01 08:39:45,711 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:39:45,711 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:39:45,711 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 08:39:45,711 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:39:45,711 - main - INFO - ==================================================
2025-05-01 08:39:45,711 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_262 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_262
2025-05-01 08:39:45,711 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_262 - INFO - Config: {
  "id": 262,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:39:46,154 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:39:46,154 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 262,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:39:46,154 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:39:47,050 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:39:47,052 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:39:47,056 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_262 - INFO - Starting model evaluation
2025-05-01 08:39:58,103 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_262 - INFO - Evaluation metrics:
  Accuracy:  0.8276
  Precision: 0.6677
  Recall:    0.7797
  F1 Score:  0.7194
  IoU:       0.5617
  mAP:       0.8243
  AUC:       0.8963
2025-05-01 08:39:58,105 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_262 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_262/final_results.json
2025-05-01 08:39:58,108 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_262 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_262/final_results.json
2025-05-01 08:39:58,108 - main - INFO - 
Summary for configuration 262:
2025-05-01 08:39:58,108 - main - INFO - Accuracy: 0.8276
2025-05-01 08:39:58,108 - main - INFO - Precision: 0.6677
2025-05-01 08:39:58,108 - main - INFO - Recall: 0.7797
2025-05-01 08:39:58,108 - main - INFO - F1 Score: 0.7194
2025-05-01 08:39:58,108 - main - INFO - IoU: 0.5617
2025-05-01 08:39:58,108 - main - INFO - mAP: 0.8243
2025-05-01 08:39:58,108 - main - INFO - AUC: 0.8963
2025-05-01 08:39:58,108 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:39:58,108 - main - INFO - 
==================================================
2025-05-01 08:39:58,108 - main - INFO - Running configuration 263/756:
2025-05-01 08:39:58,108 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:39:58,108 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:39:58,108 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 08:39:58,108 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:39:58,108 - main - INFO - ==================================================
2025-05-01 08:39:58,108 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_263 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_263
2025-05-01 08:39:58,108 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_263 - INFO - Config: {
  "id": 263,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:39:58,412 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:39:58,414 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 263,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:39:58,414 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:39:59,380 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:39:59,382 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:39:59,385 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_263 - INFO - Starting model evaluation
2025-05-01 08:40:10,209 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_263 - INFO - Evaluation metrics:
  Accuracy:  0.8305
  Precision: 0.6758
  Recall:    0.7727
  F1 Score:  0.7210
  IoU:       0.5638
  mAP:       0.8269
  AUC:       0.9016
2025-05-01 08:40:10,211 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_263 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_263/final_results.json
2025-05-01 08:40:10,213 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_263 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_263/final_results.json
2025-05-01 08:40:10,213 - main - INFO - 
Summary for configuration 263:
2025-05-01 08:40:10,213 - main - INFO - Accuracy: 0.8305
2025-05-01 08:40:10,213 - main - INFO - Precision: 0.6758
2025-05-01 08:40:10,213 - main - INFO - Recall: 0.7727
2025-05-01 08:40:10,213 - main - INFO - F1 Score: 0.7210
2025-05-01 08:40:10,213 - main - INFO - IoU: 0.5638
2025-05-01 08:40:10,213 - main - INFO - mAP: 0.8269
2025-05-01 08:40:10,213 - main - INFO - AUC: 0.9016
2025-05-01 08:40:10,213 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:40:10,213 - main - INFO - 
==================================================
2025-05-01 08:40:10,213 - main - INFO - Running configuration 264/756:
2025-05-01 08:40:10,213 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:40:10,213 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 08:40:10,213 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 08:40:10,213 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:40:10,213 - main - INFO - ==================================================
2025-05-01 08:40:10,213 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_264 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01_id_264
2025-05-01 08:40:10,213 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_264 - INFO - Config: {
  "id": 264,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:40:10,512 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.01
2025-05-01 08:40:10,512 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 264,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:40:10,512 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 08:40:11,317 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8113
2025-05-01 08:40:11,318 - training.model_EfficientNet-B2_opt_Adam_lr_0.01 - INFO - Training completed after 1132.03 seconds
2025-05-01 08:40:11,322 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_264 - INFO - Starting model evaluation
2025-05-01 08:40:22,564 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_264 - INFO - Evaluation metrics:
  Accuracy:  0.8216
  Precision: 0.6559
  Recall:    0.7797
  F1 Score:  0.7125
  IoU:       0.5533
  mAP:       0.8206
  AUC:       0.8958
2025-05-01 08:40:22,566 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_264 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_264/final_results.json
2025-05-01 08:40:22,567 - training.model_EfficientNet-B2_opt_Adam_lr_0.01_id_264 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.01_id_264/final_results.json
2025-05-01 08:40:22,567 - main - INFO - 
Summary for configuration 264:
2025-05-01 08:40:22,567 - main - INFO - Accuracy: 0.8216
2025-05-01 08:40:22,567 - main - INFO - Precision: 0.6559
2025-05-01 08:40:22,567 - main - INFO - Recall: 0.7797
2025-05-01 08:40:22,567 - main - INFO - F1 Score: 0.7125
2025-05-01 08:40:22,567 - main - INFO - IoU: 0.5533
2025-05-01 08:40:22,567 - main - INFO - mAP: 0.8206
2025-05-01 08:40:22,567 - main - INFO - AUC: 0.8958
2025-05-01 08:40:22,567 - main - INFO - Training time: 1132.03 seconds
2025-05-01 08:40:22,567 - main - INFO - 
==================================================
2025-05-01 08:40:22,567 - main - INFO - Running configuration 265/756:
2025-05-01 08:40:22,567 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:40:22,567 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 08:40:22,567 - main - INFO - Scheduler: StepLR
2025-05-01 08:40:22,567 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:40:22,567 - main - INFO - ==================================================
2025-05-01 08:40:22,568 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_265 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_265
2025-05-01 08:40:22,568 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_265 - INFO - Config: {
  "id": 265,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:40:22,986 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 08:40:22,986 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 265,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:40:22,986 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 08:40:22,987 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 08:40:47,026 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 08:41:18,935 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.8636
2025-05-01 08:41:18,964 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 1 completed in 55.98s - Train Loss: 0.4785, Train Acc: 0.8260, Val Loss: 0.4448, Val Acc: 0.8636
2025-05-01 08:41:19,068 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 08:41:19,069 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 08:41:43,359 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 08:42:16,022 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation loss improved from inf to 0.4972
2025-05-01 08:42:16,061 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 2 completed in 56.99s - Train Loss: 0.4517, Train Acc: 0.8578, Val Loss: 0.4972, Val Acc: 0.8130
2025-05-01 08:42:16,164 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 08:42:16,165 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 08:42:40,744 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 08:43:12,132 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8636 to 0.8971
2025-05-01 08:43:12,170 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 3 completed in 56.01s - Train Loss: 0.4329, Train Acc: 0.8792, Val Loss: 0.4131, Val Acc: 0.8971
2025-05-01 08:43:12,271 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 08:43:12,271 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 08:43:36,341 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 08:44:09,002 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8971 to 0.9288
2025-05-01 08:44:09,040 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 4 completed in 56.77s - Train Loss: 0.4079, Train Acc: 0.9026, Val Loss: 0.3809, Val Acc: 0.9288
2025-05-01 08:44:09,139 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 08:44:09,139 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 08:44:33,388 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 08:45:05,980 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4972 to 0.3859
2025-05-01 08:45:06,029 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 5 completed in 56.89s - Train Loss: 0.3981, Train Acc: 0.9129, Val Loss: 0.3859, Val Acc: 0.9262
2025-05-01 08:45:06,124 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 08:45:06,125 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 08:45:29,927 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 08:46:01,718 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9288 to 0.9305
2025-05-01 08:46:01,772 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 6 completed in 55.65s - Train Loss: 0.3863, Train Acc: 0.9260, Val Loss: 0.3809, Val Acc: 0.9305
2025-05-01 08:46:01,868 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 08:46:01,868 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 08:46:25,244 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 08:46:58,024 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 7 completed in 56.16s - Train Loss: 0.3968, Train Acc: 0.9131, Val Loss: 0.4257, Val Acc: 0.8825
2025-05-01 08:46:58,139 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 08:46:58,139 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 08:47:22,244 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 08:47:54,456 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9305 to 0.9374
2025-05-01 08:47:54,505 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 8 completed in 56.37s - Train Loss: 0.3866, Train Acc: 0.9243, Val Loss: 0.3748, Val Acc: 0.9374
2025-05-01 08:47:54,604 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 08:47:54,605 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 08:48:18,778 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 08:48:52,118 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9374 to 0.9460
2025-05-01 08:48:52,169 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 9 completed in 57.56s - Train Loss: 0.3762, Train Acc: 0.9346, Val Loss: 0.3630, Val Acc: 0.9460
2025-05-01 08:48:52,266 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 08:48:52,266 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 08:49:16,742 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 08:49:48,862 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3859 to 0.3703
2025-05-01 08:49:48,912 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 10 completed in 56.65s - Train Loss: 0.3673, Train Acc: 0.9444, Val Loss: 0.3703, Val Acc: 0.9425
2025-05-01 08:49:49,010 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 08:49:49,010 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 08:50:13,043 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 08:50:45,317 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9460 to 0.9580
2025-05-01 08:50:45,359 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 11 completed in 56.35s - Train Loss: 0.3515, Train Acc: 0.9614, Val Loss: 0.3542, Val Acc: 0.9580
2025-05-01 08:50:45,461 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 08:50:45,461 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 08:51:10,237 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 08:51:42,514 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9580 to 0.9605
2025-05-01 08:51:42,552 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 12 completed in 57.09s - Train Loss: 0.3412, Train Acc: 0.9706, Val Loss: 0.3525, Val Acc: 0.9605
2025-05-01 08:51:42,645 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 08:51:42,645 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 08:52:07,098 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 08:52:38,441 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9605 to 0.9700
2025-05-01 08:52:38,480 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 13 completed in 55.83s - Train Loss: 0.3369, Train Acc: 0.9762, Val Loss: 0.3425, Val Acc: 0.9700
2025-05-01 08:52:38,579 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 08:52:38,580 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 08:53:02,145 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 08:53:34,702 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3703 to 0.3455
2025-05-01 08:53:34,751 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 14 completed in 56.17s - Train Loss: 0.3339, Train Acc: 0.9790, Val Loss: 0.3455, Val Acc: 0.9666
2025-05-01 08:53:34,851 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 08:53:34,851 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 08:53:59,315 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 08:54:32,075 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3455 to 0.3433
2025-05-01 08:54:32,113 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 15 completed in 57.26s - Train Loss: 0.3344, Train Acc: 0.9792, Val Loss: 0.3433, Val Acc: 0.9683
2025-05-01 08:54:32,211 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 08:54:32,212 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 08:54:56,908 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 08:55:28,659 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 16 completed in 56.45s - Train Loss: 0.3327, Train Acc: 0.9807, Val Loss: 0.3459, Val Acc: 0.9657
2025-05-01 08:55:28,758 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 08:55:28,758 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 08:55:52,802 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 08:56:25,440 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3433 to 0.3428
2025-05-01 08:56:25,481 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 17 completed in 56.72s - Train Loss: 0.3301, Train Acc: 0.9826, Val Loss: 0.3428, Val Acc: 0.9691
2025-05-01 08:56:25,582 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 08:56:25,583 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 08:56:49,344 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 08:57:21,792 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9700 to 0.9726
2025-05-01 08:57:21,840 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 18 completed in 56.26s - Train Loss: 0.3294, Train Acc: 0.9843, Val Loss: 0.3402, Val Acc: 0.9726
2025-05-01 08:57:21,936 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 08:57:21,937 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 08:57:46,315 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 08:58:18,581 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3428 to 0.3399
2025-05-01 08:58:18,630 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 19 completed in 56.69s - Train Loss: 0.3297, Train Acc: 0.9833, Val Loss: 0.3399, Val Acc: 0.9726
2025-05-01 08:58:18,731 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 08:58:18,732 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 08:58:42,917 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 08:59:15,601 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9726 to 0.9760
2025-05-01 08:59:15,649 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Epoch 20 completed in 56.92s - Train Loss: 0.3271, Train Acc: 0.9865, Val Loss: 0.3379, Val Acc: 0.9760
2025-05-01 08:59:15,746 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 08:59:15,747 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 08:59:15,751 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_265 - INFO - Starting model evaluation
2025-05-01 08:59:26,906 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_265 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.9007
  Recall:    0.9510
  F1 Score:  0.9252
  IoU:       0.8608
  mAP:       0.9659
  AUC:       0.9855
2025-05-01 08:59:26,907 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_265 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_265/final_results.json
2025-05-01 08:59:26,909 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_265 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_265/final_results.json
2025-05-01 08:59:26,909 - main - INFO - 
Summary for configuration 265:
2025-05-01 08:59:26,909 - main - INFO - Accuracy: 0.9564
2025-05-01 08:59:26,909 - main - INFO - Precision: 0.9007
2025-05-01 08:59:26,909 - main - INFO - Recall: 0.9510
2025-05-01 08:59:26,909 - main - INFO - F1 Score: 0.9252
2025-05-01 08:59:26,909 - main - INFO - IoU: 0.8608
2025-05-01 08:59:26,909 - main - INFO - mAP: 0.9659
2025-05-01 08:59:26,909 - main - INFO - AUC: 0.9855
2025-05-01 08:59:26,909 - main - INFO - Training time: 1132.66 seconds
2025-05-01 08:59:26,909 - main - INFO - 
==================================================
2025-05-01 08:59:26,909 - main - INFO - Running configuration 266/756:
2025-05-01 08:59:26,909 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:59:26,909 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 08:59:26,909 - main - INFO - Scheduler: StepLR
2025-05-01 08:59:26,909 - main - INFO - Loss Function: FocalLoss
2025-05-01 08:59:26,909 - main - INFO - ==================================================
2025-05-01 08:59:26,909 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_266 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_266
2025-05-01 08:59:26,909 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_266 - INFO - Config: {
  "id": 266,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:59:27,264 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 08:59:27,264 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 266,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 08:59:27,264 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 08:59:28,127 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 08:59:28,129 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 08:59:28,132 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_266 - INFO - Starting model evaluation
2025-05-01 08:59:39,107 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_266 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.9037
  Recall:    0.9510
  F1 Score:  0.9267
  IoU:       0.8635
  mAP:       0.9706
  AUC:       0.9890
2025-05-01 08:59:39,108 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_266 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_266/final_results.json
2025-05-01 08:59:39,110 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_266 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_266/final_results.json
2025-05-01 08:59:39,110 - main - INFO - 
Summary for configuration 266:
2025-05-01 08:59:39,110 - main - INFO - Accuracy: 0.9574
2025-05-01 08:59:39,110 - main - INFO - Precision: 0.9037
2025-05-01 08:59:39,110 - main - INFO - Recall: 0.9510
2025-05-01 08:59:39,110 - main - INFO - F1 Score: 0.9267
2025-05-01 08:59:39,110 - main - INFO - IoU: 0.8635
2025-05-01 08:59:39,110 - main - INFO - mAP: 0.9706
2025-05-01 08:59:39,110 - main - INFO - AUC: 0.9890
2025-05-01 08:59:39,110 - main - INFO - Training time: 1132.66 seconds
2025-05-01 08:59:39,110 - main - INFO - 
==================================================
2025-05-01 08:59:39,110 - main - INFO - Running configuration 267/756:
2025-05-01 08:59:39,110 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:59:39,110 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 08:59:39,110 - main - INFO - Scheduler: StepLR
2025-05-01 08:59:39,110 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 08:59:39,110 - main - INFO - ==================================================
2025-05-01 08:59:39,110 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_267 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_267
2025-05-01 08:59:39,110 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_267 - INFO - Config: {
  "id": 267,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:59:39,529 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 08:59:39,529 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 267,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 08:59:39,529 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 08:59:40,456 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 08:59:40,458 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 08:59:40,461 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_267 - INFO - Starting model evaluation
2025-05-01 08:59:51,427 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_267 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.9007
  Recall:    0.9510
  F1 Score:  0.9252
  IoU:       0.8608
  mAP:       0.9690
  AUC:       0.9867
2025-05-01 08:59:51,429 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_267 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_267/final_results.json
2025-05-01 08:59:51,431 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_267 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_267/final_results.json
2025-05-01 08:59:51,431 - main - INFO - 
Summary for configuration 267:
2025-05-01 08:59:51,431 - main - INFO - Accuracy: 0.9564
2025-05-01 08:59:51,431 - main - INFO - Precision: 0.9007
2025-05-01 08:59:51,431 - main - INFO - Recall: 0.9510
2025-05-01 08:59:51,431 - main - INFO - F1 Score: 0.9252
2025-05-01 08:59:51,431 - main - INFO - IoU: 0.8608
2025-05-01 08:59:51,431 - main - INFO - mAP: 0.9690
2025-05-01 08:59:51,431 - main - INFO - AUC: 0.9867
2025-05-01 08:59:51,431 - main - INFO - Training time: 1132.66 seconds
2025-05-01 08:59:51,431 - main - INFO - 
==================================================
2025-05-01 08:59:51,431 - main - INFO - Running configuration 268/756:
2025-05-01 08:59:51,431 - main - INFO - Model: EfficientNet-B2
2025-05-01 08:59:51,431 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 08:59:51,431 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 08:59:51,431 - main - INFO - Loss Function: CrossEntropy
2025-05-01 08:59:51,431 - main - INFO - ==================================================
2025-05-01 08:59:51,431 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_268 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_268
2025-05-01 08:59:51,431 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_268 - INFO - Config: {
  "id": 268,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:59:51,751 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 08:59:51,751 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 268,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 08:59:51,751 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 08:59:52,568 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 08:59:52,569 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 08:59:52,573 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_268 - INFO - Starting model evaluation
2025-05-01 09:00:03,363 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_268 - INFO - Evaluation metrics:
  Accuracy:  0.9495
  Precision: 0.8852
  Recall:    0.9441
  F1 Score:  0.9137
  IoU:       0.8411
  mAP:       0.9679
  AUC:       0.9870
2025-05-01 09:00:03,365 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_268 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_268/final_results.json
2025-05-01 09:00:03,366 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_268 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_268/final_results.json
2025-05-01 09:00:03,366 - main - INFO - 
Summary for configuration 268:
2025-05-01 09:00:03,366 - main - INFO - Accuracy: 0.9495
2025-05-01 09:00:03,366 - main - INFO - Precision: 0.8852
2025-05-01 09:00:03,366 - main - INFO - Recall: 0.9441
2025-05-01 09:00:03,366 - main - INFO - F1 Score: 0.9137
2025-05-01 09:00:03,366 - main - INFO - IoU: 0.8411
2025-05-01 09:00:03,366 - main - INFO - mAP: 0.9679
2025-05-01 09:00:03,366 - main - INFO - AUC: 0.9870
2025-05-01 09:00:03,366 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:00:03,366 - main - INFO - 
==================================================
2025-05-01 09:00:03,366 - main - INFO - Running configuration 269/756:
2025-05-01 09:00:03,366 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:00:03,366 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 09:00:03,366 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 09:00:03,366 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:00:03,366 - main - INFO - ==================================================
2025-05-01 09:00:03,366 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_269 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_269
2025-05-01 09:00:03,367 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_269 - INFO - Config: {
  "id": 269,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:00:03,673 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 09:00:03,673 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 269,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:00:03,673 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 09:00:04,610 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 09:00:04,611 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 09:00:04,615 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_269 - INFO - Starting model evaluation
2025-05-01 09:00:15,479 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_269 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9027
  Recall:    0.9406
  F1 Score:  0.9212
  IoU:       0.8540
  mAP:       0.9662
  AUC:       0.9863
2025-05-01 09:00:15,480 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_269 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_269/final_results.json
2025-05-01 09:00:15,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_269 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_269/final_results.json
2025-05-01 09:00:15,482 - main - INFO - 
Summary for configuration 269:
2025-05-01 09:00:15,482 - main - INFO - Accuracy: 0.9544
2025-05-01 09:00:15,482 - main - INFO - Precision: 0.9027
2025-05-01 09:00:15,482 - main - INFO - Recall: 0.9406
2025-05-01 09:00:15,482 - main - INFO - F1 Score: 0.9212
2025-05-01 09:00:15,482 - main - INFO - IoU: 0.8540
2025-05-01 09:00:15,482 - main - INFO - mAP: 0.9662
2025-05-01 09:00:15,482 - main - INFO - AUC: 0.9863
2025-05-01 09:00:15,482 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:00:15,482 - main - INFO - 
==================================================
2025-05-01 09:00:15,482 - main - INFO - Running configuration 270/756:
2025-05-01 09:00:15,482 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:00:15,482 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 09:00:15,482 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 09:00:15,482 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:00:15,482 - main - INFO - ==================================================
2025-05-01 09:00:15,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_270 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_270
2025-05-01 09:00:15,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_270 - INFO - Config: {
  "id": 270,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:00:15,863 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 09:00:15,863 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 270,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:00:15,863 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 09:00:16,719 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 09:00:16,720 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 09:00:16,724 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_270 - INFO - Starting model evaluation
2025-05-01 09:00:27,632 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_270 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9027
  Recall:    0.9406
  F1 Score:  0.9212
  IoU:       0.8540
  mAP:       0.9641
  AUC:       0.9855
2025-05-01 09:00:27,634 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_270 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_270/final_results.json
2025-05-01 09:00:27,635 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_270 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_270/final_results.json
2025-05-01 09:00:27,635 - main - INFO - 
Summary for configuration 270:
2025-05-01 09:00:27,635 - main - INFO - Accuracy: 0.9544
2025-05-01 09:00:27,636 - main - INFO - Precision: 0.9027
2025-05-01 09:00:27,636 - main - INFO - Recall: 0.9406
2025-05-01 09:00:27,636 - main - INFO - F1 Score: 0.9212
2025-05-01 09:00:27,636 - main - INFO - IoU: 0.8540
2025-05-01 09:00:27,636 - main - INFO - mAP: 0.9641
2025-05-01 09:00:27,636 - main - INFO - AUC: 0.9855
2025-05-01 09:00:27,636 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:00:27,636 - main - INFO - 
==================================================
2025-05-01 09:00:27,636 - main - INFO - Running configuration 271/756:
2025-05-01 09:00:27,636 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:00:27,636 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 09:00:27,636 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:00:27,636 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:00:27,636 - main - INFO - ==================================================
2025-05-01 09:00:27,636 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_271 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_271
2025-05-01 09:00:27,636 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_271 - INFO - Config: {
  "id": 271,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:00:27,928 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 09:00:27,928 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 271,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:00:27,928 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 09:00:28,727 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 09:00:28,728 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 09:00:28,732 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_271 - INFO - Starting model evaluation
2025-05-01 09:00:39,551 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_271 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9128
  Recall:    0.9510
  F1 Score:  0.9315
  IoU:       0.8718
  mAP:       0.9723
  AUC:       0.9891
2025-05-01 09:00:39,553 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_271 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_271/final_results.json
2025-05-01 09:00:39,554 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_271 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_271/final_results.json
2025-05-01 09:00:39,554 - main - INFO - 
Summary for configuration 271:
2025-05-01 09:00:39,554 - main - INFO - Accuracy: 0.9604
2025-05-01 09:00:39,554 - main - INFO - Precision: 0.9128
2025-05-01 09:00:39,554 - main - INFO - Recall: 0.9510
2025-05-01 09:00:39,554 - main - INFO - F1 Score: 0.9315
2025-05-01 09:00:39,554 - main - INFO - IoU: 0.8718
2025-05-01 09:00:39,554 - main - INFO - mAP: 0.9723
2025-05-01 09:00:39,554 - main - INFO - AUC: 0.9891
2025-05-01 09:00:39,554 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:00:39,554 - main - INFO - 
==================================================
2025-05-01 09:00:39,554 - main - INFO - Running configuration 272/756:
2025-05-01 09:00:39,554 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:00:39,554 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 09:00:39,554 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:00:39,554 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:00:39,554 - main - INFO - ==================================================
2025-05-01 09:00:39,555 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_272 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_272
2025-05-01 09:00:39,555 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_272 - INFO - Config: {
  "id": 272,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:00:39,952 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 09:00:39,952 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 272,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:00:39,952 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 09:00:40,751 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 09:00:40,753 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 09:00:40,756 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_272 - INFO - Starting model evaluation
2025-05-01 09:00:51,439 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_272 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.9037
  Recall:    0.9510
  F1 Score:  0.9267
  IoU:       0.8635
  mAP:       0.9674
  AUC:       0.9859
2025-05-01 09:00:51,441 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_272 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_272/final_results.json
2025-05-01 09:00:51,443 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_272 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_272/final_results.json
2025-05-01 09:00:51,443 - main - INFO - 
Summary for configuration 272:
2025-05-01 09:00:51,443 - main - INFO - Accuracy: 0.9574
2025-05-01 09:00:51,443 - main - INFO - Precision: 0.9037
2025-05-01 09:00:51,443 - main - INFO - Recall: 0.9510
2025-05-01 09:00:51,443 - main - INFO - F1 Score: 0.9267
2025-05-01 09:00:51,443 - main - INFO - IoU: 0.8635
2025-05-01 09:00:51,443 - main - INFO - mAP: 0.9674
2025-05-01 09:00:51,443 - main - INFO - AUC: 0.9859
2025-05-01 09:00:51,443 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:00:51,443 - main - INFO - 
==================================================
2025-05-01 09:00:51,443 - main - INFO - Running configuration 273/756:
2025-05-01 09:00:51,443 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:00:51,443 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 09:00:51,443 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:00:51,443 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:00:51,443 - main - INFO - ==================================================
2025-05-01 09:00:51,443 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_273 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_273
2025-05-01 09:00:51,443 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_273 - INFO - Config: {
  "id": 273,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:00:51,856 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 09:00:51,856 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 273,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:00:51,856 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 09:00:52,620 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 09:00:52,621 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 09:00:52,625 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_273 - INFO - Starting model evaluation
2025-05-01 09:01:03,519 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_273 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.8954
  Recall:    0.9580
  F1 Score:  0.9257
  IoU:       0.8616
  mAP:       0.9656
  AUC:       0.9876
2025-05-01 09:01:03,521 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_273 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_273/final_results.json
2025-05-01 09:01:03,522 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_273 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_273/final_results.json
2025-05-01 09:01:03,522 - main - INFO - 
Summary for configuration 273:
2025-05-01 09:01:03,522 - main - INFO - Accuracy: 0.9564
2025-05-01 09:01:03,522 - main - INFO - Precision: 0.8954
2025-05-01 09:01:03,522 - main - INFO - Recall: 0.9580
2025-05-01 09:01:03,522 - main - INFO - F1 Score: 0.9257
2025-05-01 09:01:03,523 - main - INFO - IoU: 0.8616
2025-05-01 09:01:03,523 - main - INFO - mAP: 0.9656
2025-05-01 09:01:03,523 - main - INFO - AUC: 0.9876
2025-05-01 09:01:03,523 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:01:03,523 - main - INFO - 
==================================================
2025-05-01 09:01:03,523 - main - INFO - Running configuration 274/756:
2025-05-01 09:01:03,523 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:01:03,523 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 09:01:03,523 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:01:03,523 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:01:03,523 - main - INFO - ==================================================
2025-05-01 09:01:03,523 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_274 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_274
2025-05-01 09:01:03,523 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_274 - INFO - Config: {
  "id": 274,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:01:03,961 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 09:01:03,961 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 274,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:01:03,961 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 09:01:04,888 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 09:01:04,889 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 09:01:04,893 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_274 - INFO - Starting model evaluation
2025-05-01 09:01:15,844 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_274 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.9033
  Recall:    0.9476
  F1 Score:  0.9249
  IoU:       0.8603
  mAP:       0.9660
  AUC:       0.9875
2025-05-01 09:01:15,845 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_274 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_274/final_results.json
2025-05-01 09:01:15,847 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_274 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_274/final_results.json
2025-05-01 09:01:15,847 - main - INFO - 
Summary for configuration 274:
2025-05-01 09:01:15,847 - main - INFO - Accuracy: 0.9564
2025-05-01 09:01:15,847 - main - INFO - Precision: 0.9033
2025-05-01 09:01:15,847 - main - INFO - Recall: 0.9476
2025-05-01 09:01:15,847 - main - INFO - F1 Score: 0.9249
2025-05-01 09:01:15,847 - main - INFO - IoU: 0.8603
2025-05-01 09:01:15,847 - main - INFO - mAP: 0.9660
2025-05-01 09:01:15,847 - main - INFO - AUC: 0.9875
2025-05-01 09:01:15,847 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:01:15,847 - main - INFO - 
==================================================
2025-05-01 09:01:15,847 - main - INFO - Running configuration 275/756:
2025-05-01 09:01:15,847 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:01:15,847 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 09:01:15,847 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:01:15,847 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:01:15,847 - main - INFO - ==================================================
2025-05-01 09:01:15,847 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_275 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_275
2025-05-01 09:01:15,847 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_275 - INFO - Config: {
  "id": 275,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:01:16,171 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 09:01:16,171 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 275,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:01:16,171 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 09:01:16,968 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 09:01:16,970 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 09:01:16,973 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_275 - INFO - Starting model evaluation
2025-05-01 09:01:28,159 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_275 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9043
  Recall:    0.9580
  F1 Score:  0.9304
  IoU:       0.8698
  mAP:       0.9703
  AUC:       0.9871
2025-05-01 09:01:28,161 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_275 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_275/final_results.json
2025-05-01 09:01:28,163 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_275 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_275/final_results.json
2025-05-01 09:01:28,163 - main - INFO - 
Summary for configuration 275:
2025-05-01 09:01:28,163 - main - INFO - Accuracy: 0.9594
2025-05-01 09:01:28,163 - main - INFO - Precision: 0.9043
2025-05-01 09:01:28,163 - main - INFO - Recall: 0.9580
2025-05-01 09:01:28,163 - main - INFO - F1 Score: 0.9304
2025-05-01 09:01:28,163 - main - INFO - IoU: 0.8698
2025-05-01 09:01:28,163 - main - INFO - mAP: 0.9703
2025-05-01 09:01:28,163 - main - INFO - AUC: 0.9871
2025-05-01 09:01:28,163 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:01:28,163 - main - INFO - 
==================================================
2025-05-01 09:01:28,163 - main - INFO - Running configuration 276/756:
2025-05-01 09:01:28,163 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:01:28,163 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 09:01:28,163 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:01:28,163 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:01:28,163 - main - INFO - ==================================================
2025-05-01 09:01:28,163 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_276 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001_id_276
2025-05-01 09:01:28,163 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_276 - INFO - Config: {
  "id": 276,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:01:28,547 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.001
2025-05-01 09:01:28,547 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 276,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:01:28,548 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 09:01:29,389 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-01 09:01:29,390 - training.model_EfficientNet-B2_opt_Adam_lr_0.001 - INFO - Training completed after 1132.66 seconds
2025-05-01 09:01:29,394 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_276 - INFO - Starting model evaluation
2025-05-01 09:01:40,250 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_276 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.8940
  Recall:    0.9441
  F1 Score:  0.9184
  IoU:       0.8491
  mAP:       0.9690
  AUC:       0.9864
2025-05-01 09:01:40,251 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_276 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_276/final_results.json
2025-05-01 09:01:40,253 - training.model_EfficientNet-B2_opt_Adam_lr_0.001_id_276 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.001_id_276/final_results.json
2025-05-01 09:01:40,253 - main - INFO - 
Summary for configuration 276:
2025-05-01 09:01:40,253 - main - INFO - Accuracy: 0.9524
2025-05-01 09:01:40,253 - main - INFO - Precision: 0.8940
2025-05-01 09:01:40,253 - main - INFO - Recall: 0.9441
2025-05-01 09:01:40,253 - main - INFO - F1 Score: 0.9184
2025-05-01 09:01:40,253 - main - INFO - IoU: 0.8491
2025-05-01 09:01:40,253 - main - INFO - mAP: 0.9690
2025-05-01 09:01:40,253 - main - INFO - AUC: 0.9864
2025-05-01 09:01:40,253 - main - INFO - Training time: 1132.66 seconds
2025-05-01 09:01:40,253 - main - INFO - 
==================================================
2025-05-01 09:01:40,253 - main - INFO - Running configuration 277/756:
2025-05-01 09:01:40,253 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:01:40,253 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:01:40,253 - main - INFO - Scheduler: StepLR
2025-05-01 09:01:40,253 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:01:40,253 - main - INFO - ==================================================
2025-05-01 09:01:40,253 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277
2025-05-01 09:01:40,253 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277 - INFO - Config: {
  "id": 277,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:01:40,594 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:01:40,594 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 277,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:01:40,594 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 09:01:40,596 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 09:02:05,152 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 09:02:38,484 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9605
2025-05-01 09:02:38,512 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 1 completed in 57.92s - Train Loss: 0.4593, Train Acc: 0.8672, Val Loss: 0.3570, Val Acc: 0.9605
2025-05-01 09:02:38,613 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 09:02:38,614 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 09:03:02,845 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 09:03:34,713 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9605 to 0.9760
2025-05-01 09:03:34,760 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 2 completed in 56.15s - Train Loss: 0.3570, Train Acc: 0.9603, Val Loss: 0.3372, Val Acc: 0.9760
2025-05-01 09:03:34,860 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 09:03:34,861 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 09:03:58,647 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 09:04:31,228 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9760 to 0.9846
2025-05-01 09:04:31,276 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 3 completed in 56.42s - Train Loss: 0.3467, Train Acc: 0.9685, Val Loss: 0.3299, Val Acc: 0.9846
2025-05-01 09:04:31,383 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 09:04:31,384 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 09:04:55,447 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 09:05:27,667 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation loss improved from inf to 0.3306
2025-05-01 09:05:27,706 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 4 completed in 56.32s - Train Loss: 0.3368, Train Acc: 0.9788, Val Loss: 0.3306, Val Acc: 0.9803
2025-05-01 09:05:27,802 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 09:05:27,803 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 09:05:52,343 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 09:06:24,776 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3306 to 0.3296
2025-05-01 09:06:24,817 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 5 completed in 57.01s - Train Loss: 0.3359, Train Acc: 0.9768, Val Loss: 0.3296, Val Acc: 0.9837
2025-05-01 09:06:24,923 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 09:06:24,923 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 09:06:48,655 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 09:07:22,237 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3296 to 0.3292
2025-05-01 09:07:22,275 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 6 completed in 57.35s - Train Loss: 0.3286, Train Acc: 0.9850, Val Loss: 0.3292, Val Acc: 0.9846
2025-05-01 09:07:22,373 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 09:07:22,374 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 09:07:48,098 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 09:08:21,530 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 7 completed in 59.16s - Train Loss: 0.3268, Train Acc: 0.9871, Val Loss: 0.3359, Val Acc: 0.9768
2025-05-01 09:08:21,656 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 09:08:21,656 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 09:08:46,725 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 09:09:22,816 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 8 completed in 61.16s - Train Loss: 0.3242, Train Acc: 0.9901, Val Loss: 0.3318, Val Acc: 0.9786
2025-05-01 09:09:22,915 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 09:09:22,916 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 09:09:48,593 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 09:10:21,952 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 9 completed in 59.04s - Train Loss: 0.3229, Train Acc: 0.9910, Val Loss: 0.3354, Val Acc: 0.9760
2025-05-01 09:10:22,061 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 09:10:22,061 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 09:10:47,716 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 09:11:22,942 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 10 completed in 60.88s - Train Loss: 0.3320, Train Acc: 0.9824, Val Loss: 0.3331, Val Acc: 0.9803
2025-05-01 09:11:23,041 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 09:11:23,042 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 09:11:45,521 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 09:12:18,652 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 11 completed in 55.61s - Train Loss: 0.3249, Train Acc: 0.9899, Val Loss: 0.3309, Val Acc: 0.9820
2025-05-01 09:12:18,789 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 09:12:18,789 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Early stopping triggered after 11 epochs
2025-05-01 09:12:18,789 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 638.06 seconds
2025-05-01 09:12:18,793 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277 - INFO - Starting model evaluation
2025-05-01 09:12:29,968 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.8974
  Recall:    0.9790
  F1 Score:  0.9365
  IoU:       0.8805
  mAP:       0.9731
  AUC:       0.9907
2025-05-01 09:12:29,970 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277/final_results.json
2025-05-01 09:12:29,972 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_277/final_results.json
2025-05-01 09:12:29,972 - main - INFO - 
Summary for configuration 277:
2025-05-01 09:12:29,972 - main - INFO - Accuracy: 0.9623
2025-05-01 09:12:29,972 - main - INFO - Precision: 0.8974
2025-05-01 09:12:29,972 - main - INFO - Recall: 0.9790
2025-05-01 09:12:29,972 - main - INFO - F1 Score: 0.9365
2025-05-01 09:12:29,972 - main - INFO - IoU: 0.8805
2025-05-01 09:12:29,972 - main - INFO - mAP: 0.9731
2025-05-01 09:12:29,972 - main - INFO - AUC: 0.9907
2025-05-01 09:12:29,972 - main - INFO - Training time: 638.06 seconds
2025-05-01 09:12:29,972 - main - INFO - 
==================================================
2025-05-01 09:12:29,972 - main - INFO - Running configuration 278/756:
2025-05-01 09:12:29,972 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:12:29,972 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:12:29,972 - main - INFO - Scheduler: StepLR
2025-05-01 09:12:29,972 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:12:29,972 - main - INFO - ==================================================
2025-05-01 09:12:29,972 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278
2025-05-01 09:12:29,972 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278 - INFO - Config: {
  "id": 278,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:12:30,331 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:12:30,331 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 278,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:12:30,331 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:12:31,267 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.9846
2025-05-01 09:12:31,268 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 09:12:54,699 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 09:13:26,637 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3292 to 0.0083
2025-05-01 09:13:26,692 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 12 completed in 55.42s - Train Loss: 0.0070, Train Acc: 0.9901, Val Loss: 0.0083, Val Acc: 0.9820
2025-05-01 09:13:26,810 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 09:13:26,810 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 09:13:52,293 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 09:14:24,337 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.0083 to 0.0080
2025-05-01 09:14:24,394 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 13 completed in 57.58s - Train Loss: 0.0068, Train Acc: 0.9923, Val Loss: 0.0080, Val Acc: 0.9837
2025-05-01 09:14:24,512 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 09:14:24,513 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 09:14:47,837 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 09:15:20,798 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 14 completed in 56.28s - Train Loss: 0.0067, Train Acc: 0.9936, Val Loss: 0.0082, Val Acc: 0.9837
2025-05-01 09:15:20,920 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 09:15:20,920 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 09:15:46,225 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 09:16:19,406 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9846 to 0.9854
2025-05-01 09:16:19,459 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 15 completed in 58.54s - Train Loss: 0.0067, Train Acc: 0.9936, Val Loss: 0.0077, Val Acc: 0.9854
2025-05-01 09:16:19,577 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 09:16:19,578 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 09:16:43,832 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 09:17:17,132 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 16 completed in 57.55s - Train Loss: 0.0065, Train Acc: 0.9940, Val Loss: 0.0081, Val Acc: 0.9854
2025-05-01 09:17:17,252 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 09:17:17,252 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 09:17:41,560 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 09:18:14,887 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9854 to 0.9871
2025-05-01 09:18:14,930 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 17 completed in 57.68s - Train Loss: 0.0067, Train Acc: 0.9927, Val Loss: 0.0077, Val Acc: 0.9871
2025-05-01 09:18:15,049 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 09:18:15,050 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 09:18:39,821 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 09:19:12,071 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 18 completed in 57.02s - Train Loss: 0.0065, Train Acc: 0.9944, Val Loss: 0.0083, Val Acc: 0.9820
2025-05-01 09:19:12,194 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 09:19:12,195 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 09:19:36,772 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 09:20:10,473 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 19 completed in 58.28s - Train Loss: 0.0065, Train Acc: 0.9946, Val Loss: 0.0080, Val Acc: 0.9854
2025-05-01 09:20:10,598 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 09:20:10,598 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 09:20:35,504 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 09:21:08,767 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Epoch 20 completed in 58.17s - Train Loss: 0.0064, Train Acc: 0.9944, Val Loss: 0.0080, Val Acc: 0.9846
2025-05-01 09:21:08,897 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 09:21:08,898 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:21:08,902 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278 - INFO - Starting model evaluation
2025-05-01 09:21:20,624 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9029
  Recall:    0.9755
  F1 Score:  0.9378
  IoU:       0.8829
  mAP:       0.9842
  AUC:       0.9923
2025-05-01 09:21:20,626 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278/final_results.json
2025-05-01 09:21:20,627 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_278/final_results.json
2025-05-01 09:21:20,627 - main - INFO - 
Summary for configuration 278:
2025-05-01 09:21:20,627 - main - INFO - Accuracy: 0.9633
2025-05-01 09:21:20,627 - main - INFO - Precision: 0.9029
2025-05-01 09:21:20,627 - main - INFO - Recall: 0.9755
2025-05-01 09:21:20,627 - main - INFO - F1 Score: 0.9378
2025-05-01 09:21:20,627 - main - INFO - IoU: 0.8829
2025-05-01 09:21:20,627 - main - INFO - mAP: 0.9842
2025-05-01 09:21:20,627 - main - INFO - AUC: 0.9923
2025-05-01 09:21:20,627 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:21:20,627 - main - INFO - 
==================================================
2025-05-01 09:21:20,627 - main - INFO - Running configuration 279/756:
2025-05-01 09:21:20,627 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:21:20,627 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:21:20,628 - main - INFO - Scheduler: StepLR
2025-05-01 09:21:20,628 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:21:20,628 - main - INFO - ==================================================
2025-05-01 09:21:20,628 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279
2025-05-01 09:21:20,628 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279 - INFO - Config: {
  "id": 279,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:21:20,913 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:21:20,913 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 279,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:21:20,913 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:21:21,706 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:21:21,708 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:21:21,712 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279 - INFO - Starting model evaluation
2025-05-01 09:21:33,265 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9082
  Recall:    0.9685
  F1 Score:  0.9374
  IoU:       0.8822
  mAP:       0.9824
  AUC:       0.9908
2025-05-01 09:21:33,267 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279/final_results.json
2025-05-01 09:21:33,268 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_279/final_results.json
2025-05-01 09:21:33,268 - main - INFO - 
Summary for configuration 279:
2025-05-01 09:21:33,268 - main - INFO - Accuracy: 0.9633
2025-05-01 09:21:33,268 - main - INFO - Precision: 0.9082
2025-05-01 09:21:33,268 - main - INFO - Recall: 0.9685
2025-05-01 09:21:33,268 - main - INFO - F1 Score: 0.9374
2025-05-01 09:21:33,268 - main - INFO - IoU: 0.8822
2025-05-01 09:21:33,268 - main - INFO - mAP: 0.9824
2025-05-01 09:21:33,268 - main - INFO - AUC: 0.9908
2025-05-01 09:21:33,268 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:21:33,268 - main - INFO - 
==================================================
2025-05-01 09:21:33,268 - main - INFO - Running configuration 280/756:
2025-05-01 09:21:33,268 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:21:33,268 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:21:33,268 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 09:21:33,268 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:21:33,268 - main - INFO - ==================================================
2025-05-01 09:21:33,269 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280
2025-05-01 09:21:33,269 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280 - INFO - Config: {
  "id": 280,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:21:33,700 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:21:33,700 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 280,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:21:33,700 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:21:34,492 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:21:34,494 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:21:34,497 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280 - INFO - Starting model evaluation
2025-05-01 09:21:45,805 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9118
  Recall:    0.9755
  F1 Score:  0.9426
  IoU:       0.8914
  mAP:       0.9832
  AUC:       0.9921
2025-05-01 09:21:45,807 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280/final_results.json
2025-05-01 09:21:45,809 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_280/final_results.json
2025-05-01 09:21:45,809 - main - INFO - 
Summary for configuration 280:
2025-05-01 09:21:45,809 - main - INFO - Accuracy: 0.9663
2025-05-01 09:21:45,809 - main - INFO - Precision: 0.9118
2025-05-01 09:21:45,809 - main - INFO - Recall: 0.9755
2025-05-01 09:21:45,809 - main - INFO - F1 Score: 0.9426
2025-05-01 09:21:45,809 - main - INFO - IoU: 0.8914
2025-05-01 09:21:45,809 - main - INFO - mAP: 0.9832
2025-05-01 09:21:45,809 - main - INFO - AUC: 0.9921
2025-05-01 09:21:45,809 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:21:45,809 - main - INFO - 
==================================================
2025-05-01 09:21:45,809 - main - INFO - Running configuration 281/756:
2025-05-01 09:21:45,809 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:21:45,809 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:21:45,809 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 09:21:45,809 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:21:45,809 - main - INFO - ==================================================
2025-05-01 09:21:45,809 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281
2025-05-01 09:21:45,809 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281 - INFO - Config: {
  "id": 281,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:21:46,170 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:21:46,171 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 281,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:21:46,171 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:21:46,966 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:21:46,967 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:21:46,971 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281 - INFO - Starting model evaluation
2025-05-01 09:21:58,088 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9139
  Recall:    0.9650
  F1 Score:  0.9388
  IoU:       0.8846
  mAP:       0.9845
  AUC:       0.9939
2025-05-01 09:21:58,089 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281/final_results.json
2025-05-01 09:21:58,091 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_281/final_results.json
2025-05-01 09:21:58,091 - main - INFO - 
Summary for configuration 281:
2025-05-01 09:21:58,091 - main - INFO - Accuracy: 0.9643
2025-05-01 09:21:58,091 - main - INFO - Precision: 0.9139
2025-05-01 09:21:58,091 - main - INFO - Recall: 0.9650
2025-05-01 09:21:58,091 - main - INFO - F1 Score: 0.9388
2025-05-01 09:21:58,091 - main - INFO - IoU: 0.8846
2025-05-01 09:21:58,091 - main - INFO - mAP: 0.9845
2025-05-01 09:21:58,091 - main - INFO - AUC: 0.9939
2025-05-01 09:21:58,091 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:21:58,091 - main - INFO - 
==================================================
2025-05-01 09:21:58,091 - main - INFO - Running configuration 282/756:
2025-05-01 09:21:58,091 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:21:58,091 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:21:58,091 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 09:21:58,091 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:21:58,091 - main - INFO - ==================================================
2025-05-01 09:21:58,091 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282
2025-05-01 09:21:58,091 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282 - INFO - Config: {
  "id": 282,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:21:58,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:21:58,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 282,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:21:58,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:21:59,394 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:21:59,396 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:21:59,399 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282 - INFO - Starting model evaluation
2025-05-01 09:22:10,913 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9142
  Recall:    0.9685
  F1 Score:  0.9406
  IoU:       0.8878
  mAP:       0.9843
  AUC:       0.9928
2025-05-01 09:22:10,915 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282/final_results.json
2025-05-01 09:22:10,916 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_282/final_results.json
2025-05-01 09:22:10,917 - main - INFO - 
Summary for configuration 282:
2025-05-01 09:22:10,917 - main - INFO - Accuracy: 0.9653
2025-05-01 09:22:10,917 - main - INFO - Precision: 0.9142
2025-05-01 09:22:10,917 - main - INFO - Recall: 0.9685
2025-05-01 09:22:10,917 - main - INFO - F1 Score: 0.9406
2025-05-01 09:22:10,917 - main - INFO - IoU: 0.8878
2025-05-01 09:22:10,917 - main - INFO - mAP: 0.9843
2025-05-01 09:22:10,917 - main - INFO - AUC: 0.9928
2025-05-01 09:22:10,917 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:22:10,917 - main - INFO - 
==================================================
2025-05-01 09:22:10,917 - main - INFO - Running configuration 283/756:
2025-05-01 09:22:10,917 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:22:10,917 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:22:10,917 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:22:10,917 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:22:10,917 - main - INFO - ==================================================
2025-05-01 09:22:10,917 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283
2025-05-01 09:22:10,917 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283 - INFO - Config: {
  "id": 283,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:22:11,305 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:22:11,305 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 283,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:22:11,305 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:22:12,229 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:22:12,231 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:22:12,235 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283 - INFO - Starting model evaluation
2025-05-01 09:22:23,680 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9115
  Recall:    0.9720
  F1 Score:  0.9408
  IoU:       0.8882
  mAP:       0.9849
  AUC:       0.9929
2025-05-01 09:22:23,682 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283/final_results.json
2025-05-01 09:22:23,683 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_283/final_results.json
2025-05-01 09:22:23,683 - main - INFO - 
Summary for configuration 283:
2025-05-01 09:22:23,683 - main - INFO - Accuracy: 0.9653
2025-05-01 09:22:23,683 - main - INFO - Precision: 0.9115
2025-05-01 09:22:23,683 - main - INFO - Recall: 0.9720
2025-05-01 09:22:23,683 - main - INFO - F1 Score: 0.9408
2025-05-01 09:22:23,683 - main - INFO - IoU: 0.8882
2025-05-01 09:22:23,683 - main - INFO - mAP: 0.9849
2025-05-01 09:22:23,683 - main - INFO - AUC: 0.9929
2025-05-01 09:22:23,683 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:22:23,683 - main - INFO - 
==================================================
2025-05-01 09:22:23,683 - main - INFO - Running configuration 284/756:
2025-05-01 09:22:23,683 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:22:23,683 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:22:23,683 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:22:23,683 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:22:23,683 - main - INFO - ==================================================
2025-05-01 09:22:23,684 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284
2025-05-01 09:22:23,684 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284 - INFO - Config: {
  "id": 284,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:22:24,128 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:22:24,128 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 284,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:22:24,129 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:22:25,034 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:22:25,036 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:22:25,040 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284 - INFO - Starting model evaluation
2025-05-01 09:22:36,713 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9026
  Recall:    0.9720
  F1 Score:  0.9360
  IoU:       0.8797
  mAP:       0.9837
  AUC:       0.9924
2025-05-01 09:22:36,715 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284/final_results.json
2025-05-01 09:22:36,716 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_284/final_results.json
2025-05-01 09:22:36,716 - main - INFO - 
Summary for configuration 284:
2025-05-01 09:22:36,716 - main - INFO - Accuracy: 0.9623
2025-05-01 09:22:36,716 - main - INFO - Precision: 0.9026
2025-05-01 09:22:36,716 - main - INFO - Recall: 0.9720
2025-05-01 09:22:36,716 - main - INFO - F1 Score: 0.9360
2025-05-01 09:22:36,716 - main - INFO - IoU: 0.8797
2025-05-01 09:22:36,716 - main - INFO - mAP: 0.9837
2025-05-01 09:22:36,716 - main - INFO - AUC: 0.9924
2025-05-01 09:22:36,716 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:22:36,716 - main - INFO - 
==================================================
2025-05-01 09:22:36,717 - main - INFO - Running configuration 285/756:
2025-05-01 09:22:36,717 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:22:36,717 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:22:36,717 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:22:36,717 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:22:36,717 - main - INFO - ==================================================
2025-05-01 09:22:36,717 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285
2025-05-01 09:22:36,717 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285 - INFO - Config: {
  "id": 285,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:22:37,124 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:22:37,124 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 285,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:22:37,124 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:22:37,923 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:22:37,924 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:22:37,928 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285 - INFO - Starting model evaluation
2025-05-01 09:22:49,478 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9118
  Recall:    0.9755
  F1 Score:  0.9426
  IoU:       0.8914
  mAP:       0.9851
  AUC:       0.9933
2025-05-01 09:22:49,480 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285/final_results.json
2025-05-01 09:22:49,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_285/final_results.json
2025-05-01 09:22:49,482 - main - INFO - 
Summary for configuration 285:
2025-05-01 09:22:49,482 - main - INFO - Accuracy: 0.9663
2025-05-01 09:22:49,482 - main - INFO - Precision: 0.9118
2025-05-01 09:22:49,482 - main - INFO - Recall: 0.9755
2025-05-01 09:22:49,482 - main - INFO - F1 Score: 0.9426
2025-05-01 09:22:49,482 - main - INFO - IoU: 0.8914
2025-05-01 09:22:49,482 - main - INFO - mAP: 0.9851
2025-05-01 09:22:49,482 - main - INFO - AUC: 0.9933
2025-05-01 09:22:49,482 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:22:49,482 - main - INFO - 
==================================================
2025-05-01 09:22:49,482 - main - INFO - Running configuration 286/756:
2025-05-01 09:22:49,482 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:22:49,482 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:22:49,482 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:22:49,482 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:22:49,482 - main - INFO - ==================================================
2025-05-01 09:22:49,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286
2025-05-01 09:22:49,482 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286 - INFO - Config: {
  "id": 286,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:22:49,869 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:22:49,869 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 286,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:22:49,870 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:22:50,805 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:22:50,807 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:22:50,810 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286 - INFO - Starting model evaluation
2025-05-01 09:23:01,887 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9088
  Recall:    0.9755
  F1 Score:  0.9410
  IoU:       0.8885
  mAP:       0.9819
  AUC:       0.9910
2025-05-01 09:23:01,889 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286/final_results.json
2025-05-01 09:23:01,890 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_286/final_results.json
2025-05-01 09:23:01,890 - main - INFO - 
Summary for configuration 286:
2025-05-01 09:23:01,890 - main - INFO - Accuracy: 0.9653
2025-05-01 09:23:01,890 - main - INFO - Precision: 0.9088
2025-05-01 09:23:01,890 - main - INFO - Recall: 0.9755
2025-05-01 09:23:01,890 - main - INFO - F1 Score: 0.9410
2025-05-01 09:23:01,890 - main - INFO - IoU: 0.8885
2025-05-01 09:23:01,890 - main - INFO - mAP: 0.9819
2025-05-01 09:23:01,890 - main - INFO - AUC: 0.9910
2025-05-01 09:23:01,890 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:23:01,890 - main - INFO - 
==================================================
2025-05-01 09:23:01,890 - main - INFO - Running configuration 287/756:
2025-05-01 09:23:01,890 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:23:01,890 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:23:01,890 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:23:01,890 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:23:01,890 - main - INFO - ==================================================
2025-05-01 09:23:01,891 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287
2025-05-01 09:23:01,891 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287 - INFO - Config: {
  "id": 287,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:23:02,149 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:23:02,149 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 287,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:23:02,150 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:23:03,031 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:23:03,033 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:23:03,037 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287 - INFO - Starting model evaluation
2025-05-01 09:23:14,493 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9115
  Recall:    0.9720
  F1 Score:  0.9408
  IoU:       0.8882
  mAP:       0.9854
  AUC:       0.9940
2025-05-01 09:23:14,495 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287/final_results.json
2025-05-01 09:23:14,497 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_287/final_results.json
2025-05-01 09:23:14,497 - main - INFO - 
Summary for configuration 287:
2025-05-01 09:23:14,497 - main - INFO - Accuracy: 0.9653
2025-05-01 09:23:14,497 - main - INFO - Precision: 0.9115
2025-05-01 09:23:14,497 - main - INFO - Recall: 0.9720
2025-05-01 09:23:14,497 - main - INFO - F1 Score: 0.9408
2025-05-01 09:23:14,497 - main - INFO - IoU: 0.8882
2025-05-01 09:23:14,497 - main - INFO - mAP: 0.9854
2025-05-01 09:23:14,497 - main - INFO - AUC: 0.9940
2025-05-01 09:23:14,497 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:23:14,497 - main - INFO - 
==================================================
2025-05-01 09:23:14,497 - main - INFO - Running configuration 288/756:
2025-05-01 09:23:14,497 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:23:14,497 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 09:23:14,497 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:23:14,497 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:23:14,497 - main - INFO - ==================================================
2025-05-01 09:23:14,497 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288
2025-05-01 09:23:14,497 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288 - INFO - Config: {
  "id": 288,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:23:14,817 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_Adam_lr_0.0001
2025-05-01 09:23:14,817 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 288,
  "model_name": "EfficientNet-B2",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:23:14,817 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 09:23:15,750 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 09:23:15,752 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001 - INFO - Training completed after 1155.55 seconds
2025-05-01 09:23:15,756 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288 - INFO - Starting model evaluation
2025-05-01 09:23:27,124 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9115
  Recall:    0.9720
  F1 Score:  0.9408
  IoU:       0.8882
  mAP:       0.9827
  AUC:       0.9910
2025-05-01 09:23:27,125 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288/final_results.json
2025-05-01 09:23:27,127 - training.model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_Adam_lr_0.0001_id_288/final_results.json
2025-05-01 09:23:27,127 - main - INFO - 
Summary for configuration 288:
2025-05-01 09:23:27,127 - main - INFO - Accuracy: 0.9653
2025-05-01 09:23:27,127 - main - INFO - Precision: 0.9115
2025-05-01 09:23:27,127 - main - INFO - Recall: 0.9720
2025-05-01 09:23:27,127 - main - INFO - F1 Score: 0.9408
2025-05-01 09:23:27,127 - main - INFO - IoU: 0.8882
2025-05-01 09:23:27,127 - main - INFO - mAP: 0.9827
2025-05-01 09:23:27,127 - main - INFO - AUC: 0.9910
2025-05-01 09:23:27,127 - main - INFO - Training time: 1155.55 seconds
2025-05-01 09:23:27,127 - main - INFO - 
==================================================
2025-05-01 09:23:27,127 - main - INFO - Running configuration 289/756:
2025-05-01 09:23:27,127 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:23:27,127 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:23:27,127 - main - INFO - Scheduler: StepLR
2025-05-01 09:23:27,127 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:23:27,127 - main - INFO - ==================================================
2025-05-01 09:23:27,127 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289
2025-05-01 09:23:27,128 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289 - INFO - Config: {
  "id": 289,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:23:27,399 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:23:27,399 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 289,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:23:27,399 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 09:23:27,400 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 09:23:52,291 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 09:24:25,127 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.5137
2025-05-01 09:24:25,155 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 1 completed in 57.76s - Train Loss: 0.7849, Train Acc: 0.5122, Val Loss: 0.7985, Val Acc: 0.5137
2025-05-01 09:24:25,278 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 09:24:25,279 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 09:24:49,853 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 09:25:23,129 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.5137 to 0.5497
2025-05-01 09:25:23,174 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 2 completed in 57.89s - Train Loss: 0.7844, Train Acc: 0.5272, Val Loss: 0.7639, Val Acc: 0.5497
2025-05-01 09:25:23,296 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 09:25:23,297 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 09:25:47,556 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 09:26:20,680 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation loss improved from inf to 0.8020
2025-05-01 09:26:20,724 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 3 completed in 57.43s - Train Loss: 0.7799, Train Acc: 0.5330, Val Loss: 0.8020, Val Acc: 0.5086
2025-05-01 09:26:20,845 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 09:26:20,845 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 09:26:46,331 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 09:27:19,891 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 4 completed in 59.05s - Train Loss: 0.7990, Train Acc: 0.5152, Val Loss: 0.8307, Val Acc: 0.4820
2025-05-01 09:27:20,020 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 09:27:20,020 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 09:27:45,077 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 09:28:17,920 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8020 to 0.8005
2025-05-01 09:28:17,959 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 5 completed in 57.94s - Train Loss: 0.7915, Train Acc: 0.5172, Val Loss: 0.8005, Val Acc: 0.5120
2025-05-01 09:28:18,057 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 09:28:18,058 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 09:28:43,783 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 09:29:17,082 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 6 completed in 59.02s - Train Loss: 0.7698, Train Acc: 0.5423, Val Loss: 0.8030, Val Acc: 0.5094
2025-05-01 09:29:17,186 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 09:29:17,187 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 09:29:41,675 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 09:30:14,878 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8005 to 0.7922
2025-05-01 09:30:14,918 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 7 completed in 57.73s - Train Loss: 0.7727, Train Acc: 0.5412, Val Loss: 0.7922, Val Acc: 0.5206
2025-05-01 09:30:15,015 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 09:30:15,016 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 09:30:40,014 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 09:31:12,785 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 8 completed in 57.77s - Train Loss: 0.7670, Train Acc: 0.5470, Val Loss: 0.8008, Val Acc: 0.5120
2025-05-01 09:31:12,889 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 09:31:12,890 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 09:31:37,411 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 09:32:10,081 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.7922 to 0.7706
2025-05-01 09:32:10,118 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 9 completed in 57.23s - Train Loss: 0.7819, Train Acc: 0.5313, Val Loss: 0.7706, Val Acc: 0.5412
2025-05-01 09:32:10,213 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 09:32:10,214 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 09:32:35,464 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 09:33:08,876 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 10 completed in 58.66s - Train Loss: 0.7710, Train Acc: 0.5420, Val Loss: 0.7976, Val Acc: 0.5154
2025-05-01 09:33:08,994 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 09:33:08,994 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 09:33:32,950 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 09:34:05,324 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 11 completed in 56.33s - Train Loss: 0.7708, Train Acc: 0.5420, Val Loss: 0.8075, Val Acc: 0.5017
2025-05-01 09:34:05,428 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 09:34:05,429 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 09:34:29,473 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 09:35:02,467 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.7706 to 0.7668
2025-05-01 09:35:02,519 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 12 completed in 57.09s - Train Loss: 0.7655, Train Acc: 0.5480, Val Loss: 0.7668, Val Acc: 0.5429
2025-05-01 09:35:02,617 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 09:35:02,618 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 09:35:27,837 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 09:36:00,737 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.5497 to 0.5575
2025-05-01 09:36:00,787 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 13 completed in 58.17s - Train Loss: 0.7610, Train Acc: 0.5534, Val Loss: 0.7538, Val Acc: 0.5575
2025-05-01 09:36:00,882 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 09:36:00,882 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 09:36:24,832 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 09:36:58,166 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 14 completed in 57.28s - Train Loss: 0.7512, Train Acc: 0.5618, Val Loss: 0.7674, Val Acc: 0.5446
2025-05-01 09:36:58,271 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 09:36:58,272 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 09:37:22,768 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 09:37:54,927 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.5575 to 0.5952
2025-05-01 09:37:54,966 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 15 completed in 56.69s - Train Loss: 0.7524, Train Acc: 0.5607, Val Loss: 0.7167, Val Acc: 0.5952
2025-05-01 09:37:55,064 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 09:37:55,065 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 09:38:19,469 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 09:38:51,614 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.5952 to 0.6029
2025-05-01 09:38:51,653 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 16 completed in 56.59s - Train Loss: 0.7387, Train Acc: 0.5749, Val Loss: 0.7088, Val Acc: 0.6029
2025-05-01 09:38:51,751 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 09:38:51,752 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 09:39:16,813 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 09:39:49,147 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6029 to 0.6115
2025-05-01 09:39:49,186 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 17 completed in 57.43s - Train Loss: 0.7184, Train Acc: 0.5940, Val Loss: 0.7010, Val Acc: 0.6115
2025-05-01 09:39:49,294 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 09:39:49,295 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 09:40:13,871 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 09:40:45,936 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6115 to 0.6166
2025-05-01 09:40:45,978 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 18 completed in 56.68s - Train Loss: 0.7158, Train Acc: 0.5967, Val Loss: 0.6936, Val Acc: 0.6166
2025-05-01 09:40:46,078 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 09:40:46,079 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 09:41:10,883 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 09:41:43,805 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6166 to 0.6355
2025-05-01 09:41:43,843 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 19 completed in 57.76s - Train Loss: 0.7130, Train Acc: 0.5991, Val Loss: 0.6755, Val Acc: 0.6355
2025-05-01 09:41:43,937 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 09:41:43,938 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 09:42:08,364 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 09:42:42,183 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.6355 to 0.6415
2025-05-01 09:42:42,222 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Epoch 20 completed in 58.28s - Train Loss: 0.7023, Train Acc: 0.6098, Val Loss: 0.6690, Val Acc: 0.6415
2025-05-01 09:42:42,321 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 09:42:42,321 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:42:42,325 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289 - INFO - Starting model evaluation
2025-05-01 09:42:53,413 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289 - INFO - Evaluation metrics:
  Accuracy:  0.6224
  Precision: 0.3942
  Recall:    0.6189
  F1 Score:  0.4816
  IoU:       0.3172
  mAP:       0.4161
  AUC:       0.6416
2025-05-01 09:42:53,415 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289/final_results.json
2025-05-01 09:42:53,416 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_289/final_results.json
2025-05-01 09:42:53,416 - main - INFO - 
Summary for configuration 289:
2025-05-01 09:42:53,416 - main - INFO - Accuracy: 0.6224
2025-05-01 09:42:53,416 - main - INFO - Precision: 0.3942
2025-05-01 09:42:53,416 - main - INFO - Recall: 0.6189
2025-05-01 09:42:53,416 - main - INFO - F1 Score: 0.4816
2025-05-01 09:42:53,416 - main - INFO - IoU: 0.3172
2025-05-01 09:42:53,416 - main - INFO - mAP: 0.4161
2025-05-01 09:42:53,416 - main - INFO - AUC: 0.6416
2025-05-01 09:42:53,416 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:42:53,416 - main - INFO - 
==================================================
2025-05-01 09:42:53,416 - main - INFO - Running configuration 290/756:
2025-05-01 09:42:53,416 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:42:53,416 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:42:53,416 - main - INFO - Scheduler: StepLR
2025-05-01 09:42:53,416 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:42:53,416 - main - INFO - ==================================================
2025-05-01 09:42:53,416 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290
2025-05-01 09:42:53,417 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290 - INFO - Config: {
  "id": 290,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:42:53,722 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:42:53,722 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 290,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:42:53,722 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:42:54,651 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:42:54,653 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:42:54,656 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290 - INFO - Starting model evaluation
2025-05-01 09:43:05,879 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290 - INFO - Evaluation metrics:
  Accuracy:  0.6264
  Precision: 0.3987
  Recall:    0.6259
  F1 Score:  0.4871
  IoU:       0.3219
  mAP:       0.4161
  AUC:       0.6409
2025-05-01 09:43:05,883 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290/final_results.json
2025-05-01 09:43:05,884 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_290/final_results.json
2025-05-01 09:43:05,884 - main - INFO - 
Summary for configuration 290:
2025-05-01 09:43:05,884 - main - INFO - Accuracy: 0.6264
2025-05-01 09:43:05,884 - main - INFO - Precision: 0.3987
2025-05-01 09:43:05,884 - main - INFO - Recall: 0.6259
2025-05-01 09:43:05,884 - main - INFO - F1 Score: 0.4871
2025-05-01 09:43:05,884 - main - INFO - IoU: 0.3219
2025-05-01 09:43:05,884 - main - INFO - mAP: 0.4161
2025-05-01 09:43:05,884 - main - INFO - AUC: 0.6409
2025-05-01 09:43:05,884 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:43:05,884 - main - INFO - 
==================================================
2025-05-01 09:43:05,884 - main - INFO - Running configuration 291/756:
2025-05-01 09:43:05,884 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:43:05,884 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:43:05,884 - main - INFO - Scheduler: StepLR
2025-05-01 09:43:05,884 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:43:05,884 - main - INFO - ==================================================
2025-05-01 09:43:05,884 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291
2025-05-01 09:43:05,885 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291 - INFO - Config: {
  "id": 291,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:43:06,300 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:43:06,300 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 291,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:43:06,300 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:43:07,104 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:43:07,105 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:43:07,109 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291 - INFO - Starting model evaluation
2025-05-01 09:43:18,257 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291 - INFO - Evaluation metrics:
  Accuracy:  0.6224
  Precision: 0.3951
  Recall:    0.6259
  F1 Score:  0.4844
  IoU:       0.3196
  mAP:       0.4047
  AUC:       0.6365
2025-05-01 09:43:18,259 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291/final_results.json
2025-05-01 09:43:18,262 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_291/final_results.json
2025-05-01 09:43:18,262 - main - INFO - 
Summary for configuration 291:
2025-05-01 09:43:18,262 - main - INFO - Accuracy: 0.6224
2025-05-01 09:43:18,262 - main - INFO - Precision: 0.3951
2025-05-01 09:43:18,262 - main - INFO - Recall: 0.6259
2025-05-01 09:43:18,262 - main - INFO - F1 Score: 0.4844
2025-05-01 09:43:18,262 - main - INFO - IoU: 0.3196
2025-05-01 09:43:18,262 - main - INFO - mAP: 0.4047
2025-05-01 09:43:18,262 - main - INFO - AUC: 0.6365
2025-05-01 09:43:18,262 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:43:18,262 - main - INFO - 
==================================================
2025-05-01 09:43:18,262 - main - INFO - Running configuration 292/756:
2025-05-01 09:43:18,262 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:43:18,262 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:43:18,262 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 09:43:18,262 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:43:18,262 - main - INFO - ==================================================
2025-05-01 09:43:18,262 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292
2025-05-01 09:43:18,263 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292 - INFO - Config: {
  "id": 292,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:43:18,595 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:43:18,595 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 292,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:43:18,595 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:43:19,518 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:43:19,519 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:43:19,523 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292 - INFO - Starting model evaluation
2025-05-01 09:43:30,621 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292 - INFO - Evaluation metrics:
  Accuracy:  0.6224
  Precision: 0.3947
  Recall:    0.6224
  F1 Score:  0.4830
  IoU:       0.3184
  mAP:       0.4182
  AUC:       0.6423
2025-05-01 09:43:30,623 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292/final_results.json
2025-05-01 09:43:30,625 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_292/final_results.json
2025-05-01 09:43:30,625 - main - INFO - 
Summary for configuration 292:
2025-05-01 09:43:30,625 - main - INFO - Accuracy: 0.6224
2025-05-01 09:43:30,625 - main - INFO - Precision: 0.3947
2025-05-01 09:43:30,625 - main - INFO - Recall: 0.6224
2025-05-01 09:43:30,625 - main - INFO - F1 Score: 0.4830
2025-05-01 09:43:30,625 - main - INFO - IoU: 0.3184
2025-05-01 09:43:30,625 - main - INFO - mAP: 0.4182
2025-05-01 09:43:30,625 - main - INFO - AUC: 0.6423
2025-05-01 09:43:30,625 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:43:30,625 - main - INFO - 
==================================================
2025-05-01 09:43:30,625 - main - INFO - Running configuration 293/756:
2025-05-01 09:43:30,626 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:43:30,626 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:43:30,626 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 09:43:30,626 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:43:30,626 - main - INFO - ==================================================
2025-05-01 09:43:30,626 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293
2025-05-01 09:43:30,626 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293 - INFO - Config: {
  "id": 293,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:43:31,029 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:43:31,029 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 293,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:43:31,029 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:43:31,956 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:43:31,957 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:43:31,962 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293 - INFO - Starting model evaluation
2025-05-01 09:43:42,986 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293 - INFO - Evaluation metrics:
  Accuracy:  0.6234
  Precision: 0.3956
  Recall:    0.6224
  F1 Score:  0.4837
  IoU:       0.3190
  mAP:       0.4157
  AUC:       0.6427
2025-05-01 09:43:42,988 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293/final_results.json
2025-05-01 09:43:42,989 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_293/final_results.json
2025-05-01 09:43:42,989 - main - INFO - 
Summary for configuration 293:
2025-05-01 09:43:42,989 - main - INFO - Accuracy: 0.6234
2025-05-01 09:43:42,989 - main - INFO - Precision: 0.3956
2025-05-01 09:43:42,989 - main - INFO - Recall: 0.6224
2025-05-01 09:43:42,989 - main - INFO - F1 Score: 0.4837
2025-05-01 09:43:42,989 - main - INFO - IoU: 0.3190
2025-05-01 09:43:42,989 - main - INFO - mAP: 0.4157
2025-05-01 09:43:42,989 - main - INFO - AUC: 0.6427
2025-05-01 09:43:42,989 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:43:42,989 - main - INFO - 
==================================================
2025-05-01 09:43:42,989 - main - INFO - Running configuration 294/756:
2025-05-01 09:43:42,989 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:43:42,989 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:43:42,989 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 09:43:42,989 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:43:42,989 - main - INFO - ==================================================
2025-05-01 09:43:42,990 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294
2025-05-01 09:43:42,990 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294 - INFO - Config: {
  "id": 294,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:43:43,278 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:43:43,278 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 294,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:43:43,278 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:43:44,107 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:43:44,108 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:43:44,112 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294 - INFO - Starting model evaluation
2025-05-01 09:43:54,928 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294 - INFO - Evaluation metrics:
  Accuracy:  0.6274
  Precision: 0.3996
  Recall:    0.6259
  F1 Score:  0.4877
  IoU:       0.3225
  mAP:       0.4149
  AUC:       0.6425
2025-05-01 09:43:54,930 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294/final_results.json
2025-05-01 09:43:54,931 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_294/final_results.json
2025-05-01 09:43:54,931 - main - INFO - 
Summary for configuration 294:
2025-05-01 09:43:54,931 - main - INFO - Accuracy: 0.6274
2025-05-01 09:43:54,931 - main - INFO - Precision: 0.3996
2025-05-01 09:43:54,931 - main - INFO - Recall: 0.6259
2025-05-01 09:43:54,931 - main - INFO - F1 Score: 0.4877
2025-05-01 09:43:54,931 - main - INFO - IoU: 0.3225
2025-05-01 09:43:54,931 - main - INFO - mAP: 0.4149
2025-05-01 09:43:54,931 - main - INFO - AUC: 0.6425
2025-05-01 09:43:54,931 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:43:54,931 - main - INFO - 
==================================================
2025-05-01 09:43:54,931 - main - INFO - Running configuration 295/756:
2025-05-01 09:43:54,931 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:43:54,931 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:43:54,931 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:43:54,932 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:43:54,932 - main - INFO - ==================================================
2025-05-01 09:43:54,932 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295
2025-05-01 09:43:54,932 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295 - INFO - Config: {
  "id": 295,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:43:55,308 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:43:55,308 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 295,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:43:55,308 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:43:56,143 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:43:56,144 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:43:56,148 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295 - INFO - Starting model evaluation
2025-05-01 09:44:07,307 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295 - INFO - Evaluation metrics:
  Accuracy:  0.6224
  Precision: 0.3947
  Recall:    0.6224
  F1 Score:  0.4830
  IoU:       0.3184
  mAP:       0.3969
  AUC:       0.6327
2025-05-01 09:44:07,308 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295/final_results.json
2025-05-01 09:44:07,310 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_295/final_results.json
2025-05-01 09:44:07,310 - main - INFO - 
Summary for configuration 295:
2025-05-01 09:44:07,310 - main - INFO - Accuracy: 0.6224
2025-05-01 09:44:07,310 - main - INFO - Precision: 0.3947
2025-05-01 09:44:07,310 - main - INFO - Recall: 0.6224
2025-05-01 09:44:07,310 - main - INFO - F1 Score: 0.4830
2025-05-01 09:44:07,310 - main - INFO - IoU: 0.3184
2025-05-01 09:44:07,310 - main - INFO - mAP: 0.3969
2025-05-01 09:44:07,310 - main - INFO - AUC: 0.6327
2025-05-01 09:44:07,310 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:44:07,310 - main - INFO - 
==================================================
2025-05-01 09:44:07,310 - main - INFO - Running configuration 296/756:
2025-05-01 09:44:07,310 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:44:07,310 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:44:07,310 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:44:07,310 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:44:07,310 - main - INFO - ==================================================
2025-05-01 09:44:07,310 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296
2025-05-01 09:44:07,310 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296 - INFO - Config: {
  "id": 296,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:44:07,626 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:44:07,626 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 296,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:44:07,626 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:44:08,563 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:44:08,564 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:44:08,568 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296 - INFO - Starting model evaluation
2025-05-01 09:44:19,813 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296 - INFO - Evaluation metrics:
  Accuracy:  0.6264
  Precision: 0.3987
  Recall:    0.6259
  F1 Score:  0.4871
  IoU:       0.3219
  mAP:       0.4164
  AUC:       0.6421
2025-05-01 09:44:19,815 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296/final_results.json
2025-05-01 09:44:19,816 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_296/final_results.json
2025-05-01 09:44:19,816 - main - INFO - 
Summary for configuration 296:
2025-05-01 09:44:19,816 - main - INFO - Accuracy: 0.6264
2025-05-01 09:44:19,816 - main - INFO - Precision: 0.3987
2025-05-01 09:44:19,816 - main - INFO - Recall: 0.6259
2025-05-01 09:44:19,816 - main - INFO - F1 Score: 0.4871
2025-05-01 09:44:19,816 - main - INFO - IoU: 0.3219
2025-05-01 09:44:19,816 - main - INFO - mAP: 0.4164
2025-05-01 09:44:19,816 - main - INFO - AUC: 0.6421
2025-05-01 09:44:19,816 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:44:19,816 - main - INFO - 
==================================================
2025-05-01 09:44:19,816 - main - INFO - Running configuration 297/756:
2025-05-01 09:44:19,816 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:44:19,816 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:44:19,816 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 09:44:19,816 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:44:19,816 - main - INFO - ==================================================
2025-05-01 09:44:19,817 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297
2025-05-01 09:44:19,817 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297 - INFO - Config: {
  "id": 297,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:44:20,176 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:44:20,176 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 297,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:44:20,176 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:44:20,972 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:44:20,974 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:44:20,977 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297 - INFO - Starting model evaluation
2025-05-01 09:44:32,360 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297 - INFO - Evaluation metrics:
  Accuracy:  0.6244
  Precision: 0.3964
  Recall:    0.6224
  F1 Score:  0.4844
  IoU:       0.3196
  mAP:       0.4043
  AUC:       0.6391
2025-05-01 09:44:32,361 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297/final_results.json
2025-05-01 09:44:32,363 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_297/final_results.json
2025-05-01 09:44:32,363 - main - INFO - 
Summary for configuration 297:
2025-05-01 09:44:32,363 - main - INFO - Accuracy: 0.6244
2025-05-01 09:44:32,363 - main - INFO - Precision: 0.3964
2025-05-01 09:44:32,363 - main - INFO - Recall: 0.6224
2025-05-01 09:44:32,363 - main - INFO - F1 Score: 0.4844
2025-05-01 09:44:32,363 - main - INFO - IoU: 0.3196
2025-05-01 09:44:32,363 - main - INFO - mAP: 0.4043
2025-05-01 09:44:32,363 - main - INFO - AUC: 0.6391
2025-05-01 09:44:32,363 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:44:32,363 - main - INFO - 
==================================================
2025-05-01 09:44:32,363 - main - INFO - Running configuration 298/756:
2025-05-01 09:44:32,363 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:44:32,363 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:44:32,363 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:44:32,363 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:44:32,363 - main - INFO - ==================================================
2025-05-01 09:44:32,363 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298
2025-05-01 09:44:32,363 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298 - INFO - Config: {
  "id": 298,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:44:32,795 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:44:32,795 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 298,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:44:32,795 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:44:33,726 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:44:33,727 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:44:33,731 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298 - INFO - Starting model evaluation
2025-05-01 09:44:44,838 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298 - INFO - Evaluation metrics:
  Accuracy:  0.6254
  Precision: 0.3987
  Recall:    0.6329
  F1 Score:  0.4892
  IoU:       0.3238
  mAP:       0.4183
  AUC:       0.6438
2025-05-01 09:44:44,840 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298/final_results.json
2025-05-01 09:44:44,842 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_298/final_results.json
2025-05-01 09:44:44,842 - main - INFO - 
Summary for configuration 298:
2025-05-01 09:44:44,842 - main - INFO - Accuracy: 0.6254
2025-05-01 09:44:44,842 - main - INFO - Precision: 0.3987
2025-05-01 09:44:44,842 - main - INFO - Recall: 0.6329
2025-05-01 09:44:44,842 - main - INFO - F1 Score: 0.4892
2025-05-01 09:44:44,842 - main - INFO - IoU: 0.3238
2025-05-01 09:44:44,842 - main - INFO - mAP: 0.4183
2025-05-01 09:44:44,842 - main - INFO - AUC: 0.6438
2025-05-01 09:44:44,842 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:44:44,842 - main - INFO - 
==================================================
2025-05-01 09:44:44,842 - main - INFO - Running configuration 299/756:
2025-05-01 09:44:44,842 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:44:44,842 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:44:44,842 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:44:44,842 - main - INFO - Loss Function: FocalLoss
2025-05-01 09:44:44,842 - main - INFO - ==================================================
2025-05-01 09:44:44,842 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299
2025-05-01 09:44:44,842 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299 - INFO - Config: {
  "id": 299,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:44:45,159 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:44:45,160 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 299,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 09:44:45,160 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:44:46,079 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:44:46,080 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:44:46,084 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299 - INFO - Starting model evaluation
2025-05-01 09:44:56,972 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299 - INFO - Evaluation metrics:
  Accuracy:  0.6234
  Precision: 0.3956
  Recall:    0.6224
  F1 Score:  0.4837
  IoU:       0.3190
  mAP:       0.4110
  AUC:       0.6401
2025-05-01 09:44:56,973 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299/final_results.json
2025-05-01 09:44:56,974 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_299/final_results.json
2025-05-01 09:44:56,975 - main - INFO - 
Summary for configuration 299:
2025-05-01 09:44:56,975 - main - INFO - Accuracy: 0.6234
2025-05-01 09:44:56,975 - main - INFO - Precision: 0.3956
2025-05-01 09:44:56,975 - main - INFO - Recall: 0.6224
2025-05-01 09:44:56,975 - main - INFO - F1 Score: 0.4837
2025-05-01 09:44:56,975 - main - INFO - IoU: 0.3190
2025-05-01 09:44:56,975 - main - INFO - mAP: 0.4110
2025-05-01 09:44:56,975 - main - INFO - AUC: 0.6401
2025-05-01 09:44:56,975 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:44:56,975 - main - INFO - 
==================================================
2025-05-01 09:44:56,975 - main - INFO - Running configuration 300/756:
2025-05-01 09:44:56,975 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:44:56,975 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 09:44:56,975 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 09:44:56,975 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 09:44:56,975 - main - INFO - ==================================================
2025-05-01 09:44:56,975 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300
2025-05-01 09:44:56,975 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300 - INFO - Config: {
  "id": 300,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:44:57,334 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.01
2025-05-01 09:44:57,334 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 300,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 09:44:57,334 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 09:44:58,128 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.6415
2025-05-01 09:44:58,129 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01 - INFO - Training completed after 1154.82 seconds
2025-05-01 09:44:58,133 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300 - INFO - Starting model evaluation
2025-05-01 09:45:08,922 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300 - INFO - Evaluation metrics:
  Accuracy:  0.6244
  Precision: 0.3969
  Recall:    0.6259
  F1 Score:  0.4858
  IoU:       0.3208
  mAP:       0.4137
  AUC:       0.6424
2025-05-01 09:45:08,923 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300/final_results.json
2025-05-01 09:45:08,925 - training.model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.01_id_300/final_results.json
2025-05-01 09:45:08,925 - main - INFO - 
Summary for configuration 300:
2025-05-01 09:45:08,925 - main - INFO - Accuracy: 0.6244
2025-05-01 09:45:08,925 - main - INFO - Precision: 0.3969
2025-05-01 09:45:08,925 - main - INFO - Recall: 0.6259
2025-05-01 09:45:08,925 - main - INFO - F1 Score: 0.4858
2025-05-01 09:45:08,925 - main - INFO - IoU: 0.3208
2025-05-01 09:45:08,925 - main - INFO - mAP: 0.4137
2025-05-01 09:45:08,925 - main - INFO - AUC: 0.6424
2025-05-01 09:45:08,925 - main - INFO - Training time: 1154.82 seconds
2025-05-01 09:45:08,925 - main - INFO - 
==================================================
2025-05-01 09:45:08,925 - main - INFO - Running configuration 301/756:
2025-05-01 09:45:08,925 - main - INFO - Model: EfficientNet-B2
2025-05-01 09:45:08,925 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 09:45:08,925 - main - INFO - Scheduler: StepLR
2025-05-01 09:45:08,925 - main - INFO - Loss Function: CrossEntropy
2025-05-01 09:45:08,925 - main - INFO - ==================================================
2025-05-01 09:45:08,925 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301
2025-05-01 09:45:08,925 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301 - INFO - Config: {
  "id": 301,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:45:09,381 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 09:45:09,381 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 301,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 09:45:09,381 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 09:45:09,383 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 09:45:34,417 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 09:46:08,489 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.7530
2025-05-01 09:46:08,518 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 1 completed in 59.14s - Train Loss: 0.4767, Train Acc: 0.8316, Val Loss: 0.5583, Val Acc: 0.7530
2025-05-01 09:46:08,631 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 09:46:08,631 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 09:46:34,120 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 09:47:07,025 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.7530 to 0.8105
2025-05-01 09:47:07,064 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 2 completed in 58.43s - Train Loss: 0.4771, Train Acc: 0.8340, Val Loss: 0.4906, Val Acc: 0.8105
2025-05-01 09:47:07,168 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 09:47:07,168 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 09:47:31,610 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 09:48:04,589 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8105 to 0.8859
2025-05-01 09:48:04,626 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 3 completed in 57.46s - Train Loss: 0.4474, Train Acc: 0.8612, Val Loss: 0.4235, Val Acc: 0.8859
2025-05-01 09:48:04,725 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 09:48:04,725 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 09:48:29,772 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 09:49:04,069 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation loss improved from inf to 0.4425
2025-05-01 09:49:04,107 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 4 completed in 59.38s - Train Loss: 0.4330, Train Acc: 0.8754, Val Loss: 0.4425, Val Acc: 0.8671
2025-05-01 09:49:04,206 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 09:49:04,206 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 09:49:29,033 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 09:50:02,653 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8859 to 0.9185
2025-05-01 09:50:02,715 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 5 completed in 58.51s - Train Loss: 0.4130, Train Acc: 0.8973, Val Loss: 0.3905, Val Acc: 0.9185
2025-05-01 09:50:02,819 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 09:50:02,819 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 09:50:27,280 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 09:51:00,281 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9185 to 0.9254
2025-05-01 09:51:00,334 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 6 completed in 57.51s - Train Loss: 0.3887, Train Acc: 0.9243, Val Loss: 0.3833, Val Acc: 0.9254
2025-05-01 09:51:00,431 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 09:51:00,431 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 09:51:24,831 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 09:51:57,667 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.4425 to 0.3933
2025-05-01 09:51:57,708 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 7 completed in 57.28s - Train Loss: 0.4187, Train Acc: 0.8925, Val Loss: 0.3933, Val Acc: 0.9151
2025-05-01 09:51:57,809 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 09:51:57,809 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 09:52:22,463 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 09:52:55,317 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 8 completed in 57.51s - Train Loss: 0.4014, Train Acc: 0.9093, Val Loss: 0.3933, Val Acc: 0.9194
2025-05-01 09:52:55,422 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 09:52:55,422 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 09:53:20,686 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 09:53:53,466 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 9 completed in 58.04s - Train Loss: 0.4021, Train Acc: 0.9110, Val Loss: 0.3937, Val Acc: 0.9185
2025-05-01 09:53:53,568 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 09:53:53,569 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 09:54:17,820 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 09:54:50,650 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9254 to 0.9408
2025-05-01 09:54:50,690 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 10 completed in 57.12s - Train Loss: 0.3889, Train Acc: 0.9221, Val Loss: 0.3707, Val Acc: 0.9408
2025-05-01 09:54:50,787 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 09:54:50,787 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 09:55:15,925 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 09:55:48,185 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9408 to 0.9545
2025-05-01 09:55:48,222 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 11 completed in 57.44s - Train Loss: 0.3669, Train Acc: 0.9453, Val Loss: 0.3577, Val Acc: 0.9545
2025-05-01 09:55:48,324 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 09:55:48,324 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 09:56:12,415 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 09:56:45,928 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9545 to 0.9571
2025-05-01 09:56:45,965 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 12 completed in 57.64s - Train Loss: 0.3608, Train Acc: 0.9507, Val Loss: 0.3562, Val Acc: 0.9571
2025-05-01 09:56:46,063 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 09:56:46,064 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 09:57:10,964 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 09:57:43,314 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9571 to 0.9605
2025-05-01 09:57:43,348 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 13 completed in 57.28s - Train Loss: 0.3542, Train Acc: 0.9573, Val Loss: 0.3499, Val Acc: 0.9605
2025-05-01 09:57:43,442 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 09:57:43,443 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 09:58:07,399 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 09:58:40,807 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9605 to 0.9657
2025-05-01 09:58:40,848 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 14 completed in 57.41s - Train Loss: 0.3500, Train Acc: 0.9629, Val Loss: 0.3473, Val Acc: 0.9657
2025-05-01 09:58:40,950 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 09:58:40,950 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 09:59:05,448 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 09:59:37,601 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3933 to 0.3484
2025-05-01 09:59:37,637 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 15 completed in 56.69s - Train Loss: 0.3484, Train Acc: 0.9631, Val Loss: 0.3484, Val Acc: 0.9640
2025-05-01 09:59:37,736 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 09:59:37,737 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 10:00:02,371 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 10:00:35,292 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3484 to 0.3470
2025-05-01 10:00:35,332 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 16 completed in 57.59s - Train Loss: 0.3427, Train Acc: 0.9700, Val Loss: 0.3470, Val Acc: 0.9648
2025-05-01 10:00:35,430 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 10:00:35,431 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 10:00:59,446 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 10:01:31,497 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9657 to 0.9726
2025-05-01 10:01:31,546 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 17 completed in 56.12s - Train Loss: 0.3417, Train Acc: 0.9717, Val Loss: 0.3419, Val Acc: 0.9726
2025-05-01 10:01:31,642 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 10:01:31,643 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 10:01:56,392 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 10:02:30,463 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3470 to 0.3409
2025-05-01 10:02:30,501 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 18 completed in 58.86s - Train Loss: 0.3407, Train Acc: 0.9732, Val Loss: 0.3409, Val Acc: 0.9717
2025-05-01 10:02:30,598 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 10:02:30,599 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 10:02:55,853 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 10:03:28,926 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3409 to 0.3406
2025-05-01 10:03:28,965 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 19 completed in 58.37s - Train Loss: 0.3371, Train Acc: 0.9760, Val Loss: 0.3406, Val Acc: 0.9726
2025-05-01 10:03:29,069 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 10:03:29,070 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 10:03:52,464 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 10:04:24,951 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9726 to 0.9743
2025-05-01 10:04:24,989 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Epoch 20 completed in 55.92s - Train Loss: 0.3355, Train Acc: 0.9770, Val Loss: 0.3395, Val Acc: 0.9743
2025-05-01 10:04:25,094 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 10:04:25,095 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:04:25,099 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301 - INFO - Starting model evaluation
2025-05-01 10:04:35,852 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301 - INFO - Evaluation metrics:
  Accuracy:  0.9465
  Precision: 0.8766
  Recall:    0.9441
  F1 Score:  0.9091
  IoU:       0.8333
  mAP:       0.9426
  AUC:       0.9804
2025-05-01 10:04:35,853 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301/final_results.json
2025-05-01 10:04:35,855 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_301/final_results.json
2025-05-01 10:04:35,855 - main - INFO - 
Summary for configuration 301:
2025-05-01 10:04:35,855 - main - INFO - Accuracy: 0.9465
2025-05-01 10:04:35,855 - main - INFO - Precision: 0.8766
2025-05-01 10:04:35,855 - main - INFO - Recall: 0.9441
2025-05-01 10:04:35,855 - main - INFO - F1 Score: 0.9091
2025-05-01 10:04:35,855 - main - INFO - IoU: 0.8333
2025-05-01 10:04:35,855 - main - INFO - mAP: 0.9426
2025-05-01 10:04:35,855 - main - INFO - AUC: 0.9804
2025-05-01 10:04:35,855 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:04:35,855 - main - INFO - 
==================================================
2025-05-01 10:04:35,855 - main - INFO - Running configuration 302/756:
2025-05-01 10:04:35,855 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:04:35,855 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:04:35,855 - main - INFO - Scheduler: StepLR
2025-05-01 10:04:35,855 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:04:35,855 - main - INFO - ==================================================
2025-05-01 10:04:35,856 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302
2025-05-01 10:04:35,856 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302 - INFO - Config: {
  "id": 302,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:04:36,110 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:04:36,113 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 302,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:04:36,113 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:04:37,033 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:04:37,034 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:04:37,038 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302 - INFO - Starting model evaluation
2025-05-01 10:04:47,834 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302 - INFO - Evaluation metrics:
  Accuracy:  0.9435
  Precision: 0.8706
  Recall:    0.9406
  F1 Score:  0.9042
  IoU:       0.8252
  mAP:       0.9312
  AUC:       0.9767
2025-05-01 10:04:47,836 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302/final_results.json
2025-05-01 10:04:47,837 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_302/final_results.json
2025-05-01 10:04:47,837 - main - INFO - 
Summary for configuration 302:
2025-05-01 10:04:47,838 - main - INFO - Accuracy: 0.9435
2025-05-01 10:04:47,838 - main - INFO - Precision: 0.8706
2025-05-01 10:04:47,838 - main - INFO - Recall: 0.9406
2025-05-01 10:04:47,838 - main - INFO - F1 Score: 0.9042
2025-05-01 10:04:47,838 - main - INFO - IoU: 0.8252
2025-05-01 10:04:47,838 - main - INFO - mAP: 0.9312
2025-05-01 10:04:47,838 - main - INFO - AUC: 0.9767
2025-05-01 10:04:47,838 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:04:47,838 - main - INFO - 
==================================================
2025-05-01 10:04:47,838 - main - INFO - Running configuration 303/756:
2025-05-01 10:04:47,838 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:04:47,838 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:04:47,838 - main - INFO - Scheduler: StepLR
2025-05-01 10:04:47,838 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:04:47,838 - main - INFO - ==================================================
2025-05-01 10:04:47,838 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303
2025-05-01 10:04:47,838 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303 - INFO - Config: {
  "id": 303,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:04:48,119 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:04:48,119 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 303,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:04:48,119 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:04:48,971 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:04:48,972 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:04:48,976 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303 - INFO - Starting model evaluation
2025-05-01 10:04:59,897 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303 - INFO - Evaluation metrics:
  Accuracy:  0.9465
  Precision: 0.8791
  Recall:    0.9406
  F1 Score:  0.9088
  IoU:       0.8328
  mAP:       0.9412
  AUC:       0.9786
2025-05-01 10:04:59,899 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303/final_results.json
2025-05-01 10:04:59,902 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_303/final_results.json
2025-05-01 10:04:59,902 - main - INFO - 
Summary for configuration 303:
2025-05-01 10:04:59,902 - main - INFO - Accuracy: 0.9465
2025-05-01 10:04:59,902 - main - INFO - Precision: 0.8791
2025-05-01 10:04:59,902 - main - INFO - Recall: 0.9406
2025-05-01 10:04:59,902 - main - INFO - F1 Score: 0.9088
2025-05-01 10:04:59,902 - main - INFO - IoU: 0.8328
2025-05-01 10:04:59,902 - main - INFO - mAP: 0.9412
2025-05-01 10:04:59,902 - main - INFO - AUC: 0.9786
2025-05-01 10:04:59,902 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:04:59,902 - main - INFO - 
==================================================
2025-05-01 10:04:59,902 - main - INFO - Running configuration 304/756:
2025-05-01 10:04:59,902 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:04:59,902 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:04:59,902 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:04:59,902 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:04:59,902 - main - INFO - ==================================================
2025-05-01 10:04:59,902 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304
2025-05-01 10:04:59,902 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304 - INFO - Config: {
  "id": 304,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:05:00,214 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:05:00,214 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 304,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:05:00,214 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:05:01,070 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:05:01,072 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:05:01,075 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304 - INFO - Starting model evaluation
2025-05-01 10:05:12,087 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304 - INFO - Evaluation metrics:
  Accuracy:  0.9475
  Precision: 0.8820
  Recall:    0.9406
  F1 Score:  0.9103
  IoU:       0.8354
  mAP:       0.9316
  AUC:       0.9755
2025-05-01 10:05:12,089 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304/final_results.json
2025-05-01 10:05:12,090 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_304/final_results.json
2025-05-01 10:05:12,090 - main - INFO - 
Summary for configuration 304:
2025-05-01 10:05:12,090 - main - INFO - Accuracy: 0.9475
2025-05-01 10:05:12,090 - main - INFO - Precision: 0.8820
2025-05-01 10:05:12,090 - main - INFO - Recall: 0.9406
2025-05-01 10:05:12,090 - main - INFO - F1 Score: 0.9103
2025-05-01 10:05:12,090 - main - INFO - IoU: 0.8354
2025-05-01 10:05:12,090 - main - INFO - mAP: 0.9316
2025-05-01 10:05:12,090 - main - INFO - AUC: 0.9755
2025-05-01 10:05:12,090 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:05:12,090 - main - INFO - 
==================================================
2025-05-01 10:05:12,090 - main - INFO - Running configuration 305/756:
2025-05-01 10:05:12,090 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:05:12,090 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:05:12,090 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:05:12,090 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:05:12,090 - main - INFO - ==================================================
2025-05-01 10:05:12,091 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305
2025-05-01 10:05:12,091 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305 - INFO - Config: {
  "id": 305,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:05:12,525 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:05:12,525 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 305,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:05:12,525 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:05:13,448 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:05:13,450 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:05:13,453 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305 - INFO - Starting model evaluation
2025-05-01 10:05:24,547 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305 - INFO - Evaluation metrics:
  Accuracy:  0.9465
  Precision: 0.8791
  Recall:    0.9406
  F1 Score:  0.9088
  IoU:       0.8328
  mAP:       0.9379
  AUC:       0.9795
2025-05-01 10:05:24,549 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305/final_results.json
2025-05-01 10:05:24,550 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_305/final_results.json
2025-05-01 10:05:24,550 - main - INFO - 
Summary for configuration 305:
2025-05-01 10:05:24,550 - main - INFO - Accuracy: 0.9465
2025-05-01 10:05:24,550 - main - INFO - Precision: 0.8791
2025-05-01 10:05:24,550 - main - INFO - Recall: 0.9406
2025-05-01 10:05:24,550 - main - INFO - F1 Score: 0.9088
2025-05-01 10:05:24,550 - main - INFO - IoU: 0.8328
2025-05-01 10:05:24,550 - main - INFO - mAP: 0.9379
2025-05-01 10:05:24,550 - main - INFO - AUC: 0.9795
2025-05-01 10:05:24,550 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:05:24,550 - main - INFO - 
==================================================
2025-05-01 10:05:24,550 - main - INFO - Running configuration 306/756:
2025-05-01 10:05:24,550 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:05:24,550 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:05:24,551 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:05:24,551 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:05:24,551 - main - INFO - ==================================================
2025-05-01 10:05:24,551 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306
2025-05-01 10:05:24,551 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306 - INFO - Config: {
  "id": 306,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:05:24,991 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:05:24,991 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 306,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:05:24,991 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:05:25,911 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:05:25,913 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:05:25,916 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306 - INFO - Starting model evaluation
2025-05-01 10:05:37,014 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306 - INFO - Evaluation metrics:
  Accuracy:  0.9504
  Precision: 0.8882
  Recall:    0.9441
  F1 Score:  0.9153
  IoU:       0.8438
  mAP:       0.9380
  AUC:       0.9801
2025-05-01 10:05:37,016 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306/final_results.json
2025-05-01 10:05:37,018 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_306/final_results.json
2025-05-01 10:05:37,018 - main - INFO - 
Summary for configuration 306:
2025-05-01 10:05:37,018 - main - INFO - Accuracy: 0.9504
2025-05-01 10:05:37,018 - main - INFO - Precision: 0.8882
2025-05-01 10:05:37,018 - main - INFO - Recall: 0.9441
2025-05-01 10:05:37,018 - main - INFO - F1 Score: 0.9153
2025-05-01 10:05:37,018 - main - INFO - IoU: 0.8438
2025-05-01 10:05:37,018 - main - INFO - mAP: 0.9380
2025-05-01 10:05:37,018 - main - INFO - AUC: 0.9801
2025-05-01 10:05:37,018 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:05:37,018 - main - INFO - 
==================================================
2025-05-01 10:05:37,018 - main - INFO - Running configuration 307/756:
2025-05-01 10:05:37,018 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:05:37,018 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:05:37,018 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:05:37,018 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:05:37,018 - main - INFO - ==================================================
2025-05-01 10:05:37,018 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307
2025-05-01 10:05:37,018 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307 - INFO - Config: {
  "id": 307,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:05:37,410 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:05:37,410 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 307,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:05:37,410 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:05:38,326 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:05:38,327 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:05:38,331 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307 - INFO - Starting model evaluation
2025-05-01 10:05:49,332 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.8889
  Recall:    0.9510
  F1 Score:  0.9189
  IoU:       0.8500
  mAP:       0.9244
  AUC:       0.9769
2025-05-01 10:05:49,334 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307/final_results.json
2025-05-01 10:05:49,336 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_307/final_results.json
2025-05-01 10:05:49,336 - main - INFO - 
Summary for configuration 307:
2025-05-01 10:05:49,336 - main - INFO - Accuracy: 0.9524
2025-05-01 10:05:49,336 - main - INFO - Precision: 0.8889
2025-05-01 10:05:49,336 - main - INFO - Recall: 0.9510
2025-05-01 10:05:49,336 - main - INFO - F1 Score: 0.9189
2025-05-01 10:05:49,336 - main - INFO - IoU: 0.8500
2025-05-01 10:05:49,336 - main - INFO - mAP: 0.9244
2025-05-01 10:05:49,336 - main - INFO - AUC: 0.9769
2025-05-01 10:05:49,336 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:05:49,336 - main - INFO - 
==================================================
2025-05-01 10:05:49,336 - main - INFO - Running configuration 308/756:
2025-05-01 10:05:49,336 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:05:49,336 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:05:49,336 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:05:49,336 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:05:49,336 - main - INFO - ==================================================
2025-05-01 10:05:49,336 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308
2025-05-01 10:05:49,336 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308 - INFO - Config: {
  "id": 308,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:05:49,633 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:05:49,636 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 308,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:05:49,636 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:05:50,522 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:05:50,523 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:05:50,527 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308 - INFO - Starting model evaluation
2025-05-01 10:06:01,513 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308 - INFO - Evaluation metrics:
  Accuracy:  0.9504
  Precision: 0.8806
  Recall:    0.9545
  F1 Score:  0.9161
  IoU:       0.8452
  mAP:       0.9457
  AUC:       0.9832
2025-05-01 10:06:01,515 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308/final_results.json
2025-05-01 10:06:01,516 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_308/final_results.json
2025-05-01 10:06:01,516 - main - INFO - 
Summary for configuration 308:
2025-05-01 10:06:01,516 - main - INFO - Accuracy: 0.9504
2025-05-01 10:06:01,516 - main - INFO - Precision: 0.8806
2025-05-01 10:06:01,516 - main - INFO - Recall: 0.9545
2025-05-01 10:06:01,516 - main - INFO - F1 Score: 0.9161
2025-05-01 10:06:01,516 - main - INFO - IoU: 0.8452
2025-05-01 10:06:01,516 - main - INFO - mAP: 0.9457
2025-05-01 10:06:01,516 - main - INFO - AUC: 0.9832
2025-05-01 10:06:01,516 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:06:01,516 - main - INFO - 
==================================================
2025-05-01 10:06:01,516 - main - INFO - Running configuration 309/756:
2025-05-01 10:06:01,516 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:06:01,516 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:06:01,516 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:06:01,516 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:06:01,516 - main - INFO - ==================================================
2025-05-01 10:06:01,517 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309
2025-05-01 10:06:01,517 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309 - INFO - Config: {
  "id": 309,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:06:01,929 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:06:01,929 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 309,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:06:01,929 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:06:02,824 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:06:02,828 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:06:02,832 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309 - INFO - Starting model evaluation
2025-05-01 10:06:13,704 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309 - INFO - Evaluation metrics:
  Accuracy:  0.9495
  Precision: 0.8803
  Recall:    0.9510
  F1 Score:  0.9143
  IoU:       0.8421
  mAP:       0.9365
  AUC:       0.9789
2025-05-01 10:06:13,706 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309/final_results.json
2025-05-01 10:06:13,708 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_309/final_results.json
2025-05-01 10:06:13,708 - main - INFO - 
Summary for configuration 309:
2025-05-01 10:06:13,708 - main - INFO - Accuracy: 0.9495
2025-05-01 10:06:13,708 - main - INFO - Precision: 0.8803
2025-05-01 10:06:13,708 - main - INFO - Recall: 0.9510
2025-05-01 10:06:13,708 - main - INFO - F1 Score: 0.9143
2025-05-01 10:06:13,708 - main - INFO - IoU: 0.8421
2025-05-01 10:06:13,708 - main - INFO - mAP: 0.9365
2025-05-01 10:06:13,708 - main - INFO - AUC: 0.9789
2025-05-01 10:06:13,708 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:06:13,708 - main - INFO - 
==================================================
2025-05-01 10:06:13,708 - main - INFO - Running configuration 310/756:
2025-05-01 10:06:13,708 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:06:13,708 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:06:13,708 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:06:13,708 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:06:13,708 - main - INFO - ==================================================
2025-05-01 10:06:13,708 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310
2025-05-01 10:06:13,708 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310 - INFO - Config: {
  "id": 310,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:06:14,127 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:06:14,127 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 310,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:06:14,127 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:06:14,923 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:06:14,925 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:06:14,928 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310 - INFO - Starting model evaluation
2025-05-01 10:06:25,906 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310 - INFO - Evaluation metrics:
  Accuracy:  0.9475
  Precision: 0.8746
  Recall:    0.9510
  F1 Score:  0.9112
  IoU:       0.8369
  mAP:       0.9418
  AUC:       0.9828
2025-05-01 10:06:25,907 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310/final_results.json
2025-05-01 10:06:25,909 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_310/final_results.json
2025-05-01 10:06:25,909 - main - INFO - 
Summary for configuration 310:
2025-05-01 10:06:25,909 - main - INFO - Accuracy: 0.9475
2025-05-01 10:06:25,909 - main - INFO - Precision: 0.8746
2025-05-01 10:06:25,909 - main - INFO - Recall: 0.9510
2025-05-01 10:06:25,909 - main - INFO - F1 Score: 0.9112
2025-05-01 10:06:25,909 - main - INFO - IoU: 0.8369
2025-05-01 10:06:25,909 - main - INFO - mAP: 0.9418
2025-05-01 10:06:25,909 - main - INFO - AUC: 0.9828
2025-05-01 10:06:25,909 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:06:25,909 - main - INFO - 
==================================================
2025-05-01 10:06:25,909 - main - INFO - Running configuration 311/756:
2025-05-01 10:06:25,909 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:06:25,909 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:06:25,909 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:06:25,909 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:06:25,909 - main - INFO - ==================================================
2025-05-01 10:06:25,909 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311
2025-05-01 10:06:25,909 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311 - INFO - Config: {
  "id": 311,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:06:26,227 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:06:26,227 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 311,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:06:26,227 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:06:27,072 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:06:27,074 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:06:27,077 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311 - INFO - Starting model evaluation
2025-05-01 10:06:37,958 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311 - INFO - Evaluation metrics:
  Accuracy:  0.9455
  Precision: 0.8762
  Recall:    0.9406
  F1 Score:  0.9073
  IoU:       0.8302
  mAP:       0.9205
  AUC:       0.9753
2025-05-01 10:06:37,959 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311/final_results.json
2025-05-01 10:06:37,961 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_311/final_results.json
2025-05-01 10:06:37,961 - main - INFO - 
Summary for configuration 311:
2025-05-01 10:06:37,961 - main - INFO - Accuracy: 0.9455
2025-05-01 10:06:37,961 - main - INFO - Precision: 0.8762
2025-05-01 10:06:37,961 - main - INFO - Recall: 0.9406
2025-05-01 10:06:37,961 - main - INFO - F1 Score: 0.9073
2025-05-01 10:06:37,961 - main - INFO - IoU: 0.8302
2025-05-01 10:06:37,961 - main - INFO - mAP: 0.9205
2025-05-01 10:06:37,961 - main - INFO - AUC: 0.9753
2025-05-01 10:06:37,961 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:06:37,961 - main - INFO - 
==================================================
2025-05-01 10:06:37,961 - main - INFO - Running configuration 312/756:
2025-05-01 10:06:37,961 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:06:37,961 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 10:06:37,961 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:06:37,961 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:06:37,961 - main - INFO - ==================================================
2025-05-01 10:06:37,961 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312
2025-05-01 10:06:37,961 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312 - INFO - Config: {
  "id": 312,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:06:38,351 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.001
2025-05-01 10:06:38,351 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 312,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:06:38,351 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 10:06:39,212 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9743
2025-05-01 10:06:39,214 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001 - INFO - Training completed after 1155.61 seconds
2025-05-01 10:06:39,218 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312 - INFO - Starting model evaluation
2025-05-01 10:06:50,072 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.8864
  Recall:    0.9545
  F1 Score:  0.9192
  IoU:       0.8505
  mAP:       0.9338
  AUC:       0.9799
2025-05-01 10:06:50,074 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312/final_results.json
2025-05-01 10:06:50,075 - training.model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.001_id_312/final_results.json
2025-05-01 10:06:50,075 - main - INFO - 
Summary for configuration 312:
2025-05-01 10:06:50,075 - main - INFO - Accuracy: 0.9524
2025-05-01 10:06:50,075 - main - INFO - Precision: 0.8864
2025-05-01 10:06:50,075 - main - INFO - Recall: 0.9545
2025-05-01 10:06:50,075 - main - INFO - F1 Score: 0.9192
2025-05-01 10:06:50,075 - main - INFO - IoU: 0.8505
2025-05-01 10:06:50,075 - main - INFO - mAP: 0.9338
2025-05-01 10:06:50,075 - main - INFO - AUC: 0.9799
2025-05-01 10:06:50,075 - main - INFO - Training time: 1155.61 seconds
2025-05-01 10:06:50,075 - main - INFO - 
==================================================
2025-05-01 10:06:50,075 - main - INFO - Running configuration 313/756:
2025-05-01 10:06:50,076 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:06:50,076 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:06:50,076 - main - INFO - Scheduler: StepLR
2025-05-01 10:06:50,076 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:06:50,076 - main - INFO - ==================================================
2025-05-01 10:06:50,076 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313
2025-05-01 10:06:50,076 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313 - INFO - Config: {
  "id": 313,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:06:50,435 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:06:50,435 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 313,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:06:50,435 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 10:06:50,436 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 10:07:14,470 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 10:07:46,889 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9563
2025-05-01 10:07:46,919 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 1 completed in 56.48s - Train Loss: 0.4627, Train Acc: 0.8569, Val Loss: 0.3611, Val Acc: 0.9563
2025-05-01 10:07:47,019 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 10:07:47,019 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 10:08:10,973 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 10:08:44,323 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9563 to 0.9700
2025-05-01 10:08:44,367 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 2 completed in 57.35s - Train Loss: 0.3630, Train Acc: 0.9535, Val Loss: 0.3444, Val Acc: 0.9700
2025-05-01 10:08:44,464 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 10:08:44,465 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 10:09:08,621 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 10:09:40,909 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9700 to 0.9751
2025-05-01 10:09:40,949 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 3 completed in 56.48s - Train Loss: 0.3434, Train Acc: 0.9715, Val Loss: 0.3401, Val Acc: 0.9751
2025-05-01 10:09:41,046 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 10:09:41,046 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 10:10:06,316 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 10:10:39,306 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9751 to 0.9768
2025-05-01 10:10:39,344 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 4 completed in 58.30s - Train Loss: 0.3412, Train Acc: 0.9730, Val Loss: 0.3352, Val Acc: 0.9768
2025-05-01 10:10:39,526 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 10:10:39,527 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 10:11:03,584 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 10:11:36,634 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from inf to 0.3351
2025-05-01 10:11:36,672 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 5 completed in 57.15s - Train Loss: 0.3363, Train Acc: 0.9779, Val Loss: 0.3351, Val Acc: 0.9768
2025-05-01 10:11:36,772 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 10:11:36,772 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 10:11:59,960 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 10:12:33,204 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9768 to 0.9803
2025-05-01 10:12:33,255 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 6 completed in 56.48s - Train Loss: 0.3302, Train Acc: 0.9839, Val Loss: 0.3336, Val Acc: 0.9803
2025-05-01 10:12:33,353 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 10:12:33,354 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 10:12:58,280 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 10:13:30,012 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3351 to 0.3307
2025-05-01 10:13:30,060 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 7 completed in 56.71s - Train Loss: 0.3292, Train Acc: 0.9839, Val Loss: 0.3307, Val Acc: 0.9794
2025-05-01 10:13:30,160 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 10:13:30,161 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 10:13:55,059 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 10:14:27,681 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 8 completed in 57.52s - Train Loss: 0.3256, Train Acc: 0.9869, Val Loss: 0.3385, Val Acc: 0.9743
2025-05-01 10:14:27,784 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 10:14:27,784 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 10:14:52,278 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 10:15:24,600 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 9 completed in 56.82s - Train Loss: 0.3250, Train Acc: 0.9886, Val Loss: 0.3336, Val Acc: 0.9794
2025-05-01 10:15:24,707 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 10:15:24,707 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 10:15:49,410 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 10:16:22,184 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9803 to 0.9820
2025-05-01 10:16:22,223 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 10 completed in 57.52s - Train Loss: 0.3264, Train Acc: 0.9869, Val Loss: 0.3301, Val Acc: 0.9820
2025-05-01 10:16:22,322 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 10:16:22,322 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 10:16:47,140 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 10:17:20,356 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9820 to 0.9897
2025-05-01 10:17:20,394 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 11 completed in 58.07s - Train Loss: 0.3218, Train Acc: 0.9921, Val Loss: 0.3252, Val Acc: 0.9897
2025-05-01 10:17:20,499 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 10:17:20,500 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 10:17:44,649 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 10:18:16,760 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3307 to 0.3298
2025-05-01 10:18:16,798 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 12 completed in 56.30s - Train Loss: 0.3192, Train Acc: 0.9949, Val Loss: 0.3298, Val Acc: 0.9828
2025-05-01 10:18:16,902 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 10:18:16,902 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-01 10:18:41,634 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 10:19:14,522 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3298 to 0.3271
2025-05-01 10:19:14,562 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 13 completed in 57.66s - Train Loss: 0.3185, Train Acc: 0.9953, Val Loss: 0.3271, Val Acc: 0.9863
2025-05-01 10:19:14,663 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-01 10:19:14,663 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-01 10:19:38,948 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 10:20:11,260 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3271 to 0.3258
2025-05-01 10:20:11,299 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 14 completed in 56.64s - Train Loss: 0.3181, Train Acc: 0.9957, Val Loss: 0.3258, Val Acc: 0.9880
2025-05-01 10:20:11,398 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-01 10:20:11,398 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-01 10:20:35,315 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 10:21:08,363 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 15 completed in 56.96s - Train Loss: 0.3183, Train Acc: 0.9957, Val Loss: 0.3279, Val Acc: 0.9846
2025-05-01 10:21:08,464 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-01 10:21:08,465 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-01 10:21:32,623 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 10:22:05,043 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 16 completed in 56.58s - Train Loss: 0.3176, Train Acc: 0.9961, Val Loss: 0.3274, Val Acc: 0.9863
2025-05-01 10:22:05,143 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-01 10:22:05,143 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-01 10:22:30,022 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 10:23:04,101 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 17 completed in 58.96s - Train Loss: 0.3163, Train Acc: 0.9972, Val Loss: 0.3280, Val Acc: 0.9854
2025-05-01 10:23:04,201 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-01 10:23:04,202 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-01 10:23:29,181 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 10:24:02,952 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 18 completed in 58.75s - Train Loss: 0.3162, Train Acc: 0.9972, Val Loss: 0.3294, Val Acc: 0.9820
2025-05-01 10:24:03,058 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-01 10:24:03,058 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-01 10:24:27,386 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 10:24:59,914 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 19 completed in 56.86s - Train Loss: 0.3168, Train Acc: 0.9970, Val Loss: 0.3258, Val Acc: 0.9880
2025-05-01 10:25:00,020 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-01 10:25:00,021 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Early stopping triggered after 19 epochs
2025-05-01 10:25:00,021 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1089.48 seconds
2025-05-01 10:25:00,026 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313 - INFO - Starting model evaluation
2025-05-01 10:25:11,140 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9327
  Recall:    0.9685
  F1 Score:  0.9503
  IoU:       0.9052
  mAP:       0.9873
  AUC:       0.9947
2025-05-01 10:25:11,141 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313/final_results.json
2025-05-01 10:25:11,143 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_313/final_results.json
2025-05-01 10:25:11,143 - main - INFO - 
Summary for configuration 313:
2025-05-01 10:25:11,143 - main - INFO - Accuracy: 0.9713
2025-05-01 10:25:11,143 - main - INFO - Precision: 0.9327
2025-05-01 10:25:11,143 - main - INFO - Recall: 0.9685
2025-05-01 10:25:11,143 - main - INFO - F1 Score: 0.9503
2025-05-01 10:25:11,143 - main - INFO - IoU: 0.9052
2025-05-01 10:25:11,143 - main - INFO - mAP: 0.9873
2025-05-01 10:25:11,143 - main - INFO - AUC: 0.9947
2025-05-01 10:25:11,143 - main - INFO - Training time: 1089.48 seconds
2025-05-01 10:25:11,143 - main - INFO - 
==================================================
2025-05-01 10:25:11,143 - main - INFO - Running configuration 314/756:
2025-05-01 10:25:11,143 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:25:11,143 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:25:11,143 - main - INFO - Scheduler: StepLR
2025-05-01 10:25:11,143 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:25:11,143 - main - INFO - ==================================================
2025-05-01 10:25:11,143 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314
2025-05-01 10:25:11,144 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314 - INFO - Config: {
  "id": 314,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:25:11,509 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:25:11,509 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 314,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:25:11,509 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:25:12,307 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 19, best validation accuracy: 0.9897
2025-05-01 10:25:12,308 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-01 10:25:36,374 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 10:26:09,140 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3258 to 0.0075
2025-05-01 10:26:09,179 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Epoch 20 completed in 56.87s - Train Loss: 0.0059, Train Acc: 0.9981, Val Loss: 0.0075, Val Acc: 0.9889
2025-05-01 10:26:09,279 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-01 10:26:09,280 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:26:09,284 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314 - INFO - Starting model evaluation
2025-05-01 10:26:20,033 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9295
  Recall:    0.9685
  F1 Score:  0.9486
  IoU:       0.9023
  mAP:       0.9779
  AUC:       0.9939
2025-05-01 10:26:20,034 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314/final_results.json
2025-05-01 10:26:20,036 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_314/final_results.json
2025-05-01 10:26:20,036 - main - INFO - 
Summary for configuration 314:
2025-05-01 10:26:20,036 - main - INFO - Accuracy: 0.9703
2025-05-01 10:26:20,036 - main - INFO - Precision: 0.9295
2025-05-01 10:26:20,036 - main - INFO - Recall: 0.9685
2025-05-01 10:26:20,036 - main - INFO - F1 Score: 0.9486
2025-05-01 10:26:20,036 - main - INFO - IoU: 0.9023
2025-05-01 10:26:20,036 - main - INFO - mAP: 0.9779
2025-05-01 10:26:20,036 - main - INFO - AUC: 0.9939
2025-05-01 10:26:20,036 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:26:20,036 - main - INFO - 
==================================================
2025-05-01 10:26:20,036 - main - INFO - Running configuration 315/756:
2025-05-01 10:26:20,036 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:26:20,036 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:26:20,036 - main - INFO - Scheduler: StepLR
2025-05-01 10:26:20,036 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:26:20,036 - main - INFO - ==================================================
2025-05-01 10:26:20,037 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315
2025-05-01 10:26:20,037 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315 - INFO - Config: {
  "id": 315,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:26:20,365 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:26:20,365 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 315,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:26:20,365 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:26:21,219 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:26:21,221 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:26:21,225 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315 - INFO - Starting model evaluation
2025-05-01 10:26:32,474 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315 - INFO - Evaluation metrics:
  Accuracy:  0.9722
  Precision: 0.9358
  Recall:    0.9685
  F1 Score:  0.9519
  IoU:       0.9082
  mAP:       0.9895
  AUC:       0.9953
2025-05-01 10:26:32,476 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315/final_results.json
2025-05-01 10:26:32,477 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_315/final_results.json
2025-05-01 10:26:32,477 - main - INFO - 
Summary for configuration 315:
2025-05-01 10:26:32,477 - main - INFO - Accuracy: 0.9722
2025-05-01 10:26:32,477 - main - INFO - Precision: 0.9358
2025-05-01 10:26:32,477 - main - INFO - Recall: 0.9685
2025-05-01 10:26:32,477 - main - INFO - F1 Score: 0.9519
2025-05-01 10:26:32,477 - main - INFO - IoU: 0.9082
2025-05-01 10:26:32,477 - main - INFO - mAP: 0.9895
2025-05-01 10:26:32,477 - main - INFO - AUC: 0.9953
2025-05-01 10:26:32,477 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:26:32,477 - main - INFO - 
==================================================
2025-05-01 10:26:32,477 - main - INFO - Running configuration 316/756:
2025-05-01 10:26:32,477 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:26:32,477 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:26:32,477 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:26:32,477 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:26:32,477 - main - INFO - ==================================================
2025-05-01 10:26:32,478 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316
2025-05-01 10:26:32,478 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316 - INFO - Config: {
  "id": 316,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:26:32,792 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:26:32,792 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 316,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:26:32,792 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:26:33,588 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:26:33,589 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:26:33,593 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316 - INFO - Starting model evaluation
2025-05-01 10:26:44,272 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9298
  Recall:    0.9720
  F1 Score:  0.9504
  IoU:       0.9055
  mAP:       0.9826
  AUC:       0.9943
2025-05-01 10:26:44,274 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316/final_results.json
2025-05-01 10:26:44,275 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_316/final_results.json
2025-05-01 10:26:44,275 - main - INFO - 
Summary for configuration 316:
2025-05-01 10:26:44,275 - main - INFO - Accuracy: 0.9713
2025-05-01 10:26:44,275 - main - INFO - Precision: 0.9298
2025-05-01 10:26:44,275 - main - INFO - Recall: 0.9720
2025-05-01 10:26:44,275 - main - INFO - F1 Score: 0.9504
2025-05-01 10:26:44,275 - main - INFO - IoU: 0.9055
2025-05-01 10:26:44,275 - main - INFO - mAP: 0.9826
2025-05-01 10:26:44,276 - main - INFO - AUC: 0.9943
2025-05-01 10:26:44,276 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:26:44,276 - main - INFO - 
==================================================
2025-05-01 10:26:44,276 - main - INFO - Running configuration 317/756:
2025-05-01 10:26:44,276 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:26:44,276 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:26:44,276 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:26:44,276 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:26:44,276 - main - INFO - ==================================================
2025-05-01 10:26:44,276 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317
2025-05-01 10:26:44,276 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317 - INFO - Config: {
  "id": 317,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:26:44,682 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:26:44,682 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 317,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:26:44,682 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:26:45,618 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:26:45,619 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:26:45,623 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317 - INFO - Starting model evaluation
2025-05-01 10:26:56,644 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9354
  Recall:    0.9615
  F1 Score:  0.9483
  IoU:       0.9016
  mAP:       0.9847
  AUC:       0.9939
2025-05-01 10:26:56,646 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317/final_results.json
2025-05-01 10:26:56,647 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_317/final_results.json
2025-05-01 10:26:56,647 - main - INFO - 
Summary for configuration 317:
2025-05-01 10:26:56,647 - main - INFO - Accuracy: 0.9703
2025-05-01 10:26:56,647 - main - INFO - Precision: 0.9354
2025-05-01 10:26:56,647 - main - INFO - Recall: 0.9615
2025-05-01 10:26:56,647 - main - INFO - F1 Score: 0.9483
2025-05-01 10:26:56,647 - main - INFO - IoU: 0.9016
2025-05-01 10:26:56,647 - main - INFO - mAP: 0.9847
2025-05-01 10:26:56,647 - main - INFO - AUC: 0.9939
2025-05-01 10:26:56,647 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:26:56,648 - main - INFO - 
==================================================
2025-05-01 10:26:56,648 - main - INFO - Running configuration 318/756:
2025-05-01 10:26:56,648 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:26:56,648 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:26:56,648 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:26:56,648 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:26:56,648 - main - INFO - ==================================================
2025-05-01 10:26:56,648 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318
2025-05-01 10:26:56,648 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318 - INFO - Config: {
  "id": 318,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:26:56,916 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:26:56,917 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 318,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:26:56,917 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:26:57,948 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:26:57,949 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:26:57,953 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318 - INFO - Starting model evaluation
2025-05-01 10:27:08,974 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9324
  Recall:    0.9650
  F1 Score:  0.9485
  IoU:       0.9020
  mAP:       0.9798
  AUC:       0.9944
2025-05-01 10:27:08,976 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318/final_results.json
2025-05-01 10:27:08,978 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_318/final_results.json
2025-05-01 10:27:08,978 - main - INFO - 
Summary for configuration 318:
2025-05-01 10:27:08,978 - main - INFO - Accuracy: 0.9703
2025-05-01 10:27:08,978 - main - INFO - Precision: 0.9324
2025-05-01 10:27:08,978 - main - INFO - Recall: 0.9650
2025-05-01 10:27:08,978 - main - INFO - F1 Score: 0.9485
2025-05-01 10:27:08,978 - main - INFO - IoU: 0.9020
2025-05-01 10:27:08,978 - main - INFO - mAP: 0.9798
2025-05-01 10:27:08,978 - main - INFO - AUC: 0.9944
2025-05-01 10:27:08,978 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:27:08,978 - main - INFO - 
==================================================
2025-05-01 10:27:08,978 - main - INFO - Running configuration 319/756:
2025-05-01 10:27:08,978 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:27:08,978 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:27:08,978 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:27:08,978 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:27:08,978 - main - INFO - ==================================================
2025-05-01 10:27:08,978 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319
2025-05-01 10:27:08,978 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319 - INFO - Config: {
  "id": 319,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:27:09,302 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:27:09,302 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 319,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:27:09,302 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:27:10,098 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:27:10,100 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:27:10,103 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319 - INFO - Starting model evaluation
2025-05-01 10:27:21,163 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9324
  Recall:    0.9650
  F1 Score:  0.9485
  IoU:       0.9020
  mAP:       0.9812
  AUC:       0.9938
2025-05-01 10:27:21,166 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319/final_results.json
2025-05-01 10:27:21,168 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_319/final_results.json
2025-05-01 10:27:21,168 - main - INFO - 
Summary for configuration 319:
2025-05-01 10:27:21,168 - main - INFO - Accuracy: 0.9703
2025-05-01 10:27:21,168 - main - INFO - Precision: 0.9324
2025-05-01 10:27:21,168 - main - INFO - Recall: 0.9650
2025-05-01 10:27:21,169 - main - INFO - F1 Score: 0.9485
2025-05-01 10:27:21,169 - main - INFO - IoU: 0.9020
2025-05-01 10:27:21,169 - main - INFO - mAP: 0.9812
2025-05-01 10:27:21,169 - main - INFO - AUC: 0.9938
2025-05-01 10:27:21,169 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:27:21,169 - main - INFO - 
==================================================
2025-05-01 10:27:21,169 - main - INFO - Running configuration 320/756:
2025-05-01 10:27:21,169 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:27:21,169 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:27:21,169 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:27:21,169 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:27:21,169 - main - INFO - ==================================================
2025-05-01 10:27:21,169 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320
2025-05-01 10:27:21,169 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320 - INFO - Config: {
  "id": 320,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:27:21,540 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:27:21,540 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 320,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:27:21,540 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:27:22,478 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:27:22,479 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:27:22,483 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320 - INFO - Starting model evaluation
2025-05-01 10:27:33,316 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9324
  Recall:    0.9650
  F1 Score:  0.9485
  IoU:       0.9020
  mAP:       0.9853
  AUC:       0.9948
2025-05-01 10:27:33,318 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320/final_results.json
2025-05-01 10:27:33,319 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_320/final_results.json
2025-05-01 10:27:33,319 - main - INFO - 
Summary for configuration 320:
2025-05-01 10:27:33,319 - main - INFO - Accuracy: 0.9703
2025-05-01 10:27:33,319 - main - INFO - Precision: 0.9324
2025-05-01 10:27:33,319 - main - INFO - Recall: 0.9650
2025-05-01 10:27:33,319 - main - INFO - F1 Score: 0.9485
2025-05-01 10:27:33,319 - main - INFO - IoU: 0.9020
2025-05-01 10:27:33,319 - main - INFO - mAP: 0.9853
2025-05-01 10:27:33,319 - main - INFO - AUC: 0.9948
2025-05-01 10:27:33,319 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:27:33,319 - main - INFO - 
==================================================
2025-05-01 10:27:33,319 - main - INFO - Running configuration 321/756:
2025-05-01 10:27:33,319 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:27:33,319 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:27:33,319 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:27:33,319 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:27:33,319 - main - INFO - ==================================================
2025-05-01 10:27:33,320 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321
2025-05-01 10:27:33,320 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321 - INFO - Config: {
  "id": 321,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:27:33,622 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:27:33,622 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 321,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:27:33,622 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:27:34,541 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:27:34,543 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:27:34,546 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321 - INFO - Starting model evaluation
2025-05-01 10:27:45,535 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9288
  Recall:    0.9580
  F1 Score:  0.9432
  IoU:       0.8925
  mAP:       0.9791
  AUC:       0.9943
2025-05-01 10:27:45,537 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321/final_results.json
2025-05-01 10:27:45,538 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_321/final_results.json
2025-05-01 10:27:45,539 - main - INFO - 
Summary for configuration 321:
2025-05-01 10:27:45,539 - main - INFO - Accuracy: 0.9673
2025-05-01 10:27:45,539 - main - INFO - Precision: 0.9288
2025-05-01 10:27:45,539 - main - INFO - Recall: 0.9580
2025-05-01 10:27:45,539 - main - INFO - F1 Score: 0.9432
2025-05-01 10:27:45,539 - main - INFO - IoU: 0.8925
2025-05-01 10:27:45,539 - main - INFO - mAP: 0.9791
2025-05-01 10:27:45,539 - main - INFO - AUC: 0.9943
2025-05-01 10:27:45,539 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:27:45,539 - main - INFO - 
==================================================
2025-05-01 10:27:45,539 - main - INFO - Running configuration 322/756:
2025-05-01 10:27:45,539 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:27:45,539 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:27:45,539 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:27:45,539 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:27:45,539 - main - INFO - ==================================================
2025-05-01 10:27:45,539 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322
2025-05-01 10:27:45,539 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322 - INFO - Config: {
  "id": 322,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:27:45,831 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:27:45,831 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 322,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:27:45,831 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:27:46,679 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:27:46,680 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:27:46,684 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322 - INFO - Starting model evaluation
2025-05-01 10:27:57,745 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9293
  Recall:    0.9650
  F1 Score:  0.9468
  IoU:       0.8990
  mAP:       0.9845
  AUC:       0.9943
2025-05-01 10:27:57,747 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322/final_results.json
2025-05-01 10:27:57,749 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_322/final_results.json
2025-05-01 10:27:57,749 - main - INFO - 
Summary for configuration 322:
2025-05-01 10:27:57,749 - main - INFO - Accuracy: 0.9693
2025-05-01 10:27:57,749 - main - INFO - Precision: 0.9293
2025-05-01 10:27:57,749 - main - INFO - Recall: 0.9650
2025-05-01 10:27:57,749 - main - INFO - F1 Score: 0.9468
2025-05-01 10:27:57,749 - main - INFO - IoU: 0.8990
2025-05-01 10:27:57,749 - main - INFO - mAP: 0.9845
2025-05-01 10:27:57,749 - main - INFO - AUC: 0.9943
2025-05-01 10:27:57,749 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:27:57,749 - main - INFO - 
==================================================
2025-05-01 10:27:57,749 - main - INFO - Running configuration 323/756:
2025-05-01 10:27:57,749 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:27:57,749 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:27:57,749 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:27:57,749 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:27:57,749 - main - INFO - ==================================================
2025-05-01 10:27:57,749 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323
2025-05-01 10:27:57,749 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323 - INFO - Config: {
  "id": 323,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:27:58,121 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:27:58,121 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 323,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:27:58,121 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:27:59,046 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:27:59,048 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:27:59,051 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323 - INFO - Starting model evaluation
2025-05-01 10:28:10,048 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9322
  Recall:    0.9615
  F1 Score:  0.9466
  IoU:       0.8987
  mAP:       0.9823
  AUC:       0.9946
2025-05-01 10:28:10,050 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323/final_results.json
2025-05-01 10:28:10,052 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_323/final_results.json
2025-05-01 10:28:10,052 - main - INFO - 
Summary for configuration 323:
2025-05-01 10:28:10,052 - main - INFO - Accuracy: 0.9693
2025-05-01 10:28:10,052 - main - INFO - Precision: 0.9322
2025-05-01 10:28:10,052 - main - INFO - Recall: 0.9615
2025-05-01 10:28:10,052 - main - INFO - F1 Score: 0.9466
2025-05-01 10:28:10,052 - main - INFO - IoU: 0.8987
2025-05-01 10:28:10,052 - main - INFO - mAP: 0.9823
2025-05-01 10:28:10,052 - main - INFO - AUC: 0.9946
2025-05-01 10:28:10,052 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:28:10,052 - main - INFO - 
==================================================
2025-05-01 10:28:10,052 - main - INFO - Running configuration 324/756:
2025-05-01 10:28:10,052 - main - INFO - Model: EfficientNet-B2
2025-05-01 10:28:10,052 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 10:28:10,052 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:28:10,052 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:28:10,052 - main - INFO - ==================================================
2025-05-01 10:28:10,052 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324
2025-05-01 10:28:10,052 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324 - INFO - Config: {
  "id": 324,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:28:10,463 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_EfficientNet-B2_opt_AdamW_lr_0.0001
2025-05-01 10:28:10,463 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 324,
  "model_name": "EfficientNet-B2",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:28:10,463 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 10:28:11,227 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-01 10:28:11,228 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001 - INFO - Training completed after 1146.35 seconds
2025-05-01 10:28:11,232 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324 - INFO - Starting model evaluation
2025-05-01 10:28:22,337 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9295
  Recall:    0.9685
  F1 Score:  0.9486
  IoU:       0.9023
  mAP:       0.9866
  AUC:       0.9940
2025-05-01 10:28:22,339 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324/final_results.json
2025-05-01 10:28:22,341 - training.model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324 - INFO - Final results saved to model_results_v2/model_EfficientNet-B2_opt_AdamW_lr_0.0001_id_324/final_results.json
2025-05-01 10:28:22,341 - main - INFO - 
Summary for configuration 324:
2025-05-01 10:28:22,341 - main - INFO - Accuracy: 0.9703
2025-05-01 10:28:22,341 - main - INFO - Precision: 0.9295
2025-05-01 10:28:22,341 - main - INFO - Recall: 0.9685
2025-05-01 10:28:22,341 - main - INFO - F1 Score: 0.9486
2025-05-01 10:28:22,341 - main - INFO - IoU: 0.9023
2025-05-01 10:28:22,341 - main - INFO - mAP: 0.9866
2025-05-01 10:28:22,341 - main - INFO - AUC: 0.9940
2025-05-01 10:28:22,341 - main - INFO - Training time: 1146.35 seconds
2025-05-01 10:28:22,341 - main - INFO - 
==================================================
2025-05-01 10:28:22,341 - main - INFO - Running configuration 325/756:
2025-05-01 10:28:22,341 - main - INFO - Model: ViT-B-16
2025-05-01 10:28:22,341 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:28:22,341 - main - INFO - Scheduler: StepLR
2025-05-01 10:28:22,341 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:28:22,341 - main - INFO - ==================================================
2025-05-01 10:28:22,341 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_325 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_325
2025-05-01 10:28:22,341 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_325 - INFO - Config: {
  "id": 325,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:28:23,033 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:28:23,033 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 325,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:28:23,033 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 10:28:23,033 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 10:28:59,049 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 10:29:46,637 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.7093
2025-05-01 10:29:46,797 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 1 completed in 83.76s - Train Loss: 0.5999, Train Acc: 0.6961, Val Loss: 0.5718, Val Acc: 0.7093
2025-05-01 10:29:47,285 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 10:29:47,285 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 10:30:23,672 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 10:31:09,399 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.7093 to 0.7890
2025-05-01 10:31:09,662 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 2 completed in 82.38s - Train Loss: 0.5519, Train Acc: 0.7467, Val Loss: 0.5099, Val Acc: 0.7890
2025-05-01 10:31:10,150 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 10:31:10,151 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 10:31:46,010 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 10:32:31,157 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.7890 to 0.8139
2025-05-01 10:32:31,409 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 3 completed in 81.26s - Train Loss: 0.5049, Train Acc: 0.7967, Val Loss: 0.4869, Val Acc: 0.8139
2025-05-01 10:32:31,896 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 10:32:31,897 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 10:33:10,025 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 10:33:57,899 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation loss improved from inf to 0.5282
2025-05-01 10:33:58,154 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 4 completed in 86.26s - Train Loss: 0.4809, Train Acc: 0.8245, Val Loss: 0.5282, Val Acc: 0.7702
2025-05-01 10:33:58,658 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 10:33:58,658 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 10:34:38,272 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 10:35:24,515 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.5282 to 0.4951
2025-05-01 10:35:24,769 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 5 completed in 86.11s - Train Loss: 0.4750, Train Acc: 0.8284, Val Loss: 0.4951, Val Acc: 0.8087
2025-05-01 10:35:25,285 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 10:35:25,285 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 10:36:04,248 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 10:36:50,923 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.8139 to 0.8448
2025-05-01 10:36:51,176 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 6 completed in 85.89s - Train Loss: 0.4696, Train Acc: 0.8363, Val Loss: 0.4621, Val Acc: 0.8448
2025-05-01 10:36:51,672 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 10:36:51,673 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 10:37:30,804 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 10:38:18,832 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.8448 to 0.8559
2025-05-01 10:38:19,090 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 7 completed in 87.42s - Train Loss: 0.4407, Train Acc: 0.8672, Val Loss: 0.4560, Val Acc: 0.8559
2025-05-01 10:38:19,590 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 10:38:19,590 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 10:38:55,272 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 10:39:43,829 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.8559 to 0.8619
2025-05-01 10:39:44,096 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 8 completed in 84.51s - Train Loss: 0.4377, Train Acc: 0.8700, Val Loss: 0.4482, Val Acc: 0.8619
2025-05-01 10:39:44,627 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 10:39:44,627 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 10:40:23,563 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 10:41:15,029 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.4951 to 0.4660
2025-05-01 10:41:15,267 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 9 completed in 90.64s - Train Loss: 0.4387, Train Acc: 0.8689, Val Loss: 0.4660, Val Acc: 0.8388
2025-05-01 10:41:15,771 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 10:41:15,771 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 10:41:53,678 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 10:42:40,657 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.8619 to 0.8756
2025-05-01 10:42:40,907 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 10 completed in 85.14s - Train Loss: 0.4243, Train Acc: 0.8867, Val Loss: 0.4302, Val Acc: 0.8756
2025-05-01 10:42:41,391 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 10:42:41,392 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 10:43:19,035 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 10:44:07,044 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.8756 to 0.8997
2025-05-01 10:44:07,300 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 11 completed in 85.91s - Train Loss: 0.3967, Train Acc: 0.9136, Val Loss: 0.4097, Val Acc: 0.8997
2025-05-01 10:44:07,797 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 10:44:07,797 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 10:44:45,146 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 10:45:31,866 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.8997 to 0.9125
2025-05-01 10:45:32,117 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 12 completed in 84.32s - Train Loss: 0.3777, Train Acc: 0.9348, Val Loss: 0.4039, Val Acc: 0.9125
2025-05-01 10:45:32,619 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 10:45:32,619 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 10:46:09,350 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 10:46:56,113 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.4660 to 0.4009
2025-05-01 10:46:56,386 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 13 completed in 83.77s - Train Loss: 0.3653, Train Acc: 0.9472, Val Loss: 0.4009, Val Acc: 0.9091
2025-05-01 10:46:56,878 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 10:46:56,878 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 10:47:34,386 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 10:48:21,001 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9125 to 0.9160
2025-05-01 10:48:21,258 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 14 completed in 84.38s - Train Loss: 0.3619, Train Acc: 0.9515, Val Loss: 0.3950, Val Acc: 0.9160
2025-05-01 10:48:21,773 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 10:48:21,773 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 10:48:58,748 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 10:49:44,905 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9160 to 0.9220
2025-05-01 10:49:45,169 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 15 completed in 83.40s - Train Loss: 0.3581, Train Acc: 0.9562, Val Loss: 0.3927, Val Acc: 0.9220
2025-05-01 10:49:45,679 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 10:49:45,679 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 10:50:22,185 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 10:51:08,548 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.4009 to 0.3895
2025-05-01 10:51:08,813 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 16 completed in 83.13s - Train Loss: 0.3563, Train Acc: 0.9577, Val Loss: 0.3895, Val Acc: 0.9202
2025-05-01 10:51:09,313 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 10:51:09,313 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 10:51:46,161 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 10:52:32,280 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9220 to 0.9245
2025-05-01 10:52:32,534 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 17 completed in 83.22s - Train Loss: 0.3539, Train Acc: 0.9597, Val Loss: 0.3869, Val Acc: 0.9245
2025-05-01 10:52:33,039 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 10:52:33,039 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 10:53:09,459 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 10:53:56,033 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9245 to 0.9314
2025-05-01 10:53:56,301 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 18 completed in 83.26s - Train Loss: 0.3553, Train Acc: 0.9586, Val Loss: 0.3807, Val Acc: 0.9314
2025-05-01 10:53:56,819 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 10:53:56,820 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 10:54:34,223 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 10:55:19,048 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3895 to 0.3867
2025-05-01 10:55:19,304 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 19 completed in 82.48s - Train Loss: 0.3528, Train Acc: 0.9610, Val Loss: 0.3867, Val Acc: 0.9271
2025-05-01 10:55:19,802 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 10:55:19,802 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 10:55:57,165 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 10:56:43,011 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3867 to 0.3845
2025-05-01 10:56:43,263 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Epoch 20 completed in 83.46s - Train Loss: 0.3519, Train Acc: 0.9618, Val Loss: 0.3845, Val Acc: 0.9280
2025-05-01 10:56:43,808 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 10:56:43,808 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:56:43,810 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_325 - INFO - Starting model evaluation
2025-05-01 10:56:55,891 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_325 - INFO - Evaluation metrics:
  Accuracy:  0.8880
  Precision: 0.8289
  Recall:    0.7622
  F1 Score:  0.7942
  IoU:       0.6586
  mAP:       0.8723
  AUC:       0.9300
2025-05-01 10:56:55,893 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_325 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_325/final_results.json
2025-05-01 10:56:55,895 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_325 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_325/final_results.json
2025-05-01 10:56:55,895 - main - INFO - 
Summary for configuration 325:
2025-05-01 10:56:55,895 - main - INFO - Accuracy: 0.8880
2025-05-01 10:56:55,895 - main - INFO - Precision: 0.8289
2025-05-01 10:56:55,895 - main - INFO - Recall: 0.7622
2025-05-01 10:56:55,895 - main - INFO - F1 Score: 0.7942
2025-05-01 10:56:55,895 - main - INFO - IoU: 0.6586
2025-05-01 10:56:55,895 - main - INFO - mAP: 0.8723
2025-05-01 10:56:55,895 - main - INFO - AUC: 0.9300
2025-05-01 10:56:55,895 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:56:55,895 - main - INFO - 
==================================================
2025-05-01 10:56:55,895 - main - INFO - Running configuration 326/756:
2025-05-01 10:56:55,895 - main - INFO - Model: ViT-B-16
2025-05-01 10:56:55,895 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:56:55,895 - main - INFO - Scheduler: StepLR
2025-05-01 10:56:55,895 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:56:55,895 - main - INFO - ==================================================
2025-05-01 10:56:55,895 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_326 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_326
2025-05-01 10:56:55,895 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_326 - INFO - Config: {
  "id": 326,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:56:56,519 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:56:56,519 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 326,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:56:56,519 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:56:56,942 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:56:56,943 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:56:56,944 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_326 - INFO - Starting model evaluation
2025-05-01 10:57:09,217 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_326 - INFO - Evaluation metrics:
  Accuracy:  0.8930
  Precision: 0.8371
  Recall:    0.7727
  F1 Score:  0.8036
  IoU:       0.6717
  mAP:       0.8738
  AUC:       0.9326
2025-05-01 10:57:09,218 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_326 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_326/final_results.json
2025-05-01 10:57:09,220 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_326 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_326/final_results.json
2025-05-01 10:57:09,220 - main - INFO - 
Summary for configuration 326:
2025-05-01 10:57:09,220 - main - INFO - Accuracy: 0.8930
2025-05-01 10:57:09,220 - main - INFO - Precision: 0.8371
2025-05-01 10:57:09,220 - main - INFO - Recall: 0.7727
2025-05-01 10:57:09,220 - main - INFO - F1 Score: 0.8036
2025-05-01 10:57:09,220 - main - INFO - IoU: 0.6717
2025-05-01 10:57:09,220 - main - INFO - mAP: 0.8738
2025-05-01 10:57:09,220 - main - INFO - AUC: 0.9326
2025-05-01 10:57:09,220 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:57:09,220 - main - INFO - 
==================================================
2025-05-01 10:57:09,220 - main - INFO - Running configuration 327/756:
2025-05-01 10:57:09,220 - main - INFO - Model: ViT-B-16
2025-05-01 10:57:09,220 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:57:09,220 - main - INFO - Scheduler: StepLR
2025-05-01 10:57:09,220 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:57:09,220 - main - INFO - ==================================================
2025-05-01 10:57:09,220 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_327 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_327
2025-05-01 10:57:09,220 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_327 - INFO - Config: {
  "id": 327,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:57:09,872 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:57:09,872 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 327,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:57:09,872 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:57:10,306 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:57:10,307 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:57:10,308 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_327 - INFO - Starting model evaluation
2025-05-01 10:57:22,519 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_327 - INFO - Evaluation metrics:
  Accuracy:  0.8850
  Precision: 0.8269
  Recall:    0.7517
  F1 Score:  0.7875
  IoU:       0.6495
  mAP:       0.8610
  AUC:       0.9256
2025-05-01 10:57:22,520 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_327 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_327/final_results.json
2025-05-01 10:57:22,522 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_327 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_327/final_results.json
2025-05-01 10:57:22,522 - main - INFO - 
Summary for configuration 327:
2025-05-01 10:57:22,522 - main - INFO - Accuracy: 0.8850
2025-05-01 10:57:22,522 - main - INFO - Precision: 0.8269
2025-05-01 10:57:22,522 - main - INFO - Recall: 0.7517
2025-05-01 10:57:22,522 - main - INFO - F1 Score: 0.7875
2025-05-01 10:57:22,522 - main - INFO - IoU: 0.6495
2025-05-01 10:57:22,522 - main - INFO - mAP: 0.8610
2025-05-01 10:57:22,522 - main - INFO - AUC: 0.9256
2025-05-01 10:57:22,522 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:57:22,522 - main - INFO - 
==================================================
2025-05-01 10:57:22,522 - main - INFO - Running configuration 328/756:
2025-05-01 10:57:22,522 - main - INFO - Model: ViT-B-16
2025-05-01 10:57:22,522 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:57:22,522 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:57:22,522 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:57:22,522 - main - INFO - ==================================================
2025-05-01 10:57:22,522 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_328 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_328
2025-05-01 10:57:22,522 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_328 - INFO - Config: {
  "id": 328,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:57:23,212 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:57:23,213 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 328,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:57:23,213 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:57:23,634 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:57:23,635 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:57:23,636 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_328 - INFO - Starting model evaluation
2025-05-01 10:57:36,045 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_328 - INFO - Evaluation metrics:
  Accuracy:  0.8910
  Precision: 0.8359
  Recall:    0.7657
  F1 Score:  0.7993
  IoU:       0.6657
  mAP:       0.8708
  AUC:       0.9338
2025-05-01 10:57:36,048 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_328 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_328/final_results.json
2025-05-01 10:57:36,050 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_328 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_328/final_results.json
2025-05-01 10:57:36,050 - main - INFO - 
Summary for configuration 328:
2025-05-01 10:57:36,050 - main - INFO - Accuracy: 0.8910
2025-05-01 10:57:36,050 - main - INFO - Precision: 0.8359
2025-05-01 10:57:36,050 - main - INFO - Recall: 0.7657
2025-05-01 10:57:36,050 - main - INFO - F1 Score: 0.7993
2025-05-01 10:57:36,050 - main - INFO - IoU: 0.6657
2025-05-01 10:57:36,050 - main - INFO - mAP: 0.8708
2025-05-01 10:57:36,050 - main - INFO - AUC: 0.9338
2025-05-01 10:57:36,051 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:57:36,051 - main - INFO - 
==================================================
2025-05-01 10:57:36,051 - main - INFO - Running configuration 329/756:
2025-05-01 10:57:36,051 - main - INFO - Model: ViT-B-16
2025-05-01 10:57:36,051 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:57:36,051 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:57:36,051 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:57:36,051 - main - INFO - ==================================================
2025-05-01 10:57:36,051 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_329 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_329
2025-05-01 10:57:36,051 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_329 - INFO - Config: {
  "id": 329,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:57:36,708 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:57:36,708 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 329,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:57:36,709 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:57:37,131 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:57:37,132 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:57:37,133 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_329 - INFO - Starting model evaluation
2025-05-01 10:57:49,411 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_329 - INFO - Evaluation metrics:
  Accuracy:  0.8890
  Precision: 0.8372
  Recall:    0.7552
  F1 Score:  0.7941
  IoU:       0.6585
  mAP:       0.8670
  AUC:       0.9281
2025-05-01 10:57:49,412 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_329 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_329/final_results.json
2025-05-01 10:57:49,414 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_329 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_329/final_results.json
2025-05-01 10:57:49,414 - main - INFO - 
Summary for configuration 329:
2025-05-01 10:57:49,414 - main - INFO - Accuracy: 0.8890
2025-05-01 10:57:49,414 - main - INFO - Precision: 0.8372
2025-05-01 10:57:49,414 - main - INFO - Recall: 0.7552
2025-05-01 10:57:49,414 - main - INFO - F1 Score: 0.7941
2025-05-01 10:57:49,414 - main - INFO - IoU: 0.6585
2025-05-01 10:57:49,414 - main - INFO - mAP: 0.8670
2025-05-01 10:57:49,414 - main - INFO - AUC: 0.9281
2025-05-01 10:57:49,414 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:57:49,414 - main - INFO - 
==================================================
2025-05-01 10:57:49,414 - main - INFO - Running configuration 330/756:
2025-05-01 10:57:49,414 - main - INFO - Model: ViT-B-16
2025-05-01 10:57:49,414 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:57:49,414 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 10:57:49,414 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:57:49,414 - main - INFO - ==================================================
2025-05-01 10:57:49,414 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_330 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_330
2025-05-01 10:57:49,414 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_330 - INFO - Config: {
  "id": 330,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:57:50,090 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:57:50,090 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 330,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:57:50,090 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:57:50,514 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:57:50,514 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:57:50,518 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_330 - INFO - Starting model evaluation
2025-05-01 10:58:02,637 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_330 - INFO - Evaluation metrics:
  Accuracy:  0.8910
  Precision: 0.8235
  Recall:    0.7832
  F1 Score:  0.8029
  IoU:       0.6707
  mAP:       0.8788
  AUC:       0.9360
2025-05-01 10:58:02,639 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_330 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_330/final_results.json
2025-05-01 10:58:02,640 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_330 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_330/final_results.json
2025-05-01 10:58:02,641 - main - INFO - 
Summary for configuration 330:
2025-05-01 10:58:02,641 - main - INFO - Accuracy: 0.8910
2025-05-01 10:58:02,641 - main - INFO - Precision: 0.8235
2025-05-01 10:58:02,641 - main - INFO - Recall: 0.7832
2025-05-01 10:58:02,641 - main - INFO - F1 Score: 0.8029
2025-05-01 10:58:02,641 - main - INFO - IoU: 0.6707
2025-05-01 10:58:02,641 - main - INFO - mAP: 0.8788
2025-05-01 10:58:02,641 - main - INFO - AUC: 0.9360
2025-05-01 10:58:02,641 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:58:02,641 - main - INFO - 
==================================================
2025-05-01 10:58:02,641 - main - INFO - Running configuration 331/756:
2025-05-01 10:58:02,641 - main - INFO - Model: ViT-B-16
2025-05-01 10:58:02,641 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:58:02,641 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:58:02,641 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:58:02,641 - main - INFO - ==================================================
2025-05-01 10:58:02,641 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_331 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_331
2025-05-01 10:58:02,641 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_331 - INFO - Config: {
  "id": 331,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:58:03,294 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:58:03,294 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 331,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:58:03,294 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:58:03,694 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:58:03,695 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:58:03,696 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_331 - INFO - Starting model evaluation
2025-05-01 10:58:16,092 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_331 - INFO - Evaluation metrics:
  Accuracy:  0.8821
  Precision: 0.8224
  Recall:    0.7448
  F1 Score:  0.7817
  IoU:       0.6416
  mAP:       0.8709
  AUC:       0.9266
2025-05-01 10:58:16,095 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_331 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_331/final_results.json
2025-05-01 10:58:16,098 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_331 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_331/final_results.json
2025-05-01 10:58:16,098 - main - INFO - 
Summary for configuration 331:
2025-05-01 10:58:16,098 - main - INFO - Accuracy: 0.8821
2025-05-01 10:58:16,098 - main - INFO - Precision: 0.8224
2025-05-01 10:58:16,098 - main - INFO - Recall: 0.7448
2025-05-01 10:58:16,098 - main - INFO - F1 Score: 0.7817
2025-05-01 10:58:16,098 - main - INFO - IoU: 0.6416
2025-05-01 10:58:16,098 - main - INFO - mAP: 0.8709
2025-05-01 10:58:16,098 - main - INFO - AUC: 0.9266
2025-05-01 10:58:16,098 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:58:16,098 - main - INFO - 
==================================================
2025-05-01 10:58:16,098 - main - INFO - Running configuration 332/756:
2025-05-01 10:58:16,098 - main - INFO - Model: ViT-B-16
2025-05-01 10:58:16,098 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:58:16,098 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:58:16,098 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:58:16,098 - main - INFO - ==================================================
2025-05-01 10:58:16,098 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_332 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_332
2025-05-01 10:58:16,098 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_332 - INFO - Config: {
  "id": 332,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:58:16,740 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:58:16,740 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 332,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:58:16,740 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:58:17,146 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:58:17,147 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:58:17,148 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_332 - INFO - Starting model evaluation
2025-05-01 10:58:29,122 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_332 - INFO - Evaluation metrics:
  Accuracy:  0.8900
  Precision: 0.8405
  Recall:    0.7552
  F1 Score:  0.7956
  IoU:       0.6606
  mAP:       0.8575
  AUC:       0.9241
2025-05-01 10:58:29,123 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_332 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_332/final_results.json
2025-05-01 10:58:29,125 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_332 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_332/final_results.json
2025-05-01 10:58:29,125 - main - INFO - 
Summary for configuration 332:
2025-05-01 10:58:29,125 - main - INFO - Accuracy: 0.8900
2025-05-01 10:58:29,125 - main - INFO - Precision: 0.8405
2025-05-01 10:58:29,125 - main - INFO - Recall: 0.7552
2025-05-01 10:58:29,125 - main - INFO - F1 Score: 0.7956
2025-05-01 10:58:29,125 - main - INFO - IoU: 0.6606
2025-05-01 10:58:29,125 - main - INFO - mAP: 0.8575
2025-05-01 10:58:29,125 - main - INFO - AUC: 0.9241
2025-05-01 10:58:29,125 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:58:29,125 - main - INFO - 
==================================================
2025-05-01 10:58:29,125 - main - INFO - Running configuration 333/756:
2025-05-01 10:58:29,125 - main - INFO - Model: ViT-B-16
2025-05-01 10:58:29,125 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:58:29,125 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 10:58:29,125 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:58:29,125 - main - INFO - ==================================================
2025-05-01 10:58:29,125 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_333 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_333
2025-05-01 10:58:29,125 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_333 - INFO - Config: {
  "id": 333,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:58:29,748 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:58:29,748 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 333,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:58:29,749 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:58:30,166 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:58:30,167 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:58:30,168 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_333 - INFO - Starting model evaluation
2025-05-01 10:58:42,291 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_333 - INFO - Evaluation metrics:
  Accuracy:  0.8969
  Precision: 0.8527
  Recall:    0.7692
  F1 Score:  0.8088
  IoU:       0.6790
  mAP:       0.8769
  AUC:       0.9300
2025-05-01 10:58:42,293 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_333 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_333/final_results.json
2025-05-01 10:58:42,295 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_333 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_333/final_results.json
2025-05-01 10:58:42,295 - main - INFO - 
Summary for configuration 333:
2025-05-01 10:58:42,295 - main - INFO - Accuracy: 0.8969
2025-05-01 10:58:42,295 - main - INFO - Precision: 0.8527
2025-05-01 10:58:42,295 - main - INFO - Recall: 0.7692
2025-05-01 10:58:42,295 - main - INFO - F1 Score: 0.8088
2025-05-01 10:58:42,295 - main - INFO - IoU: 0.6790
2025-05-01 10:58:42,295 - main - INFO - mAP: 0.8769
2025-05-01 10:58:42,295 - main - INFO - AUC: 0.9300
2025-05-01 10:58:42,295 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:58:42,295 - main - INFO - 
==================================================
2025-05-01 10:58:42,295 - main - INFO - Running configuration 334/756:
2025-05-01 10:58:42,295 - main - INFO - Model: ViT-B-16
2025-05-01 10:58:42,295 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:58:42,295 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:58:42,295 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:58:42,295 - main - INFO - ==================================================
2025-05-01 10:58:42,295 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_334 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_334
2025-05-01 10:58:42,295 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_334 - INFO - Config: {
  "id": 334,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:58:42,961 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:58:42,964 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 334,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:58:42,964 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:58:43,383 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:58:43,383 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:58:43,384 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_334 - INFO - Starting model evaluation
2025-05-01 10:58:55,726 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_334 - INFO - Evaluation metrics:
  Accuracy:  0.8870
  Precision: 0.8209
  Recall:    0.7692
  F1 Score:  0.7942
  IoU:       0.6587
  mAP:       0.8719
  AUC:       0.9294
2025-05-01 10:58:55,728 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_334 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_334/final_results.json
2025-05-01 10:58:55,729 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_334 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_334/final_results.json
2025-05-01 10:58:55,729 - main - INFO - 
Summary for configuration 334:
2025-05-01 10:58:55,729 - main - INFO - Accuracy: 0.8870
2025-05-01 10:58:55,729 - main - INFO - Precision: 0.8209
2025-05-01 10:58:55,729 - main - INFO - Recall: 0.7692
2025-05-01 10:58:55,729 - main - INFO - F1 Score: 0.7942
2025-05-01 10:58:55,729 - main - INFO - IoU: 0.6587
2025-05-01 10:58:55,729 - main - INFO - mAP: 0.8719
2025-05-01 10:58:55,729 - main - INFO - AUC: 0.9294
2025-05-01 10:58:55,729 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:58:55,729 - main - INFO - 
==================================================
2025-05-01 10:58:55,729 - main - INFO - Running configuration 335/756:
2025-05-01 10:58:55,729 - main - INFO - Model: ViT-B-16
2025-05-01 10:58:55,729 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:58:55,729 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:58:55,729 - main - INFO - Loss Function: FocalLoss
2025-05-01 10:58:55,729 - main - INFO - ==================================================
2025-05-01 10:58:55,729 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_335 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_335
2025-05-01 10:58:55,730 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_335 - INFO - Config: {
  "id": 335,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:58:56,387 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:58:56,387 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 335,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 10:58:56,387 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:58:56,802 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:58:56,803 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:58:56,804 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_335 - INFO - Starting model evaluation
2025-05-01 10:59:08,996 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_335 - INFO - Evaluation metrics:
  Accuracy:  0.8880
  Precision: 0.8314
  Recall:    0.7587
  F1 Score:  0.7934
  IoU:       0.6576
  mAP:       0.8705
  AUC:       0.9312
2025-05-01 10:59:08,997 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_335 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_335/final_results.json
2025-05-01 10:59:08,999 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_335 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_335/final_results.json
2025-05-01 10:59:08,999 - main - INFO - 
Summary for configuration 335:
2025-05-01 10:59:08,999 - main - INFO - Accuracy: 0.8880
2025-05-01 10:59:08,999 - main - INFO - Precision: 0.8314
2025-05-01 10:59:08,999 - main - INFO - Recall: 0.7587
2025-05-01 10:59:08,999 - main - INFO - F1 Score: 0.7934
2025-05-01 10:59:08,999 - main - INFO - IoU: 0.6576
2025-05-01 10:59:08,999 - main - INFO - mAP: 0.8705
2025-05-01 10:59:08,999 - main - INFO - AUC: 0.9312
2025-05-01 10:59:08,999 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:59:08,999 - main - INFO - 
==================================================
2025-05-01 10:59:08,999 - main - INFO - Running configuration 336/756:
2025-05-01 10:59:08,999 - main - INFO - Model: ViT-B-16
2025-05-01 10:59:08,999 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 10:59:08,999 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 10:59:08,999 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 10:59:08,999 - main - INFO - ==================================================
2025-05-01 10:59:08,999 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_336 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01_id_336
2025-05-01 10:59:08,999 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_336 - INFO - Config: {
  "id": 336,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:59:09,613 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.01
2025-05-01 10:59:09,613 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 336,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 10:59:09,613 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 10:59:10,031 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9314
2025-05-01 10:59:10,032 - training.model_ViT-B-16_opt_SGD_lr_0.01 - INFO - Training completed after 1700.23 seconds
2025-05-01 10:59:10,033 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_336 - INFO - Starting model evaluation
2025-05-01 10:59:22,428 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_336 - INFO - Evaluation metrics:
  Accuracy:  0.8949
  Precision: 0.8462
  Recall:    0.7692
  F1 Score:  0.8059
  IoU:       0.6748
  mAP:       0.8705
  AUC:       0.9274
2025-05-01 10:59:22,430 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_336 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_336/final_results.json
2025-05-01 10:59:22,432 - training.model_ViT-B-16_opt_SGD_lr_0.01_id_336 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.01_id_336/final_results.json
2025-05-01 10:59:22,432 - main - INFO - 
Summary for configuration 336:
2025-05-01 10:59:22,432 - main - INFO - Accuracy: 0.8949
2025-05-01 10:59:22,432 - main - INFO - Precision: 0.8462
2025-05-01 10:59:22,432 - main - INFO - Recall: 0.7692
2025-05-01 10:59:22,432 - main - INFO - F1 Score: 0.8059
2025-05-01 10:59:22,432 - main - INFO - IoU: 0.6748
2025-05-01 10:59:22,432 - main - INFO - mAP: 0.8705
2025-05-01 10:59:22,432 - main - INFO - AUC: 0.9274
2025-05-01 10:59:22,432 - main - INFO - Training time: 1700.23 seconds
2025-05-01 10:59:22,432 - main - INFO - 
==================================================
2025-05-01 10:59:22,432 - main - INFO - Running configuration 337/756:
2025-05-01 10:59:22,432 - main - INFO - Model: ViT-B-16
2025-05-01 10:59:22,432 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 10:59:22,432 - main - INFO - Scheduler: StepLR
2025-05-01 10:59:22,432 - main - INFO - Loss Function: CrossEntropy
2025-05-01 10:59:22,432 - main - INFO - ==================================================
2025-05-01 10:59:22,432 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_337 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_337
2025-05-01 10:59:22,432 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_337 - INFO - Config: {
  "id": 337,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:59:23,083 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 10:59:23,083 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 337,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 10:59:23,083 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 10:59:23,084 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 10:59:59,802 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:00:46,072 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.9228
2025-05-01 11:00:46,253 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 1 completed in 83.17s - Train Loss: 0.4759, Train Acc: 0.8426, Val Loss: 0.3957, Val Acc: 0.9228
2025-05-01 11:00:46,741 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:00:46,741 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 11:01:23,167 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:02:09,030 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation loss improved from inf to 0.4543
2025-05-01 11:02:09,287 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 2 completed in 82.55s - Train Loss: 0.3862, Train Acc: 0.9348, Val Loss: 0.4543, Val Acc: 0.8516
2025-05-01 11:02:09,765 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:02:09,766 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 11:02:47,228 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:03:33,743 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9228 to 0.9554
2025-05-01 11:03:34,001 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 3 completed in 84.23s - Train Loss: 0.3631, Train Acc: 0.9541, Val Loss: 0.3557, Val Acc: 0.9554
2025-05-01 11:03:34,496 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:03:34,496 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 11:04:10,634 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:04:57,181 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.4543 to 0.3595
2025-05-01 11:04:57,437 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 4 completed in 82.94s - Train Loss: 0.3433, Train Acc: 0.9723, Val Loss: 0.3595, Val Acc: 0.9485
2025-05-01 11:04:57,918 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:04:57,918 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 11:05:34,478 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:06:21,214 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9554 to 0.9708
2025-05-01 11:06:21,472 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 5 completed in 83.55s - Train Loss: 0.3427, Train Acc: 0.9710, Val Loss: 0.3412, Val Acc: 0.9708
2025-05-01 11:06:21,964 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:06:21,965 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 11:06:59,024 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:07:44,076 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3595 to 0.3594
2025-05-01 11:07:44,326 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 6 completed in 82.36s - Train Loss: 0.3371, Train Acc: 0.9764, Val Loss: 0.3594, Val Acc: 0.9520
2025-05-01 11:07:44,816 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:07:44,816 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 11:08:22,147 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:09:07,424 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9708 to 0.9768
2025-05-01 11:09:07,679 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 7 completed in 82.86s - Train Loss: 0.3289, Train Acc: 0.9861, Val Loss: 0.3376, Val Acc: 0.9768
2025-05-01 11:09:08,184 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:09:08,184 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 11:09:43,848 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:10:29,668 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3594 to 0.3561
2025-05-01 11:10:29,939 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 8 completed in 81.76s - Train Loss: 0.3224, Train Acc: 0.9914, Val Loss: 0.3561, Val Acc: 0.9528
2025-05-01 11:10:30,423 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:10:30,423 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 11:11:07,257 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:11:53,458 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3561 to 0.3366
2025-05-01 11:11:53,719 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 9 completed in 83.30s - Train Loss: 0.3273, Train Acc: 0.9863, Val Loss: 0.3366, Val Acc: 0.9734
2025-05-01 11:11:54,207 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:11:54,207 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 11:12:30,414 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:13:17,054 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9768 to 0.9837
2025-05-01 11:13:17,319 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 10 completed in 83.11s - Train Loss: 0.3193, Train Acc: 0.9946, Val Loss: 0.3286, Val Acc: 0.9837
2025-05-01 11:13:17,816 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:13:17,817 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 11:13:54,276 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:14:41,120 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9837 to 0.9854
2025-05-01 11:14:41,379 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 11 completed in 83.56s - Train Loss: 0.3178, Train Acc: 0.9964, Val Loss: 0.3278, Val Acc: 0.9854
2025-05-01 11:14:41,880 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:14:41,881 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 11:15:18,516 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:16:03,773 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9854 to 0.9889
2025-05-01 11:16:04,031 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 12 completed in 82.15s - Train Loss: 0.3165, Train Acc: 0.9970, Val Loss: 0.3253, Val Acc: 0.9889
2025-05-01 11:16:04,514 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:16:04,514 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 11:16:41,632 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 11:17:27,571 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3366 to 0.3286
2025-05-01 11:17:27,830 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 13 completed in 83.32s - Train Loss: 0.3164, Train Acc: 0.9970, Val Loss: 0.3286, Val Acc: 0.9837
2025-05-01 11:17:28,310 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 11:17:28,311 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 11:18:04,433 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 11:18:50,952 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3286 to 0.3245
2025-05-01 11:18:51,202 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 14 completed in 82.89s - Train Loss: 0.3161, Train Acc: 0.9974, Val Loss: 0.3245, Val Acc: 0.9889
2025-05-01 11:18:51,684 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 11:18:51,685 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 11:19:29,548 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 11:20:15,301 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 15 completed in 83.62s - Train Loss: 0.3154, Train Acc: 0.9981, Val Loss: 0.3258, Val Acc: 0.9880
2025-05-01 11:20:15,814 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 11:20:15,814 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 11:20:54,540 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 11:21:41,084 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 16 completed in 85.27s - Train Loss: 0.3151, Train Acc: 0.9985, Val Loss: 0.3258, Val Acc: 0.9880
2025-05-01 11:21:41,602 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 11:21:41,603 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 11:22:17,087 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 11:23:01,856 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 17 completed in 80.25s - Train Loss: 0.3152, Train Acc: 0.9985, Val Loss: 0.3268, Val Acc: 0.9871
2025-05-01 11:23:02,352 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 11:23:02,352 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 11:23:38,750 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 11:24:25,169 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 18 completed in 82.82s - Train Loss: 0.3151, Train Acc: 0.9985, Val Loss: 0.3256, Val Acc: 0.9889
2025-05-01 11:24:25,683 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 11:24:25,684 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 11:25:02,534 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 11:25:49,329 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Epoch 19 completed in 83.65s - Train Loss: 0.3150, Train Acc: 0.9985, Val Loss: 0.3264, Val Acc: 0.9889
2025-05-01 11:25:49,814 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 11:25:49,815 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Early stopping triggered after 19 epochs
2025-05-01 11:25:49,815 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.25 seconds
2025-05-01 11:25:49,816 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_337 - INFO - Starting model evaluation
2025-05-01 11:26:02,219 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_337 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9366
  Recall:    0.9301
  F1 Score:  0.9333
  IoU:       0.8750
  mAP:       0.9850
  AUC:       0.9938
2025-05-01 11:26:02,220 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_337 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_337/final_results.json
2025-05-01 11:26:02,222 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_337 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_337/final_results.json
2025-05-01 11:26:02,222 - main - INFO - 
Summary for configuration 337:
2025-05-01 11:26:02,222 - main - INFO - Accuracy: 0.9623
2025-05-01 11:26:02,222 - main - INFO - Precision: 0.9366
2025-05-01 11:26:02,222 - main - INFO - Recall: 0.9301
2025-05-01 11:26:02,222 - main - INFO - F1 Score: 0.9333
2025-05-01 11:26:02,222 - main - INFO - IoU: 0.8750
2025-05-01 11:26:02,222 - main - INFO - mAP: 0.9850
2025-05-01 11:26:02,222 - main - INFO - AUC: 0.9938
2025-05-01 11:26:02,222 - main - INFO - Training time: 1586.25 seconds
2025-05-01 11:26:02,222 - main - INFO - 
==================================================
2025-05-01 11:26:02,222 - main - INFO - Running configuration 338/756:
2025-05-01 11:26:02,222 - main - INFO - Model: ViT-B-16
2025-05-01 11:26:02,222 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:26:02,222 - main - INFO - Scheduler: StepLR
2025-05-01 11:26:02,222 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:26:02,222 - main - INFO - ==================================================
2025-05-01 11:26:02,223 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_338 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_338
2025-05-01 11:26:02,223 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_338 - INFO - Config: {
  "id": 338,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:26:02,920 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:26:02,920 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 338,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:26:02,920 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:26:03,341 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 19, best validation accuracy: 0.9889
2025-05-01 11:26:03,342 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 11:26:03,734 - training.model_ViT-B-16_opt_SGD_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.30 GiB is allocated by PyTorch, and 321.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:26:03,734 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:26:04,257 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 11:26:04,257 - main - ERROR - Error running configuration 338: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.30 GiB is allocated by PyTorch, and 321.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:26:04,257 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.30 GiB is allocated by PyTorch, and 321.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:26:04,259 - main - INFO - 
==================================================
2025-05-01 11:26:04,259 - main - INFO - Running configuration 339/756:
2025-05-01 11:26:04,259 - main - INFO - Model: ViT-B-16
2025-05-01 11:26:04,259 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:26:04,259 - main - INFO - Scheduler: StepLR
2025-05-01 11:26:04,259 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:26:04,259 - main - INFO - ==================================================
2025-05-01 11:26:04,259 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_339 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_339
2025-05-01 11:26:04,259 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_339 - INFO - Config: {
  "id": 339,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:26:04,949 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:26:04,950 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 339,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:26:04,950 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:26:05,383 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:26:05,383 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:26:05,385 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_339 - INFO - Starting model evaluation
2025-05-01 11:26:18,040 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_339 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9301
  Recall:    0.9301
  F1 Score:  0.9301
  IoU:       0.8693
  mAP:       0.9842
  AUC:       0.9930
2025-05-01 11:26:18,042 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_339 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_339/final_results.json
2025-05-01 11:26:18,044 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_339 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_339/final_results.json
2025-05-01 11:26:18,044 - main - INFO - 
Summary for configuration 339:
2025-05-01 11:26:18,044 - main - INFO - Accuracy: 0.9604
2025-05-01 11:26:18,044 - main - INFO - Precision: 0.9301
2025-05-01 11:26:18,044 - main - INFO - Recall: 0.9301
2025-05-01 11:26:18,044 - main - INFO - F1 Score: 0.9301
2025-05-01 11:26:18,044 - main - INFO - IoU: 0.8693
2025-05-01 11:26:18,044 - main - INFO - mAP: 0.9842
2025-05-01 11:26:18,044 - main - INFO - AUC: 0.9930
2025-05-01 11:26:18,044 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:26:18,044 - main - INFO - 
==================================================
2025-05-01 11:26:18,044 - main - INFO - Running configuration 340/756:
2025-05-01 11:26:18,044 - main - INFO - Model: ViT-B-16
2025-05-01 11:26:18,044 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:26:18,044 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:26:18,044 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:26:18,044 - main - INFO - ==================================================
2025-05-01 11:26:18,044 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_340 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_340
2025-05-01 11:26:18,044 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_340 - INFO - Config: {
  "id": 340,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:26:18,662 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:26:18,662 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 340,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:26:18,662 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:26:19,090 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:26:19,091 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:26:19,092 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_340 - INFO - Starting model evaluation
2025-05-01 11:26:31,728 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_340 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9338
  Recall:    0.9371
  F1 Score:  0.9354
  IoU:       0.8787
  mAP:       0.9845
  AUC:       0.9934
2025-05-01 11:26:31,730 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_340 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_340/final_results.json
2025-05-01 11:26:31,731 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_340 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_340/final_results.json
2025-05-01 11:26:31,731 - main - INFO - 
Summary for configuration 340:
2025-05-01 11:26:31,731 - main - INFO - Accuracy: 0.9633
2025-05-01 11:26:31,731 - main - INFO - Precision: 0.9338
2025-05-01 11:26:31,731 - main - INFO - Recall: 0.9371
2025-05-01 11:26:31,731 - main - INFO - F1 Score: 0.9354
2025-05-01 11:26:31,731 - main - INFO - IoU: 0.8787
2025-05-01 11:26:31,731 - main - INFO - mAP: 0.9845
2025-05-01 11:26:31,731 - main - INFO - AUC: 0.9934
2025-05-01 11:26:31,731 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:26:31,731 - main - INFO - 
==================================================
2025-05-01 11:26:31,731 - main - INFO - Running configuration 341/756:
2025-05-01 11:26:31,731 - main - INFO - Model: ViT-B-16
2025-05-01 11:26:31,731 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:26:31,731 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:26:31,731 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:26:31,731 - main - INFO - ==================================================
2025-05-01 11:26:31,732 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_341 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_341
2025-05-01 11:26:31,732 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_341 - INFO - Config: {
  "id": 341,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:26:32,383 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:26:32,383 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 341,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:26:32,383 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:26:32,786 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:26:32,786 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:26:32,788 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_341 - INFO - Starting model evaluation
2025-05-01 11:26:44,878 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_341 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9301
  Recall:    0.9301
  F1 Score:  0.9301
  IoU:       0.8693
  mAP:       0.9826
  AUC:       0.9926
2025-05-01 11:26:44,880 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_341 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_341/final_results.json
2025-05-01 11:26:44,881 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_341 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_341/final_results.json
2025-05-01 11:26:44,881 - main - INFO - 
Summary for configuration 341:
2025-05-01 11:26:44,881 - main - INFO - Accuracy: 0.9604
2025-05-01 11:26:44,881 - main - INFO - Precision: 0.9301
2025-05-01 11:26:44,882 - main - INFO - Recall: 0.9301
2025-05-01 11:26:44,882 - main - INFO - F1 Score: 0.9301
2025-05-01 11:26:44,882 - main - INFO - IoU: 0.8693
2025-05-01 11:26:44,882 - main - INFO - mAP: 0.9826
2025-05-01 11:26:44,882 - main - INFO - AUC: 0.9926
2025-05-01 11:26:44,882 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:26:44,882 - main - INFO - 
==================================================
2025-05-01 11:26:44,882 - main - INFO - Running configuration 342/756:
2025-05-01 11:26:44,882 - main - INFO - Model: ViT-B-16
2025-05-01 11:26:44,882 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:26:44,882 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:26:44,882 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:26:44,882 - main - INFO - ==================================================
2025-05-01 11:26:44,882 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_342 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_342
2025-05-01 11:26:44,882 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_342 - INFO - Config: {
  "id": 342,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:26:45,535 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:26:45,535 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 342,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:26:45,536 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:26:45,948 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:26:45,949 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:26:45,950 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_342 - INFO - Starting model evaluation
2025-05-01 11:26:58,518 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_342 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9366
  Recall:    0.9301
  F1 Score:  0.9333
  IoU:       0.8750
  mAP:       0.9857
  AUC:       0.9936
2025-05-01 11:26:58,519 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_342 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_342/final_results.json
2025-05-01 11:26:58,521 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_342 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_342/final_results.json
2025-05-01 11:26:58,521 - main - INFO - 
Summary for configuration 342:
2025-05-01 11:26:58,521 - main - INFO - Accuracy: 0.9623
2025-05-01 11:26:58,521 - main - INFO - Precision: 0.9366
2025-05-01 11:26:58,521 - main - INFO - Recall: 0.9301
2025-05-01 11:26:58,521 - main - INFO - F1 Score: 0.9333
2025-05-01 11:26:58,521 - main - INFO - IoU: 0.8750
2025-05-01 11:26:58,521 - main - INFO - mAP: 0.9857
2025-05-01 11:26:58,521 - main - INFO - AUC: 0.9936
2025-05-01 11:26:58,521 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:26:58,521 - main - INFO - 
==================================================
2025-05-01 11:26:58,521 - main - INFO - Running configuration 343/756:
2025-05-01 11:26:58,521 - main - INFO - Model: ViT-B-16
2025-05-01 11:26:58,521 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:26:58,521 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:26:58,521 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:26:58,521 - main - INFO - ==================================================
2025-05-01 11:26:58,521 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_343 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_343
2025-05-01 11:26:58,521 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_343 - INFO - Config: {
  "id": 343,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:26:59,175 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:26:59,175 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 343,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:26:59,175 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:26:59,581 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:26:59,582 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:26:59,583 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_343 - INFO - Starting model evaluation
2025-05-01 11:27:11,601 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_343 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9364
  Recall:    0.9266
  F1 Score:  0.9315
  IoU:       0.8717
  mAP:       0.9828
  AUC:       0.9925
2025-05-01 11:27:11,603 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_343 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_343/final_results.json
2025-05-01 11:27:11,604 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_343 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_343/final_results.json
2025-05-01 11:27:11,604 - main - INFO - 
Summary for configuration 343:
2025-05-01 11:27:11,604 - main - INFO - Accuracy: 0.9613
2025-05-01 11:27:11,604 - main - INFO - Precision: 0.9364
2025-05-01 11:27:11,604 - main - INFO - Recall: 0.9266
2025-05-01 11:27:11,604 - main - INFO - F1 Score: 0.9315
2025-05-01 11:27:11,604 - main - INFO - IoU: 0.8717
2025-05-01 11:27:11,604 - main - INFO - mAP: 0.9828
2025-05-01 11:27:11,604 - main - INFO - AUC: 0.9925
2025-05-01 11:27:11,604 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:27:11,604 - main - INFO - 
==================================================
2025-05-01 11:27:11,604 - main - INFO - Running configuration 344/756:
2025-05-01 11:27:11,604 - main - INFO - Model: ViT-B-16
2025-05-01 11:27:11,604 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:27:11,604 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:27:11,605 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:27:11,605 - main - INFO - ==================================================
2025-05-01 11:27:11,605 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_344 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_344
2025-05-01 11:27:11,605 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_344 - INFO - Config: {
  "id": 344,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:27:12,221 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:27:12,222 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 344,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:27:12,222 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:27:12,647 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:27:12,647 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:27:12,649 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_344 - INFO - Starting model evaluation
2025-05-01 11:27:24,786 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_344 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9333
  Recall:    0.9301
  F1 Score:  0.9317
  IoU:       0.8721
  mAP:       0.9854
  AUC:       0.9938
2025-05-01 11:27:24,788 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_344 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_344/final_results.json
2025-05-01 11:27:24,790 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_344 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_344/final_results.json
2025-05-01 11:27:24,790 - main - INFO - 
Summary for configuration 344:
2025-05-01 11:27:24,790 - main - INFO - Accuracy: 0.9613
2025-05-01 11:27:24,790 - main - INFO - Precision: 0.9333
2025-05-01 11:27:24,790 - main - INFO - Recall: 0.9301
2025-05-01 11:27:24,790 - main - INFO - F1 Score: 0.9317
2025-05-01 11:27:24,790 - main - INFO - IoU: 0.8721
2025-05-01 11:27:24,790 - main - INFO - mAP: 0.9854
2025-05-01 11:27:24,790 - main - INFO - AUC: 0.9938
2025-05-01 11:27:24,790 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:27:24,790 - main - INFO - 
==================================================
2025-05-01 11:27:24,790 - main - INFO - Running configuration 345/756:
2025-05-01 11:27:24,790 - main - INFO - Model: ViT-B-16
2025-05-01 11:27:24,790 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:27:24,790 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:27:24,790 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:27:24,790 - main - INFO - ==================================================
2025-05-01 11:27:24,790 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_345 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_345
2025-05-01 11:27:24,790 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_345 - INFO - Config: {
  "id": 345,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:27:25,407 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:27:25,407 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 345,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:27:25,407 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:27:25,822 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:27:25,822 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:27:25,824 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_345 - INFO - Starting model evaluation
2025-05-01 11:27:38,240 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_345 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9329
  Recall:    0.9231
  F1 Score:  0.9279
  IoU:       0.8656
  mAP:       0.9825
  AUC:       0.9924
2025-05-01 11:27:38,241 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_345 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_345/final_results.json
2025-05-01 11:27:38,243 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_345 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_345/final_results.json
2025-05-01 11:27:38,243 - main - INFO - 
Summary for configuration 345:
2025-05-01 11:27:38,243 - main - INFO - Accuracy: 0.9594
2025-05-01 11:27:38,243 - main - INFO - Precision: 0.9329
2025-05-01 11:27:38,243 - main - INFO - Recall: 0.9231
2025-05-01 11:27:38,243 - main - INFO - F1 Score: 0.9279
2025-05-01 11:27:38,243 - main - INFO - IoU: 0.8656
2025-05-01 11:27:38,243 - main - INFO - mAP: 0.9825
2025-05-01 11:27:38,243 - main - INFO - AUC: 0.9924
2025-05-01 11:27:38,243 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:27:38,243 - main - INFO - 
==================================================
2025-05-01 11:27:38,243 - main - INFO - Running configuration 346/756:
2025-05-01 11:27:38,243 - main - INFO - Model: ViT-B-16
2025-05-01 11:27:38,243 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:27:38,243 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:27:38,243 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:27:38,243 - main - INFO - ==================================================
2025-05-01 11:27:38,243 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_346 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_346
2025-05-01 11:27:38,243 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_346 - INFO - Config: {
  "id": 346,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:27:38,860 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:27:38,860 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 346,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:27:38,860 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:27:39,283 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:27:39,283 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:27:39,285 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_346 - INFO - Starting model evaluation
2025-05-01 11:27:51,531 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_346 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9303
  Recall:    0.9336
  F1 Score:  0.9319
  IoU:       0.8725
  mAP:       0.9814
  AUC:       0.9921
2025-05-01 11:27:51,533 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_346 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_346/final_results.json
2025-05-01 11:27:51,534 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_346 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_346/final_results.json
2025-05-01 11:27:51,534 - main - INFO - 
Summary for configuration 346:
2025-05-01 11:27:51,534 - main - INFO - Accuracy: 0.9613
2025-05-01 11:27:51,534 - main - INFO - Precision: 0.9303
2025-05-01 11:27:51,534 - main - INFO - Recall: 0.9336
2025-05-01 11:27:51,535 - main - INFO - F1 Score: 0.9319
2025-05-01 11:27:51,535 - main - INFO - IoU: 0.8725
2025-05-01 11:27:51,535 - main - INFO - mAP: 0.9814
2025-05-01 11:27:51,535 - main - INFO - AUC: 0.9921
2025-05-01 11:27:51,535 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:27:51,535 - main - INFO - 
==================================================
2025-05-01 11:27:51,535 - main - INFO - Running configuration 347/756:
2025-05-01 11:27:51,535 - main - INFO - Model: ViT-B-16
2025-05-01 11:27:51,535 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:27:51,535 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:27:51,535 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:27:51,535 - main - INFO - ==================================================
2025-05-01 11:27:51,535 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_347 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_347
2025-05-01 11:27:51,535 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_347 - INFO - Config: {
  "id": 347,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:27:52,198 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:27:52,199 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 347,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:27:52,199 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:27:52,604 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:27:52,604 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:27:52,605 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_347 - INFO - Starting model evaluation
2025-05-01 11:28:04,714 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_347 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9368
  Recall:    0.9336
  F1 Score:  0.9352
  IoU:       0.8783
  mAP:       0.9827
  AUC:       0.9924
2025-05-01 11:28:04,715 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_347 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_347/final_results.json
2025-05-01 11:28:04,717 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_347 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_347/final_results.json
2025-05-01 11:28:04,717 - main - INFO - 
Summary for configuration 347:
2025-05-01 11:28:04,717 - main - INFO - Accuracy: 0.9633
2025-05-01 11:28:04,717 - main - INFO - Precision: 0.9368
2025-05-01 11:28:04,717 - main - INFO - Recall: 0.9336
2025-05-01 11:28:04,717 - main - INFO - F1 Score: 0.9352
2025-05-01 11:28:04,717 - main - INFO - IoU: 0.8783
2025-05-01 11:28:04,717 - main - INFO - mAP: 0.9827
2025-05-01 11:28:04,717 - main - INFO - AUC: 0.9924
2025-05-01 11:28:04,717 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:28:04,717 - main - INFO - 
==================================================
2025-05-01 11:28:04,717 - main - INFO - Running configuration 348/756:
2025-05-01 11:28:04,717 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:04,717 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:28:04,717 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:28:04,717 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:04,717 - main - INFO - ==================================================
2025-05-01 11:28:04,717 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_348 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_348
2025-05-01 11:28:04,718 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_348 - INFO - Config: {
  "id": 348,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:05,357 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-01 11:28:05,357 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 348,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:05,358 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 11:28:05,766 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-01 11:28:05,771 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-01 11:28:05,774 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_348 - INFO - Starting model evaluation
2025-05-01 11:28:17,900 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_348 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9359
  Recall:    0.9196
  F1 Score:  0.9277
  IoU:       0.8651
  mAP:       0.9844
  AUC:       0.9932
2025-05-01 11:28:17,902 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_348 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_348/final_results.json
2025-05-01 11:28:17,904 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_348 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_348/final_results.json
2025-05-01 11:28:17,904 - main - INFO - 
Summary for configuration 348:
2025-05-01 11:28:17,904 - main - INFO - Accuracy: 0.9594
2025-05-01 11:28:17,904 - main - INFO - Precision: 0.9359
2025-05-01 11:28:17,904 - main - INFO - Recall: 0.9196
2025-05-01 11:28:17,904 - main - INFO - F1 Score: 0.9277
2025-05-01 11:28:17,904 - main - INFO - IoU: 0.8651
2025-05-01 11:28:17,904 - main - INFO - mAP: 0.9844
2025-05-01 11:28:17,904 - main - INFO - AUC: 0.9932
2025-05-01 11:28:17,904 - main - INFO - Training time: 1586.64 seconds
2025-05-01 11:28:17,904 - main - INFO - 
==================================================
2025-05-01 11:28:17,904 - main - INFO - Running configuration 349/756:
2025-05-01 11:28:17,904 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:17,904 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:17,904 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:17,904 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:17,904 - main - INFO - ==================================================
2025-05-01 11:28:17,904 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_349 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_349
2025-05-01 11:28:17,904 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_349 - INFO - Config: {
  "id": 349,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:18,524 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:18,524 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 349,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:18,524 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:28:18,524 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 11:28:18,838 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 441.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:18,838 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:19,011 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:28:19,011 - main - ERROR - Error running configuration 349: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 441.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:19,011 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 441.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:19,015 - main - INFO - 
==================================================
2025-05-01 11:28:19,015 - main - INFO - Running configuration 350/756:
2025-05-01 11:28:19,015 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:19,015 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:19,015 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:19,015 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:19,015 - main - INFO - ==================================================
2025-05-01 11:28:19,016 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_350 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_350
2025-05-01 11:28:19,016 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_350 - INFO - Config: {
  "id": 350,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:19,642 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:19,642 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 350,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:19,642 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:19,861 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:28:19,862 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 11:28:20,101 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 489.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:20,101 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:20,354 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:28:20,355 - main - ERROR - Error running configuration 350: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 489.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:20,355 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 489.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:20,355 - main - INFO - 
==================================================
2025-05-01 11:28:20,356 - main - INFO - Running configuration 351/756:
2025-05-01 11:28:20,356 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:20,356 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:20,356 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:20,356 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:20,356 - main - INFO - ==================================================
2025-05-01 11:28:20,356 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_351 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_351
2025-05-01 11:28:20,356 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_351 - INFO - Config: {
  "id": 351,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:21,000 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:21,000 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 351,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:21,000 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:21,221 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:28:21,222 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 11:28:21,334 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 425.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:21,334 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:21,641 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:28:21,641 - main - ERROR - Error running configuration 351: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 425.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:21,641 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 425.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:21,642 - main - INFO - 
==================================================
2025-05-01 11:28:21,642 - main - INFO - Running configuration 352/756:
2025-05-01 11:28:21,642 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:21,642 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:21,642 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:21,642 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:21,642 - main - INFO - ==================================================
2025-05-01 11:28:21,642 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_352 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_352
2025-05-01 11:28:21,642 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_352 - INFO - Config: {
  "id": 352,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:22,306 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:22,306 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 352,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:22,306 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:22,515 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:28:22,515 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 11:28:22,622 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.14 GiB is allocated by PyTorch, and 475.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:22,622 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:22,929 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:28:22,929 - main - ERROR - Error running configuration 352: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.14 GiB is allocated by PyTorch, and 475.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:22,929 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 4.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.14 GiB is allocated by PyTorch, and 475.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:22,930 - main - INFO - 
==================================================
2025-05-01 11:28:22,931 - main - INFO - Running configuration 353/756:
2025-05-01 11:28:22,931 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:22,931 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:22,931 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:22,931 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:22,931 - main - INFO - ==================================================
2025-05-01 11:28:22,931 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_353 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_353
2025-05-01 11:28:22,931 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_353 - INFO - Config: {
  "id": 353,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:23,624 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:23,624 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 353,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:23,624 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:23,821 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:28:23,821 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 11:28:24,028 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:24,028 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:24,285 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:28:24,286 - main - ERROR - Error running configuration 353: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:24,286 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:24,286 - main - INFO - 
==================================================
2025-05-01 11:28:24,287 - main - INFO - Running configuration 354/756:
2025-05-01 11:28:24,287 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:24,287 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:24,287 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:24,287 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:24,287 - main - INFO - ==================================================
2025-05-01 11:28:24,287 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_354 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_354
2025-05-01 11:28:24,287 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_354 - INFO - Config: {
  "id": 354,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:24,917 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:24,917 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 354,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:24,917 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:25,142 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:28:25,142 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:28:25,303 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:25,303 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:25,623 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:28:25,624 - main - ERROR - Error running configuration 354: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:25,624 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:25,625 - main - INFO - 
==================================================
2025-05-01 11:28:25,625 - main - INFO - Running configuration 355/756:
2025-05-01 11:28:25,625 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:25,625 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:25,625 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:28:25,625 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:25,625 - main - INFO - ==================================================
2025-05-01 11:28:25,625 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_355 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_355
2025-05-01 11:28:25,625 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_355 - INFO - Config: {
  "id": 355,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:26,284 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:26,284 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 355,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:26,285 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:26,497 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:28:26,498 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:28:26,614 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:26,614 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:26,927 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:28:26,927 - main - ERROR - Error running configuration 355: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:26,927 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:26,928 - main - INFO - 
==================================================
2025-05-01 11:28:26,928 - main - INFO - Running configuration 356/756:
2025-05-01 11:28:26,928 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:26,928 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:26,928 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:28:26,928 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:26,928 - main - INFO - ==================================================
2025-05-01 11:28:26,929 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_356 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_356
2025-05-01 11:28:26,929 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_356 - INFO - Config: {
  "id": 356,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:27,607 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:27,607 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 356,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:27,607 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:27,818 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:28:27,819 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:28:27,982 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:27,982 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:28,290 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:28:28,290 - main - ERROR - Error running configuration 356: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:28,290 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:28,291 - main - INFO - 
==================================================
2025-05-01 11:28:28,292 - main - INFO - Running configuration 357/756:
2025-05-01 11:28:28,292 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:28,292 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:28,292 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:28:28,292 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:28,292 - main - INFO - ==================================================
2025-05-01 11:28:28,292 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_357 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_357
2025-05-01 11:28:28,292 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_357 - INFO - Config: {
  "id": 357,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:28,916 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:28,916 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 357,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:28,916 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:29,135 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:28:29,135 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:28:29,318 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:29,318 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:29,610 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:28:29,611 - main - ERROR - Error running configuration 357: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:29,611 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:29,611 - main - INFO - 
==================================================
2025-05-01 11:28:29,612 - main - INFO - Running configuration 358/756:
2025-05-01 11:28:29,612 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:29,612 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:29,612 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:28:29,612 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:29,612 - main - INFO - ==================================================
2025-05-01 11:28:29,612 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_358 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_358
2025-05-01 11:28:29,612 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_358 - INFO - Config: {
  "id": 358,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:30,248 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:30,248 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 358,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:30,248 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:30,468 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:28:30,469 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:28:30,678 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:30,678 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:30,991 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:28:30,992 - main - ERROR - Error running configuration 358: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:30,992 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:30,993 - main - INFO - 
==================================================
2025-05-01 11:28:30,993 - main - INFO - Running configuration 359/756:
2025-05-01 11:28:30,993 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:30,993 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:30,993 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:28:30,993 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:30,993 - main - INFO - ==================================================
2025-05-01 11:28:30,993 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_359 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_359
2025-05-01 11:28:30,993 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_359 - INFO - Config: {
  "id": 359,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:31,663 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:31,664 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 359,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:31,664 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:31,894 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:28:31,895 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:28:32,065 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:32,065 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:32,380 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:28:32,381 - main - ERROR - Error running configuration 359: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:32,381 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:32,382 - main - INFO - 
==================================================
2025-05-01 11:28:32,382 - main - INFO - Running configuration 360/756:
2025-05-01 11:28:32,382 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:32,382 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 11:28:32,382 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:28:32,382 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:32,382 - main - INFO - ==================================================
2025-05-01 11:28:32,382 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_360 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_360
2025-05-01 11:28:32,382 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_360 - INFO - Config: {
  "id": 360,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:33,045 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-01 11:28:33,045 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 360,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:33,046 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-01 11:28:33,261 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:28:33,262 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:28:33,384 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:33,384 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:33,674 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:28:33,674 - main - ERROR - Error running configuration 360: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:33,674 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:33,675 - main - INFO - 
==================================================
2025-05-01 11:28:33,675 - main - INFO - Running configuration 361/756:
2025-05-01 11:28:33,675 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:33,675 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:33,675 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:33,675 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:33,675 - main - INFO - ==================================================
2025-05-01 11:28:33,676 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_361 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_361
2025-05-01 11:28:33,676 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_361 - INFO - Config: {
  "id": 361,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:34,331 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:34,331 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 361,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:34,331 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:28:34,332 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 11:28:34,504 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:34,504 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:34,710 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:28:34,710 - main - ERROR - Error running configuration 361: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:34,710 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:34,711 - main - INFO - 
==================================================
2025-05-01 11:28:34,711 - main - INFO - Running configuration 362/756:
2025-05-01 11:28:34,711 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:34,711 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:34,711 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:34,711 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:34,711 - main - INFO - ==================================================
2025-05-01 11:28:34,712 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_362 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_362
2025-05-01 11:28:34,712 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_362 - INFO - Config: {
  "id": 362,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:35,370 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:35,370 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 362,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:35,370 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:35,575 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:28:35,576 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 11:28:35,753 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:35,753 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:36,040 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:28:36,040 - main - ERROR - Error running configuration 362: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:36,040 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:36,041 - main - INFO - 
==================================================
2025-05-01 11:28:36,041 - main - INFO - Running configuration 363/756:
2025-05-01 11:28:36,041 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:36,041 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:36,041 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:36,041 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:36,041 - main - INFO - ==================================================
2025-05-01 11:28:36,041 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_363 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_363
2025-05-01 11:28:36,041 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_363 - INFO - Config: {
  "id": 363,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:36,668 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:36,668 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 363,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:36,668 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:36,886 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:28:36,887 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 11:28:37,006 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:37,006 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:37,286 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:28:37,286 - main - ERROR - Error running configuration 363: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:37,286 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:37,287 - main - INFO - 
==================================================
2025-05-01 11:28:37,287 - main - INFO - Running configuration 364/756:
2025-05-01 11:28:37,287 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:37,287 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:37,287 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:37,287 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:37,287 - main - INFO - ==================================================
2025-05-01 11:28:37,288 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_364 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_364
2025-05-01 11:28:37,288 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_364 - INFO - Config: {
  "id": 364,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:37,954 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:37,954 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 364,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:37,954 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:38,166 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:28:38,166 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 11:28:38,290 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:38,290 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:38,583 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:28:38,584 - main - ERROR - Error running configuration 364: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:38,584 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:38,585 - main - INFO - 
==================================================
2025-05-01 11:28:38,585 - main - INFO - Running configuration 365/756:
2025-05-01 11:28:38,585 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:38,585 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:38,585 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:38,585 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:38,585 - main - INFO - ==================================================
2025-05-01 11:28:38,585 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_365 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_365
2025-05-01 11:28:38,585 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_365 - INFO - Config: {
  "id": 365,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:39,212 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:39,212 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 365,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:39,212 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:39,434 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:28:39,434 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 11:28:39,585 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:39,585 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:39,885 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:28:39,885 - main - ERROR - Error running configuration 365: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:39,885 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:39,886 - main - INFO - 
==================================================
2025-05-01 11:28:39,886 - main - INFO - Running configuration 366/756:
2025-05-01 11:28:39,886 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:39,886 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:39,886 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:39,886 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:39,886 - main - INFO - ==================================================
2025-05-01 11:28:39,886 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_366 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_366
2025-05-01 11:28:39,886 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_366 - INFO - Config: {
  "id": 366,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:40,545 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:40,545 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 366,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:40,545 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:40,754 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:28:40,755 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 11:28:40,945 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:40,945 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:41,241 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:28:41,241 - main - ERROR - Error running configuration 366: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:41,241 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:41,242 - main - INFO - 
==================================================
2025-05-01 11:28:41,243 - main - INFO - Running configuration 367/756:
2025-05-01 11:28:41,243 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:41,243 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:41,243 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:28:41,243 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:41,243 - main - INFO - ==================================================
2025-05-01 11:28:41,243 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_367 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_367
2025-05-01 11:28:41,243 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_367 - INFO - Config: {
  "id": 367,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:41,915 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:41,915 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 367,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:41,915 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:42,127 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:28:42,128 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 11:28:42,295 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:42,295 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:42,598 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:28:42,599 - main - ERROR - Error running configuration 367: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:42,599 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:42,599 - main - INFO - 
==================================================
2025-05-01 11:28:42,600 - main - INFO - Running configuration 368/756:
2025-05-01 11:28:42,600 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:42,600 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:42,600 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:28:42,600 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:42,600 - main - INFO - ==================================================
2025-05-01 11:28:42,600 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_368 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_368
2025-05-01 11:28:42,600 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_368 - INFO - Config: {
  "id": 368,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:43,216 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:43,216 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 368,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:43,216 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:43,436 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:28:43,437 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 11:28:43,595 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:43,596 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:43,911 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:28:43,911 - main - ERROR - Error running configuration 368: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:43,911 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:43,912 - main - INFO - 
==================================================
2025-05-01 11:28:43,912 - main - INFO - Running configuration 369/756:
2025-05-01 11:28:43,912 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:43,913 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:43,913 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:28:43,913 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:43,913 - main - INFO - ==================================================
2025-05-01 11:28:43,913 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_369 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_369
2025-05-01 11:28:43,913 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_369 - INFO - Config: {
  "id": 369,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:44,550 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:44,550 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 369,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:44,550 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:44,776 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:28:44,777 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 11:28:44,972 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:44,972 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:45,279 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:28:45,280 - main - ERROR - Error running configuration 369: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:45,280 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:45,281 - main - INFO - 
==================================================
2025-05-01 11:28:45,281 - main - INFO - Running configuration 370/756:
2025-05-01 11:28:45,281 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:45,281 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:45,281 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:28:45,281 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:45,281 - main - INFO - ==================================================
2025-05-01 11:28:45,281 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_370 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_370
2025-05-01 11:28:45,281 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_370 - INFO - Config: {
  "id": 370,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:45,937 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:45,938 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 370,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:45,938 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:46,148 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:28:46,149 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 11:28:46,327 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:46,327 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:46,618 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:28:46,619 - main - ERROR - Error running configuration 370: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:46,619 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:46,620 - main - INFO - 
==================================================
2025-05-01 11:28:46,620 - main - INFO - Running configuration 371/756:
2025-05-01 11:28:46,620 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:46,620 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:46,620 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:28:46,620 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:46,620 - main - INFO - ==================================================
2025-05-01 11:28:46,620 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_371 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_371
2025-05-01 11:28:46,620 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_371 - INFO - Config: {
  "id": 371,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:47,283 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:47,283 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 371,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:47,283 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:47,492 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:28:47,493 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 11:28:47,735 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:47,735 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:48,040 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:28:48,040 - main - ERROR - Error running configuration 371: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:48,040 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:48,041 - main - INFO - 
==================================================
2025-05-01 11:28:48,041 - main - INFO - Running configuration 372/756:
2025-05-01 11:28:48,041 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:48,041 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-01 11:28:48,041 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:28:48,041 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:48,041 - main - INFO - ==================================================
2025-05-01 11:28:48,042 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_372 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_372
2025-05-01 11:28:48,042 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_372 - INFO - Config: {
  "id": 372,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:48,701 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-01 11:28:48,701 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 372,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:48,701 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-01 11:28:48,914 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:28:48,915 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 11:28:49,048 - training.model_ViT-B-16_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:49,048 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:28:49,341 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:28:49,342 - main - ERROR - Error running configuration 372: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:49,342 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:49,343 - main - INFO - 
==================================================
2025-05-01 11:28:49,343 - main - INFO - Running configuration 373/756:
2025-05-01 11:28:49,343 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:49,343 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:28:49,343 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:49,343 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:49,343 - main - INFO - ==================================================
2025-05-01 11:28:49,343 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_373 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_373
2025-05-01 11:28:49,343 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_373 - INFO - Config: {
  "id": 373,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:49,968 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:28:49,968 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 373,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:49,968 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:28:49,968 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 11:28:50,137 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:50,137 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:50,382 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:28:50,382 - main - ERROR - Error running configuration 373: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:50,382 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 419.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:50,383 - main - INFO - 
==================================================
2025-05-01 11:28:50,383 - main - INFO - Running configuration 374/756:
2025-05-01 11:28:50,383 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:50,383 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:28:50,383 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:50,383 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:50,383 - main - INFO - ==================================================
2025-05-01 11:28:50,383 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_374 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_374
2025-05-01 11:28:50,383 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_374 - INFO - Config: {
  "id": 374,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:51,004 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:28:51,005 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 374,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:51,005 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:28:51,221 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:28:51,221 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 11:28:51,335 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:51,335 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:51,636 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:28:51,637 - main - ERROR - Error running configuration 374: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:51,637 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.06 GiB is allocated by PyTorch, and 538.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:51,638 - main - INFO - 
==================================================
2025-05-01 11:28:51,638 - main - INFO - Running configuration 375/756:
2025-05-01 11:28:51,638 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:51,638 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:28:51,638 - main - INFO - Scheduler: StepLR
2025-05-01 11:28:51,638 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:51,638 - main - INFO - ==================================================
2025-05-01 11:28:51,638 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_375 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_375
2025-05-01 11:28:51,638 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_375 - INFO - Config: {
  "id": 375,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:52,298 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:28:52,298 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 375,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:52,298 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:28:52,591 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:28:52,592 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 11:28:53,178 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.01 GiB is allocated by PyTorch, and 593.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:53,178 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:53,908 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:28:53,909 - main - ERROR - Error running configuration 375: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.01 GiB is allocated by PyTorch, and 593.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:53,909 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 24.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.01 GiB is allocated by PyTorch, and 593.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:53,911 - main - INFO - 
==================================================
2025-05-01 11:28:53,911 - main - INFO - Running configuration 376/756:
2025-05-01 11:28:53,911 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:53,911 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:28:53,911 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:53,911 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:28:53,911 - main - INFO - ==================================================
2025-05-01 11:28:53,911 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_376 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_376
2025-05-01 11:28:53,911 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_376 - INFO - Config: {
  "id": 376,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:54,591 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:28:54,591 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 376,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:28:54,591 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:28:55,274 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:28:55,275 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 11:28:55,522 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 44.62 MiB is free. Including non-PyTorch memory, this process has 4.88 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 463.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:55,522 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:56,251 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:28:56,251 - main - ERROR - Error running configuration 376: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 44.62 MiB is free. Including non-PyTorch memory, this process has 4.88 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 463.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:56,251 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 44.62 MiB is free. Including non-PyTorch memory, this process has 4.88 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 463.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:56,252 - main - INFO - 
==================================================
2025-05-01 11:28:56,252 - main - INFO - Running configuration 377/756:
2025-05-01 11:28:56,252 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:56,252 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:28:56,252 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:56,252 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:28:56,252 - main - INFO - ==================================================
2025-05-01 11:28:56,252 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_377 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_377
2025-05-01 11:28:56,253 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_377 - INFO - Config: {
  "id": 377,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:56,916 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:28:56,916 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 377,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:28:56,916 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:28:57,620 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:28:57,621 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 11:28:58,024 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 58.62 MiB is free. Including non-PyTorch memory, this process has 4.86 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.09 GiB is allocated by PyTorch, and 475.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:58,024 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:28:58,745 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:28:58,745 - main - ERROR - Error running configuration 377: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 58.62 MiB is free. Including non-PyTorch memory, this process has 4.86 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.09 GiB is allocated by PyTorch, and 475.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:58,746 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 58.62 MiB is free. Including non-PyTorch memory, this process has 4.86 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.09 GiB is allocated by PyTorch, and 475.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:28:58,746 - main - INFO - 
==================================================
2025-05-01 11:28:58,747 - main - INFO - Running configuration 378/756:
2025-05-01 11:28:58,747 - main - INFO - Model: ViT-B-16
2025-05-01 11:28:58,747 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:28:58,747 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:28:58,747 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:28:58,747 - main - INFO - ==================================================
2025-05-01 11:28:58,747 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_378 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_378
2025-05-01 11:28:58,747 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_378 - INFO - Config: {
  "id": 378,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:59,401 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:28:59,401 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 378,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:28:59,401 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:29:00,110 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:29:00,111 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 11:29:00,371 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 3.97 GiB is allocated by PyTorch, and 645.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:00,371 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:01,106 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:29:01,106 - main - ERROR - Error running configuration 378: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 3.97 GiB is allocated by PyTorch, and 645.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:01,106 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 3.97 GiB is allocated by PyTorch, and 645.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:01,107 - main - INFO - 
==================================================
2025-05-01 11:29:01,107 - main - INFO - Running configuration 379/756:
2025-05-01 11:29:01,107 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:01,107 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:29:01,107 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:01,107 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:01,107 - main - INFO - ==================================================
2025-05-01 11:29:01,108 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_379 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_379
2025-05-01 11:29:01,108 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_379 - INFO - Config: {
  "id": 379,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:01,724 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:29:01,724 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 379,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:01,724 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:29:02,391 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:29:02,391 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 11:29:02,530 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.28 GiB is allocated by PyTorch, and 330.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:02,530 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:03,266 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:29:03,266 - main - ERROR - Error running configuration 379: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.28 GiB is allocated by PyTorch, and 330.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:03,266 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.28 GiB is allocated by PyTorch, and 330.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:03,267 - main - INFO - 
==================================================
2025-05-01 11:29:03,267 - main - INFO - Running configuration 380/756:
2025-05-01 11:29:03,267 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:03,267 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:29:03,267 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:03,267 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:03,267 - main - INFO - ==================================================
2025-05-01 11:29:03,267 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_380 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_380
2025-05-01 11:29:03,267 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_380 - INFO - Config: {
  "id": 380,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:03,885 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:29:03,885 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 380,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:03,885 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:29:04,585 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:29:04,586 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 11:29:04,721 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 374.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:04,721 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:05,444 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:29:05,445 - main - ERROR - Error running configuration 380: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 374.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:05,445 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 374.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:05,446 - main - INFO - 
==================================================
2025-05-01 11:29:05,446 - main - INFO - Running configuration 381/756:
2025-05-01 11:29:05,446 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:05,446 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:29:05,446 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:05,446 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:05,446 - main - INFO - ==================================================
2025-05-01 11:29:05,446 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_381 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_381
2025-05-01 11:29:05,446 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_381 - INFO - Config: {
  "id": 381,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:06,103 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:29:06,104 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 381,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:06,104 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:29:06,786 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:29:06,786 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 11:29:06,966 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.24 GiB is allocated by PyTorch, and 370.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:06,966 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:07,696 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:29:07,696 - main - ERROR - Error running configuration 381: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.24 GiB is allocated by PyTorch, and 370.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:07,696 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 4.90 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.24 GiB is allocated by PyTorch, and 370.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:07,697 - main - INFO - 
==================================================
2025-05-01 11:29:07,698 - main - INFO - Running configuration 382/756:
2025-05-01 11:29:07,698 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:07,698 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:29:07,698 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:07,698 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:07,698 - main - INFO - ==================================================
2025-05-01 11:29:07,698 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_382 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_382
2025-05-01 11:29:07,698 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_382 - INFO - Config: {
  "id": 382,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:08,341 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:29:08,341 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 382,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:08,341 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:29:09,039 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:29:09,040 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 11:29:09,239 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 384.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:09,239 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:09,962 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:29:09,962 - main - ERROR - Error running configuration 382: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 384.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:09,962 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5614, in _in_projection_packed
    proj = linear(q, w, b)
           ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 384.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:09,963 - main - INFO - 
==================================================
2025-05-01 11:29:09,963 - main - INFO - Running configuration 383/756:
2025-05-01 11:29:09,963 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:09,963 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:29:09,963 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:09,963 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:09,963 - main - INFO - ==================================================
2025-05-01 11:29:09,963 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_383 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_383
2025-05-01 11:29:09,963 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_383 - INFO - Config: {
  "id": 383,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:10,621 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:29:10,621 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 383,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:10,621 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:29:11,315 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:29:11,316 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 11:29:11,545 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 385.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:11,545 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:12,288 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:29:12,288 - main - ERROR - Error running configuration 383: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 385.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:12,289 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5614, in _in_projection_packed
    proj = linear(q, w, b)
           ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 385.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:12,289 - main - INFO - 
==================================================
2025-05-01 11:29:12,289 - main - INFO - Running configuration 384/756:
2025-05-01 11:29:12,289 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:12,290 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-01 11:29:12,290 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:12,290 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:12,290 - main - INFO - ==================================================
2025-05-01 11:29:12,290 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_384 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_384
2025-05-01 11:29:12,290 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_384 - INFO - Config: {
  "id": 384,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:12,914 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-01 11:29:12,914 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 384,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:12,914 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-01 11:29:13,599 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:29:13,600 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 11:29:13,788 - training.model_ViT-B-16_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.26 GiB is allocated by PyTorch, and 342.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:13,788 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:14,514 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:29:14,514 - main - ERROR - Error running configuration 384: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.26 GiB is allocated by PyTorch, and 342.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:14,514 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.26 GiB is allocated by PyTorch, and 342.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:14,515 - main - INFO - 
==================================================
2025-05-01 11:29:14,515 - main - INFO - Running configuration 385/756:
2025-05-01 11:29:14,515 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:14,515 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:14,515 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:14,515 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:14,515 - main - INFO - ==================================================
2025-05-01 11:29:14,516 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_385 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_385
2025-05-01 11:29:14,516 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_385 - INFO - Config: {
  "id": 385,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:15,171 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:15,171 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 385,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:15,171 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:29:15,171 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 11:29:15,302 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.19 GiB is allocated by PyTorch, and 423.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:15,302 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:15,509 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:29:15,510 - main - ERROR - Error running configuration 385: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.19 GiB is allocated by PyTorch, and 423.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:15,510 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.19 GiB is allocated by PyTorch, and 423.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:15,511 - main - INFO - 
==================================================
2025-05-01 11:29:15,511 - main - INFO - Running configuration 386/756:
2025-05-01 11:29:15,511 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:15,511 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:15,511 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:15,511 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:15,511 - main - INFO - ==================================================
2025-05-01 11:29:15,511 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_386 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_386
2025-05-01 11:29:15,511 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_386 - INFO - Config: {
  "id": 386,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:16,189 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:16,189 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 386,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:16,189 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:16,397 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:29:16,398 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 11:29:16,607 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 280.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:16,608 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:16,878 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:29:16,878 - main - ERROR - Error running configuration 386: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 280.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:16,878 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 14.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 280.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:16,879 - main - INFO - 
==================================================
2025-05-01 11:29:16,880 - main - INFO - Running configuration 387/756:
2025-05-01 11:29:16,880 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:16,880 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:16,880 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:16,880 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:16,880 - main - INFO - ==================================================
2025-05-01 11:29:16,880 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_387 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_387
2025-05-01 11:29:16,880 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_387 - INFO - Config: {
  "id": 387,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:17,519 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:17,519 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 387,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:17,519 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:17,742 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:29:17,742 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 11:29:17,879 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 353.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:17,879 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:18,142 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:29:18,142 - main - ERROR - Error running configuration 387: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 353.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:18,142 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 353.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:18,144 - main - INFO - 
==================================================
2025-05-01 11:29:18,144 - main - INFO - Running configuration 388/756:
2025-05-01 11:29:18,144 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:18,144 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:18,144 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:18,144 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:18,144 - main - INFO - ==================================================
2025-05-01 11:29:18,144 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_388 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_388
2025-05-01 11:29:18,144 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_388 - INFO - Config: {
  "id": 388,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:18,800 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:18,800 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 388,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:18,800 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:19,002 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:29:19,003 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 11:29:19,138 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 294.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:19,138 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:19,396 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:29:19,397 - main - ERROR - Error running configuration 388: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 294.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:19,397 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 294.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:19,398 - main - INFO - 
==================================================
2025-05-01 11:29:19,398 - main - INFO - Running configuration 389/756:
2025-05-01 11:29:19,398 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:19,398 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:19,398 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:19,398 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:19,398 - main - INFO - ==================================================
2025-05-01 11:29:19,398 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_389 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_389
2025-05-01 11:29:19,398 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_389 - INFO - Config: {
  "id": 389,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:20,076 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:20,076 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 389,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:20,076 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:20,289 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:29:20,289 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 11:29:20,428 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 353.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:20,428 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:20,705 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:29:20,706 - main - ERROR - Error running configuration 389: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 353.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:20,706 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 353.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:20,707 - main - INFO - 
==================================================
2025-05-01 11:29:20,707 - main - INFO - Running configuration 390/756:
2025-05-01 11:29:20,707 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:20,707 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:20,707 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:20,707 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:20,707 - main - INFO - ==================================================
2025-05-01 11:29:20,707 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_390 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_390
2025-05-01 11:29:20,707 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_390 - INFO - Config: {
  "id": 390,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:21,335 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:21,335 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 390,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:21,335 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:21,555 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:29:21,556 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:29:21,717 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 294.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:21,718 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:21,988 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:29:21,988 - main - ERROR - Error running configuration 390: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 294.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:21,988 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 294.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:21,989 - main - INFO - 
==================================================
2025-05-01 11:29:21,989 - main - INFO - Running configuration 391/756:
2025-05-01 11:29:21,989 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:21,989 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:21,989 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:21,989 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:21,989 - main - INFO - ==================================================
2025-05-01 11:29:21,989 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_391 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_391
2025-05-01 11:29:21,989 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_391 - INFO - Config: {
  "id": 391,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:22,644 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:22,644 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 391,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:22,644 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:22,854 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:29:22,854 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:29:22,982 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:22,982 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:23,251 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:29:23,252 - main - ERROR - Error running configuration 391: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:23,252 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:23,253 - main - INFO - 
==================================================
2025-05-01 11:29:23,253 - main - INFO - Running configuration 392/756:
2025-05-01 11:29:23,253 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:23,253 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:23,253 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:23,253 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:23,253 - main - INFO - ==================================================
2025-05-01 11:29:23,253 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_392 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_392
2025-05-01 11:29:23,253 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_392 - INFO - Config: {
  "id": 392,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:23,871 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:23,871 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 392,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:23,871 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:24,091 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:29:24,091 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:29:24,224 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:24,225 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:24,489 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:29:24,489 - main - ERROR - Error running configuration 392: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:24,489 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:24,490 - main - INFO - 
==================================================
2025-05-01 11:29:24,490 - main - INFO - Running configuration 393/756:
2025-05-01 11:29:24,490 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:24,490 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:24,490 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:24,490 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:24,490 - main - INFO - ==================================================
2025-05-01 11:29:24,491 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_393 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_393
2025-05-01 11:29:24,491 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_393 - INFO - Config: {
  "id": 393,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:25,142 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:25,143 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 393,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:25,143 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:25,347 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:29:25,348 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:29:25,511 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:25,511 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:25,770 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:29:25,771 - main - ERROR - Error running configuration 393: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:25,771 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:25,772 - main - INFO - 
==================================================
2025-05-01 11:29:25,772 - main - INFO - Running configuration 394/756:
2025-05-01 11:29:25,772 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:25,772 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:25,772 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:25,772 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:25,772 - main - INFO - ==================================================
2025-05-01 11:29:25,772 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_394 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_394
2025-05-01 11:29:25,772 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_394 - INFO - Config: {
  "id": 394,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:26,426 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:26,426 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 394,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:26,426 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:26,637 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:29:26,638 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:29:26,810 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:26,810 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:27,080 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:29:27,080 - main - ERROR - Error running configuration 394: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:27,080 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:27,081 - main - INFO - 
==================================================
2025-05-01 11:29:27,082 - main - INFO - Running configuration 395/756:
2025-05-01 11:29:27,082 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:27,082 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:27,082 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:27,082 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:27,082 - main - INFO - ==================================================
2025-05-01 11:29:27,082 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_395 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_395
2025-05-01 11:29:27,082 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_395 - INFO - Config: {
  "id": 395,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:27,712 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:27,713 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 395,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:27,713 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:27,933 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:29:27,934 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:29:28,124 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:28,125 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:28,408 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:29:28,408 - main - ERROR - Error running configuration 395: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:28,408 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:28,409 - main - INFO - 
==================================================
2025-05-01 11:29:28,409 - main - INFO - Running configuration 396/756:
2025-05-01 11:29:28,409 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:28,409 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-01 11:29:28,409 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:28,409 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:28,409 - main - INFO - ==================================================
2025-05-01 11:29:28,410 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_396 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_396
2025-05-01 11:29:28,410 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_396 - INFO - Config: {
  "id": 396,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:29,058 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-01 11:29:29,058 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 396,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:29,058 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-01 11:29:29,280 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:29:29,281 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:29:29,466 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:29,467 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:29,747 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:29:29,747 - main - ERROR - Error running configuration 396: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:29,747 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:29,748 - main - INFO - 
==================================================
2025-05-01 11:29:29,748 - main - INFO - Running configuration 397/756:
2025-05-01 11:29:29,748 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:29,748 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:29,748 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:29,748 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:29,748 - main - INFO - ==================================================
2025-05-01 11:29:29,749 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_397 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_397
2025-05-01 11:29:29,749 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_397 - INFO - Config: {
  "id": 397,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:30,404 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:30,404 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 397,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:30,404 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:29:30,405 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 11:29:30,564 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:30,564 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:30,781 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:29:30,781 - main - ERROR - Error running configuration 397: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:30,781 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:30,782 - main - INFO - 
==================================================
2025-05-01 11:29:30,782 - main - INFO - Running configuration 398/756:
2025-05-01 11:29:30,782 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:30,782 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:30,782 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:30,782 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:30,782 - main - INFO - ==================================================
2025-05-01 11:29:30,783 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_398 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_398
2025-05-01 11:29:30,783 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_398 - INFO - Config: {
  "id": 398,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:31,445 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:31,445 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 398,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:31,445 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:31,661 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:29:31,661 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 11:29:31,853 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:31,853 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:32,106 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:29:32,106 - main - ERROR - Error running configuration 398: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:32,106 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:32,107 - main - INFO - 
==================================================
2025-05-01 11:29:32,107 - main - INFO - Running configuration 399/756:
2025-05-01 11:29:32,107 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:32,107 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:32,107 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:32,107 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:32,107 - main - INFO - ==================================================
2025-05-01 11:29:32,108 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_399 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_399
2025-05-01 11:29:32,108 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_399 - INFO - Config: {
  "id": 399,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:32,782 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:32,782 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 399,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:32,782 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:32,993 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:29:32,994 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 11:29:33,163 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:33,163 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:33,425 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:29:33,425 - main - ERROR - Error running configuration 399: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:33,425 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:33,426 - main - INFO - 
==================================================
2025-05-01 11:29:33,426 - main - INFO - Running configuration 400/756:
2025-05-01 11:29:33,426 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:33,426 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:33,426 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:33,426 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:33,426 - main - INFO - ==================================================
2025-05-01 11:29:33,427 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_400 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_400
2025-05-01 11:29:33,427 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_400 - INFO - Config: {
  "id": 400,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:34,047 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:34,047 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 400,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:34,047 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:34,265 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:29:34,266 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 11:29:34,400 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:34,401 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:34,666 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:29:34,666 - main - ERROR - Error running configuration 400: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:34,667 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:34,668 - main - INFO - 
==================================================
2025-05-01 11:29:34,668 - main - INFO - Running configuration 401/756:
2025-05-01 11:29:34,668 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:34,668 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:34,668 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:34,668 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:34,668 - main - INFO - ==================================================
2025-05-01 11:29:34,668 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_401 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_401
2025-05-01 11:29:34,668 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_401 - INFO - Config: {
  "id": 401,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:35,318 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:35,319 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 401,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:35,319 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:35,527 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:29:35,528 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 11:29:35,687 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:35,687 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:35,965 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:29:35,965 - main - ERROR - Error running configuration 401: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:35,965 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:35,966 - main - INFO - 
==================================================
2025-05-01 11:29:35,966 - main - INFO - Running configuration 402/756:
2025-05-01 11:29:35,966 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:35,966 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:35,966 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:35,966 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:35,966 - main - INFO - ==================================================
2025-05-01 11:29:35,967 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_402 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_402
2025-05-01 11:29:35,967 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_402 - INFO - Config: {
  "id": 402,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:36,644 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:36,644 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 402,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:36,644 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:36,858 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:29:36,858 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 11:29:36,972 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:36,973 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:37,215 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:29:37,215 - main - ERROR - Error running configuration 402: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:37,215 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:37,217 - main - INFO - 
==================================================
2025-05-01 11:29:37,217 - main - INFO - Running configuration 403/756:
2025-05-01 11:29:37,217 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:37,217 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:37,217 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:37,217 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:37,217 - main - INFO - ==================================================
2025-05-01 11:29:37,217 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_403 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_403
2025-05-01 11:29:37,217 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_403 - INFO - Config: {
  "id": 403,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:37,854 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:37,854 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 403,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:37,854 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:38,072 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:29:38,073 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 11:29:38,239 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:38,240 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:38,505 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:29:38,506 - main - ERROR - Error running configuration 403: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:38,506 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:38,507 - main - INFO - 
==================================================
2025-05-01 11:29:38,507 - main - INFO - Running configuration 404/756:
2025-05-01 11:29:38,507 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:38,507 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:38,507 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:38,507 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:38,507 - main - INFO - ==================================================
2025-05-01 11:29:38,507 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_404 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_404
2025-05-01 11:29:38,507 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_404 - INFO - Config: {
  "id": 404,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:39,164 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:39,164 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 404,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:39,164 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:39,378 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:29:39,378 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 11:29:39,546 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:39,546 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:39,811 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:29:39,811 - main - ERROR - Error running configuration 404: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:39,811 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:39,812 - main - INFO - 
==================================================
2025-05-01 11:29:39,812 - main - INFO - Running configuration 405/756:
2025-05-01 11:29:39,812 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:39,812 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:39,812 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:39,812 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:39,812 - main - INFO - ==================================================
2025-05-01 11:29:39,813 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_405 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_405
2025-05-01 11:29:39,813 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_405 - INFO - Config: {
  "id": 405,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:40,429 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:40,429 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 405,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:40,429 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:40,647 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:29:40,648 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 11:29:40,846 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:40,846 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:41,089 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:29:41,089 - main - ERROR - Error running configuration 405: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:41,089 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:41,090 - main - INFO - 
==================================================
2025-05-01 11:29:41,090 - main - INFO - Running configuration 406/756:
2025-05-01 11:29:41,090 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:41,090 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:41,090 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:41,090 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:41,090 - main - INFO - ==================================================
2025-05-01 11:29:41,090 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_406 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_406
2025-05-01 11:29:41,090 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_406 - INFO - Config: {
  "id": 406,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:41,733 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:41,734 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 406,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:41,734 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:41,954 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:29:41,954 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 11:29:42,106 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:42,107 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:42,370 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:29:42,370 - main - ERROR - Error running configuration 406: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:42,370 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:42,371 - main - INFO - 
==================================================
2025-05-01 11:29:42,371 - main - INFO - Running configuration 407/756:
2025-05-01 11:29:42,371 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:42,371 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:42,371 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:42,371 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:42,371 - main - INFO - ==================================================
2025-05-01 11:29:42,372 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_407 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_407
2025-05-01 11:29:42,372 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_407 - INFO - Config: {
  "id": 407,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:43,022 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:43,022 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 407,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:43,022 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:43,235 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:29:43,235 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 11:29:43,420 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:43,420 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:43,692 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:29:43,693 - main - ERROR - Error running configuration 407: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:43,693 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:43,694 - main - INFO - 
==================================================
2025-05-01 11:29:43,694 - main - INFO - Running configuration 408/756:
2025-05-01 11:29:43,694 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:43,694 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-01 11:29:43,694 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:43,694 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:43,694 - main - INFO - ==================================================
2025-05-01 11:29:43,694 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_408 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_408
2025-05-01 11:29:43,694 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_408 - INFO - Config: {
  "id": 408,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:44,363 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-01 11:29:44,363 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 408,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:44,363 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-01 11:29:44,556 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:29:44,557 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 11:29:44,716 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:44,717 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:29:44,968 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:29:44,968 - main - ERROR - Error running configuration 408: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:44,968 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:44,969 - main - INFO - 
==================================================
2025-05-01 11:29:44,970 - main - INFO - Running configuration 409/756:
2025-05-01 11:29:44,970 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:44,970 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:44,970 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:44,970 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:44,970 - main - INFO - ==================================================
2025-05-01 11:29:44,970 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_409 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_409
2025-05-01 11:29:44,970 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_409 - INFO - Config: {
  "id": 409,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:45,608 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:45,608 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 409,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:45,608 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:29:45,608 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 11:29:45,761 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:45,761 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:45,953 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:29:45,953 - main - ERROR - Error running configuration 409: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:45,953 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:45,954 - main - INFO - 
==================================================
2025-05-01 11:29:45,954 - main - INFO - Running configuration 410/756:
2025-05-01 11:29:45,954 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:45,954 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:45,954 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:45,954 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:45,954 - main - INFO - ==================================================
2025-05-01 11:29:45,954 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_410 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_410
2025-05-01 11:29:45,955 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_410 - INFO - Config: {
  "id": 410,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:46,606 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:46,606 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 410,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:46,607 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:29:46,820 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:29:46,820 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 11:29:47,073 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:47,074 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:47,350 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:29:47,350 - main - ERROR - Error running configuration 410: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:47,350 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:47,352 - main - INFO - 
==================================================
2025-05-01 11:29:47,352 - main - INFO - Running configuration 411/756:
2025-05-01 11:29:47,352 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:47,352 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:47,352 - main - INFO - Scheduler: StepLR
2025-05-01 11:29:47,352 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:47,352 - main - INFO - ==================================================
2025-05-01 11:29:47,352 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_411 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_411
2025-05-01 11:29:47,352 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_411 - INFO - Config: {
  "id": 411,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:48,009 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:48,009 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 411,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:48,009 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:29:48,220 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:29:48,221 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 11:29:48,341 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:48,342 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:48,602 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:29:48,603 - main - ERROR - Error running configuration 411: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:48,603 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:48,604 - main - INFO - 
==================================================
2025-05-01 11:29:48,604 - main - INFO - Running configuration 412/756:
2025-05-01 11:29:48,604 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:48,604 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:48,604 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:48,604 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:48,604 - main - INFO - ==================================================
2025-05-01 11:29:48,604 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_412 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_412
2025-05-01 11:29:48,604 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_412 - INFO - Config: {
  "id": 412,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:49,220 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:49,220 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 412,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:49,220 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:29:49,442 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:29:49,442 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 11:29:49,590 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:49,590 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:49,861 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:29:49,861 - main - ERROR - Error running configuration 412: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:49,861 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:49,862 - main - INFO - 
==================================================
2025-05-01 11:29:49,862 - main - INFO - Running configuration 413/756:
2025-05-01 11:29:49,862 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:49,862 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:49,863 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:49,863 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:49,863 - main - INFO - ==================================================
2025-05-01 11:29:49,863 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_413 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_413
2025-05-01 11:29:49,863 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_413 - INFO - Config: {
  "id": 413,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:50,538 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:50,538 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 413,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:50,538 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:29:50,759 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:29:50,760 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 11:29:50,883 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:50,883 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:51,157 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:29:51,157 - main - ERROR - Error running configuration 413: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:51,157 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 354.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:51,158 - main - INFO - 
==================================================
2025-05-01 11:29:51,158 - main - INFO - Running configuration 414/756:
2025-05-01 11:29:51,158 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:51,158 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:51,158 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:29:51,158 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:51,158 - main - INFO - ==================================================
2025-05-01 11:29:51,159 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_414 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_414
2025-05-01 11:29:51,159 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_414 - INFO - Config: {
  "id": 414,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:51,834 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:51,834 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 414,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:51,834 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:29:52,045 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:29:52,045 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 11:29:52,292 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:52,292 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:52,564 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:29:52,564 - main - ERROR - Error running configuration 414: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:52,564 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.33 GiB is allocated by PyTorch, and 293.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:52,566 - main - INFO - 
==================================================
2025-05-01 11:29:52,566 - main - INFO - Running configuration 415/756:
2025-05-01 11:29:52,566 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:52,566 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:52,566 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:52,566 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:52,566 - main - INFO - ==================================================
2025-05-01 11:29:52,566 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_415 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_415
2025-05-01 11:29:52,566 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_415 - INFO - Config: {
  "id": 415,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:53,298 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:53,298 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 415,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:29:53,298 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:29:53,510 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:29:53,514 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 11:29:54,281 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.13 GiB is allocated by PyTorch, and 491.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:54,281 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:54,953 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:29:54,953 - main - ERROR - Error running configuration 415: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.13 GiB is allocated by PyTorch, and 491.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:54,953 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 119, in forward
    return x + y
           ~~^~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.13 GiB is allocated by PyTorch, and 491.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:54,955 - main - INFO - 
==================================================
2025-05-01 11:29:54,955 - main - INFO - Running configuration 416/756:
2025-05-01 11:29:54,955 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:54,955 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:54,955 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:54,955 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:29:54,955 - main - INFO - ==================================================
2025-05-01 11:29:54,955 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_416 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_416
2025-05-01 11:29:54,955 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_416 - INFO - Config: {
  "id": 416,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:55,584 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:55,584 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 416,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:29:55,584 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:29:56,298 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:29:56,299 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 11:29:56,455 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.28 GiB is allocated by PyTorch, and 345.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:56,456 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:57,168 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:29:57,169 - main - ERROR - Error running configuration 416: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.28 GiB is allocated by PyTorch, and 345.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:57,169 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.28 GiB is allocated by PyTorch, and 345.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:57,170 - main - INFO - 
==================================================
2025-05-01 11:29:57,170 - main - INFO - Running configuration 417/756:
2025-05-01 11:29:57,170 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:57,170 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:57,170 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:29:57,170 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:29:57,170 - main - INFO - ==================================================
2025-05-01 11:29:57,170 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_417 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_417
2025-05-01 11:29:57,170 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_417 - INFO - Config: {
  "id": 417,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:57,761 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:29:57,761 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 417,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:29:57,762 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:29:58,452 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:29:58,452 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 11:29:58,632 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 521.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:58,632 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:29:59,383 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:29:59,383 - main - ERROR - Error running configuration 417: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 521.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:59,383 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 521.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:29:59,385 - main - INFO - 
==================================================
2025-05-01 11:29:59,385 - main - INFO - Running configuration 418/756:
2025-05-01 11:29:59,385 - main - INFO - Model: ViT-B-16
2025-05-01 11:29:59,385 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:29:59,385 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:29:59,385 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:29:59,385 - main - INFO - ==================================================
2025-05-01 11:29:59,385 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_418 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_418
2025-05-01 11:29:59,385 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_418 - INFO - Config: {
  "id": 418,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:00,026 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:30:00,027 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 418,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:00,027 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:30:00,715 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:00,716 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 11:30:00,928 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 346.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:00,929 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:01,654 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:01,655 - main - ERROR - Error running configuration 418: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 346.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:01,655 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 346.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:01,656 - main - INFO - 
==================================================
2025-05-01 11:30:01,656 - main - INFO - Running configuration 419/756:
2025-05-01 11:30:01,656 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:01,656 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:30:01,656 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:30:01,656 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:30:01,656 - main - INFO - ==================================================
2025-05-01 11:30:01,656 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_419 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_419
2025-05-01 11:30:01,656 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_419 - INFO - Config: {
  "id": 419,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:02,263 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:30:02,264 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 419,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:02,264 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:30:02,977 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:02,978 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 11:30:03,185 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 522.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:03,185 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:03,911 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:03,911 - main - ERROR - Error running configuration 419: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 522.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:03,911 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 522.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:03,912 - main - INFO - 
==================================================
2025-05-01 11:30:03,912 - main - INFO - Running configuration 420/756:
2025-05-01 11:30:03,912 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:03,912 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-01 11:30:03,912 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:30:03,912 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:30:03,912 - main - INFO - ==================================================
2025-05-01 11:30:03,913 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_420 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_420
2025-05-01 11:30:03,913 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_420 - INFO - Config: {
  "id": 420,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:04,549 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-01 11:30:04,549 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 420,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:04,550 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-01 11:30:05,246 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:05,246 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 11:30:05,394 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.24 GiB is allocated by PyTorch, and 387.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:05,394 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:06,112 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:06,112 - main - ERROR - Error running configuration 420: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.24 GiB is allocated by PyTorch, and 387.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:06,112 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 113, in forward
    x, _ = self.self_attention(x, x, x, need_weights=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 640.00 KiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.24 GiB is allocated by PyTorch, and 387.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:06,113 - main - INFO - 
==================================================
2025-05-01 11:30:06,114 - main - INFO - Running configuration 421/756:
2025-05-01 11:30:06,114 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:06,114 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:06,114 - main - INFO - Scheduler: StepLR
2025-05-01 11:30:06,114 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:30:06,114 - main - INFO - ==================================================
2025-05-01 11:30:06,114 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_421 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_421
2025-05-01 11:30:06,114 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_421 - INFO - Config: {
  "id": 421,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:06,743 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:06,743 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 421,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:06,743 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:30:06,744 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 11:30:06,980 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 525.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:06,980 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:07,142 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:30:07,142 - main - ERROR - Error running configuration 421: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 525.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:07,142 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 525.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:07,143 - main - INFO - 
==================================================
2025-05-01 11:30:07,143 - main - INFO - Running configuration 422/756:
2025-05-01 11:30:07,143 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:07,143 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:07,143 - main - INFO - Scheduler: StepLR
2025-05-01 11:30:07,143 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:30:07,143 - main - INFO - ==================================================
2025-05-01 11:30:07,144 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_422 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_422
2025-05-01 11:30:07,144 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_422 - INFO - Config: {
  "id": 422,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 422,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:07,768 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:07,976 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:30:07,977 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 11:30:08,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 387.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:08,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:08,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:30:08,488 - main - ERROR - Error running configuration 422: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 387.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:08,488 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 112, in forward
    x = self.ln_1(input)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 387.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:08,489 - main - INFO - 
==================================================
2025-05-01 11:30:08,490 - main - INFO - Running configuration 423/756:
2025-05-01 11:30:08,490 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:08,490 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:08,490 - main - INFO - Scheduler: StepLR
2025-05-01 11:30:08,490 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:30:08,490 - main - INFO - ==================================================
2025-05-01 11:30:08,490 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_423 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_423
2025-05-01 11:30:08,490 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_423 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:09,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:09,296 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-01 11:30:09,296 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 11:30:09,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:09,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:09,685 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:30:09,685 - main - ERROR - Error running configuration 423: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:09,685 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:09,686 - main - INFO - 
==================================================
2025-05-01 11:30:09,686 - main - INFO - Running configuration 424/756:
2025-05-01 11:30:09,687 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:09,687 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:09,687 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:30:09,687 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:30:09,687 - main - INFO - ==================================================
2025-05-01 11:30:09,687 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_424 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_424
2025-05-01 11:30:09,687 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_424 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:10,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:10,535 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-01 11:30:10,536 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:10,724 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:10,994 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:30:10,994 - main - ERROR - Error running configuration 424: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:10,994 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 119, in forward
    return x + y
           ~~^~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:10,995 - main - INFO - 
==================================================
2025-05-01 11:30:10,995 - main - INFO - Running configuration 425/756:
2025-05-01 11:30:10,995 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:10,995 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:10,995 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:30:10,995 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:30:10,995 - main - INFO - ==================================================
2025-05-01 11:30:10,995 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_425 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_425
2025-05-01 11:30:10,995 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_425 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:11,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:11,835 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.0000
2025-05-01 11:30:11,836 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:11,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:12,254 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:30:12,255 - main - ERROR - Error running configuration 425: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:12,255 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:12,256 - main - INFO - 
==================================================
2025-05-01 11:30:12,256 - main - INFO - Running configuration 426/756:
2025-05-01 11:30:12,256 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:12,256 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:12,256 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:30:12,256 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:30:12,256 - main - INFO - ==================================================
2025-05-01 11:30:12,256 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_426 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_426
2025-05-01 11:30:12,256 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_426 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:12,871 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:13,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 5, best validation accuracy: 0.0000
2025-05-01 11:30:13,093 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-01 11:30:13,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:13,485 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:30:13,485 - main - ERROR - Error running configuration 426: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,486 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 119, in forward
    return x + y
           ~~^~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:13,487 - main - INFO - 
==================================================
2025-05-01 11:30:13,487 - main - INFO - Running configuration 427/756:
2025-05-01 11:30:13,487 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:13,487 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:13,487 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:30:13,487 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:30:13,487 - main - INFO - ==================================================
2025-05-01 11:30:13,487 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_427 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_427
2025-05-01 11:30:13,487 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_427 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:14,082 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:14,302 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 6, best validation accuracy: 0.0000
2025-05-01 11:30:14,303 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,519 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:14,817 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:30:14,817 - main - ERROR - Error running configuration 427: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,817 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:14,818 - main - INFO - 
==================================================
2025-05-01 11:30:14,818 - main - INFO - Running configuration 428/756:
2025-05-01 11:30:14,818 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:14,818 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:14,818 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:30:14,818 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:30:14,818 - main - INFO - ==================================================
2025-05-01 11:30:14,819 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_428 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_428
2025-05-01 11:30:14,819 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_428 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:15,421 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:15,640 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 7, best validation accuracy: 0.0000
2025-05-01 11:30:15,641 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:15,808 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:16,106 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:30:16,107 - main - ERROR - Error running configuration 428: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:16,107 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 119, in forward
    return x + y
           ~~^~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:16,108 - main - INFO - 
==================================================
2025-05-01 11:30:16,108 - main - INFO - Running configuration 429/756:
2025-05-01 11:30:16,108 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:16,108 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:16,108 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:30:16,108 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:30:16,108 - main - INFO - ==================================================
2025-05-01 11:30:16,108 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_429 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_429
2025-05-01 11:30:16,108 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_429 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:16,763 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:16,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 8, best validation accuracy: 0.0000
2025-05-01 11:30:16,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,163 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:17,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:30:17,450 - main - ERROR - Error running configuration 429: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,450 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:17,451 - main - INFO - 
==================================================
2025-05-01 11:30:17,451 - main - INFO - Running configuration 430/756:
2025-05-01 11:30:17,451 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:17,451 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:17,451 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:30:17,451 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:30:17,451 - main - INFO - ==================================================
2025-05-01 11:30:17,451 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_430 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_430
2025-05-01 11:30:17,451 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_430 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:18,092 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 9, best validation accuracy: 0.0000
2025-05-01 11:30:18,297 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-01 11:30:18,441 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,442 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:18,730 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:30:18,730 - main - ERROR - Error running configuration 430: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,730 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 119, in forward
    return x + y
           ~~^~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:18,731 - main - INFO - 
==================================================
2025-05-01 11:30:18,732 - main - INFO - Running configuration 431/756:
2025-05-01 11:30:18,732 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:18,732 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:18,732 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:30:18,732 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:30:18,732 - main - INFO - ==================================================
2025-05-01 11:30:18,732 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_431 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_431
2025-05-01 11:30:18,732 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_431 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:19,346 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 10, best validation accuracy: 0.0000
2025-05-01 11:30:19,565 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-01 11:30:19,873 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:19,874 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:20,134 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:30:20,134 - main - ERROR - Error running configuration 431: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:20,134 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 118, in forward
    y = self.mlp(y)
        ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.10 GiB is allocated by PyTorch, and 524.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:20,135 - main - INFO - 
==================================================
2025-05-01 11:30:20,135 - main - INFO - Running configuration 432/756:
2025-05-01 11:30:20,135 - main - INFO - Model: ViT-B-16
2025-05-01 11:30:20,136 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-01 11:30:20,136 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:30:20,136 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:30:20,136 - main - INFO - ==================================================
2025-05-01 11:30:20,136 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_432 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_432
2025-05-01 11:30:20,136 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_432 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,750 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:30:20,751 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-01 11:30:20,978 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 11, best validation accuracy: 0.0000
2025-05-01 11:30:20,979 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-01 11:30:21,126 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,127 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-01 11:30:21,433 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:30:21,433 - main - ERROR - Error running configuration 432: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,433 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 298, in forward
    x = self.encoder(x)
        ^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 157, in forward
    return self.ln(self.layers(self.dropout(input)))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/vision_transformer.py", line 119, in forward
    return x + y
           ~~^~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 4.92 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.22 GiB is allocated by PyTorch, and 400.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:21,435 - main - INFO - 
==================================================
2025-05-01 11:30:21,435 - main - INFO - Running configuration 433/756:
2025-05-01 11:30:21,435 - main - INFO - Model: DenseNet121
2025-05-01 11:30:21,435 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:30:21,435 - main - INFO - Scheduler: StepLR
2025-05-01 11:30:21,435 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:30:21,435 - main - INFO - ==================================================
2025-05-01 11:30:21,435 - training.model_DenseNet121_opt_SGD_lr_0.01_id_433 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_433
2025-05-01 11:30:21,435 - training.model_DenseNet121_opt_SGD_lr_0.01_id_433 - INFO - Config: {
  "id": 433,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:21,989 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:30:21,989 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 433,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:30:21,989 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:30:21,991 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 1/20
2025-05-01 11:30:22,755 - training.model_DenseNet121_opt_SGD_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 12.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 434.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:22,755 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-01 11:30:22,807 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:30:22,808 - main - ERROR - Error running configuration 433: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 12.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 434.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:22,808 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1176, in train_model_with_resume
    loss.backward()
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 12.62 MiB is free. Including non-PyTorch memory, this process has 4.91 GiB memory in use. Process 756348 has 10.75 GiB memory in use. Of the allocated memory 4.18 GiB is allocated by PyTorch, and 434.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-01 11:30:22,809 - main - INFO - 
==================================================
2025-05-01 11:30:22,809 - main - INFO - Running configuration 434/756:
2025-05-01 11:30:22,809 - main - INFO - Model: DenseNet121
2025-05-01 11:30:22,809 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:30:22,809 - main - INFO - Scheduler: StepLR
2025-05-01 11:30:22,809 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:30:22,809 - main - INFO - ==================================================
2025-05-01 11:30:22,809 - training.model_DenseNet121_opt_SGD_lr_0.01_id_434 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_434
2025-05-01 11:30:22,809 - training.model_DenseNet121_opt_SGD_lr_0.01_id_434 - INFO - Config: {
  "id": 434,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:23,091 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:30:23,091 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 434,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:30:23,091 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:30:23,937 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-01 11:30:23,939 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 2/20
2025-05-01 11:30:54,311 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:31:32,666 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.9340
2025-05-01 11:31:32,697 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 2 completed in 68.76s - Train Loss: 0.0244, Train Acc: 0.8325, Val Loss: 0.0150, Val Acc: 0.9340
2025-05-01 11:31:32,766 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:31:32,767 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 3/20
2025-05-01 11:32:02,981 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:32:41,024 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9340 to 0.9640
2025-05-01 11:32:41,069 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 3 completed in 68.30s - Train Loss: 0.0128, Train Acc: 0.9513, Val Loss: 0.0106, Val Acc: 0.9640
2025-05-01 11:32:41,139 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:32:41,139 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 4/20
2025-05-01 11:33:10,008 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:33:44,622 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9640 to 0.9734
2025-05-01 11:33:44,667 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 4 completed in 63.53s - Train Loss: 0.0094, Train Acc: 0.9796, Val Loss: 0.0094, Val Acc: 0.9734
2025-05-01 11:33:44,734 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:33:44,734 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 5/20
2025-05-01 11:34:10,933 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:34:46,102 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9734 to 0.9786
2025-05-01 11:34:46,141 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 5 completed in 61.41s - Train Loss: 0.0080, Train Acc: 0.9852, Val Loss: 0.0084, Val Acc: 0.9786
2025-05-01 11:34:46,209 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:34:46,209 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 6/20
2025-05-01 11:35:12,243 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:35:47,626 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9786 to 0.9794
2025-05-01 11:35:47,670 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 6 completed in 61.46s - Train Loss: 0.0076, Train Acc: 0.9871, Val Loss: 0.0085, Val Acc: 0.9794
2025-05-01 11:35:47,736 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:35:47,737 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 7/20
2025-05-01 11:36:13,506 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:36:48,152 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9794 to 0.9837
2025-05-01 11:36:48,195 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 7 completed in 60.46s - Train Loss: 0.0072, Train Acc: 0.9918, Val Loss: 0.0078, Val Acc: 0.9837
2025-05-01 11:36:48,260 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:36:48,261 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 8/20
2025-05-01 11:37:14,123 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:37:47,745 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation loss improved from inf to 0.0080
2025-05-01 11:37:47,783 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 8 completed in 59.52s - Train Loss: 0.0067, Train Acc: 0.9951, Val Loss: 0.0080, Val Acc: 0.9811
2025-05-01 11:37:47,849 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-01 11:37:47,849 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 9/20
2025-05-01 11:38:14,225 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:38:48,254 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.0080 to 0.0079
2025-05-01 11:38:48,294 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 9 completed in 60.44s - Train Loss: 0.0066, Train Acc: 0.9949, Val Loss: 0.0079, Val Acc: 0.9820
2025-05-01 11:38:48,359 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-01 11:38:48,360 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 10/20
2025-05-01 11:39:14,089 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:39:48,229 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9837 to 0.9846
2025-05-01 11:39:48,269 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 10 completed in 59.91s - Train Loss: 0.0064, Train Acc: 0.9959, Val Loss: 0.0075, Val Acc: 0.9846
2025-05-01 11:39:48,349 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-01 11:39:48,349 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 11/20
2025-05-01 11:40:13,288 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:40:48,197 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.0079 to 0.0078
2025-05-01 11:40:48,237 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 11 completed in 59.89s - Train Loss: 0.0062, Train Acc: 0.9972, Val Loss: 0.0078, Val Acc: 0.9846
2025-05-01 11:40:48,303 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-01 11:40:48,303 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 12/20
2025-05-01 11:41:14,059 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:41:48,750 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.0078 to 0.0077
2025-05-01 11:41:48,789 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 12 completed in 60.49s - Train Loss: 0.0061, Train Acc: 0.9979, Val Loss: 0.0077, Val Acc: 0.9837
2025-05-01 11:41:48,858 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-01 11:41:48,859 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 13/20
2025-05-01 11:42:14,113 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 11:42:47,986 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.0077 to 0.0074
2025-05-01 11:42:48,031 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 13 completed in 59.17s - Train Loss: 0.0060, Train Acc: 0.9994, Val Loss: 0.0074, Val Acc: 0.9846
2025-05-01 11:42:48,096 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-01 11:42:48,097 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 14/20
2025-05-01 11:43:14,157 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 11:43:48,731 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 14 completed in 60.63s - Train Loss: 0.0062, Train Acc: 0.9976, Val Loss: 0.0076, Val Acc: 0.9846
2025-05-01 11:43:48,796 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-01 11:43:48,797 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 15/20
2025-05-01 11:44:13,956 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 11:44:49,018 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 15 completed in 60.22s - Train Loss: 0.0060, Train Acc: 0.9989, Val Loss: 0.0076, Val Acc: 0.9837
2025-05-01 11:44:49,088 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-01 11:44:49,088 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 16/20
2025-05-01 11:45:14,832 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 11:45:48,632 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9846 to 0.9863
2025-05-01 11:45:48,673 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 16 completed in 59.58s - Train Loss: 0.0059, Train Acc: 0.9994, Val Loss: 0.0074, Val Acc: 0.9863
2025-05-01 11:45:48,743 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-01 11:45:48,744 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 17/20
2025-05-01 11:46:13,532 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 11:46:47,839 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 17 completed in 59.10s - Train Loss: 0.0060, Train Acc: 0.9994, Val Loss: 0.0075, Val Acc: 0.9828
2025-05-01 11:46:47,911 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-01 11:46:47,911 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 18/20
2025-05-01 11:47:14,096 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 11:47:48,248 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 18 completed in 60.34s - Train Loss: 0.0060, Train Acc: 0.9991, Val Loss: 0.0077, Val Acc: 0.9837
2025-05-01 11:47:48,322 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-01 11:47:48,322 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 19/20
2025-05-01 11:48:14,080 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 11:48:48,431 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9863 to 0.9880
2025-05-01 11:48:48,471 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 19 completed in 60.15s - Train Loss: 0.0060, Train Acc: 0.9991, Val Loss: 0.0072, Val Acc: 0.9880
2025-05-01 11:48:48,542 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-01 11:48:48,543 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Starting epoch 20/20
2025-05-01 11:49:15,104 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 11:49:49,413 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Epoch 20 completed in 60.87s - Train Loss: 0.0060, Train Acc: 0.9989, Val Loss: 0.0079, Val Acc: 0.9803
2025-05-01 11:49:49,481 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-01 11:49:49,482 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:49:49,486 - training.model_DenseNet121_opt_SGD_lr_0.01_id_434 - INFO - Starting model evaluation
2025-05-01 11:50:01,225 - training.model_DenseNet121_opt_SGD_lr_0.01_id_434 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9472
  Recall:    0.9406
  F1 Score:  0.9439
  IoU:       0.8937
  mAP:       0.9890
  AUC:       0.9946
2025-05-01 11:50:01,227 - training.model_DenseNet121_opt_SGD_lr_0.01_id_434 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_434/final_results.json
2025-05-01 11:50:01,228 - training.model_DenseNet121_opt_SGD_lr_0.01_id_434 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_434/final_results.json
2025-05-01 11:50:01,228 - main - INFO - 
Summary for configuration 434:
2025-05-01 11:50:01,228 - main - INFO - Accuracy: 0.9683
2025-05-01 11:50:01,229 - main - INFO - Precision: 0.9472
2025-05-01 11:50:01,229 - main - INFO - Recall: 0.9406
2025-05-01 11:50:01,229 - main - INFO - F1 Score: 0.9439
2025-05-01 11:50:01,229 - main - INFO - IoU: 0.8937
2025-05-01 11:50:01,229 - main - INFO - mAP: 0.9890
2025-05-01 11:50:01,229 - main - INFO - AUC: 0.9946
2025-05-01 11:50:01,229 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:50:01,229 - main - INFO - 
==================================================
2025-05-01 11:50:01,229 - main - INFO - Running configuration 435/756:
2025-05-01 11:50:01,229 - main - INFO - Model: DenseNet121
2025-05-01 11:50:01,229 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:50:01,229 - main - INFO - Scheduler: StepLR
2025-05-01 11:50:01,229 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:50:01,229 - main - INFO - ==================================================
2025-05-01 11:50:01,229 - training.model_DenseNet121_opt_SGD_lr_0.01_id_435 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_435
2025-05-01 11:50:01,229 - training.model_DenseNet121_opt_SGD_lr_0.01_id_435 - INFO - Config: {
  "id": 435,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:50:01,629 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:50:01,629 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 435,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:50:01,629 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:50:02,408 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:50:02,410 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:50:02,415 - training.model_DenseNet121_opt_SGD_lr_0.01_id_435 - INFO - Starting model evaluation
2025-05-01 11:50:14,623 - training.model_DenseNet121_opt_SGD_lr_0.01_id_435 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9443
  Recall:    0.9476
  F1 Score:  0.9459
  IoU:       0.8974
  mAP:       0.9891
  AUC:       0.9951
2025-05-01 11:50:14,625 - training.model_DenseNet121_opt_SGD_lr_0.01_id_435 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_435/final_results.json
2025-05-01 11:50:14,627 - training.model_DenseNet121_opt_SGD_lr_0.01_id_435 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_435/final_results.json
2025-05-01 11:50:14,627 - main - INFO - 
Summary for configuration 435:
2025-05-01 11:50:14,627 - main - INFO - Accuracy: 0.9693
2025-05-01 11:50:14,627 - main - INFO - Precision: 0.9443
2025-05-01 11:50:14,627 - main - INFO - Recall: 0.9476
2025-05-01 11:50:14,627 - main - INFO - F1 Score: 0.9459
2025-05-01 11:50:14,627 - main - INFO - IoU: 0.8974
2025-05-01 11:50:14,627 - main - INFO - mAP: 0.9891
2025-05-01 11:50:14,627 - main - INFO - AUC: 0.9951
2025-05-01 11:50:14,627 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:50:14,627 - main - INFO - 
==================================================
2025-05-01 11:50:14,627 - main - INFO - Running configuration 436/756:
2025-05-01 11:50:14,627 - main - INFO - Model: DenseNet121
2025-05-01 11:50:14,627 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:50:14,627 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:50:14,627 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:50:14,627 - main - INFO - ==================================================
2025-05-01 11:50:14,627 - training.model_DenseNet121_opt_SGD_lr_0.01_id_436 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_436
2025-05-01 11:50:14,627 - training.model_DenseNet121_opt_SGD_lr_0.01_id_436 - INFO - Config: {
  "id": 436,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:50:14,978 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:50:14,979 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 436,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:50:14,979 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:50:15,759 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:50:15,761 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:50:15,766 - training.model_DenseNet121_opt_SGD_lr_0.01_id_436 - INFO - Starting model evaluation
2025-05-01 11:50:27,723 - training.model_DenseNet121_opt_SGD_lr_0.01_id_436 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9541
  Recall:    0.9441
  F1 Score:  0.9490
  IoU:       0.9030
  mAP:       0.9891
  AUC:       0.9945
2025-05-01 11:50:27,724 - training.model_DenseNet121_opt_SGD_lr_0.01_id_436 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_436/final_results.json
2025-05-01 11:50:27,726 - training.model_DenseNet121_opt_SGD_lr_0.01_id_436 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_436/final_results.json
2025-05-01 11:50:27,726 - main - INFO - 
Summary for configuration 436:
2025-05-01 11:50:27,726 - main - INFO - Accuracy: 0.9713
2025-05-01 11:50:27,726 - main - INFO - Precision: 0.9541
2025-05-01 11:50:27,726 - main - INFO - Recall: 0.9441
2025-05-01 11:50:27,726 - main - INFO - F1 Score: 0.9490
2025-05-01 11:50:27,726 - main - INFO - IoU: 0.9030
2025-05-01 11:50:27,726 - main - INFO - mAP: 0.9891
2025-05-01 11:50:27,726 - main - INFO - AUC: 0.9945
2025-05-01 11:50:27,726 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:50:27,726 - main - INFO - 
==================================================
2025-05-01 11:50:27,726 - main - INFO - Running configuration 437/756:
2025-05-01 11:50:27,726 - main - INFO - Model: DenseNet121
2025-05-01 11:50:27,726 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:50:27,726 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:50:27,726 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:50:27,726 - main - INFO - ==================================================
2025-05-01 11:50:27,726 - training.model_DenseNet121_opt_SGD_lr_0.01_id_437 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_437
2025-05-01 11:50:27,726 - training.model_DenseNet121_opt_SGD_lr_0.01_id_437 - INFO - Config: {
  "id": 437,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:50:28,115 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:50:28,115 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 437,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:50:28,115 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:50:28,893 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:50:28,895 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:50:28,899 - training.model_DenseNet121_opt_SGD_lr_0.01_id_437 - INFO - Starting model evaluation
2025-05-01 11:50:40,709 - training.model_DenseNet121_opt_SGD_lr_0.01_id_437 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9441
  Recall:    0.9441
  F1 Score:  0.9441
  IoU:       0.8940
  mAP:       0.9893
  AUC:       0.9951
2025-05-01 11:50:40,711 - training.model_DenseNet121_opt_SGD_lr_0.01_id_437 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_437/final_results.json
2025-05-01 11:50:40,712 - training.model_DenseNet121_opt_SGD_lr_0.01_id_437 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_437/final_results.json
2025-05-01 11:50:40,712 - main - INFO - 
Summary for configuration 437:
2025-05-01 11:50:40,712 - main - INFO - Accuracy: 0.9683
2025-05-01 11:50:40,712 - main - INFO - Precision: 0.9441
2025-05-01 11:50:40,712 - main - INFO - Recall: 0.9441
2025-05-01 11:50:40,712 - main - INFO - F1 Score: 0.9441
2025-05-01 11:50:40,712 - main - INFO - IoU: 0.8940
2025-05-01 11:50:40,712 - main - INFO - mAP: 0.9893
2025-05-01 11:50:40,712 - main - INFO - AUC: 0.9951
2025-05-01 11:50:40,712 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:50:40,712 - main - INFO - 
==================================================
2025-05-01 11:50:40,712 - main - INFO - Running configuration 438/756:
2025-05-01 11:50:40,712 - main - INFO - Model: DenseNet121
2025-05-01 11:50:40,712 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:50:40,712 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 11:50:40,712 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:50:40,712 - main - INFO - ==================================================
2025-05-01 11:50:40,712 - training.model_DenseNet121_opt_SGD_lr_0.01_id_438 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_438
2025-05-01 11:50:40,712 - training.model_DenseNet121_opt_SGD_lr_0.01_id_438 - INFO - Config: {
  "id": 438,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:50:41,078 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:50:41,078 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 438,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:50:41,078 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:50:41,854 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:50:41,856 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:50:41,861 - training.model_DenseNet121_opt_SGD_lr_0.01_id_438 - INFO - Starting model evaluation
2025-05-01 11:50:54,862 - training.model_DenseNet121_opt_SGD_lr_0.01_id_438 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9505
  Recall:    0.9406
  F1 Score:  0.9455
  IoU:       0.8967
  mAP:       0.9898
  AUC:       0.9948
2025-05-01 11:50:54,864 - training.model_DenseNet121_opt_SGD_lr_0.01_id_438 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_438/final_results.json
2025-05-01 11:50:54,865 - training.model_DenseNet121_opt_SGD_lr_0.01_id_438 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_438/final_results.json
2025-05-01 11:50:54,865 - main - INFO - 
Summary for configuration 438:
2025-05-01 11:50:54,865 - main - INFO - Accuracy: 0.9693
2025-05-01 11:50:54,865 - main - INFO - Precision: 0.9505
2025-05-01 11:50:54,865 - main - INFO - Recall: 0.9406
2025-05-01 11:50:54,865 - main - INFO - F1 Score: 0.9455
2025-05-01 11:50:54,865 - main - INFO - IoU: 0.8967
2025-05-01 11:50:54,865 - main - INFO - mAP: 0.9898
2025-05-01 11:50:54,865 - main - INFO - AUC: 0.9948
2025-05-01 11:50:54,865 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:50:54,866 - main - INFO - 
==================================================
2025-05-01 11:50:54,866 - main - INFO - Running configuration 439/756:
2025-05-01 11:50:54,866 - main - INFO - Model: DenseNet121
2025-05-01 11:50:54,866 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:50:54,866 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:50:54,866 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:50:54,866 - main - INFO - ==================================================
2025-05-01 11:50:54,866 - training.model_DenseNet121_opt_SGD_lr_0.01_id_439 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_439
2025-05-01 11:50:54,866 - training.model_DenseNet121_opt_SGD_lr_0.01_id_439 - INFO - Config: {
  "id": 439,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:50:55,182 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:50:55,183 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 439,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:50:55,183 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:50:55,961 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:50:55,963 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:50:55,968 - training.model_DenseNet121_opt_SGD_lr_0.01_id_439 - INFO - Starting model evaluation
2025-05-01 11:51:08,591 - training.model_DenseNet121_opt_SGD_lr_0.01_id_439 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9504
  Recall:    0.9371
  F1 Score:  0.9437
  IoU:       0.8933
  mAP:       0.9896
  AUC:       0.9952
2025-05-01 11:51:08,593 - training.model_DenseNet121_opt_SGD_lr_0.01_id_439 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_439/final_results.json
2025-05-01 11:51:08,595 - training.model_DenseNet121_opt_SGD_lr_0.01_id_439 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_439/final_results.json
2025-05-01 11:51:08,595 - main - INFO - 
Summary for configuration 439:
2025-05-01 11:51:08,595 - main - INFO - Accuracy: 0.9683
2025-05-01 11:51:08,595 - main - INFO - Precision: 0.9504
2025-05-01 11:51:08,595 - main - INFO - Recall: 0.9371
2025-05-01 11:51:08,595 - main - INFO - F1 Score: 0.9437
2025-05-01 11:51:08,595 - main - INFO - IoU: 0.8933
2025-05-01 11:51:08,595 - main - INFO - mAP: 0.9896
2025-05-01 11:51:08,595 - main - INFO - AUC: 0.9952
2025-05-01 11:51:08,595 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:51:08,595 - main - INFO - 
==================================================
2025-05-01 11:51:08,595 - main - INFO - Running configuration 440/756:
2025-05-01 11:51:08,595 - main - INFO - Model: DenseNet121
2025-05-01 11:51:08,595 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:51:08,595 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:51:08,595 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:51:08,595 - main - INFO - ==================================================
2025-05-01 11:51:08,595 - training.model_DenseNet121_opt_SGD_lr_0.01_id_440 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_440
2025-05-01 11:51:08,595 - training.model_DenseNet121_opt_SGD_lr_0.01_id_440 - INFO - Config: {
  "id": 440,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:51:09,048 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:51:09,048 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 440,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:51:09,048 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:51:09,826 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:51:09,828 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:51:09,832 - training.model_DenseNet121_opt_SGD_lr_0.01_id_440 - INFO - Starting model evaluation
2025-05-01 11:51:22,105 - training.model_DenseNet121_opt_SGD_lr_0.01_id_440 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9573
  Recall:    0.9406
  F1 Score:  0.9489
  IoU:       0.9027
  mAP:       0.9898
  AUC:       0.9949
2025-05-01 11:51:22,107 - training.model_DenseNet121_opt_SGD_lr_0.01_id_440 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_440/final_results.json
2025-05-01 11:51:22,108 - training.model_DenseNet121_opt_SGD_lr_0.01_id_440 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_440/final_results.json
2025-05-01 11:51:22,108 - main - INFO - 
Summary for configuration 440:
2025-05-01 11:51:22,108 - main - INFO - Accuracy: 0.9713
2025-05-01 11:51:22,108 - main - INFO - Precision: 0.9573
2025-05-01 11:51:22,108 - main - INFO - Recall: 0.9406
2025-05-01 11:51:22,108 - main - INFO - F1 Score: 0.9489
2025-05-01 11:51:22,109 - main - INFO - IoU: 0.9027
2025-05-01 11:51:22,109 - main - INFO - mAP: 0.9898
2025-05-01 11:51:22,109 - main - INFO - AUC: 0.9949
2025-05-01 11:51:22,109 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:51:22,109 - main - INFO - 
==================================================
2025-05-01 11:51:22,109 - main - INFO - Running configuration 441/756:
2025-05-01 11:51:22,109 - main - INFO - Model: DenseNet121
2025-05-01 11:51:22,109 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:51:22,109 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 11:51:22,109 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:51:22,109 - main - INFO - ==================================================
2025-05-01 11:51:22,109 - training.model_DenseNet121_opt_SGD_lr_0.01_id_441 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_441
2025-05-01 11:51:22,109 - training.model_DenseNet121_opt_SGD_lr_0.01_id_441 - INFO - Config: {
  "id": 441,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:51:22,533 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:51:22,533 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 441,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:51:22,533 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:51:23,313 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:51:23,314 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:51:23,319 - training.model_DenseNet121_opt_SGD_lr_0.01_id_441 - INFO - Starting model evaluation
2025-05-01 11:51:35,459 - training.model_DenseNet121_opt_SGD_lr_0.01_id_441 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9474
  Recall:    0.9441
  F1 Score:  0.9457
  IoU:       0.8970
  mAP:       0.9887
  AUC:       0.9944
2025-05-01 11:51:35,461 - training.model_DenseNet121_opt_SGD_lr_0.01_id_441 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_441/final_results.json
2025-05-01 11:51:35,462 - training.model_DenseNet121_opt_SGD_lr_0.01_id_441 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_441/final_results.json
2025-05-01 11:51:35,462 - main - INFO - 
Summary for configuration 441:
2025-05-01 11:51:35,462 - main - INFO - Accuracy: 0.9693
2025-05-01 11:51:35,462 - main - INFO - Precision: 0.9474
2025-05-01 11:51:35,462 - main - INFO - Recall: 0.9441
2025-05-01 11:51:35,462 - main - INFO - F1 Score: 0.9457
2025-05-01 11:51:35,462 - main - INFO - IoU: 0.8970
2025-05-01 11:51:35,462 - main - INFO - mAP: 0.9887
2025-05-01 11:51:35,462 - main - INFO - AUC: 0.9944
2025-05-01 11:51:35,462 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:51:35,462 - main - INFO - 
==================================================
2025-05-01 11:51:35,462 - main - INFO - Running configuration 442/756:
2025-05-01 11:51:35,462 - main - INFO - Model: DenseNet121
2025-05-01 11:51:35,462 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:51:35,462 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:51:35,462 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:51:35,462 - main - INFO - ==================================================
2025-05-01 11:51:35,463 - training.model_DenseNet121_opt_SGD_lr_0.01_id_442 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_442
2025-05-01 11:51:35,463 - training.model_DenseNet121_opt_SGD_lr_0.01_id_442 - INFO - Config: {
  "id": 442,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:51:35,857 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:51:35,857 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 442,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:51:35,857 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:51:36,638 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:51:36,640 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:51:36,645 - training.model_DenseNet121_opt_SGD_lr_0.01_id_442 - INFO - Starting model evaluation
2025-05-01 11:51:48,826 - training.model_DenseNet121_opt_SGD_lr_0.01_id_442 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9474
  Recall:    0.9441
  F1 Score:  0.9457
  IoU:       0.8970
  mAP:       0.9892
  AUC:       0.9945
2025-05-01 11:51:48,828 - training.model_DenseNet121_opt_SGD_lr_0.01_id_442 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_442/final_results.json
2025-05-01 11:51:48,829 - training.model_DenseNet121_opt_SGD_lr_0.01_id_442 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_442/final_results.json
2025-05-01 11:51:48,829 - main - INFO - 
Summary for configuration 442:
2025-05-01 11:51:48,829 - main - INFO - Accuracy: 0.9693
2025-05-01 11:51:48,829 - main - INFO - Precision: 0.9474
2025-05-01 11:51:48,829 - main - INFO - Recall: 0.9441
2025-05-01 11:51:48,829 - main - INFO - F1 Score: 0.9457
2025-05-01 11:51:48,829 - main - INFO - IoU: 0.8970
2025-05-01 11:51:48,829 - main - INFO - mAP: 0.9892
2025-05-01 11:51:48,829 - main - INFO - AUC: 0.9945
2025-05-01 11:51:48,829 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:51:48,829 - main - INFO - 
==================================================
2025-05-01 11:51:48,829 - main - INFO - Running configuration 443/756:
2025-05-01 11:51:48,829 - main - INFO - Model: DenseNet121
2025-05-01 11:51:48,829 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:51:48,829 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:51:48,829 - main - INFO - Loss Function: FocalLoss
2025-05-01 11:51:48,829 - main - INFO - ==================================================
2025-05-01 11:51:48,830 - training.model_DenseNet121_opt_SGD_lr_0.01_id_443 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_443
2025-05-01 11:51:48,830 - training.model_DenseNet121_opt_SGD_lr_0.01_id_443 - INFO - Config: {
  "id": 443,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:51:49,207 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:51:49,207 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 443,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 11:51:49,207 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:51:50,136 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:51:50,138 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:51:50,143 - training.model_DenseNet121_opt_SGD_lr_0.01_id_443 - INFO - Starting model evaluation
2025-05-01 11:52:02,681 - training.model_DenseNet121_opt_SGD_lr_0.01_id_443 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9541
  Recall:    0.9441
  F1 Score:  0.9490
  IoU:       0.9030
  mAP:       0.9898
  AUC:       0.9949
2025-05-01 11:52:02,686 - training.model_DenseNet121_opt_SGD_lr_0.01_id_443 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_443/final_results.json
2025-05-01 11:52:02,687 - training.model_DenseNet121_opt_SGD_lr_0.01_id_443 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_443/final_results.json
2025-05-01 11:52:02,687 - main - INFO - 
Summary for configuration 443:
2025-05-01 11:52:02,687 - main - INFO - Accuracy: 0.9713
2025-05-01 11:52:02,687 - main - INFO - Precision: 0.9541
2025-05-01 11:52:02,687 - main - INFO - Recall: 0.9441
2025-05-01 11:52:02,687 - main - INFO - F1 Score: 0.9490
2025-05-01 11:52:02,687 - main - INFO - IoU: 0.9030
2025-05-01 11:52:02,687 - main - INFO - mAP: 0.9898
2025-05-01 11:52:02,687 - main - INFO - AUC: 0.9949
2025-05-01 11:52:02,687 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:52:02,687 - main - INFO - 
==================================================
2025-05-01 11:52:02,687 - main - INFO - Running configuration 444/756:
2025-05-01 11:52:02,687 - main - INFO - Model: DenseNet121
2025-05-01 11:52:02,687 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-01 11:52:02,687 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 11:52:02,687 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 11:52:02,687 - main - INFO - ==================================================
2025-05-01 11:52:02,688 - training.model_DenseNet121_opt_SGD_lr_0.01_id_444 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_444
2025-05-01 11:52:02,688 - training.model_DenseNet121_opt_SGD_lr_0.01_id_444 - INFO - Config: {
  "id": 444,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:52:03,040 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-01 11:52:03,040 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 444,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 11:52:03,040 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-01 11:52:03,818 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-01 11:52:03,820 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-01 11:52:03,825 - training.model_DenseNet121_opt_SGD_lr_0.01_id_444 - INFO - Starting model evaluation
2025-05-01 11:52:15,960 - training.model_DenseNet121_opt_SGD_lr_0.01_id_444 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9468
  Recall:    0.9336
  F1 Score:  0.9401
  IoU:       0.8870
  mAP:       0.9875
  AUC:       0.9939
2025-05-01 11:52:15,962 - training.model_DenseNet121_opt_SGD_lr_0.01_id_444 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_444/final_results.json
2025-05-01 11:52:15,963 - training.model_DenseNet121_opt_SGD_lr_0.01_id_444 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_444/final_results.json
2025-05-01 11:52:15,963 - main - INFO - 
Summary for configuration 444:
2025-05-01 11:52:15,963 - main - INFO - Accuracy: 0.9663
2025-05-01 11:52:15,963 - main - INFO - Precision: 0.9468
2025-05-01 11:52:15,963 - main - INFO - Recall: 0.9336
2025-05-01 11:52:15,963 - main - INFO - F1 Score: 0.9401
2025-05-01 11:52:15,963 - main - INFO - IoU: 0.8870
2025-05-01 11:52:15,963 - main - INFO - mAP: 0.9875
2025-05-01 11:52:15,963 - main - INFO - AUC: 0.9939
2025-05-01 11:52:15,963 - main - INFO - Training time: 1166.24 seconds
2025-05-01 11:52:15,963 - main - INFO - 
==================================================
2025-05-01 11:52:15,964 - main - INFO - Running configuration 445/756:
2025-05-01 11:52:15,964 - main - INFO - Model: DenseNet121
2025-05-01 11:52:15,964 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 11:52:15,964 - main - INFO - Scheduler: StepLR
2025-05-01 11:52:15,964 - main - INFO - Loss Function: CrossEntropy
2025-05-01 11:52:15,964 - main - INFO - ==================================================
2025-05-01 11:52:15,964 - training.model_DenseNet121_opt_SGD_lr_0.001_id_445 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_445
2025-05-01 11:52:15,964 - training.model_DenseNet121_opt_SGD_lr_0.001_id_445 - INFO - Config: {
  "id": 445,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:52:16,354 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 11:52:16,354 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 445,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 11:52:16,354 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-01 11:52:16,355 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 1/20
2025-05-01 11:52:42,140 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:53:16,157 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.9151
2025-05-01 11:53:16,188 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 1 completed in 59.83s - Train Loss: 0.5117, Train Acc: 0.8192, Val Loss: 0.4170, Val Acc: 0.9151
2025-05-01 11:53:16,255 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-01 11:53:16,255 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 2/20
2025-05-01 11:53:42,506 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:54:17,750 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9151 to 0.9563
2025-05-01 11:54:17,792 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 2 completed in 61.54s - Train Loss: 0.3947, Train Acc: 0.9350, Val Loss: 0.3658, Val Acc: 0.9563
2025-05-01 11:54:17,858 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-01 11:54:17,859 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 3/20
2025-05-01 11:54:43,733 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:55:17,453 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9563 to 0.9717
2025-05-01 11:55:17,491 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 3 completed in 59.63s - Train Loss: 0.3581, Train Acc: 0.9680, Val Loss: 0.3485, Val Acc: 0.9717
2025-05-01 11:55:17,554 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-01 11:55:17,555 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 4/20
2025-05-01 11:55:43,764 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:56:18,636 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation loss improved from inf to 0.3421
2025-05-01 11:56:18,676 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 4 completed in 61.12s - Train Loss: 0.3414, Train Acc: 0.9788, Val Loss: 0.3421, Val Acc: 0.9717
2025-05-01 11:56:18,741 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-01 11:56:18,742 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 5/20
2025-05-01 11:56:43,702 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:57:19,516 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9717 to 0.9828
2025-05-01 11:57:19,559 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 5 completed in 60.82s - Train Loss: 0.3340, Train Acc: 0.9848, Val Loss: 0.3328, Val Acc: 0.9828
2025-05-01 11:57:19,627 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-01 11:57:19,628 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 6/20
2025-05-01 11:57:46,499 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:58:23,213 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3421 to 0.3338
2025-05-01 11:58:23,257 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 6 completed in 63.63s - Train Loss: 0.3270, Train Acc: 0.9901, Val Loss: 0.3338, Val Acc: 0.9820
2025-05-01 11:58:23,332 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-01 11:58:23,333 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 7/20
2025-05-01 11:58:51,522 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:59:28,038 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3338 to 0.3300
2025-05-01 11:59:28,080 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 7 completed in 64.75s - Train Loss: 0.3235, Train Acc: 0.9940, Val Loss: 0.3300, Val Acc: 0.9828
2025-05-01 11:59:28,146 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-01 11:59:28,147 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 8/20
2025-05-01 11:59:57,434 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 12:00:35,991 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 8 completed in 67.84s - Train Loss: 0.3213, Train Acc: 0.9955, Val Loss: 0.3311, Val Acc: 0.9820
2025-05-01 12:00:36,065 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-01 12:00:36,066 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 9/20
2025-05-01 12:01:01,740 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 12:01:35,864 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 9 completed in 59.80s - Train Loss: 0.3193, Train Acc: 0.9974, Val Loss: 0.3305, Val Acc: 0.9820
2025-05-01 12:01:35,933 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-01 12:01:35,933 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 10/20
2025-05-01 12:02:01,504 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 12:02:35,370 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 10 completed in 59.44s - Train Loss: 0.3206, Train Acc: 0.9946, Val Loss: 0.3318, Val Acc: 0.9820
2025-05-01 12:02:35,442 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-01 12:02:35,442 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 11/20
2025-05-01 12:03:01,900 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 12:03:35,783 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9828 to 0.9837
2025-05-01 12:03:35,820 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 11 completed in 60.38s - Train Loss: 0.3183, Train Acc: 0.9972, Val Loss: 0.3294, Val Acc: 0.9837
2025-05-01 12:03:35,885 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-01 12:03:35,885 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 12/20
2025-05-01 12:04:01,876 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 12:04:35,699 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 12 completed in 59.81s - Train Loss: 0.3173, Train Acc: 0.9981, Val Loss: 0.3307, Val Acc: 0.9803
2025-05-01 12:04:35,778 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-01 12:04:35,778 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 13/20
2025-05-01 12:05:01,034 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 12:05:35,055 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9837 to 0.9854
2025-05-01 12:05:35,102 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 13 completed in 59.32s - Train Loss: 0.3174, Train Acc: 0.9976, Val Loss: 0.3281, Val Acc: 0.9854
2025-05-01 12:05:35,170 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-01 12:05:35,171 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 14/20
2025-05-01 12:06:00,085 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 12:06:34,978 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3300 to 0.3278
2025-05-01 12:06:35,017 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 14 completed in 59.85s - Train Loss: 0.3162, Train Acc: 0.9987, Val Loss: 0.3278, Val Acc: 0.9846
2025-05-01 12:06:35,081 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-01 12:06:35,082 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 15/20
2025-05-01 12:07:00,999 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 12:07:34,634 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3278 to 0.3276
2025-05-01 12:07:34,673 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 15 completed in 59.59s - Train Loss: 0.3168, Train Acc: 0.9981, Val Loss: 0.3276, Val Acc: 0.9854
2025-05-01 12:07:34,739 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-01 12:07:34,740 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 16/20
2025-05-01 12:08:01,186 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 12:08:35,551 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9854 to 0.9871
2025-05-01 12:08:35,591 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 16 completed in 60.85s - Train Loss: 0.3164, Train Acc: 0.9981, Val Loss: 0.3268, Val Acc: 0.9871
2025-05-01 12:08:35,652 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-01 12:08:35,653 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 17/20
2025-05-01 12:09:01,248 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 12:09:35,347 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3276 to 0.3270
2025-05-01 12:09:35,387 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 17 completed in 59.73s - Train Loss: 0.3166, Train Acc: 0.9989, Val Loss: 0.3270, Val Acc: 0.9871
2025-05-01 12:09:35,452 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-01 12:09:35,453 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 18/20
2025-05-01 12:10:01,034 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 12:10:34,519 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3270 to 0.3259
2025-05-01 12:10:34,558 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 18 completed in 59.11s - Train Loss: 0.3167, Train Acc: 0.9981, Val Loss: 0.3259, Val Acc: 0.9837
2025-05-01 12:10:34,624 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-01 12:10:34,625 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 19/20
2025-05-01 12:11:01,235 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 12:11:35,755 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 19 completed in 61.13s - Train Loss: 0.3164, Train Acc: 0.9985, Val Loss: 0.3290, Val Acc: 0.9811
2025-05-01 12:11:35,830 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-01 12:11:35,831 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Starting epoch 20/20
2025-05-01 12:12:03,558 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 12:12:39,427 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Epoch 20 completed in 63.60s - Train Loss: 0.3159, Train Acc: 0.9989, Val Loss: 0.3305, Val Acc: 0.9803
2025-05-01 12:12:39,493 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-01 12:12:39,493 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:12:39,499 - training.model_DenseNet121_opt_SGD_lr_0.001_id_445 - INFO - Starting model evaluation
2025-05-01 12:12:51,684 - training.model_DenseNet121_opt_SGD_lr_0.001_id_445 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9317
  Recall:    0.9545
  F1 Score:  0.9430
  IoU:       0.8922
  mAP:       0.9870
  AUC:       0.9931
2025-05-01 12:12:51,686 - training.model_DenseNet121_opt_SGD_lr_0.001_id_445 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_445/final_results.json
2025-05-01 12:12:51,687 - training.model_DenseNet121_opt_SGD_lr_0.001_id_445 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_445/final_results.json
2025-05-01 12:12:51,687 - main - INFO - 
Summary for configuration 445:
2025-05-01 12:12:51,687 - main - INFO - Accuracy: 0.9673
2025-05-01 12:12:51,687 - main - INFO - Precision: 0.9317
2025-05-01 12:12:51,687 - main - INFO - Recall: 0.9545
2025-05-01 12:12:51,687 - main - INFO - F1 Score: 0.9430
2025-05-01 12:12:51,687 - main - INFO - IoU: 0.8922
2025-05-01 12:12:51,687 - main - INFO - mAP: 0.9870
2025-05-01 12:12:51,687 - main - INFO - AUC: 0.9931
2025-05-01 12:12:51,687 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:12:51,687 - main - INFO - 
==================================================
2025-05-01 12:12:51,687 - main - INFO - Running configuration 446/756:
2025-05-01 12:12:51,687 - main - INFO - Model: DenseNet121
2025-05-01 12:12:51,687 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:12:51,687 - main - INFO - Scheduler: StepLR
2025-05-01 12:12:51,687 - main - INFO - Loss Function: FocalLoss
2025-05-01 12:12:51,687 - main - INFO - ==================================================
2025-05-01 12:12:51,688 - training.model_DenseNet121_opt_SGD_lr_0.001_id_446 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_446
2025-05-01 12:12:51,688 - training.model_DenseNet121_opt_SGD_lr_0.001_id_446 - INFO - Config: {
  "id": 446,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 12:12:52,067 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:12:52,067 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 446,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 12:12:52,067 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:12:52,503 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:12:52,505 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:12:52,510 - training.model_DenseNet121_opt_SGD_lr_0.001_id_446 - INFO - Starting model evaluation
2025-05-01 12:13:05,208 - training.model_DenseNet121_opt_SGD_lr_0.001_id_446 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9189
  Recall:    0.9510
  F1 Score:  0.9347
  IoU:       0.8774
  mAP:       0.9836
  AUC:       0.9901
2025-05-01 12:13:05,209 - training.model_DenseNet121_opt_SGD_lr_0.001_id_446 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_446/final_results.json
2025-05-01 12:13:05,211 - training.model_DenseNet121_opt_SGD_lr_0.001_id_446 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_446/final_results.json
2025-05-01 12:13:05,211 - main - INFO - 
Summary for configuration 446:
2025-05-01 12:13:05,211 - main - INFO - Accuracy: 0.9623
2025-05-01 12:13:05,211 - main - INFO - Precision: 0.9189
2025-05-01 12:13:05,211 - main - INFO - Recall: 0.9510
2025-05-01 12:13:05,211 - main - INFO - F1 Score: 0.9347
2025-05-01 12:13:05,211 - main - INFO - IoU: 0.8774
2025-05-01 12:13:05,211 - main - INFO - mAP: 0.9836
2025-05-01 12:13:05,211 - main - INFO - AUC: 0.9901
2025-05-01 12:13:05,211 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:13:05,211 - main - INFO - 
==================================================
2025-05-01 12:13:05,211 - main - INFO - Running configuration 447/756:
2025-05-01 12:13:05,211 - main - INFO - Model: DenseNet121
2025-05-01 12:13:05,211 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:13:05,211 - main - INFO - Scheduler: StepLR
2025-05-01 12:13:05,211 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 12:13:05,211 - main - INFO - ==================================================
2025-05-01 12:13:05,211 - training.model_DenseNet121_opt_SGD_lr_0.001_id_447 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_447
2025-05-01 12:13:05,211 - training.model_DenseNet121_opt_SGD_lr_0.001_id_447 - INFO - Config: {
  "id": 447,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 12:13:05,498 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:13:05,498 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 447,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 12:13:05,498 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:13:06,067 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:13:06,068 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:13:06,073 - training.model_DenseNet121_opt_SGD_lr_0.001_id_447 - INFO - Starting model evaluation
2025-05-01 12:13:19,267 - training.model_DenseNet121_opt_SGD_lr_0.001_id_447 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9254
  Recall:    0.9545
  F1 Score:  0.9398
  IoU:       0.8864
  mAP:       0.9852
  AUC:       0.9924
2025-05-01 12:13:19,269 - training.model_DenseNet121_opt_SGD_lr_0.001_id_447 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_447/final_results.json
2025-05-01 12:13:19,270 - training.model_DenseNet121_opt_SGD_lr_0.001_id_447 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_447/final_results.json
2025-05-01 12:13:19,270 - main - INFO - 
Summary for configuration 447:
2025-05-01 12:13:19,270 - main - INFO - Accuracy: 0.9653
2025-05-01 12:13:19,270 - main - INFO - Precision: 0.9254
2025-05-01 12:13:19,270 - main - INFO - Recall: 0.9545
2025-05-01 12:13:19,270 - main - INFO - F1 Score: 0.9398
2025-05-01 12:13:19,270 - main - INFO - IoU: 0.8864
2025-05-01 12:13:19,270 - main - INFO - mAP: 0.9852
2025-05-01 12:13:19,270 - main - INFO - AUC: 0.9924
2025-05-01 12:13:19,270 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:13:19,270 - main - INFO - 
==================================================
2025-05-01 12:13:19,270 - main - INFO - Running configuration 448/756:
2025-05-01 12:13:19,270 - main - INFO - Model: DenseNet121
2025-05-01 12:13:19,270 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:13:19,270 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 12:13:19,270 - main - INFO - Loss Function: CrossEntropy
2025-05-01 12:13:19,270 - main - INFO - ==================================================
2025-05-01 12:13:19,271 - training.model_DenseNet121_opt_SGD_lr_0.001_id_448 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_448
2025-05-01 12:13:19,271 - training.model_DenseNet121_opt_SGD_lr_0.001_id_448 - INFO - Config: {
  "id": 448,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 12:13:19,596 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:13:19,597 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 448,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 12:13:19,597 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:13:20,021 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:13:20,023 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:13:20,027 - training.model_DenseNet121_opt_SGD_lr_0.001_id_448 - INFO - Starting model evaluation
2025-05-01 12:13:32,732 - training.model_DenseNet121_opt_SGD_lr_0.001_id_448 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9226
  Recall:    0.9580
  F1 Score:  0.9400
  IoU:       0.8867
  mAP:       0.9855
  AUC:       0.9925
2025-05-01 12:13:32,733 - training.model_DenseNet121_opt_SGD_lr_0.001_id_448 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_448/final_results.json
2025-05-01 12:13:32,735 - training.model_DenseNet121_opt_SGD_lr_0.001_id_448 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_448/final_results.json
2025-05-01 12:13:32,735 - main - INFO - 
Summary for configuration 448:
2025-05-01 12:13:32,735 - main - INFO - Accuracy: 0.9653
2025-05-01 12:13:32,735 - main - INFO - Precision: 0.9226
2025-05-01 12:13:32,735 - main - INFO - Recall: 0.9580
2025-05-01 12:13:32,735 - main - INFO - F1 Score: 0.9400
2025-05-01 12:13:32,735 - main - INFO - IoU: 0.8867
2025-05-01 12:13:32,735 - main - INFO - mAP: 0.9855
2025-05-01 12:13:32,735 - main - INFO - AUC: 0.9925
2025-05-01 12:13:32,735 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:13:32,735 - main - INFO - 
==================================================
2025-05-01 12:13:32,735 - main - INFO - Running configuration 449/756:
2025-05-01 12:13:32,735 - main - INFO - Model: DenseNet121
2025-05-01 12:13:32,735 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:13:32,735 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 12:13:32,735 - main - INFO - Loss Function: FocalLoss
2025-05-01 12:13:32,735 - main - INFO - ==================================================
2025-05-01 12:13:32,735 - training.model_DenseNet121_opt_SGD_lr_0.001_id_449 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_449
2025-05-01 12:13:32,736 - training.model_DenseNet121_opt_SGD_lr_0.001_id_449 - INFO - Config: {
  "id": 449,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 12:13:33,021 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:13:33,021 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 449,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 12:13:33,021 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:13:33,510 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:13:33,512 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:13:33,517 - training.model_DenseNet121_opt_SGD_lr_0.001_id_449 - INFO - Starting model evaluation
2025-05-01 12:13:46,246 - training.model_DenseNet121_opt_SGD_lr_0.001_id_449 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9128
  Recall:    0.9510
  F1 Score:  0.9315
  IoU:       0.8718
  mAP:       0.9849
  AUC:       0.9920
2025-05-01 12:13:46,248 - training.model_DenseNet121_opt_SGD_lr_0.001_id_449 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_449/final_results.json
2025-05-01 12:13:46,249 - training.model_DenseNet121_opt_SGD_lr_0.001_id_449 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_449/final_results.json
2025-05-01 12:13:46,250 - main - INFO - 
Summary for configuration 449:
2025-05-01 12:13:46,250 - main - INFO - Accuracy: 0.9604
2025-05-01 12:13:46,250 - main - INFO - Precision: 0.9128
2025-05-01 12:13:46,250 - main - INFO - Recall: 0.9510
2025-05-01 12:13:46,250 - main - INFO - F1 Score: 0.9315
2025-05-01 12:13:46,250 - main - INFO - IoU: 0.8718
2025-05-01 12:13:46,250 - main - INFO - mAP: 0.9849
2025-05-01 12:13:46,250 - main - INFO - AUC: 0.9920
2025-05-01 12:13:46,250 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:13:46,250 - main - INFO - 
==================================================
2025-05-01 12:13:46,250 - main - INFO - Running configuration 450/756:
2025-05-01 12:13:46,250 - main - INFO - Model: DenseNet121
2025-05-01 12:13:46,250 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:13:46,250 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-01 12:13:46,250 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 12:13:46,250 - main - INFO - ==================================================
2025-05-01 12:13:46,250 - training.model_DenseNet121_opt_SGD_lr_0.001_id_450 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_450
2025-05-01 12:13:46,250 - training.model_DenseNet121_opt_SGD_lr_0.001_id_450 - INFO - Config: {
  "id": 450,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 12:13:46,534 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:13:46,534 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 450,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 12:13:46,534 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:13:47,043 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:13:47,045 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:13:47,049 - training.model_DenseNet121_opt_SGD_lr_0.001_id_450 - INFO - Starting model evaluation
2025-05-01 12:14:00,002 - training.model_DenseNet121_opt_SGD_lr_0.001_id_450 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9223
  Recall:    0.9545
  F1 Score:  0.9381
  IoU:       0.8835
  mAP:       0.9829
  AUC:       0.9899
2025-05-01 12:14:00,005 - training.model_DenseNet121_opt_SGD_lr_0.001_id_450 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_450/final_results.json
2025-05-01 12:14:00,007 - training.model_DenseNet121_opt_SGD_lr_0.001_id_450 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_450/final_results.json
2025-05-01 12:14:00,007 - main - INFO - 
Summary for configuration 450:
2025-05-01 12:14:00,007 - main - INFO - Accuracy: 0.9643
2025-05-01 12:14:00,007 - main - INFO - Precision: 0.9223
2025-05-01 12:14:00,007 - main - INFO - Recall: 0.9545
2025-05-01 12:14:00,007 - main - INFO - F1 Score: 0.9381
2025-05-01 12:14:00,007 - main - INFO - IoU: 0.8835
2025-05-01 12:14:00,007 - main - INFO - mAP: 0.9829
2025-05-01 12:14:00,007 - main - INFO - AUC: 0.9899
2025-05-01 12:14:00,007 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:14:00,007 - main - INFO - 
==================================================
2025-05-01 12:14:00,007 - main - INFO - Running configuration 451/756:
2025-05-01 12:14:00,007 - main - INFO - Model: DenseNet121
2025-05-01 12:14:00,007 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:14:00,007 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 12:14:00,007 - main - INFO - Loss Function: CrossEntropy
2025-05-01 12:14:00,007 - main - INFO - ==================================================
2025-05-01 12:14:00,007 - training.model_DenseNet121_opt_SGD_lr_0.001_id_451 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_451
2025-05-01 12:14:00,007 - training.model_DenseNet121_opt_SGD_lr_0.001_id_451 - INFO - Config: {
  "id": 451,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 12:14:00,378 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:14:00,378 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 451,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 12:14:00,378 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:14:00,799 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:14:00,801 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:14:00,806 - training.model_DenseNet121_opt_SGD_lr_0.001_id_451 - INFO - Starting model evaluation
2025-05-01 12:14:13,226 - training.model_DenseNet121_opt_SGD_lr_0.001_id_451 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9252
  Recall:    0.9510
  F1 Score:  0.9379
  IoU:       0.8831
  mAP:       0.9854
  AUC:       0.9912
2025-05-01 12:14:13,228 - training.model_DenseNet121_opt_SGD_lr_0.001_id_451 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_451/final_results.json
2025-05-01 12:14:13,229 - training.model_DenseNet121_opt_SGD_lr_0.001_id_451 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_451/final_results.json
2025-05-01 12:14:13,229 - main - INFO - 
Summary for configuration 451:
2025-05-01 12:14:13,229 - main - INFO - Accuracy: 0.9643
2025-05-01 12:14:13,229 - main - INFO - Precision: 0.9252
2025-05-01 12:14:13,229 - main - INFO - Recall: 0.9510
2025-05-01 12:14:13,229 - main - INFO - F1 Score: 0.9379
2025-05-01 12:14:13,229 - main - INFO - IoU: 0.8831
2025-05-01 12:14:13,229 - main - INFO - mAP: 0.9854
2025-05-01 12:14:13,229 - main - INFO - AUC: 0.9912
2025-05-01 12:14:13,229 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:14:13,229 - main - INFO - 
==================================================
2025-05-01 12:14:13,229 - main - INFO - Running configuration 452/756:
2025-05-01 12:14:13,229 - main - INFO - Model: DenseNet121
2025-05-01 12:14:13,229 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:14:13,229 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 12:14:13,229 - main - INFO - Loss Function: FocalLoss
2025-05-01 12:14:13,229 - main - INFO - ==================================================
2025-05-01 12:14:13,230 - training.model_DenseNet121_opt_SGD_lr_0.001_id_452 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_452
2025-05-01 12:14:13,230 - training.model_DenseNet121_opt_SGD_lr_0.001_id_452 - INFO - Config: {
  "id": 452,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 12:14:13,625 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:14:13,625 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 452,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 12:14:13,625 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:14:14,075 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:14:14,076 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:14:14,081 - training.model_DenseNet121_opt_SGD_lr_0.001_id_452 - INFO - Starting model evaluation
2025-05-01 12:14:26,583 - training.model_DenseNet121_opt_SGD_lr_0.001_id_452 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9220
  Recall:    0.9510
  F1 Score:  0.9363
  IoU:       0.8803
  mAP:       0.9837
  AUC:       0.9905
2025-05-01 12:14:26,586 - training.model_DenseNet121_opt_SGD_lr_0.001_id_452 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_452/final_results.json
2025-05-01 12:14:26,588 - training.model_DenseNet121_opt_SGD_lr_0.001_id_452 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_452/final_results.json
2025-05-01 12:14:26,588 - main - INFO - 
Summary for configuration 452:
2025-05-01 12:14:26,588 - main - INFO - Accuracy: 0.9633
2025-05-01 12:14:26,588 - main - INFO - Precision: 0.9220
2025-05-01 12:14:26,588 - main - INFO - Recall: 0.9510
2025-05-01 12:14:26,588 - main - INFO - F1 Score: 0.9363
2025-05-01 12:14:26,588 - main - INFO - IoU: 0.8803
2025-05-01 12:14:26,589 - main - INFO - mAP: 0.9837
2025-05-01 12:14:26,589 - main - INFO - AUC: 0.9905
2025-05-01 12:14:26,589 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:14:26,589 - main - INFO - 
==================================================
2025-05-01 12:14:26,589 - main - INFO - Running configuration 453/756:
2025-05-01 12:14:26,589 - main - INFO - Model: DenseNet121
2025-05-01 12:14:26,589 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:14:26,589 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-01 12:14:26,589 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 12:14:26,589 - main - INFO - ==================================================
2025-05-01 12:14:26,589 - training.model_DenseNet121_opt_SGD_lr_0.001_id_453 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_453
2025-05-01 12:14:26,589 - training.model_DenseNet121_opt_SGD_lr_0.001_id_453 - INFO - Config: {
  "id": 453,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 12:14:26,955 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:14:26,955 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 453,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 12:14:26,955 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:14:27,378 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:14:27,380 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:14:27,384 - training.model_DenseNet121_opt_SGD_lr_0.001_id_453 - INFO - Starting model evaluation
2025-05-01 12:14:40,117 - training.model_DenseNet121_opt_SGD_lr_0.001_id_453 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9218
  Recall:    0.9476
  F1 Score:  0.9345
  IoU:       0.8770
  mAP:       0.9831
  AUC:       0.9901
2025-05-01 12:14:40,119 - training.model_DenseNet121_opt_SGD_lr_0.001_id_453 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_453/final_results.json
2025-05-01 12:14:40,120 - training.model_DenseNet121_opt_SGD_lr_0.001_id_453 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_453/final_results.json
2025-05-01 12:14:40,120 - main - INFO - 
Summary for configuration 453:
2025-05-01 12:14:40,120 - main - INFO - Accuracy: 0.9623
2025-05-01 12:14:40,120 - main - INFO - Precision: 0.9218
2025-05-01 12:14:40,120 - main - INFO - Recall: 0.9476
2025-05-01 12:14:40,120 - main - INFO - F1 Score: 0.9345
2025-05-01 12:14:40,120 - main - INFO - IoU: 0.8770
2025-05-01 12:14:40,120 - main - INFO - mAP: 0.9831
2025-05-01 12:14:40,120 - main - INFO - AUC: 0.9901
2025-05-01 12:14:40,120 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:14:40,120 - main - INFO - 
==================================================
2025-05-01 12:14:40,120 - main - INFO - Running configuration 454/756:
2025-05-01 12:14:40,120 - main - INFO - Model: DenseNet121
2025-05-01 12:14:40,120 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:14:40,120 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 12:14:40,120 - main - INFO - Loss Function: CrossEntropy
2025-05-01 12:14:40,120 - main - INFO - ==================================================
2025-05-01 12:14:40,121 - training.model_DenseNet121_opt_SGD_lr_0.001_id_454 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_454
2025-05-01 12:14:40,121 - training.model_DenseNet121_opt_SGD_lr_0.001_id_454 - INFO - Config: {
  "id": 454,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 12:14:40,412 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:14:40,412 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 454,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 12:14:40,412 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:14:40,835 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:14:40,836 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:14:40,841 - training.model_DenseNet121_opt_SGD_lr_0.001_id_454 - INFO - Starting model evaluation
2025-05-01 12:14:53,718 - training.model_DenseNet121_opt_SGD_lr_0.001_id_454 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9218
  Recall:    0.9476
  F1 Score:  0.9345
  IoU:       0.8770
  mAP:       0.9840
  AUC:       0.9905
2025-05-01 12:14:53,720 - training.model_DenseNet121_opt_SGD_lr_0.001_id_454 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_454/final_results.json
2025-05-01 12:14:53,722 - training.model_DenseNet121_opt_SGD_lr_0.001_id_454 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_454/final_results.json
2025-05-01 12:14:53,722 - main - INFO - 
Summary for configuration 454:
2025-05-01 12:14:53,722 - main - INFO - Accuracy: 0.9623
2025-05-01 12:14:53,722 - main - INFO - Precision: 0.9218
2025-05-01 12:14:53,722 - main - INFO - Recall: 0.9476
2025-05-01 12:14:53,722 - main - INFO - F1 Score: 0.9345
2025-05-01 12:14:53,722 - main - INFO - IoU: 0.8770
2025-05-01 12:14:53,722 - main - INFO - mAP: 0.9840
2025-05-01 12:14:53,722 - main - INFO - AUC: 0.9905
2025-05-01 12:14:53,722 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:14:53,722 - main - INFO - 
==================================================
2025-05-01 12:14:53,722 - main - INFO - Running configuration 455/756:
2025-05-01 12:14:53,722 - main - INFO - Model: DenseNet121
2025-05-01 12:14:53,722 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:14:53,722 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 12:14:53,722 - main - INFO - Loss Function: FocalLoss
2025-05-01 12:14:53,722 - main - INFO - ==================================================
2025-05-01 12:14:53,722 - training.model_DenseNet121_opt_SGD_lr_0.001_id_455 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_455
2025-05-01 12:14:53,722 - training.model_DenseNet121_opt_SGD_lr_0.001_id_455 - INFO - Config: {
  "id": 455,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 12:14:54,103 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:14:54,103 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 455,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-01 12:14:54,104 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:14:54,527 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:14:54,529 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:14:54,533 - training.model_DenseNet121_opt_SGD_lr_0.001_id_455 - INFO - Starting model evaluation
2025-05-01 12:15:07,290 - training.model_DenseNet121_opt_SGD_lr_0.001_id_455 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9317
  Recall:    0.9545
  F1 Score:  0.9430
  IoU:       0.8922
  mAP:       0.9847
  AUC:       0.9907
2025-05-01 12:15:07,293 - training.model_DenseNet121_opt_SGD_lr_0.001_id_455 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_455/final_results.json
2025-05-01 12:15:07,295 - training.model_DenseNet121_opt_SGD_lr_0.001_id_455 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_455/final_results.json
2025-05-01 12:15:07,295 - main - INFO - 
Summary for configuration 455:
2025-05-01 12:15:07,295 - main - INFO - Accuracy: 0.9673
2025-05-01 12:15:07,295 - main - INFO - Precision: 0.9317
2025-05-01 12:15:07,295 - main - INFO - Recall: 0.9545
2025-05-01 12:15:07,295 - main - INFO - F1 Score: 0.9430
2025-05-01 12:15:07,295 - main - INFO - IoU: 0.8922
2025-05-01 12:15:07,296 - main - INFO - mAP: 0.9847
2025-05-01 12:15:07,296 - main - INFO - AUC: 0.9907
2025-05-01 12:15:07,296 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:15:07,296 - main - INFO - 
==================================================
2025-05-01 12:15:07,296 - main - INFO - Running configuration 456/756:
2025-05-01 12:15:07,296 - main - INFO - Model: DenseNet121
2025-05-01 12:15:07,296 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-01 12:15:07,296 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-01 12:15:07,296 - main - INFO - Loss Function: LabelSmoothing
2025-05-01 12:15:07,296 - main - INFO - ==================================================
2025-05-01 12:15:07,296 - training.model_DenseNet121_opt_SGD_lr_0.001_id_456 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001_id_456
2025-05-01 12:15:07,296 - training.model_DenseNet121_opt_SGD_lr_0.001_id_456 - INFO - Config: {
  "id": 456,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 12:15:07,664 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.001
2025-05-01 12:15:07,664 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 456,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-01 12:15:07,664 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.001/checkpoint.pt
2025-05-01 12:15:08,088 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9871
2025-05-01 12:15:08,090 - training.model_DenseNet121_opt_SGD_lr_0.001 - INFO - Training completed after 1223.07 seconds
2025-05-01 12:15:08,094 - training.model_DenseNet121_opt_SGD_lr_0.001_id_456 - INFO - Starting model evaluation
2025-05-01 12:15:20,983 - training.model_DenseNet121_opt_SGD_lr_0.001_id_456 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9128
  Recall:    0.9510
  F1 Score:  0.9315
  IoU:       0.8718
  mAP:       0.9818
  AUC:       0.9895
2025-05-01 12:15:20,985 - training.model_DenseNet121_opt_SGD_lr_0.001_id_456 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_456/final_results.json
2025-05-01 12:15:20,986 - training.model_DenseNet121_opt_SGD_lr_0.001_id_456 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.001_id_456/final_results.json
2025-05-01 12:15:20,986 - main - INFO - 
Summary for configuration 456:
2025-05-01 12:15:20,986 - main - INFO - Accuracy: 0.9604
2025-05-01 12:15:20,986 - main - INFO - Precision: 0.9128
2025-05-01 12:15:20,986 - main - INFO - Recall: 0.9510
2025-05-01 12:15:20,986 - main - INFO - F1 Score: 0.9315
2025-05-01 12:15:20,986 - main - INFO - IoU: 0.8718
2025-05-01 12:15:20,986 - main - INFO - mAP: 0.9818
2025-05-01 12:15:20,986 - main - INFO - AUC: 0.9895
2025-05-01 12:15:20,986 - main - INFO - Training time: 1223.07 seconds
2025-05-01 12:15:20,986 - main - INFO - 
==================================================
2025-05-01 12:15:20,986 - main - INFO - Running configuration 457/756:
2025-05-01 12:15:20,986 - main - INFO - Model: DenseNet121
2025-05-01 12:15:20,986 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-01 12:15:20,986 - main - INFO - Scheduler: StepLR
2025-05-01 12:15:20,986 - main - INFO - Loss Function: CrossEntropy
2025-05-01 12:15:20,986 - main - INFO - ==================================================
2025-05-01 12:15:20,987 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_457 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_457
2025-05-01 12:15:20,987 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_457 - INFO - Config: {
  "id": 457,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 12:15:21,346 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-01 12:15:21,346 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 457,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-01 12:15:21,346 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-01 12:15:21,348 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-01 12:15:47,924 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 12:16:18,030 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.7890
2025-05-01 12:16:18,059 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 1 completed in 56.71s - Train Loss: 0.6455, Train Acc: 0.6731, Val Loss: 0.5859, Val Acc: 0.7890
2025-05-01 12:16:18,137 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-01 12:16:18,137 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-01 12:16:37,266 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 12:17:03,480 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7890 to 0.8216
2025-05-01 12:17:03,524 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 2 completed in 45.39s - Train Loss: 0.5695, Train Acc: 0.7992, Val Loss: 0.5358, Val Acc: 0.8216
2025-05-01 12:17:03,600 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-01 12:17:03,600 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-01 12:17:22,578 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 12:17:49,966 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8216 to 0.8439
2025-05-01 12:17:50,010 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 3 completed in 46.41s - Train Loss: 0.5288, Train Acc: 0.8323, Val Loss: 0.5052, Val Acc: 0.8439
2025-05-01 12:17:50,089 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-01 12:17:50,089 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-01 12:18:09,631 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 12:18:35,943 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8439 to 0.8542
2025-05-01 12:18:35,998 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 4 completed in 45.91s - Train Loss: 0.5027, Train Acc: 0.8468, Val Loss: 0.4855, Val Acc: 0.8542
2025-05-01 12:18:36,074 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-01 12:18:36,074 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 00:05:01,630 - main - INFO - Starting training with device: cuda
2025-05-03 00:05:01,712 - main - INFO - Class distribution:
2025-05-03 00:05:01,713 - main - INFO - Training set: Counter({1: 2336, 0: 2326})
2025-05-03 00:05:01,713 - main - INFO - Validation set: Counter({0: 588, 1: 578})
2025-05-03 00:05:01,713 - main - INFO - Test set: Counter({0: 723, 1: 286})
2025-05-03 00:05:13,547 - main - INFO - Experiment status: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456] completed, 0 in progress, 389 pending
2025-05-03 00:05:13,547 - main - INFO - Configuration 1 already completed, skipping
2025-05-03 00:05:13,548 - main - INFO - Configuration 2 already completed, skipping
2025-05-03 00:05:13,548 - main - INFO - Configuration 3 already completed, skipping
2025-05-03 00:05:13,548 - main - INFO - Configuration 4 already completed, skipping
2025-05-03 00:05:13,548 - main - INFO - Configuration 5 already completed, skipping
2025-05-03 00:05:13,548 - main - INFO - Configuration 6 already completed, skipping
2025-05-03 00:05:13,549 - main - INFO - Configuration 7 already completed, skipping
2025-05-03 00:05:13,549 - main - INFO - Configuration 8 already completed, skipping
2025-05-03 00:05:13,549 - main - INFO - Configuration 9 already completed, skipping
2025-05-03 00:05:13,549 - main - INFO - Configuration 10 already completed, skipping
2025-05-03 00:05:13,550 - main - INFO - Configuration 11 already completed, skipping
2025-05-03 00:05:13,550 - main - INFO - Configuration 12 already completed, skipping
2025-05-03 00:05:13,550 - main - INFO - Configuration 13 already completed, skipping
2025-05-03 00:05:13,550 - main - INFO - Configuration 14 already completed, skipping
2025-05-03 00:05:13,551 - main - INFO - Configuration 15 already completed, skipping
2025-05-03 00:05:13,551 - main - INFO - Configuration 16 already completed, skipping
2025-05-03 00:05:13,551 - main - INFO - Configuration 17 already completed, skipping
2025-05-03 00:05:13,551 - main - INFO - Configuration 18 already completed, skipping
2025-05-03 00:05:13,551 - main - INFO - Configuration 19 already completed, skipping
2025-05-03 00:05:13,552 - main - INFO - Configuration 20 already completed, skipping
2025-05-03 00:05:13,552 - main - INFO - Configuration 21 already completed, skipping
2025-05-03 00:05:13,552 - main - INFO - Configuration 22 already completed, skipping
2025-05-03 00:05:13,552 - main - INFO - Configuration 23 already completed, skipping
2025-05-03 00:05:13,553 - main - INFO - Configuration 24 already completed, skipping
2025-05-03 00:05:13,553 - main - INFO - Configuration 25 already completed, skipping
2025-05-03 00:05:13,553 - main - INFO - Configuration 26 already completed, skipping
2025-05-03 00:05:13,553 - main - INFO - Configuration 27 already completed, skipping
2025-05-03 00:05:13,554 - main - INFO - Configuration 28 already completed, skipping
2025-05-03 00:05:13,554 - main - INFO - Configuration 29 already completed, skipping
2025-05-03 00:05:13,554 - main - INFO - Configuration 30 already completed, skipping
2025-05-03 00:05:13,554 - main - INFO - Configuration 31 already completed, skipping
2025-05-03 00:05:13,554 - main - INFO - Configuration 32 already completed, skipping
2025-05-03 00:05:13,555 - main - INFO - Configuration 33 already completed, skipping
2025-05-03 00:05:13,555 - main - INFO - Configuration 34 already completed, skipping
2025-05-03 00:05:13,555 - main - INFO - Configuration 35 already completed, skipping
2025-05-03 00:05:13,555 - main - INFO - Configuration 36 already completed, skipping
2025-05-03 00:05:13,555 - main - INFO - Configuration 37 already completed, skipping
2025-05-03 00:05:13,556 - main - INFO - Configuration 38 already completed, skipping
2025-05-03 00:05:13,556 - main - INFO - Configuration 39 already completed, skipping
2025-05-03 00:05:13,556 - main - INFO - Configuration 40 already completed, skipping
2025-05-03 00:05:13,556 - main - INFO - Configuration 41 already completed, skipping
2025-05-03 00:05:13,557 - main - INFO - Configuration 42 already completed, skipping
2025-05-03 00:05:13,557 - main - INFO - Configuration 43 already completed, skipping
2025-05-03 00:05:13,557 - main - INFO - Configuration 44 already completed, skipping
2025-05-03 00:05:13,557 - main - INFO - Configuration 45 already completed, skipping
2025-05-03 00:05:13,557 - main - INFO - Configuration 46 already completed, skipping
2025-05-03 00:05:13,558 - main - INFO - Configuration 47 already completed, skipping
2025-05-03 00:05:13,558 - main - INFO - Configuration 48 already completed, skipping
2025-05-03 00:05:13,558 - main - INFO - Configuration 49 already completed, skipping
2025-05-03 00:05:13,558 - main - INFO - Configuration 50 already completed, skipping
2025-05-03 00:05:13,559 - main - INFO - Configuration 51 already completed, skipping
2025-05-03 00:05:13,559 - main - INFO - Configuration 52 already completed, skipping
2025-05-03 00:05:13,559 - main - INFO - Configuration 53 already completed, skipping
2025-05-03 00:05:13,559 - main - INFO - Configuration 54 already completed, skipping
2025-05-03 00:05:13,560 - main - INFO - Configuration 55 already completed, skipping
2025-05-03 00:05:13,560 - main - INFO - Configuration 56 already completed, skipping
2025-05-03 00:05:13,560 - main - INFO - Configuration 57 already completed, skipping
2025-05-03 00:05:13,561 - main - INFO - Configuration 58 already completed, skipping
2025-05-03 00:05:13,561 - main - INFO - Configuration 59 already completed, skipping
2025-05-03 00:05:13,561 - main - INFO - Configuration 60 already completed, skipping
2025-05-03 00:05:13,561 - main - INFO - Configuration 61 already completed, skipping
2025-05-03 00:05:13,562 - main - INFO - Configuration 62 already completed, skipping
2025-05-03 00:05:13,562 - main - INFO - Configuration 63 already completed, skipping
2025-05-03 00:05:13,562 - main - INFO - Configuration 64 already completed, skipping
2025-05-03 00:05:13,562 - main - INFO - Configuration 65 already completed, skipping
2025-05-03 00:05:13,563 - main - INFO - Configuration 66 already completed, skipping
2025-05-03 00:05:13,563 - main - INFO - Configuration 67 already completed, skipping
2025-05-03 00:05:13,563 - main - INFO - Configuration 68 already completed, skipping
2025-05-03 00:05:13,563 - main - INFO - Configuration 69 already completed, skipping
2025-05-03 00:05:13,563 - main - INFO - Configuration 70 already completed, skipping
2025-05-03 00:05:13,564 - main - INFO - Configuration 71 already completed, skipping
2025-05-03 00:05:13,564 - main - INFO - Configuration 72 already completed, skipping
2025-05-03 00:05:13,564 - main - INFO - Configuration 73 already completed, skipping
2025-05-03 00:05:13,564 - main - INFO - Configuration 74 already completed, skipping
2025-05-03 00:05:13,565 - main - INFO - Configuration 75 already completed, skipping
2025-05-03 00:05:13,565 - main - INFO - Configuration 76 already completed, skipping
2025-05-03 00:05:13,565 - main - INFO - Configuration 77 already completed, skipping
2025-05-03 00:05:13,565 - main - INFO - Configuration 78 already completed, skipping
2025-05-03 00:05:13,565 - main - INFO - Configuration 79 already completed, skipping
2025-05-03 00:05:13,566 - main - INFO - Configuration 80 already completed, skipping
2025-05-03 00:05:13,566 - main - INFO - Configuration 81 already completed, skipping
2025-05-03 00:05:13,566 - main - INFO - Configuration 82 already completed, skipping
2025-05-03 00:05:13,566 - main - INFO - Configuration 83 already completed, skipping
2025-05-03 00:05:13,566 - main - INFO - Configuration 84 already completed, skipping
2025-05-03 00:05:13,567 - main - INFO - Configuration 85 already completed, skipping
2025-05-03 00:05:13,567 - main - INFO - Configuration 86 already completed, skipping
2025-05-03 00:05:13,567 - main - INFO - Configuration 87 already completed, skipping
2025-05-03 00:05:13,567 - main - INFO - Configuration 88 already completed, skipping
2025-05-03 00:05:13,568 - main - INFO - Configuration 89 already completed, skipping
2025-05-03 00:05:13,568 - main - INFO - Configuration 90 already completed, skipping
2025-05-03 00:05:13,568 - main - INFO - Configuration 91 already completed, skipping
2025-05-03 00:05:13,568 - main - INFO - Configuration 92 already completed, skipping
2025-05-03 00:05:13,569 - main - INFO - Configuration 93 already completed, skipping
2025-05-03 00:05:13,569 - main - INFO - Configuration 94 already completed, skipping
2025-05-03 00:05:13,569 - main - INFO - Configuration 95 already completed, skipping
2025-05-03 00:05:13,569 - main - INFO - Configuration 96 already completed, skipping
2025-05-03 00:05:13,570 - main - INFO - Configuration 97 already completed, skipping
2025-05-03 00:05:13,570 - main - INFO - Configuration 98 already completed, skipping
2025-05-03 00:05:13,570 - main - INFO - Configuration 99 already completed, skipping
2025-05-03 00:05:13,570 - main - INFO - Configuration 100 already completed, skipping
2025-05-03 00:05:13,571 - main - INFO - Configuration 101 already completed, skipping
2025-05-03 00:05:13,571 - main - INFO - Configuration 102 already completed, skipping
2025-05-03 00:05:13,571 - main - INFO - Configuration 103 already completed, skipping
2025-05-03 00:05:13,571 - main - INFO - Configuration 104 already completed, skipping
2025-05-03 00:05:13,572 - main - INFO - Configuration 105 already completed, skipping
2025-05-03 00:05:13,572 - main - INFO - Configuration 106 already completed, skipping
2025-05-03 00:05:13,572 - main - INFO - Configuration 107 already completed, skipping
2025-05-03 00:05:13,572 - main - INFO - Configuration 108 already completed, skipping
2025-05-03 00:05:13,573 - main - INFO - Configuration 109 already completed, skipping
2025-05-03 00:05:13,573 - main - INFO - Configuration 110 already completed, skipping
2025-05-03 00:05:13,573 - main - INFO - Configuration 111 already completed, skipping
2025-05-03 00:05:13,573 - main - INFO - Configuration 112 already completed, skipping
2025-05-03 00:05:13,574 - main - INFO - Configuration 113 already completed, skipping
2025-05-03 00:05:13,574 - main - INFO - Configuration 114 already completed, skipping
2025-05-03 00:05:13,574 - main - INFO - Configuration 115 already completed, skipping
2025-05-03 00:05:13,574 - main - INFO - Configuration 116 already completed, skipping
2025-05-03 00:05:13,574 - main - INFO - Configuration 117 already completed, skipping
2025-05-03 00:05:13,575 - main - INFO - Configuration 118 already completed, skipping
2025-05-03 00:05:13,575 - main - INFO - Configuration 119 already completed, skipping
2025-05-03 00:05:13,575 - main - INFO - Configuration 120 already completed, skipping
2025-05-03 00:05:13,575 - main - INFO - Configuration 121 already completed, skipping
2025-05-03 00:05:13,576 - main - INFO - Configuration 122 already completed, skipping
2025-05-03 00:05:13,576 - main - INFO - Configuration 123 already completed, skipping
2025-05-03 00:05:13,576 - main - INFO - Configuration 124 already completed, skipping
2025-05-03 00:05:13,576 - main - INFO - Configuration 125 already completed, skipping
2025-05-03 00:05:13,576 - main - INFO - Configuration 126 already completed, skipping
2025-05-03 00:05:13,577 - main - INFO - Configuration 127 already completed, skipping
2025-05-03 00:05:13,577 - main - INFO - Configuration 128 already completed, skipping
2025-05-03 00:05:13,577 - main - INFO - Configuration 129 already completed, skipping
2025-05-03 00:05:13,577 - main - INFO - Configuration 130 already completed, skipping
2025-05-03 00:05:13,578 - main - INFO - Configuration 131 already completed, skipping
2025-05-03 00:05:13,578 - main - INFO - Configuration 132 already completed, skipping
2025-05-03 00:05:13,578 - main - INFO - Configuration 133 already completed, skipping
2025-05-03 00:05:13,578 - main - INFO - Configuration 134 already completed, skipping
2025-05-03 00:05:13,579 - main - INFO - Configuration 135 already completed, skipping
2025-05-03 00:05:13,579 - main - INFO - Configuration 136 already completed, skipping
2025-05-03 00:05:13,579 - main - INFO - Configuration 137 already completed, skipping
2025-05-03 00:05:13,579 - main - INFO - Configuration 138 already completed, skipping
2025-05-03 00:05:13,579 - main - INFO - Configuration 139 already completed, skipping
2025-05-03 00:05:13,580 - main - INFO - Configuration 140 already completed, skipping
2025-05-03 00:05:13,580 - main - INFO - Configuration 141 already completed, skipping
2025-05-03 00:05:13,580 - main - INFO - Configuration 142 already completed, skipping
2025-05-03 00:05:13,580 - main - INFO - Configuration 143 already completed, skipping
2025-05-03 00:05:13,581 - main - INFO - Configuration 144 already completed, skipping
2025-05-03 00:05:13,581 - main - INFO - Configuration 145 already completed, skipping
2025-05-03 00:05:13,581 - main - INFO - Configuration 146 already completed, skipping
2025-05-03 00:05:13,581 - main - INFO - Configuration 147 already completed, skipping
2025-05-03 00:05:13,582 - main - INFO - Configuration 148 already completed, skipping
2025-05-03 00:05:13,582 - main - INFO - Configuration 149 already completed, skipping
2025-05-03 00:05:13,582 - main - INFO - Configuration 150 already completed, skipping
2025-05-03 00:05:13,582 - main - INFO - Configuration 151 already completed, skipping
2025-05-03 00:05:13,583 - main - INFO - Configuration 152 already completed, skipping
2025-05-03 00:05:13,583 - main - INFO - Configuration 153 already completed, skipping
2025-05-03 00:05:13,583 - main - INFO - Configuration 154 already completed, skipping
2025-05-03 00:05:13,583 - main - INFO - Configuration 155 already completed, skipping
2025-05-03 00:05:13,584 - main - INFO - Configuration 156 already completed, skipping
2025-05-03 00:05:13,584 - main - INFO - Configuration 157 already completed, skipping
2025-05-03 00:05:13,584 - main - INFO - Configuration 158 already completed, skipping
2025-05-03 00:05:13,584 - main - INFO - Configuration 159 already completed, skipping
2025-05-03 00:05:13,585 - main - INFO - Configuration 160 already completed, skipping
2025-05-03 00:05:13,585 - main - INFO - Configuration 161 already completed, skipping
2025-05-03 00:05:13,585 - main - INFO - Configuration 162 already completed, skipping
2025-05-03 00:05:13,585 - main - INFO - Configuration 163 already completed, skipping
2025-05-03 00:05:13,585 - main - INFO - Configuration 164 already completed, skipping
2025-05-03 00:05:13,586 - main - INFO - Configuration 165 already completed, skipping
2025-05-03 00:05:13,586 - main - INFO - Configuration 166 already completed, skipping
2025-05-03 00:05:13,586 - main - INFO - Configuration 167 already completed, skipping
2025-05-03 00:05:13,586 - main - INFO - Configuration 168 already completed, skipping
2025-05-03 00:05:13,587 - main - INFO - Configuration 169 already completed, skipping
2025-05-03 00:05:13,587 - main - INFO - Configuration 170 already completed, skipping
2025-05-03 00:05:13,587 - main - INFO - Configuration 171 already completed, skipping
2025-05-03 00:05:13,587 - main - INFO - Configuration 172 already completed, skipping
2025-05-03 00:05:13,588 - main - INFO - Configuration 173 already completed, skipping
2025-05-03 00:05:13,588 - main - INFO - Configuration 174 already completed, skipping
2025-05-03 00:05:13,588 - main - INFO - Configuration 175 already completed, skipping
2025-05-03 00:05:13,588 - main - INFO - Configuration 176 already completed, skipping
2025-05-03 00:05:13,588 - main - INFO - Configuration 177 already completed, skipping
2025-05-03 00:05:13,589 - main - INFO - Configuration 178 already completed, skipping
2025-05-03 00:05:13,589 - main - INFO - Configuration 179 already completed, skipping
2025-05-03 00:05:13,589 - main - INFO - Configuration 180 already completed, skipping
2025-05-03 00:05:13,589 - main - INFO - Configuration 181 already completed, skipping
2025-05-03 00:05:13,590 - main - INFO - Configuration 182 already completed, skipping
2025-05-03 00:05:13,590 - main - INFO - Configuration 183 already completed, skipping
2025-05-03 00:05:13,590 - main - INFO - Configuration 184 already completed, skipping
2025-05-03 00:05:13,590 - main - INFO - 
==================================================
2025-05-03 00:05:13,590 - main - INFO - Running configuration 185/756:
2025-05-03 00:05:13,590 - main - INFO - Model: ResNet101
2025-05-03 00:05:13,590 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 00:05:13,590 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 00:05:13,590 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:05:13,590 - main - INFO - ==================================================
2025-05-03 00:05:13,590 - training.model_ResNet101_opt_AdamW_lr_0.01_id_185 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_185
2025-05-03 00:05:13,590 - training.model_ResNet101_opt_AdamW_lr_0.01_id_185 - INFO - Config: {
  "id": 185,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:05:13,945 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-03 00:05:13,946 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 185,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:05:13,946 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 00:05:14,088 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:05:14,089 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1056.18 seconds
2025-05-03 00:05:14,092 - training.model_ResNet101_opt_AdamW_lr_0.01_id_185 - INFO - Starting model evaluation
2025-05-03 00:05:26,690 - training.model_ResNet101_opt_AdamW_lr_0.01_id_185 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2827
  AUC:       0.4934
2025-05-03 00:05:26,691 - training.model_ResNet101_opt_AdamW_lr_0.01_id_185 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_185/final_results.json
2025-05-03 00:05:26,692 - training.model_ResNet101_opt_AdamW_lr_0.01_id_185 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_185/final_results.json
2025-05-03 00:05:26,692 - main - INFO - 
Summary for configuration 185:
2025-05-03 00:05:26,692 - main - INFO - Accuracy: 0.7166
2025-05-03 00:05:26,692 - main - INFO - Precision: 0.0000
2025-05-03 00:05:26,692 - main - INFO - Recall: 0.0000
2025-05-03 00:05:26,692 - main - INFO - F1 Score: 0.0000
2025-05-03 00:05:26,692 - main - INFO - IoU: 0.0000
2025-05-03 00:05:26,692 - main - INFO - mAP: 0.2827
2025-05-03 00:05:26,692 - main - INFO - AUC: 0.4934
2025-05-03 00:05:26,693 - main - INFO - Training time: 1056.18 seconds
2025-05-03 00:05:26,693 - main - INFO - 
==================================================
2025-05-03 00:05:26,693 - main - INFO - Running configuration 186/756:
2025-05-03 00:05:26,693 - main - INFO - Model: ResNet101
2025-05-03 00:05:26,693 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 00:05:26,693 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 00:05:26,693 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:05:26,693 - main - INFO - ==================================================
2025-05-03 00:05:26,693 - training.model_ResNet101_opt_AdamW_lr_0.01_id_186 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_186
2025-05-03 00:05:26,693 - training.model_ResNet101_opt_AdamW_lr_0.01_id_186 - INFO - Config: {
  "id": 186,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:05:27,083 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-03 00:05:27,083 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 186,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:05:27,083 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 00:05:27,227 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:05:27,229 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1056.18 seconds
2025-05-03 00:05:27,232 - training.model_ResNet101_opt_AdamW_lr_0.01_id_186 - INFO - Starting model evaluation
2025-05-03 00:05:36,890 - training.model_ResNet101_opt_AdamW_lr_0.01_id_186 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2833
  AUC:       0.4921
2025-05-03 00:05:36,894 - training.model_ResNet101_opt_AdamW_lr_0.01_id_186 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_186/final_results.json
2025-05-03 00:05:36,895 - training.model_ResNet101_opt_AdamW_lr_0.01_id_186 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_186/final_results.json
2025-05-03 00:05:36,895 - main - INFO - 
Summary for configuration 186:
2025-05-03 00:05:36,895 - main - INFO - Accuracy: 0.7166
2025-05-03 00:05:36,895 - main - INFO - Precision: 0.0000
2025-05-03 00:05:36,895 - main - INFO - Recall: 0.0000
2025-05-03 00:05:36,895 - main - INFO - F1 Score: 0.0000
2025-05-03 00:05:36,895 - main - INFO - IoU: 0.0000
2025-05-03 00:05:36,895 - main - INFO - mAP: 0.2833
2025-05-03 00:05:36,895 - main - INFO - AUC: 0.4921
2025-05-03 00:05:36,895 - main - INFO - Training time: 1056.18 seconds
2025-05-03 00:05:36,895 - main - INFO - 
==================================================
2025-05-03 00:05:36,895 - main - INFO - Running configuration 187/756:
2025-05-03 00:05:36,895 - main - INFO - Model: ResNet101
2025-05-03 00:05:36,895 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 00:05:36,895 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 00:05:36,895 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:05:36,895 - main - INFO - ==================================================
2025-05-03 00:05:36,895 - training.model_ResNet101_opt_AdamW_lr_0.01_id_187 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01_id_187
2025-05-03 00:05:36,895 - training.model_ResNet101_opt_AdamW_lr_0.01_id_187 - INFO - Config: {
  "id": 187,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:05:37,180 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ResNet101_opt_AdamW_lr_0.01
2025-05-03 00:05:37,180 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 187,
  "model_name": "ResNet101",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:05:37,180 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ResNet101_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 00:05:37,328 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:05:37,330 - training.model_ResNet101_opt_AdamW_lr_0.01 - INFO - Training completed after 1056.18 seconds
2025-05-03 00:05:37,333 - training.model_ResNet101_opt_AdamW_lr_0.01_id_187 - INFO - Starting model evaluation
2025-05-03 00:05:47,045 - training.model_ResNet101_opt_AdamW_lr_0.01_id_187 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2827
  AUC:       0.4934
2025-05-03 00:05:47,049 - training.model_ResNet101_opt_AdamW_lr_0.01_id_187 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_187/final_results.json
2025-05-03 00:05:47,050 - training.model_ResNet101_opt_AdamW_lr_0.01_id_187 - INFO - Final results saved to model_results_v2/model_ResNet101_opt_AdamW_lr_0.01_id_187/final_results.json
2025-05-03 00:05:47,050 - main - INFO - 
Summary for configuration 187:
2025-05-03 00:05:47,050 - main - INFO - Accuracy: 0.7166
2025-05-03 00:05:47,050 - main - INFO - Precision: 0.0000
2025-05-03 00:05:47,050 - main - INFO - Recall: 0.0000
2025-05-03 00:05:47,050 - main - INFO - F1 Score: 0.0000
2025-05-03 00:05:47,050 - main - INFO - IoU: 0.0000
2025-05-03 00:05:47,050 - main - INFO - mAP: 0.2827
2025-05-03 00:05:47,050 - main - INFO - AUC: 0.4934
2025-05-03 00:05:47,050 - main - INFO - Training time: 1056.18 seconds
2025-05-03 00:05:47,050 - main - INFO - Configuration 188 already completed, skipping
2025-05-03 00:05:47,050 - main - INFO - Configuration 189 already completed, skipping
2025-05-03 00:05:47,050 - main - INFO - Configuration 190 already completed, skipping
2025-05-03 00:05:47,050 - main - INFO - Configuration 191 already completed, skipping
2025-05-03 00:05:47,051 - main - INFO - Configuration 192 already completed, skipping
2025-05-03 00:05:47,051 - main - INFO - Configuration 193 already completed, skipping
2025-05-03 00:05:47,051 - main - INFO - Configuration 194 already completed, skipping
2025-05-03 00:05:47,051 - main - INFO - Configuration 195 already completed, skipping
2025-05-03 00:05:47,051 - main - INFO - Configuration 196 already completed, skipping
2025-05-03 00:05:47,052 - main - INFO - Configuration 197 already completed, skipping
2025-05-03 00:05:47,052 - main - INFO - Configuration 198 already completed, skipping
2025-05-03 00:05:47,052 - main - INFO - Configuration 199 already completed, skipping
2025-05-03 00:05:47,052 - main - INFO - Configuration 200 already completed, skipping
2025-05-03 00:05:47,053 - main - INFO - Configuration 201 already completed, skipping
2025-05-03 00:05:47,053 - main - INFO - Configuration 202 already completed, skipping
2025-05-03 00:05:47,053 - main - INFO - Configuration 203 already completed, skipping
2025-05-03 00:05:47,053 - main - INFO - Configuration 204 already completed, skipping
2025-05-03 00:05:47,053 - main - INFO - Configuration 205 already completed, skipping
2025-05-03 00:05:47,054 - main - INFO - Configuration 206 already completed, skipping
2025-05-03 00:05:47,054 - main - INFO - Configuration 207 already completed, skipping
2025-05-03 00:05:47,054 - main - INFO - Configuration 208 already completed, skipping
2025-05-03 00:05:47,054 - main - INFO - Configuration 209 already completed, skipping
2025-05-03 00:05:47,055 - main - INFO - Configuration 210 already completed, skipping
2025-05-03 00:05:47,055 - main - INFO - Configuration 211 already completed, skipping
2025-05-03 00:05:47,055 - main - INFO - Configuration 212 already completed, skipping
2025-05-03 00:05:47,055 - main - INFO - Configuration 213 already completed, skipping
2025-05-03 00:05:47,055 - main - INFO - Configuration 214 already completed, skipping
2025-05-03 00:05:47,056 - main - INFO - Configuration 215 already completed, skipping
2025-05-03 00:05:47,056 - main - INFO - Configuration 216 already completed, skipping
2025-05-03 00:05:47,056 - main - INFO - Configuration 217 already completed, skipping
2025-05-03 00:05:47,056 - main - INFO - Configuration 218 already completed, skipping
2025-05-03 00:05:47,057 - main - INFO - Configuration 219 already completed, skipping
2025-05-03 00:05:47,057 - main - INFO - Configuration 220 already completed, skipping
2025-05-03 00:05:47,057 - main - INFO - Configuration 221 already completed, skipping
2025-05-03 00:05:47,057 - main - INFO - Configuration 222 already completed, skipping
2025-05-03 00:05:47,058 - main - INFO - Configuration 223 already completed, skipping
2025-05-03 00:05:47,058 - main - INFO - Configuration 224 already completed, skipping
2025-05-03 00:05:47,058 - main - INFO - Configuration 225 already completed, skipping
2025-05-03 00:05:47,058 - main - INFO - Configuration 226 already completed, skipping
2025-05-03 00:05:47,059 - main - INFO - Configuration 227 already completed, skipping
2025-05-03 00:05:47,059 - main - INFO - Configuration 228 already completed, skipping
2025-05-03 00:05:47,059 - main - INFO - Configuration 229 already completed, skipping
2025-05-03 00:05:47,059 - main - INFO - Configuration 230 already completed, skipping
2025-05-03 00:05:47,059 - main - INFO - Configuration 231 already completed, skipping
2025-05-03 00:05:47,060 - main - INFO - Configuration 232 already completed, skipping
2025-05-03 00:05:47,060 - main - INFO - Configuration 233 already completed, skipping
2025-05-03 00:05:47,061 - main - INFO - Configuration 234 already completed, skipping
2025-05-03 00:05:47,061 - main - INFO - Configuration 235 already completed, skipping
2025-05-03 00:05:47,061 - main - INFO - Configuration 236 already completed, skipping
2025-05-03 00:05:47,061 - main - INFO - Configuration 237 already completed, skipping
2025-05-03 00:05:47,061 - main - INFO - Configuration 238 already completed, skipping
2025-05-03 00:05:47,062 - main - INFO - Configuration 239 already completed, skipping
2025-05-03 00:05:47,062 - main - INFO - Configuration 240 already completed, skipping
2025-05-03 00:05:47,062 - main - INFO - Configuration 241 already completed, skipping
2025-05-03 00:05:47,062 - main - INFO - Configuration 242 already completed, skipping
2025-05-03 00:05:47,063 - main - INFO - Configuration 243 already completed, skipping
2025-05-03 00:05:47,063 - main - INFO - Configuration 244 already completed, skipping
2025-05-03 00:05:47,063 - main - INFO - Configuration 245 already completed, skipping
2025-05-03 00:05:47,063 - main - INFO - Configuration 246 already completed, skipping
2025-05-03 00:05:47,064 - main - INFO - Configuration 247 already completed, skipping
2025-05-03 00:05:47,064 - main - INFO - Configuration 248 already completed, skipping
2025-05-03 00:05:47,064 - main - INFO - Configuration 249 already completed, skipping
2025-05-03 00:05:47,064 - main - INFO - Configuration 250 already completed, skipping
2025-05-03 00:05:47,064 - main - INFO - Configuration 251 already completed, skipping
2025-05-03 00:05:47,065 - main - INFO - Configuration 252 already completed, skipping
2025-05-03 00:05:47,065 - main - INFO - Configuration 253 already completed, skipping
2025-05-03 00:05:47,065 - main - INFO - Configuration 254 already completed, skipping
2025-05-03 00:05:47,065 - main - INFO - Configuration 255 already completed, skipping
2025-05-03 00:05:47,066 - main - INFO - Configuration 256 already completed, skipping
2025-05-03 00:05:47,066 - main - INFO - Configuration 257 already completed, skipping
2025-05-03 00:05:47,066 - main - INFO - Configuration 258 already completed, skipping
2025-05-03 00:05:47,066 - main - INFO - Configuration 259 already completed, skipping
2025-05-03 00:05:47,067 - main - INFO - Configuration 260 already completed, skipping
2025-05-03 00:05:47,067 - main - INFO - Configuration 261 already completed, skipping
2025-05-03 00:05:47,067 - main - INFO - Configuration 262 already completed, skipping
2025-05-03 00:05:47,067 - main - INFO - Configuration 263 already completed, skipping
2025-05-03 00:05:47,067 - main - INFO - Configuration 264 already completed, skipping
2025-05-03 00:05:47,068 - main - INFO - Configuration 265 already completed, skipping
2025-05-03 00:05:47,068 - main - INFO - Configuration 266 already completed, skipping
2025-05-03 00:05:47,068 - main - INFO - Configuration 267 already completed, skipping
2025-05-03 00:05:47,068 - main - INFO - Configuration 268 already completed, skipping
2025-05-03 00:05:47,069 - main - INFO - Configuration 269 already completed, skipping
2025-05-03 00:05:47,069 - main - INFO - Configuration 270 already completed, skipping
2025-05-03 00:05:47,069 - main - INFO - Configuration 271 already completed, skipping
2025-05-03 00:05:47,069 - main - INFO - Configuration 272 already completed, skipping
2025-05-03 00:05:47,070 - main - INFO - Configuration 273 already completed, skipping
2025-05-03 00:05:47,070 - main - INFO - Configuration 274 already completed, skipping
2025-05-03 00:05:47,070 - main - INFO - Configuration 275 already completed, skipping
2025-05-03 00:05:47,070 - main - INFO - Configuration 276 already completed, skipping
2025-05-03 00:05:47,070 - main - INFO - Configuration 277 already completed, skipping
2025-05-03 00:05:47,071 - main - INFO - Configuration 278 already completed, skipping
2025-05-03 00:05:47,071 - main - INFO - Configuration 279 already completed, skipping
2025-05-03 00:05:47,071 - main - INFO - Configuration 280 already completed, skipping
2025-05-03 00:05:47,072 - main - INFO - Configuration 281 already completed, skipping
2025-05-03 00:05:47,072 - main - INFO - Configuration 282 already completed, skipping
2025-05-03 00:05:47,072 - main - INFO - Configuration 283 already completed, skipping
2025-05-03 00:05:47,072 - main - INFO - Configuration 284 already completed, skipping
2025-05-03 00:05:47,073 - main - INFO - Configuration 285 already completed, skipping
2025-05-03 00:05:47,073 - main - INFO - Configuration 286 already completed, skipping
2025-05-03 00:05:47,073 - main - INFO - Configuration 287 already completed, skipping
2025-05-03 00:05:47,073 - main - INFO - Configuration 288 already completed, skipping
2025-05-03 00:05:47,074 - main - INFO - Configuration 289 already completed, skipping
2025-05-03 00:05:47,074 - main - INFO - Configuration 290 already completed, skipping
2025-05-03 00:05:47,074 - main - INFO - Configuration 291 already completed, skipping
2025-05-03 00:05:47,074 - main - INFO - Configuration 292 already completed, skipping
2025-05-03 00:05:47,074 - main - INFO - Configuration 293 already completed, skipping
2025-05-03 00:05:47,075 - main - INFO - Configuration 294 already completed, skipping
2025-05-03 00:05:47,075 - main - INFO - Configuration 295 already completed, skipping
2025-05-03 00:05:47,075 - main - INFO - Configuration 296 already completed, skipping
2025-05-03 00:05:47,075 - main - INFO - Configuration 297 already completed, skipping
2025-05-03 00:05:47,075 - main - INFO - Configuration 298 already completed, skipping
2025-05-03 00:05:47,076 - main - INFO - Configuration 299 already completed, skipping
2025-05-03 00:05:47,076 - main - INFO - Configuration 300 already completed, skipping
2025-05-03 00:05:47,076 - main - INFO - Configuration 301 already completed, skipping
2025-05-03 00:05:47,076 - main - INFO - Configuration 302 already completed, skipping
2025-05-03 00:05:47,077 - main - INFO - Configuration 303 already completed, skipping
2025-05-03 00:05:47,077 - main - INFO - Configuration 304 already completed, skipping
2025-05-03 00:05:47,077 - main - INFO - Configuration 305 already completed, skipping
2025-05-03 00:05:47,077 - main - INFO - Configuration 306 already completed, skipping
2025-05-03 00:05:47,077 - main - INFO - Configuration 307 already completed, skipping
2025-05-03 00:05:47,078 - main - INFO - Configuration 308 already completed, skipping
2025-05-03 00:05:47,078 - main - INFO - Configuration 309 already completed, skipping
2025-05-03 00:05:47,078 - main - INFO - Configuration 310 already completed, skipping
2025-05-03 00:05:47,078 - main - INFO - Configuration 311 already completed, skipping
2025-05-03 00:05:47,079 - main - INFO - Configuration 312 already completed, skipping
2025-05-03 00:05:47,079 - main - INFO - Configuration 313 already completed, skipping
2025-05-03 00:05:47,079 - main - INFO - Configuration 314 already completed, skipping
2025-05-03 00:05:47,079 - main - INFO - Configuration 315 already completed, skipping
2025-05-03 00:05:47,080 - main - INFO - Configuration 316 already completed, skipping
2025-05-03 00:05:47,080 - main - INFO - Configuration 317 already completed, skipping
2025-05-03 00:05:47,080 - main - INFO - Configuration 318 already completed, skipping
2025-05-03 00:05:47,080 - main - INFO - Configuration 319 already completed, skipping
2025-05-03 00:05:47,080 - main - INFO - Configuration 320 already completed, skipping
2025-05-03 00:05:47,081 - main - INFO - Configuration 321 already completed, skipping
2025-05-03 00:05:47,081 - main - INFO - Configuration 322 already completed, skipping
2025-05-03 00:05:47,081 - main - INFO - Configuration 323 already completed, skipping
2025-05-03 00:05:47,081 - main - INFO - Configuration 324 already completed, skipping
2025-05-03 00:05:47,082 - main - INFO - Configuration 325 already completed, skipping
2025-05-03 00:05:47,083 - main - INFO - Configuration 326 already completed, skipping
2025-05-03 00:05:47,083 - main - INFO - Configuration 327 already completed, skipping
2025-05-03 00:05:47,083 - main - INFO - Configuration 328 already completed, skipping
2025-05-03 00:05:47,084 - main - INFO - Configuration 329 already completed, skipping
2025-05-03 00:05:47,084 - main - INFO - Configuration 330 already completed, skipping
2025-05-03 00:05:47,084 - main - INFO - Configuration 331 already completed, skipping
2025-05-03 00:05:47,085 - main - INFO - Configuration 332 already completed, skipping
2025-05-03 00:05:47,085 - main - INFO - Configuration 333 already completed, skipping
2025-05-03 00:05:47,085 - main - INFO - Configuration 334 already completed, skipping
2025-05-03 00:05:47,086 - main - INFO - Configuration 335 already completed, skipping
2025-05-03 00:05:47,086 - main - INFO - Configuration 336 already completed, skipping
2025-05-03 00:05:47,086 - main - INFO - Configuration 337 already completed, skipping
2025-05-03 00:05:47,086 - main - INFO - 
==================================================
2025-05-03 00:05:47,086 - main - INFO - Running configuration 338/756:
2025-05-03 00:05:47,087 - main - INFO - Model: ViT-B-16
2025-05-03 00:05:47,087 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 00:05:47,087 - main - INFO - Scheduler: StepLR
2025-05-03 00:05:47,087 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:05:47,087 - main - INFO - ==================================================
2025-05-03 00:05:47,087 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_338 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001_id_338
2025-05-03 00:05:47,087 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_338 - INFO - Config: {
  "id": 338,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:05:47,558 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.001
2025-05-03 00:05:47,561 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 338,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:05:47,561 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 00:05:47,769 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9889
2025-05-03 00:05:47,770 - training.model_ViT-B-16_opt_SGD_lr_0.001 - INFO - Training completed after 1586.64 seconds
2025-05-03 00:05:47,771 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_338 - INFO - Starting model evaluation
2025-05-03 00:05:57,164 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_338 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9333
  Recall:    0.9301
  F1 Score:  0.9317
  IoU:       0.8721
  mAP:       0.9829
  AUC:       0.9925
2025-05-03 00:05:57,166 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_338 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_338/final_results.json
2025-05-03 00:05:57,167 - training.model_ViT-B-16_opt_SGD_lr_0.001_id_338 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.001_id_338/final_results.json
2025-05-03 00:05:57,167 - main - INFO - 
Summary for configuration 338:
2025-05-03 00:05:57,168 - main - INFO - Accuracy: 0.9613
2025-05-03 00:05:57,168 - main - INFO - Precision: 0.9333
2025-05-03 00:05:57,168 - main - INFO - Recall: 0.9301
2025-05-03 00:05:57,168 - main - INFO - F1 Score: 0.9317
2025-05-03 00:05:57,168 - main - INFO - IoU: 0.8721
2025-05-03 00:05:57,168 - main - INFO - mAP: 0.9829
2025-05-03 00:05:57,168 - main - INFO - AUC: 0.9925
2025-05-03 00:05:57,168 - main - INFO - Training time: 1586.64 seconds
2025-05-03 00:05:57,168 - main - INFO - Configuration 339 already completed, skipping
2025-05-03 00:05:57,168 - main - INFO - Configuration 340 already completed, skipping
2025-05-03 00:05:57,168 - main - INFO - Configuration 341 already completed, skipping
2025-05-03 00:05:57,168 - main - INFO - Configuration 342 already completed, skipping
2025-05-03 00:05:57,169 - main - INFO - Configuration 343 already completed, skipping
2025-05-03 00:05:57,169 - main - INFO - Configuration 344 already completed, skipping
2025-05-03 00:05:57,169 - main - INFO - Configuration 345 already completed, skipping
2025-05-03 00:05:57,169 - main - INFO - Configuration 346 already completed, skipping
2025-05-03 00:05:57,170 - main - INFO - Configuration 347 already completed, skipping
2025-05-03 00:05:57,170 - main - INFO - Configuration 348 already completed, skipping
2025-05-03 00:05:57,170 - main - INFO - 
==================================================
2025-05-03 00:05:57,170 - main - INFO - Running configuration 349/756:
2025-05-03 00:05:57,170 - main - INFO - Model: ViT-B-16
2025-05-03 00:05:57,170 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:05:57,170 - main - INFO - Scheduler: StepLR
2025-05-03 00:05:57,170 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:05:57,170 - main - INFO - ==================================================
2025-05-03 00:05:57,170 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_349 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_349
2025-05-03 00:05:57,170 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_349 - INFO - Config: {
  "id": 349,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:05:57,655 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:05:57,656 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 349,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:05:57,656 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:05:57,732 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 12, best validation accuracy: 0.0000
2025-05-03 00:05:57,733 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 00:06:25,417 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 00:06:59,859 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.8225
2025-05-03 00:07:00,028 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Epoch 13 completed in 62.30s - Train Loss: 0.6120, Train Acc: 0.7145, Val Loss: 0.5371, Val Acc: 0.8225
2025-05-03 00:07:00,523 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 00:07:00,523 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 00:07:28,132 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 00:08:02,842 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8225 to 0.8688
2025-05-03 00:08:03,100 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Epoch 14 completed in 62.58s - Train Loss: 0.5101, Train Acc: 0.8503, Val Loss: 0.4852, Val Acc: 0.8688
2025-05-03 00:08:03,577 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 00:08:03,577 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 00:08:31,119 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 00:09:05,511 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8688 to 0.8791
2025-05-03 00:09:05,783 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Epoch 15 completed in 62.21s - Train Loss: 0.4711, Train Acc: 0.8711, Val Loss: 0.4590, Val Acc: 0.8791
2025-05-03 00:09:06,271 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 00:09:06,271 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 00:09:33,901 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 00:10:07,969 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8791 to 0.8919
2025-05-03 00:10:08,228 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Epoch 16 completed in 61.96s - Train Loss: 0.4446, Train Acc: 0.8912, Val Loss: 0.4411, Val Acc: 0.8919
2025-05-03 00:10:08,718 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 00:10:08,718 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 00:10:36,600 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 00:11:10,698 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8919 to 0.9057
2025-05-03 00:11:10,947 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Epoch 17 completed in 62.23s - Train Loss: 0.4241, Train Acc: 0.9101, Val Loss: 0.4255, Val Acc: 0.9057
2025-05-03 00:11:11,429 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 00:11:11,430 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 00:11:39,164 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 00:12:13,359 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9057 to 0.9142
2025-05-03 00:12:13,622 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Epoch 18 completed in 62.19s - Train Loss: 0.4077, Train Acc: 0.9279, Val Loss: 0.4109, Val Acc: 0.9142
2025-05-03 00:12:14,108 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 00:12:14,109 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 00:12:41,739 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 00:13:15,861 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9142 to 0.9254
2025-05-03 00:13:16,125 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Epoch 19 completed in 62.02s - Train Loss: 0.3945, Train Acc: 0.9374, Val Loss: 0.4010, Val Acc: 0.9254
2025-05-03 00:13:16,607 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 00:13:16,607 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 00:13:43,790 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 00:14:18,337 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9254 to 0.9348
2025-05-03 00:14:18,623 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Epoch 20 completed in 62.02s - Train Loss: 0.3821, Train Acc: 0.9509, Val Loss: 0.3913, Val Acc: 0.9348
2025-05-03 00:14:19,123 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 00:14:19,124 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:14:19,125 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_349 - INFO - Starting model evaluation
2025-05-03 00:14:28,413 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_349 - INFO - Evaluation metrics:
  Accuracy:  0.9286
  Precision: 0.8452
  Recall:    0.9161
  F1 Score:  0.8792
  IoU:       0.7844
  mAP:       0.9448
  AUC:       0.9733
2025-05-03 00:14:28,418 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_349 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_349/final_results.json
2025-05-03 00:14:28,419 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_349 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_349/final_results.json
2025-05-03 00:14:28,419 - main - INFO - 
Summary for configuration 349:
2025-05-03 00:14:28,419 - main - INFO - Accuracy: 0.9286
2025-05-03 00:14:28,419 - main - INFO - Precision: 0.8452
2025-05-03 00:14:28,419 - main - INFO - Recall: 0.9161
2025-05-03 00:14:28,419 - main - INFO - F1 Score: 0.8792
2025-05-03 00:14:28,419 - main - INFO - IoU: 0.7844
2025-05-03 00:14:28,419 - main - INFO - mAP: 0.9448
2025-05-03 00:14:28,419 - main - INFO - AUC: 0.9733
2025-05-03 00:14:28,419 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:14:28,419 - main - INFO - 
==================================================
2025-05-03 00:14:28,419 - main - INFO - Running configuration 350/756:
2025-05-03 00:14:28,419 - main - INFO - Model: ViT-B-16
2025-05-03 00:14:28,419 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:14:28,419 - main - INFO - Scheduler: StepLR
2025-05-03 00:14:28,419 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:14:28,419 - main - INFO - ==================================================
2025-05-03 00:14:28,420 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_350 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_350
2025-05-03 00:14:28,420 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_350 - INFO - Config: {
  "id": 350,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:14:29,059 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:14:29,059 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 350,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:14:29,059 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:14:29,182 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:14:29,182 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:14:29,183 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_350 - INFO - Starting model evaluation
2025-05-03 00:14:38,404 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_350 - INFO - Evaluation metrics:
  Accuracy:  0.9267
  Precision: 0.8397
  Recall:    0.9161
  F1 Score:  0.8763
  IoU:       0.7798
  mAP:       0.9467
  AUC:       0.9741
2025-05-03 00:14:38,409 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_350 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_350/final_results.json
2025-05-03 00:14:38,410 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_350 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_350/final_results.json
2025-05-03 00:14:38,410 - main - INFO - 
Summary for configuration 350:
2025-05-03 00:14:38,410 - main - INFO - Accuracy: 0.9267
2025-05-03 00:14:38,410 - main - INFO - Precision: 0.8397
2025-05-03 00:14:38,410 - main - INFO - Recall: 0.9161
2025-05-03 00:14:38,410 - main - INFO - F1 Score: 0.8763
2025-05-03 00:14:38,410 - main - INFO - IoU: 0.7798
2025-05-03 00:14:38,410 - main - INFO - mAP: 0.9467
2025-05-03 00:14:38,410 - main - INFO - AUC: 0.9741
2025-05-03 00:14:38,410 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:14:38,410 - main - INFO - 
==================================================
2025-05-03 00:14:38,410 - main - INFO - Running configuration 351/756:
2025-05-03 00:14:38,410 - main - INFO - Model: ViT-B-16
2025-05-03 00:14:38,410 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:14:38,410 - main - INFO - Scheduler: StepLR
2025-05-03 00:14:38,410 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:14:38,410 - main - INFO - ==================================================
2025-05-03 00:14:38,410 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_351 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_351
2025-05-03 00:14:38,411 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_351 - INFO - Config: {
  "id": 351,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:14:38,884 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:14:38,884 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 351,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:14:38,884 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:14:39,006 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:14:39,007 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:14:39,008 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_351 - INFO - Starting model evaluation
2025-05-03 00:14:48,158 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_351 - INFO - Evaluation metrics:
  Accuracy:  0.9227
  Precision: 0.8333
  Recall:    0.9091
  F1 Score:  0.8696
  IoU:       0.7692
  mAP:       0.9451
  AUC:       0.9739
2025-05-03 00:14:48,163 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_351 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_351/final_results.json
2025-05-03 00:14:48,164 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_351 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_351/final_results.json
2025-05-03 00:14:48,164 - main - INFO - 
Summary for configuration 351:
2025-05-03 00:14:48,164 - main - INFO - Accuracy: 0.9227
2025-05-03 00:14:48,164 - main - INFO - Precision: 0.8333
2025-05-03 00:14:48,164 - main - INFO - Recall: 0.9091
2025-05-03 00:14:48,164 - main - INFO - F1 Score: 0.8696
2025-05-03 00:14:48,164 - main - INFO - IoU: 0.7692
2025-05-03 00:14:48,164 - main - INFO - mAP: 0.9451
2025-05-03 00:14:48,164 - main - INFO - AUC: 0.9739
2025-05-03 00:14:48,164 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:14:48,164 - main - INFO - 
==================================================
2025-05-03 00:14:48,164 - main - INFO - Running configuration 352/756:
2025-05-03 00:14:48,164 - main - INFO - Model: ViT-B-16
2025-05-03 00:14:48,164 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:14:48,164 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 00:14:48,164 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:14:48,164 - main - INFO - ==================================================
2025-05-03 00:14:48,165 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_352 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_352
2025-05-03 00:14:48,165 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_352 - INFO - Config: {
  "id": 352,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:14:48,645 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:14:48,645 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 352,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:14:48,645 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:14:48,769 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:14:48,769 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:14:48,770 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_352 - INFO - Starting model evaluation
2025-05-03 00:14:58,249 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_352 - INFO - Evaluation metrics:
  Accuracy:  0.9306
  Precision: 0.8462
  Recall:    0.9231
  F1 Score:  0.8829
  IoU:       0.7904
  mAP:       0.9440
  AUC:       0.9729
2025-05-03 00:14:58,251 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_352 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_352/final_results.json
2025-05-03 00:14:58,253 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_352 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_352/final_results.json
2025-05-03 00:14:58,253 - main - INFO - 
Summary for configuration 352:
2025-05-03 00:14:58,253 - main - INFO - Accuracy: 0.9306
2025-05-03 00:14:58,253 - main - INFO - Precision: 0.8462
2025-05-03 00:14:58,253 - main - INFO - Recall: 0.9231
2025-05-03 00:14:58,253 - main - INFO - F1 Score: 0.8829
2025-05-03 00:14:58,253 - main - INFO - IoU: 0.7904
2025-05-03 00:14:58,253 - main - INFO - mAP: 0.9440
2025-05-03 00:14:58,253 - main - INFO - AUC: 0.9729
2025-05-03 00:14:58,253 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:14:58,253 - main - INFO - 
==================================================
2025-05-03 00:14:58,253 - main - INFO - Running configuration 353/756:
2025-05-03 00:14:58,253 - main - INFO - Model: ViT-B-16
2025-05-03 00:14:58,253 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:14:58,253 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 00:14:58,253 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:14:58,253 - main - INFO - ==================================================
2025-05-03 00:14:58,253 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_353 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_353
2025-05-03 00:14:58,255 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_353 - INFO - Config: {
  "id": 353,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:14:58,729 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:14:58,729 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 353,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:14:58,729 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:14:58,851 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:14:58,852 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:14:58,853 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_353 - INFO - Starting model evaluation
2025-05-03 00:15:08,124 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_353 - INFO - Evaluation metrics:
  Accuracy:  0.9257
  Precision: 0.8371
  Recall:    0.9161
  F1 Score:  0.8748
  IoU:       0.7774
  mAP:       0.9460
  AUC:       0.9738
2025-05-03 00:15:08,130 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_353 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_353/final_results.json
2025-05-03 00:15:08,132 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_353 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_353/final_results.json
2025-05-03 00:15:08,132 - main - INFO - 
Summary for configuration 353:
2025-05-03 00:15:08,132 - main - INFO - Accuracy: 0.9257
2025-05-03 00:15:08,132 - main - INFO - Precision: 0.8371
2025-05-03 00:15:08,132 - main - INFO - Recall: 0.9161
2025-05-03 00:15:08,132 - main - INFO - F1 Score: 0.8748
2025-05-03 00:15:08,132 - main - INFO - IoU: 0.7774
2025-05-03 00:15:08,132 - main - INFO - mAP: 0.9460
2025-05-03 00:15:08,132 - main - INFO - AUC: 0.9738
2025-05-03 00:15:08,132 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:15:08,132 - main - INFO - 
==================================================
2025-05-03 00:15:08,132 - main - INFO - Running configuration 354/756:
2025-05-03 00:15:08,132 - main - INFO - Model: ViT-B-16
2025-05-03 00:15:08,132 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:15:08,132 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 00:15:08,132 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:15:08,132 - main - INFO - ==================================================
2025-05-03 00:15:08,132 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_354 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_354
2025-05-03 00:15:08,132 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_354 - INFO - Config: {
  "id": 354,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:15:08,619 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:15:08,620 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 354,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:15:08,620 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:15:08,742 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:15:08,742 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:15:08,743 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_354 - INFO - Starting model evaluation
2025-05-03 00:15:17,699 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_354 - INFO - Evaluation metrics:
  Accuracy:  0.9316
  Precision: 0.8511
  Recall:    0.9196
  F1 Score:  0.8840
  IoU:       0.7922
  mAP:       0.9479
  AUC:       0.9747
2025-05-03 00:15:17,703 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_354 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_354/final_results.json
2025-05-03 00:15:17,705 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_354 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_354/final_results.json
2025-05-03 00:15:17,705 - main - INFO - 
Summary for configuration 354:
2025-05-03 00:15:17,705 - main - INFO - Accuracy: 0.9316
2025-05-03 00:15:17,705 - main - INFO - Precision: 0.8511
2025-05-03 00:15:17,705 - main - INFO - Recall: 0.9196
2025-05-03 00:15:17,705 - main - INFO - F1 Score: 0.8840
2025-05-03 00:15:17,705 - main - INFO - IoU: 0.7922
2025-05-03 00:15:17,705 - main - INFO - mAP: 0.9479
2025-05-03 00:15:17,705 - main - INFO - AUC: 0.9747
2025-05-03 00:15:17,705 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:15:17,705 - main - INFO - 
==================================================
2025-05-03 00:15:17,705 - main - INFO - Running configuration 355/756:
2025-05-03 00:15:17,705 - main - INFO - Model: ViT-B-16
2025-05-03 00:15:17,705 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:15:17,705 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 00:15:17,705 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:15:17,705 - main - INFO - ==================================================
2025-05-03 00:15:17,705 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_355 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_355
2025-05-03 00:15:17,705 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_355 - INFO - Config: {
  "id": 355,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:15:18,207 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:15:18,207 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 355,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:15:18,207 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:15:18,327 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:15:18,328 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:15:18,329 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_355 - INFO - Starting model evaluation
2025-05-03 00:15:27,536 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_355 - INFO - Evaluation metrics:
  Accuracy:  0.9267
  Precision: 0.8419
  Recall:    0.9126
  F1 Score:  0.8758
  IoU:       0.7791
  mAP:       0.9433
  AUC:       0.9727
2025-05-03 00:15:27,538 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_355 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_355/final_results.json
2025-05-03 00:15:27,539 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_355 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_355/final_results.json
2025-05-03 00:15:27,539 - main - INFO - 
Summary for configuration 355:
2025-05-03 00:15:27,539 - main - INFO - Accuracy: 0.9267
2025-05-03 00:15:27,539 - main - INFO - Precision: 0.8419
2025-05-03 00:15:27,539 - main - INFO - Recall: 0.9126
2025-05-03 00:15:27,539 - main - INFO - F1 Score: 0.8758
2025-05-03 00:15:27,539 - main - INFO - IoU: 0.7791
2025-05-03 00:15:27,539 - main - INFO - mAP: 0.9433
2025-05-03 00:15:27,539 - main - INFO - AUC: 0.9727
2025-05-03 00:15:27,539 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:15:27,539 - main - INFO - 
==================================================
2025-05-03 00:15:27,539 - main - INFO - Running configuration 356/756:
2025-05-03 00:15:27,539 - main - INFO - Model: ViT-B-16
2025-05-03 00:15:27,539 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:15:27,539 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 00:15:27,539 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:15:27,539 - main - INFO - ==================================================
2025-05-03 00:15:27,539 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_356 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_356
2025-05-03 00:15:27,550 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_356 - INFO - Config: {
  "id": 356,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:15:28,065 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:15:28,065 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 356,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:15:28,065 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:15:28,187 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:15:28,188 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:15:28,189 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_356 - INFO - Starting model evaluation
2025-05-03 00:15:37,557 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_356 - INFO - Evaluation metrics:
  Accuracy:  0.9247
  Precision: 0.8365
  Recall:    0.9126
  F1 Score:  0.8729
  IoU:       0.7745
  mAP:       0.9449
  AUC:       0.9732
2025-05-03 00:15:37,561 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_356 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_356/final_results.json
2025-05-03 00:15:37,563 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_356 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_356/final_results.json
2025-05-03 00:15:37,563 - main - INFO - 
Summary for configuration 356:
2025-05-03 00:15:37,563 - main - INFO - Accuracy: 0.9247
2025-05-03 00:15:37,563 - main - INFO - Precision: 0.8365
2025-05-03 00:15:37,563 - main - INFO - Recall: 0.9126
2025-05-03 00:15:37,563 - main - INFO - F1 Score: 0.8729
2025-05-03 00:15:37,563 - main - INFO - IoU: 0.7745
2025-05-03 00:15:37,563 - main - INFO - mAP: 0.9449
2025-05-03 00:15:37,563 - main - INFO - AUC: 0.9732
2025-05-03 00:15:37,563 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:15:37,563 - main - INFO - 
==================================================
2025-05-03 00:15:37,563 - main - INFO - Running configuration 357/756:
2025-05-03 00:15:37,563 - main - INFO - Model: ViT-B-16
2025-05-03 00:15:37,563 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:15:37,563 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 00:15:37,563 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:15:37,563 - main - INFO - ==================================================
2025-05-03 00:15:37,563 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_357 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_357
2025-05-03 00:15:37,563 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_357 - INFO - Config: {
  "id": 357,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:15:38,061 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:15:38,061 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 357,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:15:38,061 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:15:38,180 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:15:38,181 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:15:38,182 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_357 - INFO - Starting model evaluation
2025-05-03 00:15:47,740 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_357 - INFO - Evaluation metrics:
  Accuracy:  0.9267
  Precision: 0.8419
  Recall:    0.9126
  F1 Score:  0.8758
  IoU:       0.7791
  mAP:       0.9473
  AUC:       0.9752
2025-05-03 00:15:47,744 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_357 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_357/final_results.json
2025-05-03 00:15:47,746 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_357 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_357/final_results.json
2025-05-03 00:15:47,746 - main - INFO - 
Summary for configuration 357:
2025-05-03 00:15:47,746 - main - INFO - Accuracy: 0.9267
2025-05-03 00:15:47,746 - main - INFO - Precision: 0.8419
2025-05-03 00:15:47,746 - main - INFO - Recall: 0.9126
2025-05-03 00:15:47,746 - main - INFO - F1 Score: 0.8758
2025-05-03 00:15:47,746 - main - INFO - IoU: 0.7791
2025-05-03 00:15:47,746 - main - INFO - mAP: 0.9473
2025-05-03 00:15:47,746 - main - INFO - AUC: 0.9752
2025-05-03 00:15:47,746 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:15:47,746 - main - INFO - 
==================================================
2025-05-03 00:15:47,746 - main - INFO - Running configuration 358/756:
2025-05-03 00:15:47,746 - main - INFO - Model: ViT-B-16
2025-05-03 00:15:47,746 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:15:47,746 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 00:15:47,746 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:15:47,746 - main - INFO - ==================================================
2025-05-03 00:15:47,746 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_358 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_358
2025-05-03 00:15:47,746 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_358 - INFO - Config: {
  "id": 358,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:15:48,269 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:15:48,269 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 358,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:15:48,269 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:15:48,389 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:15:48,389 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:15:48,391 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_358 - INFO - Starting model evaluation
2025-05-03 00:15:57,934 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_358 - INFO - Evaluation metrics:
  Accuracy:  0.9277
  Precision: 0.8447
  Recall:    0.9126
  F1 Score:  0.8773
  IoU:       0.7814
  mAP:       0.9436
  AUC:       0.9718
2025-05-03 00:15:57,948 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_358 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_358/final_results.json
2025-05-03 00:15:57,949 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_358 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_358/final_results.json
2025-05-03 00:15:57,949 - main - INFO - 
Summary for configuration 358:
2025-05-03 00:15:57,949 - main - INFO - Accuracy: 0.9277
2025-05-03 00:15:57,949 - main - INFO - Precision: 0.8447
2025-05-03 00:15:57,949 - main - INFO - Recall: 0.9126
2025-05-03 00:15:57,949 - main - INFO - F1 Score: 0.8773
2025-05-03 00:15:57,949 - main - INFO - IoU: 0.7814
2025-05-03 00:15:57,949 - main - INFO - mAP: 0.9436
2025-05-03 00:15:57,949 - main - INFO - AUC: 0.9718
2025-05-03 00:15:57,949 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:15:57,949 - main - INFO - 
==================================================
2025-05-03 00:15:57,949 - main - INFO - Running configuration 359/756:
2025-05-03 00:15:57,949 - main - INFO - Model: ViT-B-16
2025-05-03 00:15:57,949 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:15:57,949 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 00:15:57,949 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:15:57,949 - main - INFO - ==================================================
2025-05-03 00:15:57,949 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_359 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_359
2025-05-03 00:15:57,950 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_359 - INFO - Config: {
  "id": 359,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:15:58,456 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:15:58,457 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 359,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:15:58,457 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:15:58,576 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:15:58,577 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:15:58,578 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_359 - INFO - Starting model evaluation
2025-05-03 00:16:32,156 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_359 - INFO - Evaluation metrics:
  Accuracy:  0.9306
  Precision: 0.8506
  Recall:    0.9161
  F1 Score:  0.8822
  IoU:       0.7892
  mAP:       0.9454
  AUC:       0.9739
2025-05-03 00:16:32,159 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_359 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_359/final_results.json
2025-05-03 00:16:32,160 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_359 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_359/final_results.json
2025-05-03 00:16:32,160 - main - INFO - 
Summary for configuration 359:
2025-05-03 00:16:32,160 - main - INFO - Accuracy: 0.9306
2025-05-03 00:16:32,160 - main - INFO - Precision: 0.8506
2025-05-03 00:16:32,160 - main - INFO - Recall: 0.9161
2025-05-03 00:16:32,160 - main - INFO - F1 Score: 0.8822
2025-05-03 00:16:32,160 - main - INFO - IoU: 0.7892
2025-05-03 00:16:32,160 - main - INFO - mAP: 0.9454
2025-05-03 00:16:32,160 - main - INFO - AUC: 0.9739
2025-05-03 00:16:32,160 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:16:32,160 - main - INFO - 
==================================================
2025-05-03 00:16:32,161 - main - INFO - Running configuration 360/756:
2025-05-03 00:16:32,161 - main - INFO - Model: ViT-B-16
2025-05-03 00:16:32,161 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 00:16:32,161 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 00:16:32,161 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:16:32,161 - main - INFO - ==================================================
2025-05-03 00:16:32,161 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_360 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001_id_360
2025-05-03 00:16:32,161 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_360 - INFO - Config: {
  "id": 360,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:16:32,974 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_SGD_lr_0.0001
2025-05-03 00:16:32,974 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 360,
  "model_name": "ViT-B-16",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:16:32,974 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 00:16:33,093 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9348
2025-05-03 00:16:33,094 - training.model_ViT-B-16_opt_SGD_lr_0.0001 - INFO - Training completed after 502.99 seconds
2025-05-03 00:16:33,095 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_360 - INFO - Starting model evaluation
2025-05-03 00:16:44,803 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_360 - INFO - Evaluation metrics:
  Accuracy:  0.9267
  Precision: 0.8442
  Recall:    0.9091
  F1 Score:  0.8754
  IoU:       0.7784
  mAP:       0.9469
  AUC:       0.9749
2025-05-03 00:16:44,807 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_360 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_360/final_results.json
2025-05-03 00:16:44,808 - training.model_ViT-B-16_opt_SGD_lr_0.0001_id_360 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_SGD_lr_0.0001_id_360/final_results.json
2025-05-03 00:16:44,808 - main - INFO - 
Summary for configuration 360:
2025-05-03 00:16:44,808 - main - INFO - Accuracy: 0.9267
2025-05-03 00:16:44,809 - main - INFO - Precision: 0.8442
2025-05-03 00:16:44,809 - main - INFO - Recall: 0.9091
2025-05-03 00:16:44,809 - main - INFO - F1 Score: 0.8754
2025-05-03 00:16:44,809 - main - INFO - IoU: 0.7784
2025-05-03 00:16:44,809 - main - INFO - mAP: 0.9469
2025-05-03 00:16:44,809 - main - INFO - AUC: 0.9749
2025-05-03 00:16:44,809 - main - INFO - Training time: 502.99 seconds
2025-05-03 00:16:44,809 - main - INFO - 
==================================================
2025-05-03 00:16:44,809 - main - INFO - Running configuration 361/756:
2025-05-03 00:16:44,809 - main - INFO - Model: ViT-B-16
2025-05-03 00:16:44,809 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:16:44,809 - main - INFO - Scheduler: StepLR
2025-05-03 00:16:44,809 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:16:44,809 - main - INFO - ==================================================
2025-05-03 00:16:44,809 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_361 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_361
2025-05-03 00:16:44,809 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_361 - INFO - Config: {
  "id": 361,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:16:45,330 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:16:45,333 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 361,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:16:45,333 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:16:45,446 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 12, best validation accuracy: 0.0000
2025-05-03 00:16:45,446 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 00:17:17,311 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 00:17:55,921 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.4957
2025-05-03 00:17:56,093 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Epoch 13 completed in 70.65s - Train Loss: 0.8147, Train Acc: 0.4976, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 00:17:56,814 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 00:17:56,814 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 00:18:28,389 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 00:19:06,714 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.4957 to 0.5043
2025-05-03 00:19:07,004 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Epoch 14 completed in 70.19s - Train Loss: 0.8006, Train Acc: 0.5122, Val Loss: 0.8067, Val Acc: 0.5043
2025-05-03 00:19:07,745 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 00:19:07,745 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 00:19:39,451 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 00:20:17,155 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Validation loss improved from inf to 0.6942
2025-05-03 00:20:17,414 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Epoch 15 completed in 69.67s - Train Loss: 0.7406, Train Acc: 0.4865, Val Loss: 0.6942, Val Acc: 0.5043
2025-05-03 00:20:18,205 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 00:20:18,205 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 00:20:51,083 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 00:21:30,714 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Epoch 16 completed in 72.51s - Train Loss: 0.6992, Train Acc: 0.4936, Val Loss: 0.7075, Val Acc: 0.4957
2025-05-03 00:21:31,453 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 00:21:31,453 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 00:22:03,343 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 00:22:42,133 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.6942 to 0.6932
2025-05-03 00:22:42,384 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Epoch 17 completed in 70.93s - Train Loss: 0.7036, Train Acc: 0.4936, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 00:22:43,129 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 00:22:43,130 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 00:23:14,545 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 00:23:55,307 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Epoch 18 completed in 72.18s - Train Loss: 0.6975, Train Acc: 0.4994, Val Loss: 0.6934, Val Acc: 0.4957
2025-05-03 00:23:56,085 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 00:23:56,085 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 00:26:24,549 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 00:29:57,252 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Epoch 19 completed in 361.17s - Train Loss: 0.6997, Train Acc: 0.4946, Val Loss: 0.6988, Val Acc: 0.4957
2025-05-03 00:29:58,139 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 00:29:58,140 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 00:32:28,504 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 00:35:55,187 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Epoch 20 completed in 357.05s - Train Loss: 0.6993, Train Acc: 0.4989, Val Loss: 0.6942, Val Acc: 0.5009
2025-05-03 00:35:56,148 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 00:35:56,149 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:35:56,151 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_361 - INFO - Starting model evaluation
2025-05-03 00:36:58,577 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_361 - INFO - Evaluation metrics:
  Accuracy:  0.7126
  Precision: 0.1667
  Recall:    0.0035
  F1 Score:  0.0068
  IoU:       0.0034
  mAP:       0.2578
  AUC:       0.4338
2025-05-03 00:36:58,579 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_361 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_361/final_results.json
2025-05-03 00:36:58,581 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_361 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_361/final_results.json
2025-05-03 00:36:58,581 - main - INFO - 
Summary for configuration 361:
2025-05-03 00:36:58,581 - main - INFO - Accuracy: 0.7126
2025-05-03 00:36:58,581 - main - INFO - Precision: 0.1667
2025-05-03 00:36:58,581 - main - INFO - Recall: 0.0035
2025-05-03 00:36:58,581 - main - INFO - F1 Score: 0.0068
2025-05-03 00:36:58,581 - main - INFO - IoU: 0.0034
2025-05-03 00:36:58,581 - main - INFO - mAP: 0.2578
2025-05-03 00:36:58,581 - main - INFO - AUC: 0.4338
2025-05-03 00:36:58,581 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:36:58,581 - main - INFO - 
==================================================
2025-05-03 00:36:58,581 - main - INFO - Running configuration 362/756:
2025-05-03 00:36:58,581 - main - INFO - Model: ViT-B-16
2025-05-03 00:36:58,581 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:36:58,581 - main - INFO - Scheduler: StepLR
2025-05-03 00:36:58,581 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:36:58,581 - main - INFO - ==================================================
2025-05-03 00:36:58,581 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_362 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_362
2025-05-03 00:36:58,581 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_362 - INFO - Config: {
  "id": 362,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:36:59,591 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:36:59,591 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 362,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:36:59,591 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:36:59,899 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:36:59,900 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:36:59,903 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_362 - INFO - Starting model evaluation
2025-05-03 00:38:01,871 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_362 - INFO - Evaluation metrics:
  Accuracy:  0.7116
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2622
  AUC:       0.4493
2025-05-03 00:38:01,874 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_362 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_362/final_results.json
2025-05-03 00:38:01,876 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_362 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_362/final_results.json
2025-05-03 00:38:01,876 - main - INFO - 
Summary for configuration 362:
2025-05-03 00:38:01,876 - main - INFO - Accuracy: 0.7116
2025-05-03 00:38:01,876 - main - INFO - Precision: 0.0000
2025-05-03 00:38:01,876 - main - INFO - Recall: 0.0000
2025-05-03 00:38:01,876 - main - INFO - F1 Score: 0.0000
2025-05-03 00:38:01,876 - main - INFO - IoU: 0.0000
2025-05-03 00:38:01,876 - main - INFO - mAP: 0.2622
2025-05-03 00:38:01,876 - main - INFO - AUC: 0.4493
2025-05-03 00:38:01,876 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:38:01,877 - main - INFO - 
==================================================
2025-05-03 00:38:01,877 - main - INFO - Running configuration 363/756:
2025-05-03 00:38:01,877 - main - INFO - Model: ViT-B-16
2025-05-03 00:38:01,877 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:38:01,877 - main - INFO - Scheduler: StepLR
2025-05-03 00:38:01,877 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:38:01,877 - main - INFO - ==================================================
2025-05-03 00:38:01,877 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_363 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_363
2025-05-03 00:38:01,879 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_363 - INFO - Config: {
  "id": 363,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:38:02,717 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:38:02,717 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 363,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:38:02,717 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:38:02,914 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:38:02,915 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:38:02,916 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_363 - INFO - Starting model evaluation
2025-05-03 00:39:04,942 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_363 - INFO - Evaluation metrics:
  Accuracy:  0.7096
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2785
  AUC:       0.4761
2025-05-03 00:39:04,944 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_363 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_363/final_results.json
2025-05-03 00:39:04,946 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_363 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_363/final_results.json
2025-05-03 00:39:04,947 - main - INFO - 
Summary for configuration 363:
2025-05-03 00:39:04,947 - main - INFO - Accuracy: 0.7096
2025-05-03 00:39:04,947 - main - INFO - Precision: 0.0000
2025-05-03 00:39:04,947 - main - INFO - Recall: 0.0000
2025-05-03 00:39:04,947 - main - INFO - F1 Score: 0.0000
2025-05-03 00:39:04,947 - main - INFO - IoU: 0.0000
2025-05-03 00:39:04,947 - main - INFO - mAP: 0.2785
2025-05-03 00:39:04,947 - main - INFO - AUC: 0.4761
2025-05-03 00:39:04,947 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:39:04,947 - main - INFO - 
==================================================
2025-05-03 00:39:04,947 - main - INFO - Running configuration 364/756:
2025-05-03 00:39:04,947 - main - INFO - Model: ViT-B-16
2025-05-03 00:39:04,947 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:39:04,947 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 00:39:04,947 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:39:04,947 - main - INFO - ==================================================
2025-05-03 00:39:04,947 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_364 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_364
2025-05-03 00:39:04,948 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_364 - INFO - Config: {
  "id": 364,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:39:06,088 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:39:06,089 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 364,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:39:06,089 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:39:06,390 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:39:06,391 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:39:06,394 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_364 - INFO - Starting model evaluation
2025-05-03 00:40:09,250 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_364 - INFO - Evaluation metrics:
  Accuracy:  0.7076
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2652
  AUC:       0.4594
2025-05-03 00:40:09,252 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_364 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_364/final_results.json
2025-05-03 00:40:09,255 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_364 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_364/final_results.json
2025-05-03 00:40:09,255 - main - INFO - 
Summary for configuration 364:
2025-05-03 00:40:09,255 - main - INFO - Accuracy: 0.7076
2025-05-03 00:40:09,255 - main - INFO - Precision: 0.0000
2025-05-03 00:40:09,255 - main - INFO - Recall: 0.0000
2025-05-03 00:40:09,255 - main - INFO - F1 Score: 0.0000
2025-05-03 00:40:09,255 - main - INFO - IoU: 0.0000
2025-05-03 00:40:09,255 - main - INFO - mAP: 0.2652
2025-05-03 00:40:09,255 - main - INFO - AUC: 0.4594
2025-05-03 00:40:09,255 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:40:09,255 - main - INFO - 
==================================================
2025-05-03 00:40:09,255 - main - INFO - Running configuration 365/756:
2025-05-03 00:40:09,255 - main - INFO - Model: ViT-B-16
2025-05-03 00:40:09,255 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:40:09,255 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 00:40:09,255 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:40:09,255 - main - INFO - ==================================================
2025-05-03 00:40:09,255 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_365 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_365
2025-05-03 00:40:09,255 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_365 - INFO - Config: {
  "id": 365,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:40:10,353 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:40:10,353 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 365,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:40:10,354 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:40:10,669 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:40:10,670 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:40:10,672 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_365 - INFO - Starting model evaluation
2025-05-03 00:41:13,100 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_365 - INFO - Evaluation metrics:
  Accuracy:  0.7106
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2630
  AUC:       0.4501
2025-05-03 00:41:13,103 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_365 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_365/final_results.json
2025-05-03 00:41:13,105 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_365 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_365/final_results.json
2025-05-03 00:41:13,105 - main - INFO - 
Summary for configuration 365:
2025-05-03 00:41:13,105 - main - INFO - Accuracy: 0.7106
2025-05-03 00:41:13,105 - main - INFO - Precision: 0.0000
2025-05-03 00:41:13,105 - main - INFO - Recall: 0.0000
2025-05-03 00:41:13,105 - main - INFO - F1 Score: 0.0000
2025-05-03 00:41:13,105 - main - INFO - IoU: 0.0000
2025-05-03 00:41:13,105 - main - INFO - mAP: 0.2630
2025-05-03 00:41:13,105 - main - INFO - AUC: 0.4501
2025-05-03 00:41:13,106 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:41:13,106 - main - INFO - 
==================================================
2025-05-03 00:41:13,106 - main - INFO - Running configuration 366/756:
2025-05-03 00:41:13,106 - main - INFO - Model: ViT-B-16
2025-05-03 00:41:13,106 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:41:13,106 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 00:41:13,106 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:41:13,106 - main - INFO - ==================================================
2025-05-03 00:41:13,106 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_366 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_366
2025-05-03 00:41:13,107 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_366 - INFO - Config: {
  "id": 366,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:41:14,277 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:41:14,278 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 366,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:41:14,278 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:41:14,474 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:41:14,475 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:41:14,476 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_366 - INFO - Starting model evaluation
2025-05-03 00:42:17,976 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_366 - INFO - Evaluation metrics:
  Accuracy:  0.7086
  Precision: 0.1000
  Recall:    0.0035
  F1 Score:  0.0068
  IoU:       0.0034
  mAP:       0.2614
  AUC:       0.4427
2025-05-03 00:42:17,978 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_366 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_366/final_results.json
2025-05-03 00:42:17,980 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_366 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_366/final_results.json
2025-05-03 00:42:17,980 - main - INFO - 
Summary for configuration 366:
2025-05-03 00:42:17,980 - main - INFO - Accuracy: 0.7086
2025-05-03 00:42:17,980 - main - INFO - Precision: 0.1000
2025-05-03 00:42:17,980 - main - INFO - Recall: 0.0035
2025-05-03 00:42:17,980 - main - INFO - F1 Score: 0.0068
2025-05-03 00:42:17,980 - main - INFO - IoU: 0.0034
2025-05-03 00:42:17,980 - main - INFO - mAP: 0.2614
2025-05-03 00:42:17,980 - main - INFO - AUC: 0.4427
2025-05-03 00:42:17,980 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:42:17,980 - main - INFO - 
==================================================
2025-05-03 00:42:17,980 - main - INFO - Running configuration 367/756:
2025-05-03 00:42:17,980 - main - INFO - Model: ViT-B-16
2025-05-03 00:42:17,980 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:42:17,980 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 00:42:17,980 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:42:17,980 - main - INFO - ==================================================
2025-05-03 00:42:17,980 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_367 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_367
2025-05-03 00:42:17,980 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_367 - INFO - Config: {
  "id": 367,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:42:18,922 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:42:18,922 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 367,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:42:18,922 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:42:19,112 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:42:19,112 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:42:19,114 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_367 - INFO - Starting model evaluation
2025-05-03 00:43:21,243 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_367 - INFO - Evaluation metrics:
  Accuracy:  0.7106
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2576
  AUC:       0.4344
2025-05-03 00:43:21,245 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_367 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_367/final_results.json
2025-05-03 00:43:21,246 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_367 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_367/final_results.json
2025-05-03 00:43:21,247 - main - INFO - 
Summary for configuration 367:
2025-05-03 00:43:21,247 - main - INFO - Accuracy: 0.7106
2025-05-03 00:43:21,247 - main - INFO - Precision: 0.0000
2025-05-03 00:43:21,247 - main - INFO - Recall: 0.0000
2025-05-03 00:43:21,247 - main - INFO - F1 Score: 0.0000
2025-05-03 00:43:21,247 - main - INFO - IoU: 0.0000
2025-05-03 00:43:21,247 - main - INFO - mAP: 0.2576
2025-05-03 00:43:21,247 - main - INFO - AUC: 0.4344
2025-05-03 00:43:21,247 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:43:21,247 - main - INFO - 
==================================================
2025-05-03 00:43:21,247 - main - INFO - Running configuration 368/756:
2025-05-03 00:43:21,247 - main - INFO - Model: ViT-B-16
2025-05-03 00:43:21,247 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:43:21,247 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 00:43:21,247 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:43:21,247 - main - INFO - ==================================================
2025-05-03 00:43:21,247 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_368 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_368
2025-05-03 00:43:21,249 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_368 - INFO - Config: {
  "id": 368,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:43:21,915 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:43:21,915 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 368,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:43:21,915 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:43:22,105 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:43:22,106 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:43:22,107 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_368 - INFO - Starting model evaluation
2025-05-03 00:44:25,118 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_368 - INFO - Evaluation metrics:
  Accuracy:  0.7096
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2628
  AUC:       0.4511
2025-05-03 00:44:25,120 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_368 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_368/final_results.json
2025-05-03 00:44:25,122 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_368 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_368/final_results.json
2025-05-03 00:44:25,122 - main - INFO - 
Summary for configuration 368:
2025-05-03 00:44:25,122 - main - INFO - Accuracy: 0.7096
2025-05-03 00:44:25,122 - main - INFO - Precision: 0.0000
2025-05-03 00:44:25,123 - main - INFO - Recall: 0.0000
2025-05-03 00:44:25,123 - main - INFO - F1 Score: 0.0000
2025-05-03 00:44:25,123 - main - INFO - IoU: 0.0000
2025-05-03 00:44:25,123 - main - INFO - mAP: 0.2628
2025-05-03 00:44:25,123 - main - INFO - AUC: 0.4511
2025-05-03 00:44:25,123 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:44:25,123 - main - INFO - 
==================================================
2025-05-03 00:44:25,123 - main - INFO - Running configuration 369/756:
2025-05-03 00:44:25,123 - main - INFO - Model: ViT-B-16
2025-05-03 00:44:25,123 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:44:25,123 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 00:44:25,123 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:44:25,123 - main - INFO - ==================================================
2025-05-03 00:44:25,123 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_369 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_369
2025-05-03 00:44:25,126 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_369 - INFO - Config: {
  "id": 369,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:44:25,863 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:44:25,863 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 369,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:44:25,863 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:44:26,052 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:44:26,053 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:44:26,054 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_369 - INFO - Starting model evaluation
2025-05-03 00:45:28,017 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_369 - INFO - Evaluation metrics:
  Accuracy:  0.7106
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2591
  AUC:       0.4405
2025-05-03 00:45:28,019 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_369 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_369/final_results.json
2025-05-03 00:45:28,021 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_369 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_369/final_results.json
2025-05-03 00:45:28,021 - main - INFO - 
Summary for configuration 369:
2025-05-03 00:45:28,021 - main - INFO - Accuracy: 0.7106
2025-05-03 00:45:28,021 - main - INFO - Precision: 0.0000
2025-05-03 00:45:28,021 - main - INFO - Recall: 0.0000
2025-05-03 00:45:28,021 - main - INFO - F1 Score: 0.0000
2025-05-03 00:45:28,021 - main - INFO - IoU: 0.0000
2025-05-03 00:45:28,021 - main - INFO - mAP: 0.2591
2025-05-03 00:45:28,021 - main - INFO - AUC: 0.4405
2025-05-03 00:45:28,021 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:45:28,021 - main - INFO - 
==================================================
2025-05-03 00:45:28,021 - main - INFO - Running configuration 370/756:
2025-05-03 00:45:28,021 - main - INFO - Model: ViT-B-16
2025-05-03 00:45:28,021 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:45:28,021 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 00:45:28,021 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:45:28,021 - main - INFO - ==================================================
2025-05-03 00:45:28,022 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_370 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_370
2025-05-03 00:45:28,022 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_370 - INFO - Config: {
  "id": 370,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:45:28,825 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:45:28,826 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 370,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:45:28,826 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:45:29,015 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:45:29,015 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:45:29,017 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_370 - INFO - Starting model evaluation
2025-05-03 00:46:31,501 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_370 - INFO - Evaluation metrics:
  Accuracy:  0.7106
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2538
  AUC:       0.4285
2025-05-03 00:46:31,504 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_370 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_370/final_results.json
2025-05-03 00:46:31,506 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_370 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_370/final_results.json
2025-05-03 00:46:31,507 - main - INFO - 
Summary for configuration 370:
2025-05-03 00:46:31,507 - main - INFO - Accuracy: 0.7106
2025-05-03 00:46:31,507 - main - INFO - Precision: 0.0000
2025-05-03 00:46:31,507 - main - INFO - Recall: 0.0000
2025-05-03 00:46:31,507 - main - INFO - F1 Score: 0.0000
2025-05-03 00:46:31,507 - main - INFO - IoU: 0.0000
2025-05-03 00:46:31,507 - main - INFO - mAP: 0.2538
2025-05-03 00:46:31,507 - main - INFO - AUC: 0.4285
2025-05-03 00:46:31,507 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:46:31,507 - main - INFO - 
==================================================
2025-05-03 00:46:31,507 - main - INFO - Running configuration 371/756:
2025-05-03 00:46:31,507 - main - INFO - Model: ViT-B-16
2025-05-03 00:46:31,507 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:46:31,507 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 00:46:31,508 - main - INFO - Loss Function: FocalLoss
2025-05-03 00:46:31,508 - main - INFO - ==================================================
2025-05-03 00:46:31,508 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_371 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_371
2025-05-03 00:46:31,508 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_371 - INFO - Config: {
  "id": 371,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:46:32,315 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:46:32,315 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 371,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 00:46:32,316 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:46:32,616 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:46:32,617 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:46:32,619 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_371 - INFO - Starting model evaluation
2025-05-03 00:47:37,019 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_371 - INFO - Evaluation metrics:
  Accuracy:  0.7126
  Precision: 0.1667
  Recall:    0.0035
  F1 Score:  0.0068
  IoU:       0.0034
  mAP:       0.2706
  AUC:       0.4604
2025-05-03 00:47:37,021 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_371 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_371/final_results.json
2025-05-03 00:47:37,024 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_371 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_371/final_results.json
2025-05-03 00:47:37,024 - main - INFO - 
Summary for configuration 371:
2025-05-03 00:47:37,024 - main - INFO - Accuracy: 0.7126
2025-05-03 00:47:37,024 - main - INFO - Precision: 0.1667
2025-05-03 00:47:37,024 - main - INFO - Recall: 0.0035
2025-05-03 00:47:37,024 - main - INFO - F1 Score: 0.0068
2025-05-03 00:47:37,024 - main - INFO - IoU: 0.0034
2025-05-03 00:47:37,025 - main - INFO - mAP: 0.2706
2025-05-03 00:47:37,025 - main - INFO - AUC: 0.4604
2025-05-03 00:47:37,025 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:47:37,025 - main - INFO - 
==================================================
2025-05-03 00:47:37,025 - main - INFO - Running configuration 372/756:
2025-05-03 00:47:37,025 - main - INFO - Model: ViT-B-16
2025-05-03 00:47:37,025 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 00:47:37,025 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 00:47:37,025 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 00:47:37,025 - main - INFO - ==================================================
2025-05-03 00:47:37,025 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_372 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01_id_372
2025-05-03 00:47:37,028 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_372 - INFO - Config: {
  "id": 372,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:47:37,953 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.01
2025-05-03 00:47:37,954 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 372,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 00:47:37,954 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 00:47:38,262 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 00:47:38,264 - training.model_ViT-B-16_opt_Adam_lr_0.01 - INFO - Training completed after 1151.75 seconds
2025-05-03 00:47:38,266 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_372 - INFO - Starting model evaluation
2025-05-03 00:48:41,349 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_372 - INFO - Evaluation metrics:
  Accuracy:  0.7096
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2700
  AUC:       0.4716
2025-05-03 00:48:41,351 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_372 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_372/final_results.json
2025-05-03 00:48:41,353 - training.model_ViT-B-16_opt_Adam_lr_0.01_id_372 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.01_id_372/final_results.json
2025-05-03 00:48:41,354 - main - INFO - 
Summary for configuration 372:
2025-05-03 00:48:41,354 - main - INFO - Accuracy: 0.7096
2025-05-03 00:48:41,354 - main - INFO - Precision: 0.0000
2025-05-03 00:48:41,354 - main - INFO - Recall: 0.0000
2025-05-03 00:48:41,354 - main - INFO - F1 Score: 0.0000
2025-05-03 00:48:41,354 - main - INFO - IoU: 0.0000
2025-05-03 00:48:41,354 - main - INFO - mAP: 0.2700
2025-05-03 00:48:41,354 - main - INFO - AUC: 0.4716
2025-05-03 00:48:41,354 - main - INFO - Training time: 1151.75 seconds
2025-05-03 00:48:41,354 - main - INFO - 
==================================================
2025-05-03 00:48:41,354 - main - INFO - Running configuration 373/756:
2025-05-03 00:48:41,354 - main - INFO - Model: ViT-B-16
2025-05-03 00:48:41,354 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 00:48:41,354 - main - INFO - Scheduler: StepLR
2025-05-03 00:48:41,354 - main - INFO - Loss Function: CrossEntropy
2025-05-03 00:48:41,354 - main - INFO - ==================================================
2025-05-03 00:48:41,354 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_373 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_373
2025-05-03 00:48:41,354 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_373 - INFO - Config: {
  "id": 373,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:48:42,166 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 00:48:42,166 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 373,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 00:48:42,166 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 00:48:42,686 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 12, best validation accuracy: 0.0000
2025-05-03 00:48:42,687 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 00:51:18,059 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 00:54:52,679 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.5043
2025-05-03 00:54:52,916 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Epoch 13 completed in 370.23s - Train Loss: 0.8170, Train Acc: 0.4946, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 00:54:53,747 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 00:54:53,747 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 00:57:25,940 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 01:01:04,511 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Validation loss improved from inf to 0.8169
2025-05-03 01:01:04,857 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Epoch 14 completed in 371.11s - Train Loss: 0.8101, Train Acc: 0.5030, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 01:01:05,852 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 01:01:05,852 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 01:03:39,873 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 01:07:09,761 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.8169 to 0.8169
2025-05-03 01:07:10,075 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Epoch 15 completed in 364.22s - Train Loss: 0.8135, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 01:07:11,073 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 01:07:11,074 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 01:09:47,120 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 01:13:23,103 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.8169 to 0.8167
2025-05-03 01:13:23,422 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Epoch 16 completed in 372.35s - Train Loss: 0.8098, Train Acc: 0.5019, Val Loss: 0.8167, Val Acc: 0.4957
2025-05-03 01:13:24,442 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 01:13:24,442 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 01:16:00,963 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 01:19:38,195 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.8167 to 0.6929
2025-05-03 01:19:38,536 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Epoch 17 completed in 374.09s - Train Loss: 0.7084, Train Acc: 0.4949, Val Loss: 0.6929, Val Acc: 0.5043
2025-05-03 01:19:39,305 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 01:19:39,305 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 01:22:17,268 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 01:25:55,710 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.6929 to 0.6926
2025-05-03 01:25:56,069 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Epoch 18 completed in 376.76s - Train Loss: 0.6963, Train Acc: 0.5084, Val Loss: 0.6926, Val Acc: 0.4871
2025-05-03 01:25:56,905 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 01:25:56,906 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 01:28:35,271 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 01:32:16,524 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Epoch 19 completed in 379.62s - Train Loss: 0.6952, Train Acc: 0.4893, Val Loss: 0.6931, Val Acc: 0.5043
2025-05-03 01:32:17,425 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 01:32:17,425 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 01:34:54,811 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 01:38:32,052 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Epoch 20 completed in 374.63s - Train Loss: 0.6946, Train Acc: 0.5062, Val Loss: 0.6934, Val Acc: 0.5043
2025-05-03 01:38:33,031 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 01:38:33,032 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:38:33,034 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_373 - INFO - Starting model evaluation
2025-05-03 01:39:37,465 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_373 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2796
  AUC:       0.4843
2025-05-03 01:39:37,468 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_373 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_373/final_results.json
2025-05-03 01:39:37,470 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_373 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_373/final_results.json
2025-05-03 01:39:37,470 - main - INFO - 
Summary for configuration 373:
2025-05-03 01:39:37,470 - main - INFO - Accuracy: 0.7166
2025-05-03 01:39:37,470 - main - INFO - Precision: 0.0000
2025-05-03 01:39:37,470 - main - INFO - Recall: 0.0000
2025-05-03 01:39:37,470 - main - INFO - F1 Score: 0.0000
2025-05-03 01:39:37,470 - main - INFO - IoU: 0.0000
2025-05-03 01:39:37,470 - main - INFO - mAP: 0.2796
2025-05-03 01:39:37,470 - main - INFO - AUC: 0.4843
2025-05-03 01:39:37,470 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:39:37,470 - main - INFO - 
==================================================
2025-05-03 01:39:37,470 - main - INFO - Running configuration 374/756:
2025-05-03 01:39:37,470 - main - INFO - Model: ViT-B-16
2025-05-03 01:39:37,470 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:39:37,470 - main - INFO - Scheduler: StepLR
2025-05-03 01:39:37,470 - main - INFO - Loss Function: FocalLoss
2025-05-03 01:39:37,470 - main - INFO - ==================================================
2025-05-03 01:39:37,471 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_374 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_374
2025-05-03 01:39:37,471 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_374 - INFO - Config: {
  "id": 374,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 01:39:38,471 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:39:38,471 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 374,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 01:39:38,471 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:39:38,685 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:39:38,685 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:39:38,686 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_374 - INFO - Starting model evaluation
2025-05-03 01:40:41,798 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_374 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2794
  AUC:       0.4850
2025-05-03 01:40:41,800 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_374 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_374/final_results.json
2025-05-03 01:40:41,801 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_374 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_374/final_results.json
2025-05-03 01:40:41,801 - main - INFO - 
Summary for configuration 374:
2025-05-03 01:40:41,801 - main - INFO - Accuracy: 0.7166
2025-05-03 01:40:41,801 - main - INFO - Precision: 0.0000
2025-05-03 01:40:41,801 - main - INFO - Recall: 0.0000
2025-05-03 01:40:41,801 - main - INFO - F1 Score: 0.0000
2025-05-03 01:40:41,801 - main - INFO - IoU: 0.0000
2025-05-03 01:40:41,801 - main - INFO - mAP: 0.2794
2025-05-03 01:40:41,801 - main - INFO - AUC: 0.4850
2025-05-03 01:40:41,801 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:40:41,801 - main - INFO - 
==================================================
2025-05-03 01:40:41,801 - main - INFO - Running configuration 375/756:
2025-05-03 01:40:41,801 - main - INFO - Model: ViT-B-16
2025-05-03 01:40:41,801 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:40:41,801 - main - INFO - Scheduler: StepLR
2025-05-03 01:40:41,801 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 01:40:41,801 - main - INFO - ==================================================
2025-05-03 01:40:41,801 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_375 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_375
2025-05-03 01:40:41,802 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_375 - INFO - Config: {
  "id": 375,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 01:40:42,510 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:40:42,510 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 375,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 01:40:42,510 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:40:42,700 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:40:42,700 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:40:42,701 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_375 - INFO - Starting model evaluation
2025-05-03 01:41:47,788 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_375 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2799
  AUC:       0.4856
2025-05-03 01:41:47,791 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_375 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_375/final_results.json
2025-05-03 01:41:47,793 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_375 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_375/final_results.json
2025-05-03 01:41:47,793 - main - INFO - 
Summary for configuration 375:
2025-05-03 01:41:47,793 - main - INFO - Accuracy: 0.7166
2025-05-03 01:41:47,793 - main - INFO - Precision: 0.0000
2025-05-03 01:41:47,793 - main - INFO - Recall: 0.0000
2025-05-03 01:41:47,793 - main - INFO - F1 Score: 0.0000
2025-05-03 01:41:47,793 - main - INFO - IoU: 0.0000
2025-05-03 01:41:47,793 - main - INFO - mAP: 0.2799
2025-05-03 01:41:47,793 - main - INFO - AUC: 0.4856
2025-05-03 01:41:47,793 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:41:47,793 - main - INFO - 
==================================================
2025-05-03 01:41:47,793 - main - INFO - Running configuration 376/756:
2025-05-03 01:41:47,793 - main - INFO - Model: ViT-B-16
2025-05-03 01:41:47,793 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:41:47,793 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 01:41:47,793 - main - INFO - Loss Function: CrossEntropy
2025-05-03 01:41:47,793 - main - INFO - ==================================================
2025-05-03 01:41:47,794 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_376 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_376
2025-05-03 01:41:47,794 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_376 - INFO - Config: {
  "id": 376,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 01:41:48,509 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:41:48,509 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 376,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 01:41:48,509 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:41:48,698 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:41:48,698 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:41:48,699 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_376 - INFO - Starting model evaluation
2025-05-03 01:42:51,283 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_376 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2796
  AUC:       0.4846
2025-05-03 01:42:51,285 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_376 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_376/final_results.json
2025-05-03 01:42:51,287 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_376 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_376/final_results.json
2025-05-03 01:42:51,287 - main - INFO - 
Summary for configuration 376:
2025-05-03 01:42:51,288 - main - INFO - Accuracy: 0.7166
2025-05-03 01:42:51,288 - main - INFO - Precision: 0.0000
2025-05-03 01:42:51,288 - main - INFO - Recall: 0.0000
2025-05-03 01:42:51,288 - main - INFO - F1 Score: 0.0000
2025-05-03 01:42:51,288 - main - INFO - IoU: 0.0000
2025-05-03 01:42:51,288 - main - INFO - mAP: 0.2796
2025-05-03 01:42:51,288 - main - INFO - AUC: 0.4846
2025-05-03 01:42:51,288 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:42:51,288 - main - INFO - 
==================================================
2025-05-03 01:42:51,288 - main - INFO - Running configuration 377/756:
2025-05-03 01:42:51,288 - main - INFO - Model: ViT-B-16
2025-05-03 01:42:51,288 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:42:51,288 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 01:42:51,288 - main - INFO - Loss Function: FocalLoss
2025-05-03 01:42:51,288 - main - INFO - ==================================================
2025-05-03 01:42:51,288 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_377 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_377
2025-05-03 01:42:51,292 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_377 - INFO - Config: {
  "id": 377,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 01:42:52,087 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:42:52,087 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 377,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 01:42:52,087 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:42:52,276 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:42:52,276 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:42:52,277 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_377 - INFO - Starting model evaluation
2025-05-03 01:43:58,504 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_377 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2791
  AUC:       0.4843
2025-05-03 01:43:58,506 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_377 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_377/final_results.json
2025-05-03 01:43:58,508 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_377 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_377/final_results.json
2025-05-03 01:43:58,508 - main - INFO - 
Summary for configuration 377:
2025-05-03 01:43:58,508 - main - INFO - Accuracy: 0.7166
2025-05-03 01:43:58,508 - main - INFO - Precision: 0.0000
2025-05-03 01:43:58,508 - main - INFO - Recall: 0.0000
2025-05-03 01:43:58,508 - main - INFO - F1 Score: 0.0000
2025-05-03 01:43:58,508 - main - INFO - IoU: 0.0000
2025-05-03 01:43:58,508 - main - INFO - mAP: 0.2791
2025-05-03 01:43:58,508 - main - INFO - AUC: 0.4843
2025-05-03 01:43:58,508 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:43:58,508 - main - INFO - 
==================================================
2025-05-03 01:43:58,508 - main - INFO - Running configuration 378/756:
2025-05-03 01:43:58,508 - main - INFO - Model: ViT-B-16
2025-05-03 01:43:58,508 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:43:58,508 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 01:43:58,508 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 01:43:58,508 - main - INFO - ==================================================
2025-05-03 01:43:58,509 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_378 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_378
2025-05-03 01:43:58,510 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_378 - INFO - Config: {
  "id": 378,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 01:43:59,324 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:43:59,324 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 378,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 01:43:59,324 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:43:59,513 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:43:59,513 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:43:59,514 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_378 - INFO - Starting model evaluation
2025-05-03 01:45:03,906 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_378 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2797
  AUC:       0.4847
2025-05-03 01:45:03,908 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_378 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_378/final_results.json
2025-05-03 01:45:03,911 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_378 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_378/final_results.json
2025-05-03 01:45:03,911 - main - INFO - 
Summary for configuration 378:
2025-05-03 01:45:03,911 - main - INFO - Accuracy: 0.7166
2025-05-03 01:45:03,911 - main - INFO - Precision: 0.0000
2025-05-03 01:45:03,911 - main - INFO - Recall: 0.0000
2025-05-03 01:45:03,911 - main - INFO - F1 Score: 0.0000
2025-05-03 01:45:03,911 - main - INFO - IoU: 0.0000
2025-05-03 01:45:03,911 - main - INFO - mAP: 0.2797
2025-05-03 01:45:03,911 - main - INFO - AUC: 0.4847
2025-05-03 01:45:03,911 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:45:03,911 - main - INFO - 
==================================================
2025-05-03 01:45:03,911 - main - INFO - Running configuration 379/756:
2025-05-03 01:45:03,911 - main - INFO - Model: ViT-B-16
2025-05-03 01:45:03,911 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:45:03,911 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 01:45:03,911 - main - INFO - Loss Function: CrossEntropy
2025-05-03 01:45:03,911 - main - INFO - ==================================================
2025-05-03 01:45:03,912 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_379 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_379
2025-05-03 01:45:03,912 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_379 - INFO - Config: {
  "id": 379,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 01:45:04,687 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:45:04,687 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 379,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 01:45:04,687 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:45:04,868 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:45:04,868 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:45:04,869 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_379 - INFO - Starting model evaluation
2025-05-03 01:46:09,466 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_379 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2788
  AUC:       0.4835
2025-05-03 01:46:09,469 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_379 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_379/final_results.json
2025-05-03 01:46:09,471 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_379 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_379/final_results.json
2025-05-03 01:46:09,471 - main - INFO - 
Summary for configuration 379:
2025-05-03 01:46:09,471 - main - INFO - Accuracy: 0.7166
2025-05-03 01:46:09,471 - main - INFO - Precision: 0.0000
2025-05-03 01:46:09,471 - main - INFO - Recall: 0.0000
2025-05-03 01:46:09,471 - main - INFO - F1 Score: 0.0000
2025-05-03 01:46:09,471 - main - INFO - IoU: 0.0000
2025-05-03 01:46:09,471 - main - INFO - mAP: 0.2788
2025-05-03 01:46:09,471 - main - INFO - AUC: 0.4835
2025-05-03 01:46:09,471 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:46:09,471 - main - INFO - 
==================================================
2025-05-03 01:46:09,471 - main - INFO - Running configuration 380/756:
2025-05-03 01:46:09,471 - main - INFO - Model: ViT-B-16
2025-05-03 01:46:09,471 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:46:09,471 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 01:46:09,471 - main - INFO - Loss Function: FocalLoss
2025-05-03 01:46:09,471 - main - INFO - ==================================================
2025-05-03 01:46:09,472 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_380 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_380
2025-05-03 01:46:09,472 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_380 - INFO - Config: {
  "id": 380,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 01:46:10,449 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:46:10,450 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 380,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 01:46:10,450 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:46:10,737 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:46:10,738 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:46:10,741 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_380 - INFO - Starting model evaluation
2025-05-03 01:47:13,716 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_380 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2799
  AUC:       0.4847
2025-05-03 01:47:13,719 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_380 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_380/final_results.json
2025-05-03 01:47:13,722 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_380 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_380/final_results.json
2025-05-03 01:47:13,722 - main - INFO - 
Summary for configuration 380:
2025-05-03 01:47:13,722 - main - INFO - Accuracy: 0.7166
2025-05-03 01:47:13,722 - main - INFO - Precision: 0.0000
2025-05-03 01:47:13,722 - main - INFO - Recall: 0.0000
2025-05-03 01:47:13,722 - main - INFO - F1 Score: 0.0000
2025-05-03 01:47:13,722 - main - INFO - IoU: 0.0000
2025-05-03 01:47:13,722 - main - INFO - mAP: 0.2799
2025-05-03 01:47:13,722 - main - INFO - AUC: 0.4847
2025-05-03 01:47:13,722 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:47:13,722 - main - INFO - 
==================================================
2025-05-03 01:47:13,722 - main - INFO - Running configuration 381/756:
2025-05-03 01:47:13,722 - main - INFO - Model: ViT-B-16
2025-05-03 01:47:13,722 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:47:13,722 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 01:47:13,722 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 01:47:13,722 - main - INFO - ==================================================
2025-05-03 01:47:13,723 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_381 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_381
2025-05-03 01:47:13,723 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_381 - INFO - Config: {
  "id": 381,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 01:47:14,774 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:47:14,774 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 381,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 01:47:14,774 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:47:14,961 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:47:14,962 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:47:14,963 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_381 - INFO - Starting model evaluation
2025-05-03 01:48:18,658 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_381 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2799
  AUC:       0.4843
2025-05-03 01:48:18,661 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_381 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_381/final_results.json
2025-05-03 01:48:18,664 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_381 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_381/final_results.json
2025-05-03 01:48:18,664 - main - INFO - 
Summary for configuration 381:
2025-05-03 01:48:18,664 - main - INFO - Accuracy: 0.7166
2025-05-03 01:48:18,664 - main - INFO - Precision: 0.0000
2025-05-03 01:48:18,664 - main - INFO - Recall: 0.0000
2025-05-03 01:48:18,664 - main - INFO - F1 Score: 0.0000
2025-05-03 01:48:18,664 - main - INFO - IoU: 0.0000
2025-05-03 01:48:18,664 - main - INFO - mAP: 0.2799
2025-05-03 01:48:18,664 - main - INFO - AUC: 0.4843
2025-05-03 01:48:18,664 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:48:18,664 - main - INFO - 
==================================================
2025-05-03 01:48:18,664 - main - INFO - Running configuration 382/756:
2025-05-03 01:48:18,664 - main - INFO - Model: ViT-B-16
2025-05-03 01:48:18,664 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:48:18,664 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 01:48:18,664 - main - INFO - Loss Function: CrossEntropy
2025-05-03 01:48:18,664 - main - INFO - ==================================================
2025-05-03 01:48:18,664 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_382 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_382
2025-05-03 01:48:18,664 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_382 - INFO - Config: {
  "id": 382,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 01:48:19,351 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:48:19,351 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 382,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 01:48:19,351 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:48:19,532 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:48:19,532 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:48:19,533 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_382 - INFO - Starting model evaluation
2025-05-03 01:49:22,762 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_382 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2798
  AUC:       0.4848
2025-05-03 01:49:22,765 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_382 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_382/final_results.json
2025-05-03 01:49:22,767 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_382 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_382/final_results.json
2025-05-03 01:49:22,767 - main - INFO - 
Summary for configuration 382:
2025-05-03 01:49:22,767 - main - INFO - Accuracy: 0.7166
2025-05-03 01:49:22,767 - main - INFO - Precision: 0.0000
2025-05-03 01:49:22,767 - main - INFO - Recall: 0.0000
2025-05-03 01:49:22,767 - main - INFO - F1 Score: 0.0000
2025-05-03 01:49:22,767 - main - INFO - IoU: 0.0000
2025-05-03 01:49:22,767 - main - INFO - mAP: 0.2798
2025-05-03 01:49:22,767 - main - INFO - AUC: 0.4848
2025-05-03 01:49:22,767 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:49:22,767 - main - INFO - 
==================================================
2025-05-03 01:49:22,767 - main - INFO - Running configuration 383/756:
2025-05-03 01:49:22,767 - main - INFO - Model: ViT-B-16
2025-05-03 01:49:22,767 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:49:22,767 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 01:49:22,767 - main - INFO - Loss Function: FocalLoss
2025-05-03 01:49:22,767 - main - INFO - ==================================================
2025-05-03 01:49:22,768 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_383 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_383
2025-05-03 01:49:22,768 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_383 - INFO - Config: {
  "id": 383,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 01:49:23,770 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:49:23,770 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 383,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 01:49:23,771 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:49:24,066 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:49:24,067 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:49:24,069 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_383 - INFO - Starting model evaluation
2025-05-03 01:50:28,206 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_383 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2805
  AUC:       0.4853
2025-05-03 01:50:28,209 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_383 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_383/final_results.json
2025-05-03 01:50:28,211 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_383 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_383/final_results.json
2025-05-03 01:50:28,211 - main - INFO - 
Summary for configuration 383:
2025-05-03 01:50:28,211 - main - INFO - Accuracy: 0.7166
2025-05-03 01:50:28,211 - main - INFO - Precision: 0.0000
2025-05-03 01:50:28,211 - main - INFO - Recall: 0.0000
2025-05-03 01:50:28,211 - main - INFO - F1 Score: 0.0000
2025-05-03 01:50:28,211 - main - INFO - IoU: 0.0000
2025-05-03 01:50:28,211 - main - INFO - mAP: 0.2805
2025-05-03 01:50:28,212 - main - INFO - AUC: 0.4853
2025-05-03 01:50:28,212 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:50:28,212 - main - INFO - 
==================================================
2025-05-03 01:50:28,212 - main - INFO - Running configuration 384/756:
2025-05-03 01:50:28,212 - main - INFO - Model: ViT-B-16
2025-05-03 01:50:28,212 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 01:50:28,212 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 01:50:28,212 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 01:50:28,212 - main - INFO - ==================================================
2025-05-03 01:50:28,212 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_384 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001_id_384
2025-05-03 01:50:28,212 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_384 - INFO - Config: {
  "id": 384,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 01:50:29,126 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.001
2025-05-03 01:50:29,126 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 384,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 01:50:29,127 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 01:50:29,308 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 01:50:29,308 - training.model_ViT-B-16_opt_Adam_lr_0.001 - INFO - Training completed after 2992.21 seconds
2025-05-03 01:50:29,310 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_384 - INFO - Starting model evaluation
2025-05-03 01:51:35,166 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_384 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2800
  AUC:       0.4855
2025-05-03 01:51:35,168 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_384 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_384/final_results.json
2025-05-03 01:51:35,170 - training.model_ViT-B-16_opt_Adam_lr_0.001_id_384 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.001_id_384/final_results.json
2025-05-03 01:51:35,170 - main - INFO - 
Summary for configuration 384:
2025-05-03 01:51:35,170 - main - INFO - Accuracy: 0.7166
2025-05-03 01:51:35,171 - main - INFO - Precision: 0.0000
2025-05-03 01:51:35,171 - main - INFO - Recall: 0.0000
2025-05-03 01:51:35,171 - main - INFO - F1 Score: 0.0000
2025-05-03 01:51:35,171 - main - INFO - IoU: 0.0000
2025-05-03 01:51:35,171 - main - INFO - mAP: 0.2800
2025-05-03 01:51:35,171 - main - INFO - AUC: 0.4855
2025-05-03 01:51:35,171 - main - INFO - Training time: 2992.21 seconds
2025-05-03 01:51:35,171 - main - INFO - 
==================================================
2025-05-03 01:51:35,171 - main - INFO - Running configuration 385/756:
2025-05-03 01:51:35,171 - main - INFO - Model: ViT-B-16
2025-05-03 01:51:35,171 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 01:51:35,171 - main - INFO - Scheduler: StepLR
2025-05-03 01:51:35,171 - main - INFO - Loss Function: CrossEntropy
2025-05-03 01:51:35,171 - main - INFO - ==================================================
2025-05-03 01:51:35,171 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_385 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_385
2025-05-03 01:51:35,171 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_385 - INFO - Config: {
  "id": 385,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 01:51:35,996 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 01:51:35,996 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 385,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 01:51:35,996 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 01:51:36,323 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 12, best validation accuracy: 0.0000
2025-05-03 01:51:36,324 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 01:54:11,827 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 01:57:47,255 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.8053
2025-05-03 01:57:47,481 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Epoch 13 completed in 371.16s - Train Loss: 0.5009, Train Acc: 0.8024, Val Loss: 0.4962, Val Acc: 0.8053
2025-05-03 01:57:48,306 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 01:57:48,307 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 02:00:23,329 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 02:04:04,406 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Validation loss improved from inf to 0.5528
2025-05-03 02:04:04,691 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Epoch 14 completed in 376.38s - Train Loss: 0.4522, Train Acc: 0.8582, Val Loss: 0.5528, Val Acc: 0.7513
2025-05-03 02:04:05,501 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 02:04:05,501 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 02:06:41,562 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 02:10:19,413 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.8053 to 0.8782
2025-05-03 02:10:19,705 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Epoch 15 completed in 374.20s - Train Loss: 0.4380, Train Acc: 0.8724, Val Loss: 0.4263, Val Acc: 0.8782
2025-05-03 02:10:20,514 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 02:10:20,514 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 02:12:59,027 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 02:16:42,597 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.8782 to 0.8945
2025-05-03 02:16:42,920 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Epoch 16 completed in 382.41s - Train Loss: 0.4124, Train Acc: 0.8927, Val Loss: 0.4069, Val Acc: 0.8945
2025-05-03 02:16:43,659 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 02:16:43,659 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 02:19:20,066 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 02:22:58,317 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.5528 to 0.4249
2025-05-03 02:22:58,564 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Epoch 17 completed in 374.90s - Train Loss: 0.4004, Train Acc: 0.9099, Val Loss: 0.4249, Val Acc: 0.8842
2025-05-03 02:22:59,314 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 02:22:59,315 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 02:25:35,735 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 02:29:11,231 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.8945 to 0.8988
2025-05-03 02:29:11,565 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Epoch 18 completed in 372.25s - Train Loss: 0.4181, Train Acc: 0.8932, Val Loss: 0.4079, Val Acc: 0.8988
2025-05-03 02:29:12,326 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 02:29:12,326 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 02:31:52,166 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 02:35:30,666 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Epoch 19 completed in 378.34s - Train Loss: 0.3900, Train Acc: 0.9202, Val Loss: 0.4330, Val Acc: 0.8748
2025-05-03 02:35:31,452 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 02:35:31,452 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 02:38:12,708 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 02:41:49,417 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Epoch 20 completed in 377.96s - Train Loss: 0.4127, Train Acc: 0.8968, Val Loss: 0.5376, Val Acc: 0.7667
2025-05-03 02:41:50,292 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 02:41:50,292 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:41:50,294 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_385 - INFO - Starting model evaluation
2025-05-03 02:42:54,168 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_385 - INFO - Evaluation metrics:
  Accuracy:  0.8394
  Precision: 0.8780
  Recall:    0.5035
  F1 Score:  0.6400
  IoU:       0.4706
  mAP:       0.8023
  AUC:       0.9014
2025-05-03 02:42:54,170 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_385 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_385/final_results.json
2025-05-03 02:42:54,172 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_385 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_385/final_results.json
2025-05-03 02:42:54,172 - main - INFO - 
Summary for configuration 385:
2025-05-03 02:42:54,172 - main - INFO - Accuracy: 0.8394
2025-05-03 02:42:54,172 - main - INFO - Precision: 0.8780
2025-05-03 02:42:54,172 - main - INFO - Recall: 0.5035
2025-05-03 02:42:54,172 - main - INFO - F1 Score: 0.6400
2025-05-03 02:42:54,172 - main - INFO - IoU: 0.4706
2025-05-03 02:42:54,172 - main - INFO - mAP: 0.8023
2025-05-03 02:42:54,172 - main - INFO - AUC: 0.9014
2025-05-03 02:42:54,173 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:42:54,173 - main - INFO - 
==================================================
2025-05-03 02:42:54,173 - main - INFO - Running configuration 386/756:
2025-05-03 02:42:54,173 - main - INFO - Model: ViT-B-16
2025-05-03 02:42:54,173 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:42:54,173 - main - INFO - Scheduler: StepLR
2025-05-03 02:42:54,173 - main - INFO - Loss Function: FocalLoss
2025-05-03 02:42:54,173 - main - INFO - ==================================================
2025-05-03 02:42:54,173 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_386 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_386
2025-05-03 02:42:54,173 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_386 - INFO - Config: {
  "id": 386,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 02:42:55,318 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:42:55,318 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 386,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 02:42:55,318 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:42:55,629 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:42:55,630 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:42:55,633 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_386 - INFO - Starting model evaluation
2025-05-03 02:44:00,147 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_386 - INFO - Evaluation metrics:
  Accuracy:  0.8335
  Precision: 0.8471
  Recall:    0.5035
  F1 Score:  0.6316
  IoU:       0.4615
  mAP:       0.7928
  AUC:       0.8958
2025-05-03 02:44:00,150 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_386 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_386/final_results.json
2025-05-03 02:44:00,152 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_386 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_386/final_results.json
2025-05-03 02:44:00,152 - main - INFO - 
Summary for configuration 386:
2025-05-03 02:44:00,152 - main - INFO - Accuracy: 0.8335
2025-05-03 02:44:00,152 - main - INFO - Precision: 0.8471
2025-05-03 02:44:00,152 - main - INFO - Recall: 0.5035
2025-05-03 02:44:00,152 - main - INFO - F1 Score: 0.6316
2025-05-03 02:44:00,152 - main - INFO - IoU: 0.4615
2025-05-03 02:44:00,152 - main - INFO - mAP: 0.7928
2025-05-03 02:44:00,153 - main - INFO - AUC: 0.8958
2025-05-03 02:44:00,153 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:44:00,153 - main - INFO - 
==================================================
2025-05-03 02:44:00,153 - main - INFO - Running configuration 387/756:
2025-05-03 02:44:00,153 - main - INFO - Model: ViT-B-16
2025-05-03 02:44:00,153 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:44:00,153 - main - INFO - Scheduler: StepLR
2025-05-03 02:44:00,153 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 02:44:00,153 - main - INFO - ==================================================
2025-05-03 02:44:00,153 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_387 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_387
2025-05-03 02:44:00,153 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_387 - INFO - Config: {
  "id": 387,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 02:44:00,781 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:44:00,781 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 387,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 02:44:00,781 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:44:00,971 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:44:00,972 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:44:00,973 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_387 - INFO - Starting model evaluation
2025-05-03 02:45:05,734 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_387 - INFO - Evaluation metrics:
  Accuracy:  0.8385
  Precision: 0.8596
  Recall:    0.5140
  F1 Score:  0.6433
  IoU:       0.4742
  mAP:       0.7924
  AUC:       0.8980
2025-05-03 02:45:05,737 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_387 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_387/final_results.json
2025-05-03 02:45:05,740 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_387 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_387/final_results.json
2025-05-03 02:45:05,740 - main - INFO - 
Summary for configuration 387:
2025-05-03 02:45:05,740 - main - INFO - Accuracy: 0.8385
2025-05-03 02:45:05,740 - main - INFO - Precision: 0.8596
2025-05-03 02:45:05,740 - main - INFO - Recall: 0.5140
2025-05-03 02:45:05,740 - main - INFO - F1 Score: 0.6433
2025-05-03 02:45:05,740 - main - INFO - IoU: 0.4742
2025-05-03 02:45:05,740 - main - INFO - mAP: 0.7924
2025-05-03 02:45:05,740 - main - INFO - AUC: 0.8980
2025-05-03 02:45:05,740 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:45:05,740 - main - INFO - 
==================================================
2025-05-03 02:45:05,740 - main - INFO - Running configuration 388/756:
2025-05-03 02:45:05,740 - main - INFO - Model: ViT-B-16
2025-05-03 02:45:05,740 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:45:05,740 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 02:45:05,740 - main - INFO - Loss Function: CrossEntropy
2025-05-03 02:45:05,740 - main - INFO - ==================================================
2025-05-03 02:45:05,740 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_388 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_388
2025-05-03 02:45:05,740 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_388 - INFO - Config: {
  "id": 388,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 02:45:06,776 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:45:06,776 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 388,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 02:45:06,776 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:45:07,058 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:45:07,059 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:45:07,062 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_388 - INFO - Starting model evaluation
2025-05-03 02:46:12,335 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_388 - INFO - Evaluation metrics:
  Accuracy:  0.8285
  Precision: 0.8553
  Recall:    0.4755
  F1 Score:  0.6112
  IoU:       0.4401
  mAP:       0.7966
  AUC:       0.8945
2025-05-03 02:46:12,338 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_388 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_388/final_results.json
2025-05-03 02:46:12,341 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_388 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_388/final_results.json
2025-05-03 02:46:12,341 - main - INFO - 
Summary for configuration 388:
2025-05-03 02:46:12,341 - main - INFO - Accuracy: 0.8285
2025-05-03 02:46:12,341 - main - INFO - Precision: 0.8553
2025-05-03 02:46:12,341 - main - INFO - Recall: 0.4755
2025-05-03 02:46:12,341 - main - INFO - F1 Score: 0.6112
2025-05-03 02:46:12,341 - main - INFO - IoU: 0.4401
2025-05-03 02:46:12,341 - main - INFO - mAP: 0.7966
2025-05-03 02:46:12,341 - main - INFO - AUC: 0.8945
2025-05-03 02:46:12,341 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:46:12,341 - main - INFO - 
==================================================
2025-05-03 02:46:12,341 - main - INFO - Running configuration 389/756:
2025-05-03 02:46:12,341 - main - INFO - Model: ViT-B-16
2025-05-03 02:46:12,341 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:46:12,341 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 02:46:12,341 - main - INFO - Loss Function: FocalLoss
2025-05-03 02:46:12,341 - main - INFO - ==================================================
2025-05-03 02:46:12,342 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_389 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_389
2025-05-03 02:46:12,343 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_389 - INFO - Config: {
  "id": 389,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 02:46:13,213 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:46:13,213 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 389,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 02:46:13,213 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:46:13,508 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:46:13,509 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:46:13,511 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_389 - INFO - Starting model evaluation
2025-05-03 02:47:18,846 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_389 - INFO - Evaluation metrics:
  Accuracy:  0.8394
  Precision: 0.8875
  Recall:    0.4965
  F1 Score:  0.6368
  IoU:       0.4671
  mAP:       0.7957
  AUC:       0.8981
2025-05-03 02:47:18,848 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_389 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_389/final_results.json
2025-05-03 02:47:18,851 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_389 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_389/final_results.json
2025-05-03 02:47:18,851 - main - INFO - 
Summary for configuration 389:
2025-05-03 02:47:18,851 - main - INFO - Accuracy: 0.8394
2025-05-03 02:47:18,851 - main - INFO - Precision: 0.8875
2025-05-03 02:47:18,851 - main - INFO - Recall: 0.4965
2025-05-03 02:47:18,851 - main - INFO - F1 Score: 0.6368
2025-05-03 02:47:18,851 - main - INFO - IoU: 0.4671
2025-05-03 02:47:18,851 - main - INFO - mAP: 0.7957
2025-05-03 02:47:18,851 - main - INFO - AUC: 0.8981
2025-05-03 02:47:18,851 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:47:18,851 - main - INFO - 
==================================================
2025-05-03 02:47:18,851 - main - INFO - Running configuration 390/756:
2025-05-03 02:47:18,851 - main - INFO - Model: ViT-B-16
2025-05-03 02:47:18,851 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:47:18,851 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 02:47:18,851 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 02:47:18,851 - main - INFO - ==================================================
2025-05-03 02:47:18,852 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_390 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_390
2025-05-03 02:47:18,852 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_390 - INFO - Config: {
  "id": 390,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 02:47:19,690 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:47:19,693 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 390,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 02:47:19,693 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:47:19,887 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:47:19,888 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:47:19,889 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_390 - INFO - Starting model evaluation
2025-05-03 02:48:25,318 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_390 - INFO - Evaluation metrics:
  Accuracy:  0.8305
  Precision: 0.8571
  Recall:    0.4825
  F1 Score:  0.6174
  IoU:       0.4466
  mAP:       0.7964
  AUC:       0.8940
2025-05-03 02:48:25,320 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_390 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_390/final_results.json
2025-05-03 02:48:25,322 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_390 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_390/final_results.json
2025-05-03 02:48:25,323 - main - INFO - 
Summary for configuration 390:
2025-05-03 02:48:25,323 - main - INFO - Accuracy: 0.8305
2025-05-03 02:48:25,323 - main - INFO - Precision: 0.8571
2025-05-03 02:48:25,323 - main - INFO - Recall: 0.4825
2025-05-03 02:48:25,323 - main - INFO - F1 Score: 0.6174
2025-05-03 02:48:25,323 - main - INFO - IoU: 0.4466
2025-05-03 02:48:25,323 - main - INFO - mAP: 0.7964
2025-05-03 02:48:25,323 - main - INFO - AUC: 0.8940
2025-05-03 02:48:25,323 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:48:25,323 - main - INFO - 
==================================================
2025-05-03 02:48:25,323 - main - INFO - Running configuration 391/756:
2025-05-03 02:48:25,323 - main - INFO - Model: ViT-B-16
2025-05-03 02:48:25,323 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:48:25,323 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 02:48:25,323 - main - INFO - Loss Function: CrossEntropy
2025-05-03 02:48:25,323 - main - INFO - ==================================================
2025-05-03 02:48:25,324 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_391 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_391
2025-05-03 02:48:25,324 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_391 - INFO - Config: {
  "id": 391,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 02:48:26,305 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:48:26,305 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 391,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 02:48:26,305 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:48:26,489 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:48:26,490 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:48:26,491 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_391 - INFO - Starting model evaluation
2025-05-03 02:49:30,124 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_391 - INFO - Evaluation metrics:
  Accuracy:  0.8345
  Precision: 0.8650
  Recall:    0.4930
  F1 Score:  0.6281
  IoU:       0.4578
  mAP:       0.7994
  AUC:       0.8996
2025-05-03 02:49:30,127 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_391 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_391/final_results.json
2025-05-03 02:49:30,130 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_391 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_391/final_results.json
2025-05-03 02:49:30,130 - main - INFO - 
Summary for configuration 391:
2025-05-03 02:49:30,130 - main - INFO - Accuracy: 0.8345
2025-05-03 02:49:30,130 - main - INFO - Precision: 0.8650
2025-05-03 02:49:30,130 - main - INFO - Recall: 0.4930
2025-05-03 02:49:30,130 - main - INFO - F1 Score: 0.6281
2025-05-03 02:49:30,130 - main - INFO - IoU: 0.4578
2025-05-03 02:49:30,130 - main - INFO - mAP: 0.7994
2025-05-03 02:49:30,130 - main - INFO - AUC: 0.8996
2025-05-03 02:49:30,130 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:49:30,130 - main - INFO - 
==================================================
2025-05-03 02:49:30,130 - main - INFO - Running configuration 392/756:
2025-05-03 02:49:30,130 - main - INFO - Model: ViT-B-16
2025-05-03 02:49:30,130 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:49:30,130 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 02:49:30,130 - main - INFO - Loss Function: FocalLoss
2025-05-03 02:49:30,130 - main - INFO - ==================================================
2025-05-03 02:49:30,130 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_392 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_392
2025-05-03 02:49:30,130 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_392 - INFO - Config: {
  "id": 392,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 02:49:31,045 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:49:31,045 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 392,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 02:49:31,045 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:49:31,334 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:49:31,336 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:49:31,338 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_392 - INFO - Starting model evaluation
2025-05-03 02:50:31,089 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_392 - INFO - Evaluation metrics:
  Accuracy:  0.8256
  Precision: 0.8313
  Recall:    0.4825
  F1 Score:  0.6106
  IoU:       0.4395
  mAP:       0.7677
  AUC:       0.8864
2025-05-03 02:50:31,091 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_392 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_392/final_results.json
2025-05-03 02:50:31,093 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_392 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_392/final_results.json
2025-05-03 02:50:31,093 - main - INFO - 
Summary for configuration 392:
2025-05-03 02:50:31,093 - main - INFO - Accuracy: 0.8256
2025-05-03 02:50:31,093 - main - INFO - Precision: 0.8313
2025-05-03 02:50:31,093 - main - INFO - Recall: 0.4825
2025-05-03 02:50:31,093 - main - INFO - F1 Score: 0.6106
2025-05-03 02:50:31,093 - main - INFO - IoU: 0.4395
2025-05-03 02:50:31,093 - main - INFO - mAP: 0.7677
2025-05-03 02:50:31,093 - main - INFO - AUC: 0.8864
2025-05-03 02:50:31,093 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:50:31,093 - main - INFO - 
==================================================
2025-05-03 02:50:31,093 - main - INFO - Running configuration 393/756:
2025-05-03 02:50:31,093 - main - INFO - Model: ViT-B-16
2025-05-03 02:50:31,093 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:50:31,093 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 02:50:31,093 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 02:50:31,093 - main - INFO - ==================================================
2025-05-03 02:50:31,094 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_393 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_393
2025-05-03 02:50:31,094 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_393 - INFO - Config: {
  "id": 393,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 02:50:31,670 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:50:31,670 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 393,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 02:50:31,670 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:50:31,853 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:50:31,854 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:50:31,855 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_393 - INFO - Starting model evaluation
2025-05-03 02:51:36,749 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_393 - INFO - Evaluation metrics:
  Accuracy:  0.8375
  Precision: 0.8765
  Recall:    0.4965
  F1 Score:  0.6339
  IoU:       0.4641
  mAP:       0.7914
  AUC:       0.8959
2025-05-03 02:51:36,752 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_393 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_393/final_results.json
2025-05-03 02:51:36,754 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_393 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_393/final_results.json
2025-05-03 02:51:36,754 - main - INFO - 
Summary for configuration 393:
2025-05-03 02:51:36,755 - main - INFO - Accuracy: 0.8375
2025-05-03 02:51:36,755 - main - INFO - Precision: 0.8765
2025-05-03 02:51:36,755 - main - INFO - Recall: 0.4965
2025-05-03 02:51:36,755 - main - INFO - F1 Score: 0.6339
2025-05-03 02:51:36,755 - main - INFO - IoU: 0.4641
2025-05-03 02:51:36,755 - main - INFO - mAP: 0.7914
2025-05-03 02:51:36,755 - main - INFO - AUC: 0.8959
2025-05-03 02:51:36,755 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:51:36,755 - main - INFO - 
==================================================
2025-05-03 02:51:36,755 - main - INFO - Running configuration 394/756:
2025-05-03 02:51:36,755 - main - INFO - Model: ViT-B-16
2025-05-03 02:51:36,755 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:51:36,755 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 02:51:36,755 - main - INFO - Loss Function: CrossEntropy
2025-05-03 02:51:36,755 - main - INFO - ==================================================
2025-05-03 02:51:36,759 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_394 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_394
2025-05-03 02:51:36,759 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_394 - INFO - Config: {
  "id": 394,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 02:51:37,869 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:51:37,870 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 394,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 02:51:37,870 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:51:38,052 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:51:38,053 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:51:38,054 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_394 - INFO - Starting model evaluation
2025-05-03 02:52:42,383 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_394 - INFO - Evaluation metrics:
  Accuracy:  0.8375
  Precision: 0.8675
  Recall:    0.5035
  F1 Score:  0.6372
  IoU:       0.4675
  mAP:       0.8081
  AUC:       0.8992
2025-05-03 02:52:42,386 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_394 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_394/final_results.json
2025-05-03 02:52:42,389 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_394 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_394/final_results.json
2025-05-03 02:52:42,389 - main - INFO - 
Summary for configuration 394:
2025-05-03 02:52:42,389 - main - INFO - Accuracy: 0.8375
2025-05-03 02:52:42,389 - main - INFO - Precision: 0.8675
2025-05-03 02:52:42,389 - main - INFO - Recall: 0.5035
2025-05-03 02:52:42,389 - main - INFO - F1 Score: 0.6372
2025-05-03 02:52:42,389 - main - INFO - IoU: 0.4675
2025-05-03 02:52:42,389 - main - INFO - mAP: 0.8081
2025-05-03 02:52:42,389 - main - INFO - AUC: 0.8992
2025-05-03 02:52:42,389 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:52:42,389 - main - INFO - 
==================================================
2025-05-03 02:52:42,389 - main - INFO - Running configuration 395/756:
2025-05-03 02:52:42,389 - main - INFO - Model: ViT-B-16
2025-05-03 02:52:42,389 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:52:42,389 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 02:52:42,389 - main - INFO - Loss Function: FocalLoss
2025-05-03 02:52:42,389 - main - INFO - ==================================================
2025-05-03 02:52:42,390 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_395 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_395
2025-05-03 02:52:42,390 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_395 - INFO - Config: {
  "id": 395,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 02:52:43,428 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:52:43,432 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 395,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 02:52:43,433 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:52:43,729 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:52:43,730 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:52:43,733 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_395 - INFO - Starting model evaluation
2025-05-03 02:53:47,467 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_395 - INFO - Evaluation metrics:
  Accuracy:  0.8335
  Precision: 0.8554
  Recall:    0.4965
  F1 Score:  0.6283
  IoU:       0.4581
  mAP:       0.7955
  AUC:       0.8994
2025-05-03 02:53:47,470 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_395 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_395/final_results.json
2025-05-03 02:53:47,473 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_395 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_395/final_results.json
2025-05-03 02:53:47,473 - main - INFO - 
Summary for configuration 395:
2025-05-03 02:53:47,473 - main - INFO - Accuracy: 0.8335
2025-05-03 02:53:47,473 - main - INFO - Precision: 0.8554
2025-05-03 02:53:47,473 - main - INFO - Recall: 0.4965
2025-05-03 02:53:47,473 - main - INFO - F1 Score: 0.6283
2025-05-03 02:53:47,473 - main - INFO - IoU: 0.4581
2025-05-03 02:53:47,473 - main - INFO - mAP: 0.7955
2025-05-03 02:53:47,473 - main - INFO - AUC: 0.8994
2025-05-03 02:53:47,473 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:53:47,473 - main - INFO - 
==================================================
2025-05-03 02:53:47,473 - main - INFO - Running configuration 396/756:
2025-05-03 02:53:47,473 - main - INFO - Model: ViT-B-16
2025-05-03 02:53:47,473 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 02:53:47,473 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 02:53:47,473 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 02:53:47,473 - main - INFO - ==================================================
2025-05-03 02:53:47,474 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_396 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001_id_396
2025-05-03 02:53:47,475 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_396 - INFO - Config: {
  "id": 396,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 02:53:48,282 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_Adam_lr_0.0001
2025-05-03 02:53:48,282 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 396,
  "model_name": "ViT-B-16",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 02:53:48,282 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 02:53:48,465 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8988
2025-05-03 02:53:48,465 - training.model_ViT-B-16_opt_Adam_lr_0.0001 - INFO - Training completed after 3014.98 seconds
2025-05-03 02:53:48,466 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_396 - INFO - Starting model evaluation
2025-05-03 02:54:52,803 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_396 - INFO - Evaluation metrics:
  Accuracy:  0.8325
  Precision: 0.8679
  Recall:    0.4825
  F1 Score:  0.6202
  IoU:       0.4495
  mAP:       0.7939
  AUC:       0.8977
2025-05-03 02:54:52,806 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_396 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_396/final_results.json
2025-05-03 02:54:52,809 - training.model_ViT-B-16_opt_Adam_lr_0.0001_id_396 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_Adam_lr_0.0001_id_396/final_results.json
2025-05-03 02:54:52,809 - main - INFO - 
Summary for configuration 396:
2025-05-03 02:54:52,809 - main - INFO - Accuracy: 0.8325
2025-05-03 02:54:52,809 - main - INFO - Precision: 0.8679
2025-05-03 02:54:52,809 - main - INFO - Recall: 0.4825
2025-05-03 02:54:52,809 - main - INFO - F1 Score: 0.6202
2025-05-03 02:54:52,809 - main - INFO - IoU: 0.4495
2025-05-03 02:54:52,809 - main - INFO - mAP: 0.7939
2025-05-03 02:54:52,809 - main - INFO - AUC: 0.8977
2025-05-03 02:54:52,809 - main - INFO - Training time: 3014.98 seconds
2025-05-03 02:54:52,809 - main - INFO - 
==================================================
2025-05-03 02:54:52,809 - main - INFO - Running configuration 397/756:
2025-05-03 02:54:52,809 - main - INFO - Model: ViT-B-16
2025-05-03 02:54:52,809 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 02:54:52,809 - main - INFO - Scheduler: StepLR
2025-05-03 02:54:52,809 - main - INFO - Loss Function: CrossEntropy
2025-05-03 02:54:52,809 - main - INFO - ==================================================
2025-05-03 02:54:52,809 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_397 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_397
2025-05-03 02:54:52,810 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_397 - INFO - Config: {
  "id": 397,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 02:54:53,950 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 02:54:53,950 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 397,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 02:54:53,950 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 02:54:54,279 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 12, best validation accuracy: 0.0000
2025-05-03 02:54:54,280 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 02:57:26,257 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:01:04,744 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.4957
2025-05-03 03:01:04,990 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Epoch 13 completed in 370.71s - Train Loss: 0.8084, Train Acc: 0.5047, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:01:06,006 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:01:06,006 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 03:03:49,416 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:08:46,638 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Validation loss improved from inf to 0.8169
2025-05-03 03:08:47,015 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Epoch 14 completed in 461.01s - Train Loss: 0.8128, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:08:47,784 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:08:47,784 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 03:09:28,604 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:10:25,127 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Epoch 15 completed in 97.34s - Train Loss: 0.8128, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:10:25,974 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:10:25,974 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 03:11:10,017 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:12:06,155 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Epoch 16 completed in 100.18s - Train Loss: 0.8128, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:12:07,045 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:12:07,046 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 03:12:51,495 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:13:46,893 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Epoch 17 completed in 99.85s - Train Loss: 0.8109, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:13:48,005 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:13:48,005 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 03:14:32,018 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 03:15:27,661 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Epoch 18 completed in 99.66s - Train Loss: 0.8116, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:15:28,763 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 03:15:28,763 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 03:16:15,790 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 03:17:14,233 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Epoch 19 completed in 105.47s - Train Loss: 0.8135, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:17:15,322 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 03:17:15,323 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 19 epochs
2025-05-03 03:17:15,323 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1341.91 seconds
2025-05-03 03:17:15,324 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_397 - INFO - Starting model evaluation
2025-05-03 03:17:27,964 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_397 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:17:27,966 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_397 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_397/final_results.json
2025-05-03 03:17:27,967 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_397 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_397/final_results.json
2025-05-03 03:17:27,967 - main - INFO - 
Summary for configuration 397:
2025-05-03 03:17:27,967 - main - INFO - Accuracy: 0.2834
2025-05-03 03:17:27,967 - main - INFO - Precision: 0.2834
2025-05-03 03:17:27,967 - main - INFO - Recall: 1.0000
2025-05-03 03:17:27,967 - main - INFO - F1 Score: 0.4417
2025-05-03 03:17:27,967 - main - INFO - IoU: 0.2834
2025-05-03 03:17:27,967 - main - INFO - mAP: 0.2834
2025-05-03 03:17:27,967 - main - INFO - AUC: 0.5000
2025-05-03 03:17:27,967 - main - INFO - Training time: 1341.91 seconds
2025-05-03 03:17:27,967 - main - INFO - 
==================================================
2025-05-03 03:17:27,967 - main - INFO - Running configuration 398/756:
2025-05-03 03:17:27,967 - main - INFO - Model: ViT-B-16
2025-05-03 03:17:27,967 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:17:27,967 - main - INFO - Scheduler: StepLR
2025-05-03 03:17:27,967 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:17:27,967 - main - INFO - ==================================================
2025-05-03 03:17:27,967 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_398 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_398
2025-05-03 03:17:27,967 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_398 - INFO - Config: {
  "id": 398,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:17:28,534 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:17:28,534 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 398,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:17:28,534 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:17:28,806 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 19, best validation accuracy: 0.4957
2025-05-03 03:17:28,807 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 03:18:15,885 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 03:19:10,355 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8169 to 0.0912
2025-05-03 03:19:10,713 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Epoch 20 completed in 101.91s - Train Loss: 0.0905, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 03:19:13,051 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 03:19:13,051 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:19:13,052 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_398 - INFO - Starting model evaluation
2025-05-03 03:19:28,848 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_398 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:19:28,850 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_398 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_398/final_results.json
2025-05-03 03:19:28,853 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_398 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_398/final_results.json
2025-05-03 03:19:28,853 - main - INFO - 
Summary for configuration 398:
2025-05-03 03:19:28,853 - main - INFO - Accuracy: 0.2834
2025-05-03 03:19:28,853 - main - INFO - Precision: 0.2834
2025-05-03 03:19:28,853 - main - INFO - Recall: 1.0000
2025-05-03 03:19:28,853 - main - INFO - F1 Score: 0.4417
2025-05-03 03:19:28,853 - main - INFO - IoU: 0.2834
2025-05-03 03:19:28,853 - main - INFO - mAP: 0.2834
2025-05-03 03:19:28,853 - main - INFO - AUC: 0.5000
2025-05-03 03:19:28,853 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:19:28,853 - main - INFO - 
==================================================
2025-05-03 03:19:28,853 - main - INFO - Running configuration 399/756:
2025-05-03 03:19:28,853 - main - INFO - Model: ViT-B-16
2025-05-03 03:19:28,853 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:19:28,853 - main - INFO - Scheduler: StepLR
2025-05-03 03:19:28,853 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:19:28,853 - main - INFO - ==================================================
2025-05-03 03:19:28,853 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_399 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_399
2025-05-03 03:19:28,854 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_399 - INFO - Config: {
  "id": 399,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:19:29,451 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:19:29,451 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 399,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:19:29,451 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:19:29,728 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:19:29,729 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:19:29,730 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_399 - INFO - Starting model evaluation
2025-05-03 03:19:46,755 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_399 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:19:46,757 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_399 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_399/final_results.json
2025-05-03 03:19:46,759 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_399 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_399/final_results.json
2025-05-03 03:19:46,759 - main - INFO - 
Summary for configuration 399:
2025-05-03 03:19:46,759 - main - INFO - Accuracy: 0.2834
2025-05-03 03:19:46,759 - main - INFO - Precision: 0.2834
2025-05-03 03:19:46,759 - main - INFO - Recall: 1.0000
2025-05-03 03:19:46,759 - main - INFO - F1 Score: 0.4417
2025-05-03 03:19:46,759 - main - INFO - IoU: 0.2834
2025-05-03 03:19:46,759 - main - INFO - mAP: 0.2834
2025-05-03 03:19:46,759 - main - INFO - AUC: 0.5000
2025-05-03 03:19:46,759 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:19:46,759 - main - INFO - 
==================================================
2025-05-03 03:19:46,759 - main - INFO - Running configuration 400/756:
2025-05-03 03:19:46,759 - main - INFO - Model: ViT-B-16
2025-05-03 03:19:46,760 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:19:46,760 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:19:46,760 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:19:46,760 - main - INFO - ==================================================
2025-05-03 03:19:46,760 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_400 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_400
2025-05-03 03:19:46,761 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_400 - INFO - Config: {
  "id": 400,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:19:47,558 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:19:47,558 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 400,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:19:47,558 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:19:47,802 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:19:47,802 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:19:47,803 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_400 - INFO - Starting model evaluation
2025-05-03 03:19:58,578 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_400 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:19:58,580 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_400 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_400/final_results.json
2025-05-03 03:19:58,581 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_400 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_400/final_results.json
2025-05-03 03:19:58,581 - main - INFO - 
Summary for configuration 400:
2025-05-03 03:19:58,581 - main - INFO - Accuracy: 0.2834
2025-05-03 03:19:58,581 - main - INFO - Precision: 0.2834
2025-05-03 03:19:58,581 - main - INFO - Recall: 1.0000
2025-05-03 03:19:58,581 - main - INFO - F1 Score: 0.4417
2025-05-03 03:19:58,581 - main - INFO - IoU: 0.2834
2025-05-03 03:19:58,581 - main - INFO - mAP: 0.2834
2025-05-03 03:19:58,581 - main - INFO - AUC: 0.5000
2025-05-03 03:19:58,581 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:19:58,582 - main - INFO - 
==================================================
2025-05-03 03:19:58,582 - main - INFO - Running configuration 401/756:
2025-05-03 03:19:58,582 - main - INFO - Model: ViT-B-16
2025-05-03 03:19:58,582 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:19:58,582 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:19:58,582 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:19:58,582 - main - INFO - ==================================================
2025-05-03 03:19:58,582 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_401 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_401
2025-05-03 03:19:58,582 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_401 - INFO - Config: {
  "id": 401,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:19:59,308 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:19:59,308 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 401,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:19:59,308 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:19:59,523 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:19:59,523 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:19:59,525 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_401 - INFO - Starting model evaluation
2025-05-03 03:20:09,914 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_401 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:20:09,916 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_401 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_401/final_results.json
2025-05-03 03:20:09,917 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_401 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_401/final_results.json
2025-05-03 03:20:09,917 - main - INFO - 
Summary for configuration 401:
2025-05-03 03:20:09,917 - main - INFO - Accuracy: 0.2834
2025-05-03 03:20:09,917 - main - INFO - Precision: 0.2834
2025-05-03 03:20:09,917 - main - INFO - Recall: 1.0000
2025-05-03 03:20:09,917 - main - INFO - F1 Score: 0.4417
2025-05-03 03:20:09,917 - main - INFO - IoU: 0.2834
2025-05-03 03:20:09,917 - main - INFO - mAP: 0.2834
2025-05-03 03:20:09,917 - main - INFO - AUC: 0.5000
2025-05-03 03:20:09,917 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:20:09,917 - main - INFO - 
==================================================
2025-05-03 03:20:09,917 - main - INFO - Running configuration 402/756:
2025-05-03 03:20:09,917 - main - INFO - Model: ViT-B-16
2025-05-03 03:20:09,917 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:20:09,917 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:20:09,917 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:20:09,917 - main - INFO - ==================================================
2025-05-03 03:20:09,917 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_402 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_402
2025-05-03 03:20:09,917 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_402 - INFO - Config: {
  "id": 402,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:20:10,431 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:20:10,431 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 402,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:20:10,432 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:20:10,840 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:20:10,841 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:20:10,842 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_402 - INFO - Starting model evaluation
2025-05-03 03:20:21,609 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_402 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:20:21,611 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_402 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_402/final_results.json
2025-05-03 03:20:21,613 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_402 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_402/final_results.json
2025-05-03 03:20:21,613 - main - INFO - 
Summary for configuration 402:
2025-05-03 03:20:21,613 - main - INFO - Accuracy: 0.2834
2025-05-03 03:20:21,613 - main - INFO - Precision: 0.2834
2025-05-03 03:20:21,613 - main - INFO - Recall: 1.0000
2025-05-03 03:20:21,613 - main - INFO - F1 Score: 0.4417
2025-05-03 03:20:21,613 - main - INFO - IoU: 0.2834
2025-05-03 03:20:21,613 - main - INFO - mAP: 0.2834
2025-05-03 03:20:21,613 - main - INFO - AUC: 0.5000
2025-05-03 03:20:21,613 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:20:21,613 - main - INFO - 
==================================================
2025-05-03 03:20:21,613 - main - INFO - Running configuration 403/756:
2025-05-03 03:20:21,613 - main - INFO - Model: ViT-B-16
2025-05-03 03:20:21,613 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:20:21,613 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:20:21,613 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:20:21,613 - main - INFO - ==================================================
2025-05-03 03:20:21,615 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_403 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_403
2025-05-03 03:20:21,615 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_403 - INFO - Config: {
  "id": 403,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:20:22,196 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:20:22,197 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 403,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:20:22,197 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:20:22,406 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:20:22,407 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:20:22,408 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_403 - INFO - Starting model evaluation
2025-05-03 03:20:33,006 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_403 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:20:33,007 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_403 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_403/final_results.json
2025-05-03 03:20:33,008 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_403 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_403/final_results.json
2025-05-03 03:20:33,008 - main - INFO - 
Summary for configuration 403:
2025-05-03 03:20:33,008 - main - INFO - Accuracy: 0.2834
2025-05-03 03:20:33,008 - main - INFO - Precision: 0.2834
2025-05-03 03:20:33,008 - main - INFO - Recall: 1.0000
2025-05-03 03:20:33,008 - main - INFO - F1 Score: 0.4417
2025-05-03 03:20:33,008 - main - INFO - IoU: 0.2834
2025-05-03 03:20:33,008 - main - INFO - mAP: 0.2834
2025-05-03 03:20:33,008 - main - INFO - AUC: 0.5000
2025-05-03 03:20:33,008 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:20:33,008 - main - INFO - 
==================================================
2025-05-03 03:20:33,008 - main - INFO - Running configuration 404/756:
2025-05-03 03:20:33,008 - main - INFO - Model: ViT-B-16
2025-05-03 03:20:33,008 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:20:33,008 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:20:33,008 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:20:33,008 - main - INFO - ==================================================
2025-05-03 03:20:33,008 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_404 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_404
2025-05-03 03:20:33,010 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_404 - INFO - Config: {
  "id": 404,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:20:33,650 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:20:33,651 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 404,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:20:33,651 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:20:33,857 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:20:33,858 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:20:33,859 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_404 - INFO - Starting model evaluation
2025-05-03 03:20:44,627 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_404 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:20:44,628 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_404 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_404/final_results.json
2025-05-03 03:20:44,630 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_404 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_404/final_results.json
2025-05-03 03:20:44,630 - main - INFO - 
Summary for configuration 404:
2025-05-03 03:20:44,630 - main - INFO - Accuracy: 0.2834
2025-05-03 03:20:44,630 - main - INFO - Precision: 0.2834
2025-05-03 03:20:44,630 - main - INFO - Recall: 1.0000
2025-05-03 03:20:44,630 - main - INFO - F1 Score: 0.4417
2025-05-03 03:20:44,630 - main - INFO - IoU: 0.2834
2025-05-03 03:20:44,630 - main - INFO - mAP: 0.2834
2025-05-03 03:20:44,630 - main - INFO - AUC: 0.5000
2025-05-03 03:20:44,630 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:20:44,630 - main - INFO - 
==================================================
2025-05-03 03:20:44,630 - main - INFO - Running configuration 405/756:
2025-05-03 03:20:44,630 - main - INFO - Model: ViT-B-16
2025-05-03 03:20:44,630 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:20:44,630 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:20:44,630 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:20:44,630 - main - INFO - ==================================================
2025-05-03 03:20:44,630 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_405 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_405
2025-05-03 03:20:44,632 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_405 - INFO - Config: {
  "id": 405,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:20:45,127 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:20:45,128 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 405,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:20:45,128 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:20:45,331 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:20:45,332 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:20:45,333 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_405 - INFO - Starting model evaluation
2025-05-03 03:20:55,783 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_405 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:20:55,784 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_405 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_405/final_results.json
2025-05-03 03:20:55,785 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_405 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_405/final_results.json
2025-05-03 03:20:55,785 - main - INFO - 
Summary for configuration 405:
2025-05-03 03:20:55,785 - main - INFO - Accuracy: 0.2834
2025-05-03 03:20:55,785 - main - INFO - Precision: 0.2834
2025-05-03 03:20:55,785 - main - INFO - Recall: 1.0000
2025-05-03 03:20:55,785 - main - INFO - F1 Score: 0.4417
2025-05-03 03:20:55,785 - main - INFO - IoU: 0.2834
2025-05-03 03:20:55,785 - main - INFO - mAP: 0.2834
2025-05-03 03:20:55,785 - main - INFO - AUC: 0.5000
2025-05-03 03:20:55,785 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:20:55,785 - main - INFO - 
==================================================
2025-05-03 03:20:55,785 - main - INFO - Running configuration 406/756:
2025-05-03 03:20:55,785 - main - INFO - Model: ViT-B-16
2025-05-03 03:20:55,785 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:20:55,785 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:20:55,785 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:20:55,785 - main - INFO - ==================================================
2025-05-03 03:20:55,786 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_406 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_406
2025-05-03 03:20:55,787 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_406 - INFO - Config: {
  "id": 406,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:20:56,266 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:20:56,267 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 406,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:20:56,267 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:20:56,586 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:20:56,587 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:20:56,588 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_406 - INFO - Starting model evaluation
2025-05-03 03:21:07,275 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_406 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:21:07,276 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_406 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_406/final_results.json
2025-05-03 03:21:07,277 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_406 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_406/final_results.json
2025-05-03 03:21:07,277 - main - INFO - 
Summary for configuration 406:
2025-05-03 03:21:07,277 - main - INFO - Accuracy: 0.2834
2025-05-03 03:21:07,277 - main - INFO - Precision: 0.2834
2025-05-03 03:21:07,277 - main - INFO - Recall: 1.0000
2025-05-03 03:21:07,277 - main - INFO - F1 Score: 0.4417
2025-05-03 03:21:07,277 - main - INFO - IoU: 0.2834
2025-05-03 03:21:07,277 - main - INFO - mAP: 0.2834
2025-05-03 03:21:07,277 - main - INFO - AUC: 0.5000
2025-05-03 03:21:07,277 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:21:07,277 - main - INFO - 
==================================================
2025-05-03 03:21:07,277 - main - INFO - Running configuration 407/756:
2025-05-03 03:21:07,277 - main - INFO - Model: ViT-B-16
2025-05-03 03:21:07,277 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:21:07,277 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:21:07,277 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:21:07,277 - main - INFO - ==================================================
2025-05-03 03:21:07,278 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_407 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_407
2025-05-03 03:21:07,279 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_407 - INFO - Config: {
  "id": 407,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:21:07,757 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:21:07,757 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 407,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:21:07,757 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:21:08,040 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:21:08,041 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:21:08,042 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_407 - INFO - Starting model evaluation
2025-05-03 03:21:18,485 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_407 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:21:18,486 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_407 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_407/final_results.json
2025-05-03 03:21:18,487 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_407 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_407/final_results.json
2025-05-03 03:21:18,487 - main - INFO - 
Summary for configuration 407:
2025-05-03 03:21:18,487 - main - INFO - Accuracy: 0.2834
2025-05-03 03:21:18,487 - main - INFO - Precision: 0.2834
2025-05-03 03:21:18,487 - main - INFO - Recall: 1.0000
2025-05-03 03:21:18,487 - main - INFO - F1 Score: 0.4417
2025-05-03 03:21:18,487 - main - INFO - IoU: 0.2834
2025-05-03 03:21:18,487 - main - INFO - mAP: 0.2834
2025-05-03 03:21:18,487 - main - INFO - AUC: 0.5000
2025-05-03 03:21:18,487 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:21:18,487 - main - INFO - 
==================================================
2025-05-03 03:21:18,487 - main - INFO - Running configuration 408/756:
2025-05-03 03:21:18,487 - main - INFO - Model: ViT-B-16
2025-05-03 03:21:18,487 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 03:21:18,488 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:21:18,488 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:21:18,488 - main - INFO - ==================================================
2025-05-03 03:21:18,488 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_408 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01_id_408
2025-05-03 03:21:18,488 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_408 - INFO - Config: {
  "id": 408,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:21:19,047 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.01
2025-05-03 03:21:19,048 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 408,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:21:19,048 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 03:21:19,378 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 03:21:19,379 - training.model_ViT-B-16_opt_AdamW_lr_0.01 - INFO - Training completed after 1443.82 seconds
2025-05-03 03:21:19,380 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_408 - INFO - Starting model evaluation
2025-05-03 03:21:29,987 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_408 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:21:29,988 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_408 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_408/final_results.json
2025-05-03 03:21:29,989 - training.model_ViT-B-16_opt_AdamW_lr_0.01_id_408 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.01_id_408/final_results.json
2025-05-03 03:21:29,989 - main - INFO - 
Summary for configuration 408:
2025-05-03 03:21:29,989 - main - INFO - Accuracy: 0.2834
2025-05-03 03:21:29,989 - main - INFO - Precision: 0.2834
2025-05-03 03:21:29,989 - main - INFO - Recall: 1.0000
2025-05-03 03:21:29,989 - main - INFO - F1 Score: 0.4417
2025-05-03 03:21:29,989 - main - INFO - IoU: 0.2834
2025-05-03 03:21:29,989 - main - INFO - mAP: 0.2834
2025-05-03 03:21:29,989 - main - INFO - AUC: 0.5000
2025-05-03 03:21:29,989 - main - INFO - Training time: 1443.82 seconds
2025-05-03 03:21:29,989 - main - INFO - 
==================================================
2025-05-03 03:21:29,989 - main - INFO - Running configuration 409/756:
2025-05-03 03:21:29,989 - main - INFO - Model: ViT-B-16
2025-05-03 03:21:29,989 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:21:29,989 - main - INFO - Scheduler: StepLR
2025-05-03 03:21:29,989 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:21:29,989 - main - INFO - ==================================================
2025-05-03 03:21:29,989 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_409 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_409
2025-05-03 03:21:29,992 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_409 - INFO - Config: {
  "id": 409,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:21:30,466 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:21:30,466 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 409,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:21:30,466 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:21:31,167 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 12, best validation accuracy: 0.0000
2025-05-03 03:21:31,168 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 03:22:07,093 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:22:51,258 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.4957
2025-05-03 03:22:51,427 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Epoch 13 completed in 80.26s - Train Loss: 0.8138, Train Acc: 0.4994, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:22:52,421 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:22:52,421 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 03:23:28,067 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:24:11,485 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Validation loss improved from inf to 0.8169
2025-05-03 03:24:11,810 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Epoch 14 completed in 79.39s - Train Loss: 0.8128, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:24:12,860 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:24:12,860 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 03:24:48,722 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:25:32,544 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Epoch 15 completed in 79.68s - Train Loss: 0.8128, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:25:33,288 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:25:33,288 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 03:26:08,793 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:26:53,484 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Epoch 16 completed in 80.20s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:26:54,256 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:26:54,256 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 03:27:30,032 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:28:13,359 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Epoch 17 completed in 79.10s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:28:14,127 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:28:14,127 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 03:28:50,452 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 03:29:34,140 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.8169 to 0.8169
2025-05-03 03:29:34,399 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Epoch 18 completed in 80.27s - Train Loss: 0.8103, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:29:35,207 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 03:29:35,208 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 03:30:10,605 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 03:30:53,987 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.4957 to 0.5043
2025-05-03 03:30:54,244 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Epoch 19 completed in 79.04s - Train Loss: 0.8136, Train Acc: 0.4996, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 03:30:54,992 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 03:30:54,992 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 03:31:31,154 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 03:32:14,798 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Epoch 20 completed in 79.81s - Train Loss: 0.8317, Train Acc: 0.4809, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 03:32:15,560 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 03:32:15,560 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:32:15,562 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_409 - INFO - Starting model evaluation
2025-05-03 03:32:26,195 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_409 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:32:26,196 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_409 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_409/final_results.json
2025-05-03 03:32:26,197 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_409 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_409/final_results.json
2025-05-03 03:32:26,197 - main - INFO - 
Summary for configuration 409:
2025-05-03 03:32:26,197 - main - INFO - Accuracy: 0.2834
2025-05-03 03:32:26,197 - main - INFO - Precision: 0.2834
2025-05-03 03:32:26,197 - main - INFO - Recall: 1.0000
2025-05-03 03:32:26,197 - main - INFO - F1 Score: 0.4417
2025-05-03 03:32:26,197 - main - INFO - IoU: 0.2834
2025-05-03 03:32:26,197 - main - INFO - mAP: 0.2834
2025-05-03 03:32:26,197 - main - INFO - AUC: 0.5000
2025-05-03 03:32:26,197 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:32:26,197 - main - INFO - 
==================================================
2025-05-03 03:32:26,197 - main - INFO - Running configuration 410/756:
2025-05-03 03:32:26,197 - main - INFO - Model: ViT-B-16
2025-05-03 03:32:26,197 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:32:26,197 - main - INFO - Scheduler: StepLR
2025-05-03 03:32:26,197 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:32:26,197 - main - INFO - ==================================================
2025-05-03 03:32:26,199 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_410 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_410
2025-05-03 03:32:26,199 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_410 - INFO - Config: {
  "id": 410,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:32:26,731 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:32:26,731 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 410,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:32:26,731 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:32:26,946 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:32:26,946 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:32:26,948 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_410 - INFO - Starting model evaluation
2025-05-03 03:32:37,872 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_410 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:32:37,875 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_410 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_410/final_results.json
2025-05-03 03:32:37,877 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_410 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_410/final_results.json
2025-05-03 03:32:37,877 - main - INFO - 
Summary for configuration 410:
2025-05-03 03:32:37,877 - main - INFO - Accuracy: 0.2834
2025-05-03 03:32:37,877 - main - INFO - Precision: 0.2834
2025-05-03 03:32:37,877 - main - INFO - Recall: 1.0000
2025-05-03 03:32:37,877 - main - INFO - F1 Score: 0.4417
2025-05-03 03:32:37,877 - main - INFO - IoU: 0.2834
2025-05-03 03:32:37,877 - main - INFO - mAP: 0.2834
2025-05-03 03:32:37,877 - main - INFO - AUC: 0.5000
2025-05-03 03:32:37,877 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:32:37,877 - main - INFO - 
==================================================
2025-05-03 03:32:37,877 - main - INFO - Running configuration 411/756:
2025-05-03 03:32:37,877 - main - INFO - Model: ViT-B-16
2025-05-03 03:32:37,877 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:32:37,877 - main - INFO - Scheduler: StepLR
2025-05-03 03:32:37,877 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:32:37,877 - main - INFO - ==================================================
2025-05-03 03:32:37,877 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_411 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_411
2025-05-03 03:32:37,877 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_411 - INFO - Config: {
  "id": 411,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:32:38,358 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:32:38,358 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 411,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:32:38,358 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:32:38,565 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:32:38,566 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:32:38,567 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_411 - INFO - Starting model evaluation
2025-05-03 03:32:49,159 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_411 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:32:49,160 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_411 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_411/final_results.json
2025-05-03 03:32:49,162 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_411 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_411/final_results.json
2025-05-03 03:32:49,162 - main - INFO - 
Summary for configuration 411:
2025-05-03 03:32:49,162 - main - INFO - Accuracy: 0.2834
2025-05-03 03:32:49,162 - main - INFO - Precision: 0.2834
2025-05-03 03:32:49,162 - main - INFO - Recall: 1.0000
2025-05-03 03:32:49,162 - main - INFO - F1 Score: 0.4417
2025-05-03 03:32:49,162 - main - INFO - IoU: 0.2834
2025-05-03 03:32:49,162 - main - INFO - mAP: 0.2834
2025-05-03 03:32:49,162 - main - INFO - AUC: 0.5000
2025-05-03 03:32:49,162 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:32:49,162 - main - INFO - 
==================================================
2025-05-03 03:32:49,162 - main - INFO - Running configuration 412/756:
2025-05-03 03:32:49,162 - main - INFO - Model: ViT-B-16
2025-05-03 03:32:49,162 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:32:49,162 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:32:49,162 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:32:49,162 - main - INFO - ==================================================
2025-05-03 03:32:49,163 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_412 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_412
2025-05-03 03:32:49,164 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_412 - INFO - Config: {
  "id": 412,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:32:49,810 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:32:49,811 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 412,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:32:49,811 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:32:50,018 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:32:50,019 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:32:50,020 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_412 - INFO - Starting model evaluation
2025-05-03 03:33:00,388 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_412 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:33:00,389 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_412 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_412/final_results.json
2025-05-03 03:33:00,390 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_412 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_412/final_results.json
2025-05-03 03:33:00,390 - main - INFO - 
Summary for configuration 412:
2025-05-03 03:33:00,390 - main - INFO - Accuracy: 0.2834
2025-05-03 03:33:00,390 - main - INFO - Precision: 0.2834
2025-05-03 03:33:00,390 - main - INFO - Recall: 1.0000
2025-05-03 03:33:00,390 - main - INFO - F1 Score: 0.4417
2025-05-03 03:33:00,390 - main - INFO - IoU: 0.2834
2025-05-03 03:33:00,390 - main - INFO - mAP: 0.2834
2025-05-03 03:33:00,390 - main - INFO - AUC: 0.5000
2025-05-03 03:33:00,390 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:33:00,390 - main - INFO - 
==================================================
2025-05-03 03:33:00,390 - main - INFO - Running configuration 413/756:
2025-05-03 03:33:00,390 - main - INFO - Model: ViT-B-16
2025-05-03 03:33:00,390 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:33:00,390 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:33:00,390 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:33:00,390 - main - INFO - ==================================================
2025-05-03 03:33:00,392 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_413 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_413
2025-05-03 03:33:00,392 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_413 - INFO - Config: {
  "id": 413,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:33:00,870 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:33:00,870 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 413,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:33:00,870 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:33:01,284 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:33:01,285 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:33:01,286 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_413 - INFO - Starting model evaluation
2025-05-03 03:33:12,021 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_413 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:33:12,022 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_413 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_413/final_results.json
2025-05-03 03:33:12,023 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_413 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_413/final_results.json
2025-05-03 03:33:12,023 - main - INFO - 
Summary for configuration 413:
2025-05-03 03:33:12,023 - main - INFO - Accuracy: 0.2834
2025-05-03 03:33:12,023 - main - INFO - Precision: 0.2834
2025-05-03 03:33:12,023 - main - INFO - Recall: 1.0000
2025-05-03 03:33:12,023 - main - INFO - F1 Score: 0.4417
2025-05-03 03:33:12,023 - main - INFO - IoU: 0.2834
2025-05-03 03:33:12,023 - main - INFO - mAP: 0.2834
2025-05-03 03:33:12,023 - main - INFO - AUC: 0.5000
2025-05-03 03:33:12,023 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:33:12,023 - main - INFO - 
==================================================
2025-05-03 03:33:12,023 - main - INFO - Running configuration 414/756:
2025-05-03 03:33:12,023 - main - INFO - Model: ViT-B-16
2025-05-03 03:33:12,023 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:33:12,023 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:33:12,023 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:33:12,023 - main - INFO - ==================================================
2025-05-03 03:33:12,023 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_414 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_414
2025-05-03 03:33:12,025 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_414 - INFO - Config: {
  "id": 414,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:33:12,518 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:33:12,518 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 414,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:33:12,518 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:33:12,918 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:33:12,919 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:33:12,920 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_414 - INFO - Starting model evaluation
2025-05-03 03:33:23,512 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_414 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:33:23,513 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_414 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_414/final_results.json
2025-05-03 03:33:23,514 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_414 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_414/final_results.json
2025-05-03 03:33:23,514 - main - INFO - 
Summary for configuration 414:
2025-05-03 03:33:23,515 - main - INFO - Accuracy: 0.2834
2025-05-03 03:33:23,515 - main - INFO - Precision: 0.2834
2025-05-03 03:33:23,515 - main - INFO - Recall: 1.0000
2025-05-03 03:33:23,515 - main - INFO - F1 Score: 0.4417
2025-05-03 03:33:23,515 - main - INFO - IoU: 0.2834
2025-05-03 03:33:23,515 - main - INFO - mAP: 0.2834
2025-05-03 03:33:23,515 - main - INFO - AUC: 0.5000
2025-05-03 03:33:23,515 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:33:23,515 - main - INFO - 
==================================================
2025-05-03 03:33:23,515 - main - INFO - Running configuration 415/756:
2025-05-03 03:33:23,515 - main - INFO - Model: ViT-B-16
2025-05-03 03:33:23,515 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:33:23,515 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:33:23,515 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:33:23,515 - main - INFO - ==================================================
2025-05-03 03:33:23,515 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_415 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_415
2025-05-03 03:33:23,515 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_415 - INFO - Config: {
  "id": 415,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:33:23,991 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:33:23,991 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 415,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:33:23,991 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:33:24,400 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:33:24,400 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:33:24,402 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_415 - INFO - Starting model evaluation
2025-05-03 03:33:34,895 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_415 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:33:34,897 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_415 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_415/final_results.json
2025-05-03 03:33:34,898 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_415 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_415/final_results.json
2025-05-03 03:33:34,898 - main - INFO - 
Summary for configuration 415:
2025-05-03 03:33:34,898 - main - INFO - Accuracy: 0.2834
2025-05-03 03:33:34,898 - main - INFO - Precision: 0.2834
2025-05-03 03:33:34,898 - main - INFO - Recall: 1.0000
2025-05-03 03:33:34,898 - main - INFO - F1 Score: 0.4417
2025-05-03 03:33:34,898 - main - INFO - IoU: 0.2834
2025-05-03 03:33:34,898 - main - INFO - mAP: 0.2834
2025-05-03 03:33:34,898 - main - INFO - AUC: 0.5000
2025-05-03 03:33:34,898 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:33:34,898 - main - INFO - 
==================================================
2025-05-03 03:33:34,898 - main - INFO - Running configuration 416/756:
2025-05-03 03:33:34,898 - main - INFO - Model: ViT-B-16
2025-05-03 03:33:34,898 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:33:34,898 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:33:34,898 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:33:34,898 - main - INFO - ==================================================
2025-05-03 03:33:34,900 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_416 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_416
2025-05-03 03:33:34,900 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_416 - INFO - Config: {
  "id": 416,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:33:35,373 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:33:35,373 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 416,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:33:35,373 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:33:35,573 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:33:35,574 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:33:35,575 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_416 - INFO - Starting model evaluation
2025-05-03 03:33:46,333 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_416 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:33:46,335 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_416 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_416/final_results.json
2025-05-03 03:33:46,337 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_416 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_416/final_results.json
2025-05-03 03:33:46,337 - main - INFO - 
Summary for configuration 416:
2025-05-03 03:33:46,337 - main - INFO - Accuracy: 0.2834
2025-05-03 03:33:46,337 - main - INFO - Precision: 0.2834
2025-05-03 03:33:46,337 - main - INFO - Recall: 1.0000
2025-05-03 03:33:46,337 - main - INFO - F1 Score: 0.4417
2025-05-03 03:33:46,337 - main - INFO - IoU: 0.2834
2025-05-03 03:33:46,337 - main - INFO - mAP: 0.2834
2025-05-03 03:33:46,337 - main - INFO - AUC: 0.5000
2025-05-03 03:33:46,337 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:33:46,337 - main - INFO - 
==================================================
2025-05-03 03:33:46,337 - main - INFO - Running configuration 417/756:
2025-05-03 03:33:46,337 - main - INFO - Model: ViT-B-16
2025-05-03 03:33:46,337 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:33:46,337 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:33:46,337 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:33:46,337 - main - INFO - ==================================================
2025-05-03 03:33:46,338 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_417 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_417
2025-05-03 03:33:46,338 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_417 - INFO - Config: {
  "id": 417,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:33:46,945 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:33:46,945 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 417,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:33:46,945 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:33:47,146 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:33:47,146 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:33:47,147 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_417 - INFO - Starting model evaluation
2025-05-03 03:33:57,851 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_417 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:33:57,852 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_417 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_417/final_results.json
2025-05-03 03:33:57,853 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_417 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_417/final_results.json
2025-05-03 03:33:57,853 - main - INFO - 
Summary for configuration 417:
2025-05-03 03:33:57,853 - main - INFO - Accuracy: 0.2834
2025-05-03 03:33:57,853 - main - INFO - Precision: 0.2834
2025-05-03 03:33:57,853 - main - INFO - Recall: 1.0000
2025-05-03 03:33:57,853 - main - INFO - F1 Score: 0.4417
2025-05-03 03:33:57,853 - main - INFO - IoU: 0.2834
2025-05-03 03:33:57,853 - main - INFO - mAP: 0.2834
2025-05-03 03:33:57,853 - main - INFO - AUC: 0.5000
2025-05-03 03:33:57,853 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:33:57,853 - main - INFO - 
==================================================
2025-05-03 03:33:57,853 - main - INFO - Running configuration 418/756:
2025-05-03 03:33:57,853 - main - INFO - Model: ViT-B-16
2025-05-03 03:33:57,853 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:33:57,853 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:33:57,853 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:33:57,853 - main - INFO - ==================================================
2025-05-03 03:33:57,855 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_418 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_418
2025-05-03 03:33:57,855 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_418 - INFO - Config: {
  "id": 418,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:33:58,483 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:33:58,483 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 418,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:33:58,483 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:33:58,684 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:33:58,684 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:33:58,685 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_418 - INFO - Starting model evaluation
2025-05-03 03:34:09,443 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_418 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:34:09,444 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_418 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_418/final_results.json
2025-05-03 03:34:09,445 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_418 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_418/final_results.json
2025-05-03 03:34:09,445 - main - INFO - 
Summary for configuration 418:
2025-05-03 03:34:09,445 - main - INFO - Accuracy: 0.2834
2025-05-03 03:34:09,445 - main - INFO - Precision: 0.2834
2025-05-03 03:34:09,445 - main - INFO - Recall: 1.0000
2025-05-03 03:34:09,445 - main - INFO - F1 Score: 0.4417
2025-05-03 03:34:09,445 - main - INFO - IoU: 0.2834
2025-05-03 03:34:09,445 - main - INFO - mAP: 0.2834
2025-05-03 03:34:09,445 - main - INFO - AUC: 0.5000
2025-05-03 03:34:09,445 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:34:09,445 - main - INFO - 
==================================================
2025-05-03 03:34:09,445 - main - INFO - Running configuration 419/756:
2025-05-03 03:34:09,445 - main - INFO - Model: ViT-B-16
2025-05-03 03:34:09,445 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:34:09,445 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:34:09,445 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:34:09,445 - main - INFO - ==================================================
2025-05-03 03:34:09,445 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_419 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_419
2025-05-03 03:34:09,447 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_419 - INFO - Config: {
  "id": 419,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:34:09,943 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:34:09,943 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 419,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:34:09,943 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:34:10,144 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:34:10,145 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:34:10,146 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_419 - INFO - Starting model evaluation
2025-05-03 03:34:21,121 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_419 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:34:21,123 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_419 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_419/final_results.json
2025-05-03 03:34:21,124 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_419 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_419/final_results.json
2025-05-03 03:34:21,124 - main - INFO - 
Summary for configuration 419:
2025-05-03 03:34:21,124 - main - INFO - Accuracy: 0.2834
2025-05-03 03:34:21,124 - main - INFO - Precision: 0.2834
2025-05-03 03:34:21,124 - main - INFO - Recall: 1.0000
2025-05-03 03:34:21,124 - main - INFO - F1 Score: 0.4417
2025-05-03 03:34:21,124 - main - INFO - IoU: 0.2834
2025-05-03 03:34:21,124 - main - INFO - mAP: 0.2834
2025-05-03 03:34:21,124 - main - INFO - AUC: 0.5000
2025-05-03 03:34:21,124 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:34:21,124 - main - INFO - 
==================================================
2025-05-03 03:34:21,124 - main - INFO - Running configuration 420/756:
2025-05-03 03:34:21,124 - main - INFO - Model: ViT-B-16
2025-05-03 03:34:21,124 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 03:34:21,124 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:34:21,124 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:34:21,124 - main - INFO - ==================================================
2025-05-03 03:34:21,124 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_420 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001_id_420
2025-05-03 03:34:21,135 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_420 - INFO - Config: {
  "id": 420,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:34:21,607 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.001
2025-05-03 03:34:21,607 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 420,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:34:21,608 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 03:34:21,808 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 03:34:21,809 - training.model_ViT-B-16_opt_AdamW_lr_0.001 - INFO - Training completed after 646.35 seconds
2025-05-03 03:34:21,810 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_420 - INFO - Starting model evaluation
2025-05-03 03:34:32,527 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_420 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 03:34:32,529 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_420 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_420/final_results.json
2025-05-03 03:34:32,530 - training.model_ViT-B-16_opt_AdamW_lr_0.001_id_420 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.001_id_420/final_results.json
2025-05-03 03:34:32,530 - main - INFO - 
Summary for configuration 420:
2025-05-03 03:34:32,530 - main - INFO - Accuracy: 0.2834
2025-05-03 03:34:32,530 - main - INFO - Precision: 0.2834
2025-05-03 03:34:32,530 - main - INFO - Recall: 1.0000
2025-05-03 03:34:32,530 - main - INFO - F1 Score: 0.4417
2025-05-03 03:34:32,530 - main - INFO - IoU: 0.2834
2025-05-03 03:34:32,530 - main - INFO - mAP: 0.2834
2025-05-03 03:34:32,530 - main - INFO - AUC: 0.5000
2025-05-03 03:34:32,530 - main - INFO - Training time: 646.35 seconds
2025-05-03 03:34:32,530 - main - INFO - 
==================================================
2025-05-03 03:34:32,530 - main - INFO - Running configuration 421/756:
2025-05-03 03:34:32,530 - main - INFO - Model: ViT-B-16
2025-05-03 03:34:32,530 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:34:32,530 - main - INFO - Scheduler: StepLR
2025-05-03 03:34:32,530 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:34:32,530 - main - INFO - ==================================================
2025-05-03 03:34:32,532 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_421 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_421
2025-05-03 03:34:32,532 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_421 - INFO - Config: {
  "id": 421,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:34:33,049 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:34:33,049 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 421,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:34:33,049 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:34:33,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 12, best validation accuracy: 0.0000
2025-05-03 03:34:33,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 03:35:09,276 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:35:53,450 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.5523
2025-05-03 03:35:53,665 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 13 completed in 80.43s - Train Loss: 0.7501, Train Acc: 0.5234, Val Loss: 0.7202, Val Acc: 0.5523
2025-05-03 03:35:54,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:35:54,449 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 03:36:30,678 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:37:13,286 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.5523 to 0.6295
2025-05-03 03:37:13,545 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 14 completed in 79.10s - Train Loss: 0.6555, Train Acc: 0.6169, Val Loss: 0.6484, Val Acc: 0.6295
2025-05-03 03:37:14,345 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:37:14,345 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 03:37:50,077 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:38:33,347 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.6295 to 0.6921
2025-05-03 03:38:33,611 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 15 completed in 79.27s - Train Loss: 0.5771, Train Acc: 0.7169, Val Loss: 0.6012, Val Acc: 0.6921
2025-05-03 03:38:34,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:38:34,400 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 03:39:09,953 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:39:54,123 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.6921 to 0.7633
2025-05-03 03:39:54,373 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 16 completed in 79.97s - Train Loss: 0.5554, Train Acc: 0.7372, Val Loss: 0.5431, Val Acc: 0.7633
2025-05-03 03:39:55,101 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:39:55,102 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 03:40:30,907 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:41:14,352 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from inf to 0.5709
2025-05-03 03:41:14,608 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 17 completed in 79.51s - Train Loss: 0.5305, Train Acc: 0.7681, Val Loss: 0.5709, Val Acc: 0.7196
2025-05-03 03:41:15,328 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:41:15,328 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 03:41:51,362 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 03:42:34,985 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.5709 to 0.5505
2025-05-03 03:42:35,254 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 18 completed in 79.93s - Train Loss: 0.5325, Train Acc: 0.7701, Val Loss: 0.5505, Val Acc: 0.7470
2025-05-03 03:42:36,015 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 03:42:36,015 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 03:43:12,131 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 03:43:56,180 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.7633 to 0.7736
2025-05-03 03:43:56,437 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 19 completed in 80.42s - Train Loss: 0.5391, Train Acc: 0.7649, Val Loss: 0.5193, Val Acc: 0.7736
2025-05-03 03:43:57,220 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 03:43:57,220 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 03:44:33,187 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 03:45:17,371 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.7736 to 0.8208
2025-05-03 03:45:17,635 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Epoch 20 completed in 80.41s - Train Loss: 0.5111, Train Acc: 0.7930, Val Loss: 0.4866, Val Acc: 0.8208
2025-05-03 03:45:18,386 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 03:45:18,386 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:18,388 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_421 - INFO - Starting model evaluation
2025-05-03 03:45:28,925 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_421 - INFO - Evaluation metrics:
  Accuracy:  0.8107
  Precision: 0.6518
  Recall:    0.7133
  F1 Score:  0.6811
  IoU:       0.5165
  mAP:       0.7527
  AUC:       0.8565
2025-05-03 03:45:28,927 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_421 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_421/final_results.json
2025-05-03 03:45:28,929 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_421 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_421/final_results.json
2025-05-03 03:45:28,929 - main - INFO - 
Summary for configuration 421:
2025-05-03 03:45:28,929 - main - INFO - Accuracy: 0.8107
2025-05-03 03:45:28,929 - main - INFO - Precision: 0.6518
2025-05-03 03:45:28,929 - main - INFO - Recall: 0.7133
2025-05-03 03:45:28,929 - main - INFO - F1 Score: 0.6811
2025-05-03 03:45:28,929 - main - INFO - IoU: 0.5165
2025-05-03 03:45:28,929 - main - INFO - mAP: 0.7527
2025-05-03 03:45:28,929 - main - INFO - AUC: 0.8565
2025-05-03 03:45:28,929 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:45:28,929 - main - INFO - 
==================================================
2025-05-03 03:45:28,929 - main - INFO - Running configuration 422/756:
2025-05-03 03:45:28,929 - main - INFO - Model: ViT-B-16
2025-05-03 03:45:28,929 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:45:28,929 - main - INFO - Scheduler: StepLR
2025-05-03 03:45:28,929 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:45:28,929 - main - INFO - ==================================================
2025-05-03 03:45:28,932 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_422 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_422
2025-05-03 03:45:28,932 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_422 - INFO - Config: {
  "id": 422,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:45:29,598 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:29,599 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 422,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:45:29,599 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:29,811 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:29,811 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:29,812 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_422 - INFO - Starting model evaluation
2025-05-03 03:45:40,567 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_422 - INFO - Evaluation metrics:
  Accuracy:  0.8147
  Precision: 0.6623
  Recall:    0.7063
  F1 Score:  0.6836
  IoU:       0.5193
  mAP:       0.7729
  AUC:       0.8522
2025-05-03 03:45:40,569 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_422 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_422/final_results.json
2025-05-03 03:45:40,570 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_422 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_422/final_results.json
2025-05-03 03:45:40,570 - main - INFO - 
Summary for configuration 422:
2025-05-03 03:45:40,570 - main - INFO - Accuracy: 0.8147
2025-05-03 03:45:40,570 - main - INFO - Precision: 0.6623
2025-05-03 03:45:40,570 - main - INFO - Recall: 0.7063
2025-05-03 03:45:40,570 - main - INFO - F1 Score: 0.6836
2025-05-03 03:45:40,570 - main - INFO - IoU: 0.5193
2025-05-03 03:45:40,570 - main - INFO - mAP: 0.7729
2025-05-03 03:45:40,570 - main - INFO - AUC: 0.8522
2025-05-03 03:45:40,570 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:45:40,570 - main - INFO - 
==================================================
2025-05-03 03:45:40,570 - main - INFO - Running configuration 423/756:
2025-05-03 03:45:40,570 - main - INFO - Model: ViT-B-16
2025-05-03 03:45:40,570 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:45:40,570 - main - INFO - Scheduler: StepLR
2025-05-03 03:45:40,570 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:45:40,570 - main - INFO - ==================================================
2025-05-03 03:45:40,571 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_423 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_423
2025-05-03 03:45:40,571 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_423 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 423,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:45:41,177 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:41,473 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:41,473 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:41,475 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_423 - INFO - Starting model evaluation
2025-05-03 03:45:52,231 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_423 - INFO - Evaluation metrics:
  Accuracy:  0.8028
  Precision: 0.6399
  Recall:    0.6958
  F1 Score:  0.6667
  IoU:       0.5000
  mAP:       0.7574
  AUC:       0.8498
2025-05-03 03:45:52,233 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_423 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_423/final_results.json
2025-05-03 03:45:52,234 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_423 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_423/final_results.json
2025-05-03 03:45:52,234 - main - INFO - 
Summary for configuration 423:
2025-05-03 03:45:52,234 - main - INFO - Accuracy: 0.8028
2025-05-03 03:45:52,234 - main - INFO - Precision: 0.6399
2025-05-03 03:45:52,234 - main - INFO - Recall: 0.6958
2025-05-03 03:45:52,234 - main - INFO - F1 Score: 0.6667
2025-05-03 03:45:52,234 - main - INFO - IoU: 0.5000
2025-05-03 03:45:52,234 - main - INFO - mAP: 0.7574
2025-05-03 03:45:52,234 - main - INFO - AUC: 0.8498
2025-05-03 03:45:52,234 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:45:52,234 - main - INFO - 
==================================================
2025-05-03 03:45:52,234 - main - INFO - Running configuration 424/756:
2025-05-03 03:45:52,234 - main - INFO - Model: ViT-B-16
2025-05-03 03:45:52,234 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:45:52,234 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:45:52,234 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:45:52,234 - main - INFO - ==================================================
2025-05-03 03:45:52,236 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_424 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_424
2025-05-03 03:45:52,236 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_424 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:45:52,739 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 424,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:45:52,740 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:45:52,949 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:45:52,951 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_424 - INFO - Starting model evaluation
2025-05-03 03:46:03,555 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_424 - INFO - Evaluation metrics:
  Accuracy:  0.8157
  Precision: 0.6572
  Recall:    0.7308
  F1 Score:  0.6921
  IoU:       0.5291
  mAP:       0.7808
  AUC:       0.8607
2025-05-03 03:46:03,557 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_424 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_424/final_results.json
2025-05-03 03:46:03,559 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_424 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_424/final_results.json
2025-05-03 03:46:03,559 - main - INFO - 
Summary for configuration 424:
2025-05-03 03:46:03,559 - main - INFO - Accuracy: 0.8157
2025-05-03 03:46:03,559 - main - INFO - Precision: 0.6572
2025-05-03 03:46:03,559 - main - INFO - Recall: 0.7308
2025-05-03 03:46:03,559 - main - INFO - F1 Score: 0.6921
2025-05-03 03:46:03,559 - main - INFO - IoU: 0.5291
2025-05-03 03:46:03,559 - main - INFO - mAP: 0.7808
2025-05-03 03:46:03,559 - main - INFO - AUC: 0.8607
2025-05-03 03:46:03,559 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:46:03,559 - main - INFO - 
==================================================
2025-05-03 03:46:03,559 - main - INFO - Running configuration 425/756:
2025-05-03 03:46:03,559 - main - INFO - Model: ViT-B-16
2025-05-03 03:46:03,559 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:46:03,559 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:46:03,559 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:46:03,559 - main - INFO - ==================================================
2025-05-03 03:46:03,559 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_425 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_425
2025-05-03 03:46:03,570 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_425 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 425,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:04,044 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:04,460 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:04,461 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_425 - INFO - Starting model evaluation
2025-05-03 03:46:15,145 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_425 - INFO - Evaluation metrics:
  Accuracy:  0.8226
  Precision: 0.6766
  Recall:    0.7168
  F1 Score:  0.6961
  IoU:       0.5339
  mAP:       0.7728
  AUC:       0.8568
2025-05-03 03:46:15,147 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_425 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_425/final_results.json
2025-05-03 03:46:15,148 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_425 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_425/final_results.json
2025-05-03 03:46:15,148 - main - INFO - 
Summary for configuration 425:
2025-05-03 03:46:15,149 - main - INFO - Accuracy: 0.8226
2025-05-03 03:46:15,149 - main - INFO - Precision: 0.6766
2025-05-03 03:46:15,149 - main - INFO - Recall: 0.7168
2025-05-03 03:46:15,149 - main - INFO - F1 Score: 0.6961
2025-05-03 03:46:15,149 - main - INFO - IoU: 0.5339
2025-05-03 03:46:15,149 - main - INFO - mAP: 0.7728
2025-05-03 03:46:15,149 - main - INFO - AUC: 0.8568
2025-05-03 03:46:15,149 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:46:15,149 - main - INFO - 
==================================================
2025-05-03 03:46:15,149 - main - INFO - Running configuration 426/756:
2025-05-03 03:46:15,149 - main - INFO - Model: ViT-B-16
2025-05-03 03:46:15,149 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:46:15,149 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 03:46:15,149 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:46:15,149 - main - INFO - ==================================================
2025-05-03 03:46:15,149 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_426 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_426
2025-05-03 03:46:15,149 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_426 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 426,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:15,623 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:16,037 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:16,038 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:16,039 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_426 - INFO - Starting model evaluation
2025-05-03 03:46:26,646 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_426 - INFO - Evaluation metrics:
  Accuracy:  0.8137
  Precision: 0.6591
  Recall:    0.7098
  F1 Score:  0.6835
  IoU:       0.5192
  mAP:       0.7506
  AUC:       0.8463
2025-05-03 03:46:26,648 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_426 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_426/final_results.json
2025-05-03 03:46:26,649 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_426 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_426/final_results.json
2025-05-03 03:46:26,649 - main - INFO - 
Summary for configuration 426:
2025-05-03 03:46:26,650 - main - INFO - Accuracy: 0.8137
2025-05-03 03:46:26,650 - main - INFO - Precision: 0.6591
2025-05-03 03:46:26,650 - main - INFO - Recall: 0.7098
2025-05-03 03:46:26,650 - main - INFO - F1 Score: 0.6835
2025-05-03 03:46:26,650 - main - INFO - IoU: 0.5192
2025-05-03 03:46:26,650 - main - INFO - mAP: 0.7506
2025-05-03 03:46:26,650 - main - INFO - AUC: 0.8463
2025-05-03 03:46:26,650 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:46:26,650 - main - INFO - 
==================================================
2025-05-03 03:46:26,650 - main - INFO - Running configuration 427/756:
2025-05-03 03:46:26,650 - main - INFO - Model: ViT-B-16
2025-05-03 03:46:26,650 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:46:26,650 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:46:26,650 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:46:26,650 - main - INFO - ==================================================
2025-05-03 03:46:26,650 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_427 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_427
2025-05-03 03:46:26,651 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_427 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 427,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:46:27,132 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:27,542 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:27,543 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:27,544 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_427 - INFO - Starting model evaluation
2025-05-03 03:46:37,734 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_427 - INFO - Evaluation metrics:
  Accuracy:  0.8216
  Precision: 0.6721
  Recall:    0.7238
  F1 Score:  0.6970
  IoU:       0.5349
  mAP:       0.7698
  AUC:       0.8601
2025-05-03 03:46:37,735 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_427 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_427/final_results.json
2025-05-03 03:46:37,737 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_427 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_427/final_results.json
2025-05-03 03:46:37,737 - main - INFO - 
Summary for configuration 427:
2025-05-03 03:46:37,737 - main - INFO - Accuracy: 0.8216
2025-05-03 03:46:37,737 - main - INFO - Precision: 0.6721
2025-05-03 03:46:37,737 - main - INFO - Recall: 0.7238
2025-05-03 03:46:37,737 - main - INFO - F1 Score: 0.6970
2025-05-03 03:46:37,737 - main - INFO - IoU: 0.5349
2025-05-03 03:46:37,737 - main - INFO - mAP: 0.7698
2025-05-03 03:46:37,737 - main - INFO - AUC: 0.8601
2025-05-03 03:46:37,737 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:46:37,737 - main - INFO - 
==================================================
2025-05-03 03:46:37,737 - main - INFO - Running configuration 428/756:
2025-05-03 03:46:37,737 - main - INFO - Model: ViT-B-16
2025-05-03 03:46:37,737 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:46:37,737 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:46:37,737 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:46:37,737 - main - INFO - ==================================================
2025-05-03 03:46:37,737 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_428 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_428
2025-05-03 03:46:37,737 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_428 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 428,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:46:38,413 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:38,633 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:38,634 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:38,635 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_428 - INFO - Starting model evaluation
2025-05-03 03:46:48,929 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_428 - INFO - Evaluation metrics:
  Accuracy:  0.8067
  Precision: 0.6472
  Recall:    0.6993
  F1 Score:  0.6723
  IoU:       0.5063
  mAP:       0.7521
  AUC:       0.8496
2025-05-03 03:46:48,931 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_428 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_428/final_results.json
2025-05-03 03:46:48,932 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_428 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_428/final_results.json
2025-05-03 03:46:48,932 - main - INFO - 
Summary for configuration 428:
2025-05-03 03:46:48,932 - main - INFO - Accuracy: 0.8067
2025-05-03 03:46:48,932 - main - INFO - Precision: 0.6472
2025-05-03 03:46:48,932 - main - INFO - Recall: 0.6993
2025-05-03 03:46:48,932 - main - INFO - F1 Score: 0.6723
2025-05-03 03:46:48,932 - main - INFO - IoU: 0.5063
2025-05-03 03:46:48,932 - main - INFO - mAP: 0.7521
2025-05-03 03:46:48,932 - main - INFO - AUC: 0.8496
2025-05-03 03:46:48,932 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:46:48,932 - main - INFO - 
==================================================
2025-05-03 03:46:48,932 - main - INFO - Running configuration 429/756:
2025-05-03 03:46:48,932 - main - INFO - Model: ViT-B-16
2025-05-03 03:46:48,932 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:46:48,932 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 03:46:48,932 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:46:48,932 - main - INFO - ==================================================
2025-05-03 03:46:48,933 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_429 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_429
2025-05-03 03:46:48,934 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_429 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 429,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:46:49,488 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:46:49,690 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:46:49,691 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:46:49,692 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_429 - INFO - Starting model evaluation
2025-05-03 03:47:00,330 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_429 - INFO - Evaluation metrics:
  Accuracy:  0.8107
  Precision: 0.6518
  Recall:    0.7133
  F1 Score:  0.6811
  IoU:       0.5165
  mAP:       0.7520
  AUC:       0.8500
2025-05-03 03:47:00,331 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_429 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_429/final_results.json
2025-05-03 03:47:00,333 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_429 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_429/final_results.json
2025-05-03 03:47:00,333 - main - INFO - 
Summary for configuration 429:
2025-05-03 03:47:00,333 - main - INFO - Accuracy: 0.8107
2025-05-03 03:47:00,333 - main - INFO - Precision: 0.6518
2025-05-03 03:47:00,333 - main - INFO - Recall: 0.7133
2025-05-03 03:47:00,333 - main - INFO - F1 Score: 0.6811
2025-05-03 03:47:00,333 - main - INFO - IoU: 0.5165
2025-05-03 03:47:00,333 - main - INFO - mAP: 0.7520
2025-05-03 03:47:00,333 - main - INFO - AUC: 0.8500
2025-05-03 03:47:00,333 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:47:00,333 - main - INFO - 
==================================================
2025-05-03 03:47:00,333 - main - INFO - Running configuration 430/756:
2025-05-03 03:47:00,333 - main - INFO - Model: ViT-B-16
2025-05-03 03:47:00,333 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:47:00,333 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:47:00,333 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:47:00,333 - main - INFO - ==================================================
2025-05-03 03:47:00,335 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_430 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_430
2025-05-03 03:47:00,335 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_430 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 430,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:00,806 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:01,025 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:01,026 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:01,027 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_430 - INFO - Starting model evaluation
2025-05-03 03:47:11,654 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_430 - INFO - Evaluation metrics:
  Accuracy:  0.8147
  Precision: 0.6623
  Recall:    0.7063
  F1 Score:  0.6836
  IoU:       0.5193
  mAP:       0.7585
  AUC:       0.8530
2025-05-03 03:47:11,656 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_430 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_430/final_results.json
2025-05-03 03:47:11,657 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_430 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_430/final_results.json
2025-05-03 03:47:11,657 - main - INFO - 
Summary for configuration 430:
2025-05-03 03:47:11,657 - main - INFO - Accuracy: 0.8147
2025-05-03 03:47:11,657 - main - INFO - Precision: 0.6623
2025-05-03 03:47:11,657 - main - INFO - Recall: 0.7063
2025-05-03 03:47:11,657 - main - INFO - F1 Score: 0.6836
2025-05-03 03:47:11,657 - main - INFO - IoU: 0.5193
2025-05-03 03:47:11,657 - main - INFO - mAP: 0.7585
2025-05-03 03:47:11,657 - main - INFO - AUC: 0.8530
2025-05-03 03:47:11,657 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:47:11,657 - main - INFO - 
==================================================
2025-05-03 03:47:11,657 - main - INFO - Running configuration 431/756:
2025-05-03 03:47:11,657 - main - INFO - Model: ViT-B-16
2025-05-03 03:47:11,657 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:47:11,657 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:47:11,657 - main - INFO - Loss Function: FocalLoss
2025-05-03 03:47:11,657 - main - INFO - ==================================================
2025-05-03 03:47:11,658 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_431 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_431
2025-05-03 03:47:11,659 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_431 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 431,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 03:47:12,198 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:12,401 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:12,402 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:12,403 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_431 - INFO - Starting model evaluation
2025-05-03 03:47:23,207 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_431 - INFO - Evaluation metrics:
  Accuracy:  0.8176
  Precision: 0.6656
  Recall:    0.7168
  F1 Score:  0.6902
  IoU:       0.5270
  mAP:       0.7695
  AUC:       0.8525
2025-05-03 03:47:23,209 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_431 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_431/final_results.json
2025-05-03 03:47:23,211 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_431 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_431/final_results.json
2025-05-03 03:47:23,211 - main - INFO - 
Summary for configuration 431:
2025-05-03 03:47:23,211 - main - INFO - Accuracy: 0.8176
2025-05-03 03:47:23,211 - main - INFO - Precision: 0.6656
2025-05-03 03:47:23,211 - main - INFO - Recall: 0.7168
2025-05-03 03:47:23,211 - main - INFO - F1 Score: 0.6902
2025-05-03 03:47:23,211 - main - INFO - IoU: 0.5270
2025-05-03 03:47:23,211 - main - INFO - mAP: 0.7695
2025-05-03 03:47:23,211 - main - INFO - AUC: 0.8525
2025-05-03 03:47:23,211 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:47:23,211 - main - INFO - 
==================================================
2025-05-03 03:47:23,211 - main - INFO - Running configuration 432/756:
2025-05-03 03:47:23,211 - main - INFO - Model: ViT-B-16
2025-05-03 03:47:23,211 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 03:47:23,211 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 03:47:23,211 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 03:47:23,211 - main - INFO - ==================================================
2025-05-03 03:47:23,211 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_432 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001_id_432
2025-05-03 03:47:23,212 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_432 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,785 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ViT-B-16_opt_AdamW_lr_0.0001
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 432,
  "model_name": "ViT-B-16",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 03:47:23,786 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8208
2025-05-03 03:47:23,988 - training.model_ViT-B-16_opt_AdamW_lr_0.0001 - INFO - Training completed after 646.64 seconds
2025-05-03 03:47:23,990 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_432 - INFO - Starting model evaluation
2025-05-03 03:47:34,652 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_432 - INFO - Evaluation metrics:
  Accuracy:  0.8176
  Precision: 0.6667
  Recall:    0.7133
  F1 Score:  0.6892
  IoU:       0.5258
  mAP:       0.7699
  AUC:       0.8574
2025-05-03 03:47:34,654 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_432 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_432/final_results.json
2025-05-03 03:47:34,656 - training.model_ViT-B-16_opt_AdamW_lr_0.0001_id_432 - INFO - Final results saved to model_results_v2/model_ViT-B-16_opt_AdamW_lr_0.0001_id_432/final_results.json
2025-05-03 03:47:34,656 - main - INFO - 
Summary for configuration 432:
2025-05-03 03:47:34,656 - main - INFO - Accuracy: 0.8176
2025-05-03 03:47:34,656 - main - INFO - Precision: 0.6667
2025-05-03 03:47:34,656 - main - INFO - Recall: 0.7133
2025-05-03 03:47:34,656 - main - INFO - F1 Score: 0.6892
2025-05-03 03:47:34,656 - main - INFO - IoU: 0.5258
2025-05-03 03:47:34,656 - main - INFO - mAP: 0.7699
2025-05-03 03:47:34,656 - main - INFO - AUC: 0.8574
2025-05-03 03:47:34,656 - main - INFO - Training time: 646.64 seconds
2025-05-03 03:47:34,656 - main - INFO - 
==================================================
2025-05-03 03:47:34,656 - main - INFO - Running configuration 433/756:
2025-05-03 03:47:34,656 - main - INFO - Model: DenseNet121
2025-05-03 03:47:34,656 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 03:47:34,656 - main - INFO - Scheduler: StepLR
2025-05-03 03:47:34,656 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:47:34,656 - main - INFO - ==================================================
2025-05-03 03:47:34,656 - training.model_DenseNet121_opt_SGD_lr_0.01_id_433 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01_id_433
2025-05-03 03:47:34,658 - training.model_DenseNet121_opt_SGD_lr_0.01_id_433 - INFO - Config: {
  "id": 433,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:34,787 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.01
2025-05-03 03:47:34,788 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 433,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:34,788 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 03:47:34,940 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9880
2025-05-03 03:47:34,942 - training.model_DenseNet121_opt_SGD_lr_0.01 - INFO - Training completed after 1166.24 seconds
2025-05-03 03:47:34,946 - training.model_DenseNet121_opt_SGD_lr_0.01_id_433 - INFO - Starting model evaluation
2025-05-03 03:47:46,308 - training.model_DenseNet121_opt_SGD_lr_0.01_id_433 - INFO - Evaluation metrics:
  Accuracy:  0.9713
  Precision: 0.9573
  Recall:    0.9406
  F1 Score:  0.9489
  IoU:       0.9027
  mAP:       0.9893
  AUC:       0.9951
2025-05-03 03:47:46,309 - training.model_DenseNet121_opt_SGD_lr_0.01_id_433 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_433/final_results.json
2025-05-03 03:47:46,311 - training.model_DenseNet121_opt_SGD_lr_0.01_id_433 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.01_id_433/final_results.json
2025-05-03 03:47:46,311 - main - INFO - 
Summary for configuration 433:
2025-05-03 03:47:46,311 - main - INFO - Accuracy: 0.9713
2025-05-03 03:47:46,311 - main - INFO - Precision: 0.9573
2025-05-03 03:47:46,311 - main - INFO - Recall: 0.9406
2025-05-03 03:47:46,311 - main - INFO - F1 Score: 0.9489
2025-05-03 03:47:46,311 - main - INFO - IoU: 0.9027
2025-05-03 03:47:46,311 - main - INFO - mAP: 0.9893
2025-05-03 03:47:46,311 - main - INFO - AUC: 0.9951
2025-05-03 03:47:46,311 - main - INFO - Training time: 1166.24 seconds
2025-05-03 03:47:46,311 - main - INFO - Configuration 434 already completed, skipping
2025-05-03 03:47:46,311 - main - INFO - Configuration 435 already completed, skipping
2025-05-03 03:47:46,312 - main - INFO - Configuration 436 already completed, skipping
2025-05-03 03:47:46,312 - main - INFO - Configuration 437 already completed, skipping
2025-05-03 03:47:46,312 - main - INFO - Configuration 438 already completed, skipping
2025-05-03 03:47:46,313 - main - INFO - Configuration 439 already completed, skipping
2025-05-03 03:47:46,313 - main - INFO - Configuration 440 already completed, skipping
2025-05-03 03:47:46,313 - main - INFO - Configuration 441 already completed, skipping
2025-05-03 03:47:46,314 - main - INFO - Configuration 442 already completed, skipping
2025-05-03 03:47:46,314 - main - INFO - Configuration 443 already completed, skipping
2025-05-03 03:47:46,314 - main - INFO - Configuration 444 already completed, skipping
2025-05-03 03:47:46,315 - main - INFO - Configuration 445 already completed, skipping
2025-05-03 03:47:46,315 - main - INFO - Configuration 446 already completed, skipping
2025-05-03 03:47:46,315 - main - INFO - Configuration 447 already completed, skipping
2025-05-03 03:47:46,316 - main - INFO - Configuration 448 already completed, skipping
2025-05-03 03:47:46,316 - main - INFO - Configuration 449 already completed, skipping
2025-05-03 03:47:46,316 - main - INFO - Configuration 450 already completed, skipping
2025-05-03 03:47:46,317 - main - INFO - Configuration 451 already completed, skipping
2025-05-03 03:47:46,317 - main - INFO - Configuration 452 already completed, skipping
2025-05-03 03:47:46,317 - main - INFO - Configuration 453 already completed, skipping
2025-05-03 03:47:46,318 - main - INFO - Configuration 454 already completed, skipping
2025-05-03 03:47:46,318 - main - INFO - Configuration 455 already completed, skipping
2025-05-03 03:47:46,318 - main - INFO - Configuration 456 already completed, skipping
2025-05-03 03:47:46,319 - main - INFO - 
==================================================
2025-05-03 03:47:46,319 - main - INFO - Running configuration 457/756:
2025-05-03 03:47:46,319 - main - INFO - Model: DenseNet121
2025-05-03 03:47:46,319 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 03:47:46,319 - main - INFO - Scheduler: StepLR
2025-05-03 03:47:46,319 - main - INFO - Loss Function: CrossEntropy
2025-05-03 03:47:46,319 - main - INFO - ==================================================
2025-05-03 03:47:46,319 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_457 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_457
2025-05-03 03:47:46,319 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_457 - INFO - Config: {
  "id": 457,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:46,451 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 03:47:46,454 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 457,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 03:47:46,454 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 03:47:46,611 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 4, best validation accuracy: 0.8542
2025-05-03 03:47:46,612 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 03:48:10,436 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 03:48:42,633 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8542 to 0.8662
2025-05-03 03:48:42,672 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 5 completed in 56.06s - Train Loss: 0.4816, Train Acc: 0.8636, Val Loss: 0.4667, Val Acc: 0.8662
2025-05-03 03:48:42,738 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 03:48:42,738 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 03:49:05,911 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 03:49:37,534 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8662 to 0.8834
2025-05-03 03:49:37,573 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 6 completed in 54.84s - Train Loss: 0.4652, Train Acc: 0.8762, Val Loss: 0.4529, Val Acc: 0.8834
2025-05-03 03:49:37,639 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 03:49:37,639 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 03:50:01,611 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 03:50:32,538 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8834 to 0.8851
2025-05-03 03:50:32,578 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 7 completed in 54.94s - Train Loss: 0.4511, Train Acc: 0.8904, Val Loss: 0.4409, Val Acc: 0.8851
2025-05-03 03:50:32,643 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 03:50:32,643 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 03:50:56,863 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 03:51:29,761 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8851 to 0.9039
2025-05-03 03:51:29,821 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 8 completed in 57.18s - Train Loss: 0.4407, Train Acc: 0.8955, Val Loss: 0.4307, Val Acc: 0.9039
2025-05-03 03:51:29,885 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 03:51:29,886 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 03:51:52,920 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 03:52:25,257 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9039 to 0.9142
2025-05-03 03:52:25,295 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 9 completed in 55.41s - Train Loss: 0.4263, Train Acc: 0.9101, Val Loss: 0.4239, Val Acc: 0.9142
2025-05-03 03:52:25,363 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 03:52:25,364 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 03:52:49,391 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 03:53:21,181 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9142 to 0.9211
2025-05-03 03:53:21,223 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 10 completed in 55.86s - Train Loss: 0.4185, Train Acc: 0.9204, Val Loss: 0.4124, Val Acc: 0.9211
2025-05-03 03:53:21,289 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 03:53:21,289 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 03:53:45,228 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 03:54:18,561 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation loss improved from inf to 0.4124
2025-05-03 03:54:18,600 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 11 completed in 57.31s - Train Loss: 0.4123, Train Acc: 0.9256, Val Loss: 0.4124, Val Acc: 0.9194
2025-05-03 03:54:18,675 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 03:54:18,675 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 03:54:41,692 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 03:55:13,268 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4124 to 0.4122
2025-05-03 03:55:13,308 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 12 completed in 54.63s - Train Loss: 0.4087, Train Acc: 0.9273, Val Loss: 0.4122, Val Acc: 0.9202
2025-05-03 03:55:13,375 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 03:55:13,375 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 03:55:36,491 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:56:07,442 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9211 to 0.9245
2025-05-03 03:56:07,482 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 13 completed in 54.11s - Train Loss: 0.4093, Train Acc: 0.9281, Val Loss: 0.4089, Val Acc: 0.9245
2025-05-03 03:56:07,547 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 03:56:07,547 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 03:56:31,214 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:57:02,490 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4122 to 0.4105
2025-05-03 03:57:02,548 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 14 completed in 55.00s - Train Loss: 0.4062, Train Acc: 0.9305, Val Loss: 0.4105, Val Acc: 0.9245
2025-05-03 03:57:02,612 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 03:57:02,613 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 03:57:26,803 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:57:58,902 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9245 to 0.9280
2025-05-03 03:57:58,944 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 15 completed in 56.33s - Train Loss: 0.4076, Train Acc: 0.9303, Val Loss: 0.4079, Val Acc: 0.9280
2025-05-03 03:57:59,014 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 03:57:59,014 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 03:58:23,882 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:58:55,499 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.9280 to 0.9322
2025-05-03 03:58:55,539 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 16 completed in 56.52s - Train Loss: 0.4055, Train Acc: 0.9305, Val Loss: 0.4058, Val Acc: 0.9322
2025-05-03 03:58:55,602 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 03:58:55,603 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 03:59:19,798 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:59:50,964 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4105 to 0.4059
2025-05-03 03:59:51,004 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 17 completed in 55.40s - Train Loss: 0.4052, Train Acc: 0.9309, Val Loss: 0.4059, Val Acc: 0.9254
2025-05-03 03:59:51,066 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 03:59:51,067 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 04:00:14,906 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 04:00:47,474 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4059 to 0.4059
2025-05-03 04:00:47,514 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 18 completed in 56.45s - Train Loss: 0.4053, Train Acc: 0.9326, Val Loss: 0.4059, Val Acc: 0.9228
2025-05-03 04:00:47,582 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 04:00:47,583 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 04:01:11,754 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 04:01:43,870 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4059 to 0.4057
2025-05-03 04:01:43,910 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 19 completed in 56.33s - Train Loss: 0.4032, Train Acc: 0.9337, Val Loss: 0.4057, Val Acc: 0.9262
2025-05-03 04:01:43,976 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 04:01:43,976 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 04:02:07,416 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 04:02:39,331 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4057 to 0.4054
2025-05-03 04:02:39,371 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Epoch 20 completed in 55.39s - Train Loss: 0.4053, Train Acc: 0.9288, Val Loss: 0.4054, Val Acc: 0.9271
2025-05-03 04:02:39,434 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 04:02:39,434 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:02:39,439 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_457 - INFO - Starting model evaluation
2025-05-03 04:02:51,719 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_457 - INFO - Evaluation metrics:
  Accuracy:  0.9237
  Precision: 0.8519
  Recall:    0.8846
  F1 Score:  0.8679
  IoU:       0.7667
  mAP:       0.9496
  AUC:       0.9706
2025-05-03 04:02:51,720 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_457 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_457/final_results.json
2025-05-03 04:02:51,722 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_457 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_457/final_results.json
2025-05-03 04:02:51,722 - main - INFO - 
Summary for configuration 457:
2025-05-03 04:02:51,722 - main - INFO - Accuracy: 0.9237
2025-05-03 04:02:51,722 - main - INFO - Precision: 0.8519
2025-05-03 04:02:51,722 - main - INFO - Recall: 0.8846
2025-05-03 04:02:51,722 - main - INFO - F1 Score: 0.8679
2025-05-03 04:02:51,722 - main - INFO - IoU: 0.7667
2025-05-03 04:02:51,722 - main - INFO - mAP: 0.9496
2025-05-03 04:02:51,722 - main - INFO - AUC: 0.9706
2025-05-03 04:02:51,722 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:02:51,722 - main - INFO - 
==================================================
2025-05-03 04:02:51,722 - main - INFO - Running configuration 458/756:
2025-05-03 04:02:51,722 - main - INFO - Model: DenseNet121
2025-05-03 04:02:51,722 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:02:51,722 - main - INFO - Scheduler: StepLR
2025-05-03 04:02:51,722 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:02:51,722 - main - INFO - ==================================================
2025-05-03 04:02:51,722 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_458 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_458
2025-05-03 04:02:51,722 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_458 - INFO - Config: {
  "id": 458,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:02:51,853 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:02:51,853 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 458,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:02:51,853 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:02:51,980 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:02:51,982 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:02:51,986 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_458 - INFO - Starting model evaluation
2025-05-03 04:03:03,417 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_458 - INFO - Evaluation metrics:
  Accuracy:  0.9187
  Precision: 0.8400
  Recall:    0.8811
  F1 Score:  0.8601
  IoU:       0.7545
  mAP:       0.9478
  AUC:       0.9713
2025-05-03 04:03:03,418 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_458 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_458/final_results.json
2025-05-03 04:03:03,420 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_458 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_458/final_results.json
2025-05-03 04:03:03,420 - main - INFO - 
Summary for configuration 458:
2025-05-03 04:03:03,420 - main - INFO - Accuracy: 0.9187
2025-05-03 04:03:03,420 - main - INFO - Precision: 0.8400
2025-05-03 04:03:03,420 - main - INFO - Recall: 0.8811
2025-05-03 04:03:03,420 - main - INFO - F1 Score: 0.8601
2025-05-03 04:03:03,420 - main - INFO - IoU: 0.7545
2025-05-03 04:03:03,420 - main - INFO - mAP: 0.9478
2025-05-03 04:03:03,420 - main - INFO - AUC: 0.9713
2025-05-03 04:03:03,420 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:03:03,420 - main - INFO - 
==================================================
2025-05-03 04:03:03,420 - main - INFO - Running configuration 459/756:
2025-05-03 04:03:03,420 - main - INFO - Model: DenseNet121
2025-05-03 04:03:03,420 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:03:03,420 - main - INFO - Scheduler: StepLR
2025-05-03 04:03:03,420 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:03:03,420 - main - INFO - ==================================================
2025-05-03 04:03:03,420 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_459 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_459
2025-05-03 04:03:03,420 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_459 - INFO - Config: {
  "id": 459,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:03:03,553 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:03:03,553 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 459,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:03:03,553 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:03:03,679 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:03:03,681 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:03:03,685 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_459 - INFO - Starting model evaluation
2025-05-03 04:03:15,947 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_459 - INFO - Evaluation metrics:
  Accuracy:  0.9158
  Precision: 0.8339
  Recall:    0.8776
  F1 Score:  0.8552
  IoU:       0.7470
  mAP:       0.9493
  AUC:       0.9713
2025-05-03 04:03:15,948 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_459 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_459/final_results.json
2025-05-03 04:03:15,950 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_459 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_459/final_results.json
2025-05-03 04:03:15,950 - main - INFO - 
Summary for configuration 459:
2025-05-03 04:03:15,950 - main - INFO - Accuracy: 0.9158
2025-05-03 04:03:15,950 - main - INFO - Precision: 0.8339
2025-05-03 04:03:15,950 - main - INFO - Recall: 0.8776
2025-05-03 04:03:15,950 - main - INFO - F1 Score: 0.8552
2025-05-03 04:03:15,950 - main - INFO - IoU: 0.7470
2025-05-03 04:03:15,950 - main - INFO - mAP: 0.9493
2025-05-03 04:03:15,950 - main - INFO - AUC: 0.9713
2025-05-03 04:03:15,950 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:03:15,950 - main - INFO - 
==================================================
2025-05-03 04:03:15,950 - main - INFO - Running configuration 460/756:
2025-05-03 04:03:15,950 - main - INFO - Model: DenseNet121
2025-05-03 04:03:15,950 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:03:15,950 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:03:15,950 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:03:15,950 - main - INFO - ==================================================
2025-05-03 04:03:15,950 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_460 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_460
2025-05-03 04:03:15,950 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_460 - INFO - Config: {
  "id": 460,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:03:16,092 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:03:16,092 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 460,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:03:16,092 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:03:16,218 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:03:16,220 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:03:16,226 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_460 - INFO - Starting model evaluation
2025-05-03 04:03:28,394 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_460 - INFO - Evaluation metrics:
  Accuracy:  0.9217
  Precision: 0.8462
  Recall:    0.8846
  F1 Score:  0.8650
  IoU:       0.7620
  mAP:       0.9493
  AUC:       0.9712
2025-05-03 04:03:28,396 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_460 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_460/final_results.json
2025-05-03 04:03:28,397 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_460 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_460/final_results.json
2025-05-03 04:03:28,397 - main - INFO - 
Summary for configuration 460:
2025-05-03 04:03:28,397 - main - INFO - Accuracy: 0.9217
2025-05-03 04:03:28,397 - main - INFO - Precision: 0.8462
2025-05-03 04:03:28,397 - main - INFO - Recall: 0.8846
2025-05-03 04:03:28,397 - main - INFO - F1 Score: 0.8650
2025-05-03 04:03:28,397 - main - INFO - IoU: 0.7620
2025-05-03 04:03:28,398 - main - INFO - mAP: 0.9493
2025-05-03 04:03:28,398 - main - INFO - AUC: 0.9712
2025-05-03 04:03:28,398 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:03:28,398 - main - INFO - 
==================================================
2025-05-03 04:03:28,398 - main - INFO - Running configuration 461/756:
2025-05-03 04:03:28,398 - main - INFO - Model: DenseNet121
2025-05-03 04:03:28,398 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:03:28,398 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:03:28,398 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:03:28,398 - main - INFO - ==================================================
2025-05-03 04:03:28,398 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_461 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_461
2025-05-03 04:03:28,398 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_461 - INFO - Config: {
  "id": 461,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:03:28,549 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:03:28,549 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 461,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:03:28,549 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:03:28,675 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:03:28,677 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:03:28,681 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_461 - INFO - Starting model evaluation
2025-05-03 04:03:40,156 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_461 - INFO - Evaluation metrics:
  Accuracy:  0.9217
  Precision: 0.8485
  Recall:    0.8811
  F1 Score:  0.8645
  IoU:       0.7613
  mAP:       0.9469
  AUC:       0.9687
2025-05-03 04:03:40,157 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_461 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_461/final_results.json
2025-05-03 04:03:40,159 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_461 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_461/final_results.json
2025-05-03 04:03:40,159 - main - INFO - 
Summary for configuration 461:
2025-05-03 04:03:40,159 - main - INFO - Accuracy: 0.9217
2025-05-03 04:03:40,159 - main - INFO - Precision: 0.8485
2025-05-03 04:03:40,159 - main - INFO - Recall: 0.8811
2025-05-03 04:03:40,159 - main - INFO - F1 Score: 0.8645
2025-05-03 04:03:40,159 - main - INFO - IoU: 0.7613
2025-05-03 04:03:40,159 - main - INFO - mAP: 0.9469
2025-05-03 04:03:40,159 - main - INFO - AUC: 0.9687
2025-05-03 04:03:40,159 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:03:40,159 - main - INFO - 
==================================================
2025-05-03 04:03:40,159 - main - INFO - Running configuration 462/756:
2025-05-03 04:03:40,159 - main - INFO - Model: DenseNet121
2025-05-03 04:03:40,159 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:03:40,159 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:03:40,159 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:03:40,159 - main - INFO - ==================================================
2025-05-03 04:03:40,159 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_462 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_462
2025-05-03 04:03:40,159 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_462 - INFO - Config: {
  "id": 462,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:03:40,290 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:03:40,290 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 462,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:03:40,290 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:03:40,496 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:03:40,497 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:03:40,502 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_462 - INFO - Starting model evaluation
2025-05-03 04:03:52,811 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_462 - INFO - Evaluation metrics:
  Accuracy:  0.9217
  Precision: 0.8416
  Recall:    0.8916
  F1 Score:  0.8659
  IoU:       0.7635
  mAP:       0.9504
  AUC:       0.9721
2025-05-03 04:03:52,812 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_462 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_462/final_results.json
2025-05-03 04:03:52,814 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_462 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_462/final_results.json
2025-05-03 04:03:52,814 - main - INFO - 
Summary for configuration 462:
2025-05-03 04:03:52,814 - main - INFO - Accuracy: 0.9217
2025-05-03 04:03:52,814 - main - INFO - Precision: 0.8416
2025-05-03 04:03:52,814 - main - INFO - Recall: 0.8916
2025-05-03 04:03:52,814 - main - INFO - F1 Score: 0.8659
2025-05-03 04:03:52,814 - main - INFO - IoU: 0.7635
2025-05-03 04:03:52,814 - main - INFO - mAP: 0.9504
2025-05-03 04:03:52,814 - main - INFO - AUC: 0.9721
2025-05-03 04:03:52,814 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:03:52,814 - main - INFO - 
==================================================
2025-05-03 04:03:52,814 - main - INFO - Running configuration 463/756:
2025-05-03 04:03:52,814 - main - INFO - Model: DenseNet121
2025-05-03 04:03:52,814 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:03:52,814 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:03:52,814 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:03:52,814 - main - INFO - ==================================================
2025-05-03 04:03:52,814 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_463 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_463
2025-05-03 04:03:52,814 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_463 - INFO - Config: {
  "id": 463,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:03:52,943 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:03:52,943 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 463,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:03:52,943 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:03:53,071 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:03:53,073 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:03:53,077 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_463 - INFO - Starting model evaluation
2025-05-03 04:04:04,960 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_463 - INFO - Evaluation metrics:
  Accuracy:  0.9247
  Precision: 0.8523
  Recall:    0.8881
  F1 Score:  0.8699
  IoU:       0.7697
  mAP:       0.9487
  AUC:       0.9705
2025-05-03 04:04:04,961 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_463 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_463/final_results.json
2025-05-03 04:04:04,963 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_463 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_463/final_results.json
2025-05-03 04:04:04,963 - main - INFO - 
Summary for configuration 463:
2025-05-03 04:04:04,963 - main - INFO - Accuracy: 0.9247
2025-05-03 04:04:04,963 - main - INFO - Precision: 0.8523
2025-05-03 04:04:04,963 - main - INFO - Recall: 0.8881
2025-05-03 04:04:04,963 - main - INFO - F1 Score: 0.8699
2025-05-03 04:04:04,963 - main - INFO - IoU: 0.7697
2025-05-03 04:04:04,963 - main - INFO - mAP: 0.9487
2025-05-03 04:04:04,963 - main - INFO - AUC: 0.9705
2025-05-03 04:04:04,963 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:04:04,963 - main - INFO - 
==================================================
2025-05-03 04:04:04,963 - main - INFO - Running configuration 464/756:
2025-05-03 04:04:04,963 - main - INFO - Model: DenseNet121
2025-05-03 04:04:04,963 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:04:04,963 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:04:04,963 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:04:04,963 - main - INFO - ==================================================
2025-05-03 04:04:04,963 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_464 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_464
2025-05-03 04:04:04,963 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_464 - INFO - Config: {
  "id": 464,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:04:05,095 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:04:05,095 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 464,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:04:05,095 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:04:05,221 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:04:05,223 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:04:05,227 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_464 - INFO - Starting model evaluation
2025-05-03 04:04:17,153 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_464 - INFO - Evaluation metrics:
  Accuracy:  0.9247
  Precision: 0.8547
  Recall:    0.8846
  F1 Score:  0.8694
  IoU:       0.7690
  mAP:       0.9489
  AUC:       0.9697
2025-05-03 04:04:17,155 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_464 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_464/final_results.json
2025-05-03 04:04:17,156 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_464 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_464/final_results.json
2025-05-03 04:04:17,156 - main - INFO - 
Summary for configuration 464:
2025-05-03 04:04:17,156 - main - INFO - Accuracy: 0.9247
2025-05-03 04:04:17,156 - main - INFO - Precision: 0.8547
2025-05-03 04:04:17,156 - main - INFO - Recall: 0.8846
2025-05-03 04:04:17,156 - main - INFO - F1 Score: 0.8694
2025-05-03 04:04:17,156 - main - INFO - IoU: 0.7690
2025-05-03 04:04:17,156 - main - INFO - mAP: 0.9489
2025-05-03 04:04:17,156 - main - INFO - AUC: 0.9697
2025-05-03 04:04:17,156 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:04:17,156 - main - INFO - 
==================================================
2025-05-03 04:04:17,156 - main - INFO - Running configuration 465/756:
2025-05-03 04:04:17,156 - main - INFO - Model: DenseNet121
2025-05-03 04:04:17,156 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:04:17,156 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:04:17,156 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:04:17,156 - main - INFO - ==================================================
2025-05-03 04:04:17,157 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_465 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_465
2025-05-03 04:04:17,157 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_465 - INFO - Config: {
  "id": 465,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:04:17,285 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:04:17,285 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 465,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:04:17,285 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:04:17,411 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:04:17,412 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:04:17,417 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_465 - INFO - Starting model evaluation
2025-05-03 04:04:29,591 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_465 - INFO - Evaluation metrics:
  Accuracy:  0.9227
  Precision: 0.8467
  Recall:    0.8881
  F1 Score:  0.8669
  IoU:       0.7651
  mAP:       0.9501
  AUC:       0.9705
2025-05-03 04:04:29,592 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_465 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_465/final_results.json
2025-05-03 04:04:29,594 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_465 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_465/final_results.json
2025-05-03 04:04:29,594 - main - INFO - 
Summary for configuration 465:
2025-05-03 04:04:29,594 - main - INFO - Accuracy: 0.9227
2025-05-03 04:04:29,594 - main - INFO - Precision: 0.8467
2025-05-03 04:04:29,594 - main - INFO - Recall: 0.8881
2025-05-03 04:04:29,594 - main - INFO - F1 Score: 0.8669
2025-05-03 04:04:29,594 - main - INFO - IoU: 0.7651
2025-05-03 04:04:29,594 - main - INFO - mAP: 0.9501
2025-05-03 04:04:29,594 - main - INFO - AUC: 0.9705
2025-05-03 04:04:29,594 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:04:29,594 - main - INFO - 
==================================================
2025-05-03 04:04:29,594 - main - INFO - Running configuration 466/756:
2025-05-03 04:04:29,594 - main - INFO - Model: DenseNet121
2025-05-03 04:04:29,594 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:04:29,594 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:04:29,594 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:04:29,594 - main - INFO - ==================================================
2025-05-03 04:04:29,594 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_466 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_466
2025-05-03 04:04:29,594 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_466 - INFO - Config: {
  "id": 466,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:04:29,725 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:04:29,725 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 466,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:04:29,725 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:04:29,851 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:04:29,852 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:04:29,857 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_466 - INFO - Starting model evaluation
2025-05-03 04:04:41,563 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_466 - INFO - Evaluation metrics:
  Accuracy:  0.9257
  Precision: 0.8576
  Recall:    0.8846
  F1 Score:  0.8709
  IoU:       0.7713
  mAP:       0.9475
  AUC:       0.9698
2025-05-03 04:04:41,565 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_466 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_466/final_results.json
2025-05-03 04:04:41,566 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_466 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_466/final_results.json
2025-05-03 04:04:41,567 - main - INFO - 
Summary for configuration 466:
2025-05-03 04:04:41,567 - main - INFO - Accuracy: 0.9257
2025-05-03 04:04:41,567 - main - INFO - Precision: 0.8576
2025-05-03 04:04:41,567 - main - INFO - Recall: 0.8846
2025-05-03 04:04:41,567 - main - INFO - F1 Score: 0.8709
2025-05-03 04:04:41,567 - main - INFO - IoU: 0.7713
2025-05-03 04:04:41,567 - main - INFO - mAP: 0.9475
2025-05-03 04:04:41,567 - main - INFO - AUC: 0.9698
2025-05-03 04:04:41,567 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:04:41,567 - main - INFO - 
==================================================
2025-05-03 04:04:41,567 - main - INFO - Running configuration 467/756:
2025-05-03 04:04:41,567 - main - INFO - Model: DenseNet121
2025-05-03 04:04:41,567 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:04:41,567 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:04:41,567 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:04:41,567 - main - INFO - ==================================================
2025-05-03 04:04:41,567 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_467 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_467
2025-05-03 04:04:41,567 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_467 - INFO - Config: {
  "id": 467,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:04:41,696 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:04:41,697 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 467,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:04:41,697 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:04:41,822 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:04:41,824 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:04:41,828 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_467 - INFO - Starting model evaluation
2025-05-03 04:04:53,738 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_467 - INFO - Evaluation metrics:
  Accuracy:  0.9227
  Precision: 0.8490
  Recall:    0.8846
  F1 Score:  0.8664
  IoU:       0.7644
  mAP:       0.9496
  AUC:       0.9706
2025-05-03 04:04:53,740 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_467 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_467/final_results.json
2025-05-03 04:04:53,741 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_467 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_467/final_results.json
2025-05-03 04:04:53,741 - main - INFO - 
Summary for configuration 467:
2025-05-03 04:04:53,741 - main - INFO - Accuracy: 0.9227
2025-05-03 04:04:53,741 - main - INFO - Precision: 0.8490
2025-05-03 04:04:53,741 - main - INFO - Recall: 0.8846
2025-05-03 04:04:53,741 - main - INFO - F1 Score: 0.8664
2025-05-03 04:04:53,741 - main - INFO - IoU: 0.7644
2025-05-03 04:04:53,741 - main - INFO - mAP: 0.9496
2025-05-03 04:04:53,741 - main - INFO - AUC: 0.9706
2025-05-03 04:04:53,741 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:04:53,742 - main - INFO - 
==================================================
2025-05-03 04:04:53,742 - main - INFO - Running configuration 468/756:
2025-05-03 04:04:53,742 - main - INFO - Model: DenseNet121
2025-05-03 04:04:53,742 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 04:04:53,742 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:04:53,742 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:04:53,742 - main - INFO - ==================================================
2025-05-03 04:04:53,742 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_468 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001_id_468
2025-05-03 04:04:53,742 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_468 - INFO - Config: {
  "id": 468,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:04:53,872 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_SGD_lr_0.0001
2025-05-03 04:04:53,872 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 468,
  "model_name": "DenseNet121",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:04:53,872 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 04:04:53,997 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9322
2025-05-03 04:04:53,999 - training.model_DenseNet121_opt_SGD_lr_0.0001 - INFO - Training completed after 1087.41 seconds
2025-05-03 04:04:54,003 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_468 - INFO - Starting model evaluation
2025-05-03 04:05:05,827 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_468 - INFO - Evaluation metrics:
  Accuracy:  0.9237
  Precision: 0.8495
  Recall:    0.8881
  F1 Score:  0.8684
  IoU:       0.7674
  mAP:       0.9503
  AUC:       0.9705
2025-05-03 04:05:05,828 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_468 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_468/final_results.json
2025-05-03 04:05:05,830 - training.model_DenseNet121_opt_SGD_lr_0.0001_id_468 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_SGD_lr_0.0001_id_468/final_results.json
2025-05-03 04:05:05,830 - main - INFO - 
Summary for configuration 468:
2025-05-03 04:05:05,830 - main - INFO - Accuracy: 0.9237
2025-05-03 04:05:05,830 - main - INFO - Precision: 0.8495
2025-05-03 04:05:05,830 - main - INFO - Recall: 0.8881
2025-05-03 04:05:05,830 - main - INFO - F1 Score: 0.8684
2025-05-03 04:05:05,830 - main - INFO - IoU: 0.7674
2025-05-03 04:05:05,830 - main - INFO - mAP: 0.9503
2025-05-03 04:05:05,830 - main - INFO - AUC: 0.9705
2025-05-03 04:05:05,830 - main - INFO - Training time: 1087.41 seconds
2025-05-03 04:05:05,830 - main - INFO - 
==================================================
2025-05-03 04:05:05,830 - main - INFO - Running configuration 469/756:
2025-05-03 04:05:05,830 - main - INFO - Model: DenseNet121
2025-05-03 04:05:05,830 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:05:05,830 - main - INFO - Scheduler: StepLR
2025-05-03 04:05:05,830 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:05:05,830 - main - INFO - ==================================================
2025-05-03 04:05:05,830 - training.model_DenseNet121_opt_Adam_lr_0.01_id_469 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_469
2025-05-03 04:05:05,830 - training.model_DenseNet121_opt_Adam_lr_0.01_id_469 - INFO - Config: {
  "id": 469,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:05:05,960 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:05:05,960 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 469,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:05:05,960 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-03 04:05:05,961 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 1/20
2025-05-03 04:05:29,002 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 04:06:01,746 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.5043
2025-05-03 04:06:01,777 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 1 completed in 55.82s - Train Loss: 0.8037, Train Acc: 0.4974, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 04:06:01,880 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 04:06:01,881 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 2/20
2025-05-03 04:06:26,432 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 04:06:58,202 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5043 to 0.5746
2025-05-03 04:06:58,243 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 2 completed in 56.36s - Train Loss: 0.6764, Train Acc: 0.5828, Val Loss: 0.6839, Val Acc: 0.5746
2025-05-03 04:06:58,347 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 04:06:58,347 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 3/20
2025-05-03 04:07:22,067 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 04:07:54,157 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation loss improved from inf to 0.6942
2025-05-03 04:07:54,196 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 3 completed in 55.85s - Train Loss: 0.6582, Train Acc: 0.6066, Val Loss: 0.6942, Val Acc: 0.5240
2025-05-03 04:07:54,286 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 04:07:54,286 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 4/20
2025-05-03 04:08:17,911 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 04:08:49,847 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 4 completed in 55.56s - Train Loss: 0.6445, Train Acc: 0.6214, Val Loss: 0.7253, Val Acc: 0.5395
2025-05-03 04:08:49,950 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 04:08:49,951 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 5/20
2025-05-03 04:09:13,739 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 04:09:45,691 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 5 completed in 55.74s - Train Loss: 0.6376, Train Acc: 0.6255, Val Loss: 0.7613, Val Acc: 0.5189
2025-05-03 04:09:45,797 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 04:09:45,798 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 6/20
2025-05-03 04:10:08,987 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 04:10:42,079 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 6 completed in 56.28s - Train Loss: 0.6621, Train Acc: 0.6045, Val Loss: 0.7390, Val Acc: 0.4957
2025-05-03 04:10:42,177 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 04:10:42,177 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 7/20
2025-05-03 04:11:05,976 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 04:11:38,363 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5746 to 0.6655
2025-05-03 04:11:38,406 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 7 completed in 56.23s - Train Loss: 0.6198, Train Acc: 0.6557, Val Loss: 0.6392, Val Acc: 0.6655
2025-05-03 04:11:38,501 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 04:11:38,501 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 8/20
2025-05-03 04:12:02,089 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 04:12:33,837 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.6942 to 0.6613
2025-05-03 04:12:33,878 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 8 completed in 55.38s - Train Loss: 0.5824, Train Acc: 0.7042, Val Loss: 0.6613, Val Acc: 0.6141
2025-05-03 04:12:33,973 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 04:12:33,974 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 9/20
2025-05-03 04:12:58,084 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 04:13:29,895 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.6655 to 0.7221
2025-05-03 04:13:29,929 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 9 completed in 55.96s - Train Loss: 0.5541, Train Acc: 0.7411, Val Loss: 0.5769, Val Acc: 0.7221
2025-05-03 04:13:30,027 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 04:13:30,027 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 10/20
2025-05-03 04:13:53,801 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 04:14:25,812 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7221 to 0.7727
2025-05-03 04:14:25,853 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 10 completed in 55.83s - Train Loss: 0.5491, Train Acc: 0.7527, Val Loss: 0.5320, Val Acc: 0.7727
2025-05-03 04:14:25,947 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 04:14:25,948 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 11/20
2025-05-03 04:14:50,079 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 04:15:22,396 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7727 to 0.8096
2025-05-03 04:15:22,436 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 11 completed in 56.49s - Train Loss: 0.5118, Train Acc: 0.7930, Val Loss: 0.4995, Val Acc: 0.8096
2025-05-03 04:15:22,532 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 04:15:22,532 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 12/20
2025-05-03 04:15:46,425 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 04:16:18,537 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8096 to 0.8165
2025-05-03 04:16:18,577 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 12 completed in 56.04s - Train Loss: 0.4971, Train Acc: 0.8132, Val Loss: 0.4893, Val Acc: 0.8165
2025-05-03 04:16:18,676 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 04:16:18,677 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 04:16:42,990 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 04:17:15,354 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8165 to 0.8268
2025-05-03 04:17:15,392 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 13 completed in 56.72s - Train Loss: 0.4945, Train Acc: 0.8181, Val Loss: 0.4844, Val Acc: 0.8268
2025-05-03 04:17:15,487 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 04:17:15,487 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 04:17:40,210 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 04:18:12,116 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8268 to 0.8379
2025-05-03 04:18:12,158 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 14 completed in 56.67s - Train Loss: 0.4889, Train Acc: 0.8198, Val Loss: 0.4725, Val Acc: 0.8379
2025-05-03 04:18:12,262 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 04:18:12,263 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 04:18:36,441 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 04:19:08,750 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.6613 to 0.4721
2025-05-03 04:19:08,790 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 15 completed in 56.53s - Train Loss: 0.4796, Train Acc: 0.8327, Val Loss: 0.4721, Val Acc: 0.8370
2025-05-03 04:19:08,888 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 04:19:08,889 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 04:19:32,153 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 04:20:04,292 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8379 to 0.8422
2025-05-03 04:20:04,334 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 16 completed in 55.44s - Train Loss: 0.4722, Train Acc: 0.8402, Val Loss: 0.4688, Val Acc: 0.8422
2025-05-03 04:20:04,427 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 04:20:04,428 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 04:20:28,102 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 04:21:00,637 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8422 to 0.8611
2025-05-03 04:21:00,678 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 17 completed in 56.25s - Train Loss: 0.4632, Train Acc: 0.8468, Val Loss: 0.4529, Val Acc: 0.8611
2025-05-03 04:21:00,781 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 04:21:00,782 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 04:21:24,997 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 04:21:57,677 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.4721 to 0.4550
2025-05-03 04:21:57,720 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 18 completed in 56.94s - Train Loss: 0.4632, Train Acc: 0.8479, Val Loss: 0.4550, Val Acc: 0.8525
2025-05-03 04:21:57,817 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 04:21:57,817 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 04:22:22,287 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 04:22:54,257 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8611 to 0.8645
2025-05-03 04:22:54,300 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 19 completed in 56.48s - Train Loss: 0.4515, Train Acc: 0.8580, Val Loss: 0.4469, Val Acc: 0.8645
2025-05-03 04:22:54,396 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 04:22:54,396 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 04:23:17,385 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 04:23:48,340 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.8645 to 0.8705
2025-05-03 04:23:48,382 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Epoch 20 completed in 53.99s - Train Loss: 0.4425, Train Acc: 0.8689, Val Loss: 0.4409, Val Acc: 0.8705
2025-05-03 04:23:48,479 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 04:23:48,480 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:23:48,485 - training.model_DenseNet121_opt_Adam_lr_0.01_id_469 - INFO - Starting model evaluation
2025-05-03 04:24:00,358 - training.model_DenseNet121_opt_Adam_lr_0.01_id_469 - INFO - Evaluation metrics:
  Accuracy:  0.8434
  Precision: 0.6850
  Recall:    0.8287
  F1 Score:  0.7500
  IoU:       0.6000
  mAP:       0.7728
  AUC:       0.8997
2025-05-03 04:24:00,359 - training.model_DenseNet121_opt_Adam_lr_0.01_id_469 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_469/final_results.json
2025-05-03 04:24:00,361 - training.model_DenseNet121_opt_Adam_lr_0.01_id_469 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_469/final_results.json
2025-05-03 04:24:00,361 - main - INFO - 
Summary for configuration 469:
2025-05-03 04:24:00,361 - main - INFO - Accuracy: 0.8434
2025-05-03 04:24:00,361 - main - INFO - Precision: 0.6850
2025-05-03 04:24:00,361 - main - INFO - Recall: 0.8287
2025-05-03 04:24:00,361 - main - INFO - F1 Score: 0.7500
2025-05-03 04:24:00,361 - main - INFO - IoU: 0.6000
2025-05-03 04:24:00,361 - main - INFO - mAP: 0.7728
2025-05-03 04:24:00,361 - main - INFO - AUC: 0.8997
2025-05-03 04:24:00,361 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:24:00,361 - main - INFO - 
==================================================
2025-05-03 04:24:00,361 - main - INFO - Running configuration 470/756:
2025-05-03 04:24:00,361 - main - INFO - Model: DenseNet121
2025-05-03 04:24:00,361 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:24:00,361 - main - INFO - Scheduler: StepLR
2025-05-03 04:24:00,361 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:24:00,361 - main - INFO - ==================================================
2025-05-03 04:24:00,361 - training.model_DenseNet121_opt_Adam_lr_0.01_id_470 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_470
2025-05-03 04:24:00,361 - training.model_DenseNet121_opt_Adam_lr_0.01_id_470 - INFO - Config: {
  "id": 470,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:24:00,665 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:24:00,665 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 470,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:24:00,665 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:24:00,838 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:24:00,839 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:24:00,844 - training.model_DenseNet121_opt_Adam_lr_0.01_id_470 - INFO - Starting model evaluation
2025-05-03 04:24:12,866 - training.model_DenseNet121_opt_Adam_lr_0.01_id_470 - INFO - Evaluation metrics:
  Accuracy:  0.8474
  Precision: 0.6941
  Recall:    0.8252
  F1 Score:  0.7540
  IoU:       0.6051
  mAP:       0.7692
  AUC:       0.9008
2025-05-03 04:24:12,868 - training.model_DenseNet121_opt_Adam_lr_0.01_id_470 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_470/final_results.json
2025-05-03 04:24:12,869 - training.model_DenseNet121_opt_Adam_lr_0.01_id_470 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_470/final_results.json
2025-05-03 04:24:12,870 - main - INFO - 
Summary for configuration 470:
2025-05-03 04:24:12,870 - main - INFO - Accuracy: 0.8474
2025-05-03 04:24:12,870 - main - INFO - Precision: 0.6941
2025-05-03 04:24:12,870 - main - INFO - Recall: 0.8252
2025-05-03 04:24:12,870 - main - INFO - F1 Score: 0.7540
2025-05-03 04:24:12,870 - main - INFO - IoU: 0.6051
2025-05-03 04:24:12,870 - main - INFO - mAP: 0.7692
2025-05-03 04:24:12,870 - main - INFO - AUC: 0.9008
2025-05-03 04:24:12,870 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:24:12,870 - main - INFO - 
==================================================
2025-05-03 04:24:12,870 - main - INFO - Running configuration 471/756:
2025-05-03 04:24:12,870 - main - INFO - Model: DenseNet121
2025-05-03 04:24:12,870 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:24:12,870 - main - INFO - Scheduler: StepLR
2025-05-03 04:24:12,870 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:24:12,870 - main - INFO - ==================================================
2025-05-03 04:24:12,870 - training.model_DenseNet121_opt_Adam_lr_0.01_id_471 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_471
2025-05-03 04:24:12,870 - training.model_DenseNet121_opt_Adam_lr_0.01_id_471 - INFO - Config: {
  "id": 471,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:24:13,117 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:24:13,117 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 471,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:24:13,118 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:24:13,290 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:24:13,291 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:24:13,296 - training.model_DenseNet121_opt_Adam_lr_0.01_id_471 - INFO - Starting model evaluation
2025-05-03 04:24:24,597 - training.model_DenseNet121_opt_Adam_lr_0.01_id_471 - INFO - Evaluation metrics:
  Accuracy:  0.8454
  Precision: 0.6901
  Recall:    0.8252
  F1 Score:  0.7516
  IoU:       0.6020
  mAP:       0.7668
  AUC:       0.8968
2025-05-03 04:24:24,598 - training.model_DenseNet121_opt_Adam_lr_0.01_id_471 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_471/final_results.json
2025-05-03 04:24:24,600 - training.model_DenseNet121_opt_Adam_lr_0.01_id_471 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_471/final_results.json
2025-05-03 04:24:24,600 - main - INFO - 
Summary for configuration 471:
2025-05-03 04:24:24,600 - main - INFO - Accuracy: 0.8454
2025-05-03 04:24:24,600 - main - INFO - Precision: 0.6901
2025-05-03 04:24:24,600 - main - INFO - Recall: 0.8252
2025-05-03 04:24:24,600 - main - INFO - F1 Score: 0.7516
2025-05-03 04:24:24,600 - main - INFO - IoU: 0.6020
2025-05-03 04:24:24,600 - main - INFO - mAP: 0.7668
2025-05-03 04:24:24,600 - main - INFO - AUC: 0.8968
2025-05-03 04:24:24,600 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:24:24,600 - main - INFO - 
==================================================
2025-05-03 04:24:24,600 - main - INFO - Running configuration 472/756:
2025-05-03 04:24:24,600 - main - INFO - Model: DenseNet121
2025-05-03 04:24:24,600 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:24:24,600 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:24:24,600 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:24:24,600 - main - INFO - ==================================================
2025-05-03 04:24:24,600 - training.model_DenseNet121_opt_Adam_lr_0.01_id_472 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_472
2025-05-03 04:24:24,600 - training.model_DenseNet121_opt_Adam_lr_0.01_id_472 - INFO - Config: {
  "id": 472,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:24:24,732 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:24:24,732 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 472,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:24:24,732 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:24:24,905 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:24:24,907 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:24:24,911 - training.model_DenseNet121_opt_Adam_lr_0.01_id_472 - INFO - Starting model evaluation
2025-05-03 04:24:36,438 - training.model_DenseNet121_opt_Adam_lr_0.01_id_472 - INFO - Evaluation metrics:
  Accuracy:  0.8424
  Precision: 0.6841
  Recall:    0.8252
  F1 Score:  0.7480
  IoU:       0.5975
  mAP:       0.7571
  AUC:       0.8963
2025-05-03 04:24:36,440 - training.model_DenseNet121_opt_Adam_lr_0.01_id_472 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_472/final_results.json
2025-05-03 04:24:36,442 - training.model_DenseNet121_opt_Adam_lr_0.01_id_472 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_472/final_results.json
2025-05-03 04:24:36,442 - main - INFO - 
Summary for configuration 472:
2025-05-03 04:24:36,442 - main - INFO - Accuracy: 0.8424
2025-05-03 04:24:36,442 - main - INFO - Precision: 0.6841
2025-05-03 04:24:36,442 - main - INFO - Recall: 0.8252
2025-05-03 04:24:36,442 - main - INFO - F1 Score: 0.7480
2025-05-03 04:24:36,442 - main - INFO - IoU: 0.5975
2025-05-03 04:24:36,442 - main - INFO - mAP: 0.7571
2025-05-03 04:24:36,442 - main - INFO - AUC: 0.8963
2025-05-03 04:24:36,442 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:24:36,442 - main - INFO - 
==================================================
2025-05-03 04:24:36,442 - main - INFO - Running configuration 473/756:
2025-05-03 04:24:36,442 - main - INFO - Model: DenseNet121
2025-05-03 04:24:36,442 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:24:36,442 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:24:36,442 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:24:36,442 - main - INFO - ==================================================
2025-05-03 04:24:36,442 - training.model_DenseNet121_opt_Adam_lr_0.01_id_473 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_473
2025-05-03 04:24:36,442 - training.model_DenseNet121_opt_Adam_lr_0.01_id_473 - INFO - Config: {
  "id": 473,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:24:36,827 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:24:36,827 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 473,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:24:36,827 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:24:36,999 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:24:37,001 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:24:37,005 - training.model_DenseNet121_opt_Adam_lr_0.01_id_473 - INFO - Starting model evaluation
2025-05-03 04:24:48,335 - training.model_DenseNet121_opt_Adam_lr_0.01_id_473 - INFO - Evaluation metrics:
  Accuracy:  0.8474
  Precision: 0.6941
  Recall:    0.8252
  F1 Score:  0.7540
  IoU:       0.6051
  mAP:       0.7682
  AUC:       0.8980
2025-05-03 04:24:48,337 - training.model_DenseNet121_opt_Adam_lr_0.01_id_473 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_473/final_results.json
2025-05-03 04:24:48,338 - training.model_DenseNet121_opt_Adam_lr_0.01_id_473 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_473/final_results.json
2025-05-03 04:24:48,338 - main - INFO - 
Summary for configuration 473:
2025-05-03 04:24:48,338 - main - INFO - Accuracy: 0.8474
2025-05-03 04:24:48,338 - main - INFO - Precision: 0.6941
2025-05-03 04:24:48,338 - main - INFO - Recall: 0.8252
2025-05-03 04:24:48,338 - main - INFO - F1 Score: 0.7540
2025-05-03 04:24:48,338 - main - INFO - IoU: 0.6051
2025-05-03 04:24:48,338 - main - INFO - mAP: 0.7682
2025-05-03 04:24:48,338 - main - INFO - AUC: 0.8980
2025-05-03 04:24:48,338 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:24:48,338 - main - INFO - 
==================================================
2025-05-03 04:24:48,338 - main - INFO - Running configuration 474/756:
2025-05-03 04:24:48,338 - main - INFO - Model: DenseNet121
2025-05-03 04:24:48,338 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:24:48,338 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:24:48,338 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:24:48,338 - main - INFO - ==================================================
2025-05-03 04:24:48,339 - training.model_DenseNet121_opt_Adam_lr_0.01_id_474 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_474
2025-05-03 04:24:48,339 - training.model_DenseNet121_opt_Adam_lr_0.01_id_474 - INFO - Config: {
  "id": 474,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:24:48,468 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:24:48,469 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 474,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:24:48,469 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:24:48,641 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:24:48,642 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:24:48,647 - training.model_DenseNet121_opt_Adam_lr_0.01_id_474 - INFO - Starting model evaluation
2025-05-03 04:25:00,055 - training.model_DenseNet121_opt_Adam_lr_0.01_id_474 - INFO - Evaluation metrics:
  Accuracy:  0.8424
  Precision: 0.6830
  Recall:    0.8287
  F1 Score:  0.7488
  IoU:       0.5985
  mAP:       0.7684
  AUC:       0.8972
2025-05-03 04:25:00,057 - training.model_DenseNet121_opt_Adam_lr_0.01_id_474 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_474/final_results.json
2025-05-03 04:25:00,059 - training.model_DenseNet121_opt_Adam_lr_0.01_id_474 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_474/final_results.json
2025-05-03 04:25:00,059 - main - INFO - 
Summary for configuration 474:
2025-05-03 04:25:00,059 - main - INFO - Accuracy: 0.8424
2025-05-03 04:25:00,059 - main - INFO - Precision: 0.6830
2025-05-03 04:25:00,059 - main - INFO - Recall: 0.8287
2025-05-03 04:25:00,060 - main - INFO - F1 Score: 0.7488
2025-05-03 04:25:00,060 - main - INFO - IoU: 0.5985
2025-05-03 04:25:00,060 - main - INFO - mAP: 0.7684
2025-05-03 04:25:00,060 - main - INFO - AUC: 0.8972
2025-05-03 04:25:00,060 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:25:00,060 - main - INFO - 
==================================================
2025-05-03 04:25:00,060 - main - INFO - Running configuration 475/756:
2025-05-03 04:25:00,060 - main - INFO - Model: DenseNet121
2025-05-03 04:25:00,060 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:25:00,060 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:25:00,060 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:25:00,060 - main - INFO - ==================================================
2025-05-03 04:25:00,060 - training.model_DenseNet121_opt_Adam_lr_0.01_id_475 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_475
2025-05-03 04:25:00,060 - training.model_DenseNet121_opt_Adam_lr_0.01_id_475 - INFO - Config: {
  "id": 475,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:25:00,192 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:25:00,192 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 475,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:25:00,192 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:25:00,639 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:25:00,641 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:25:00,645 - training.model_DenseNet121_opt_Adam_lr_0.01_id_475 - INFO - Starting model evaluation
2025-05-03 04:25:12,343 - training.model_DenseNet121_opt_Adam_lr_0.01_id_475 - INFO - Evaluation metrics:
  Accuracy:  0.8434
  Precision: 0.6860
  Recall:    0.8252
  F1 Score:  0.7492
  IoU:       0.5990
  mAP:       0.7742
  AUC:       0.8986
2025-05-03 04:25:12,344 - training.model_DenseNet121_opt_Adam_lr_0.01_id_475 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_475/final_results.json
2025-05-03 04:25:12,346 - training.model_DenseNet121_opt_Adam_lr_0.01_id_475 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_475/final_results.json
2025-05-03 04:25:12,346 - main - INFO - 
Summary for configuration 475:
2025-05-03 04:25:12,346 - main - INFO - Accuracy: 0.8434
2025-05-03 04:25:12,346 - main - INFO - Precision: 0.6860
2025-05-03 04:25:12,346 - main - INFO - Recall: 0.8252
2025-05-03 04:25:12,346 - main - INFO - F1 Score: 0.7492
2025-05-03 04:25:12,346 - main - INFO - IoU: 0.5990
2025-05-03 04:25:12,346 - main - INFO - mAP: 0.7742
2025-05-03 04:25:12,346 - main - INFO - AUC: 0.8986
2025-05-03 04:25:12,346 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:25:12,346 - main - INFO - 
==================================================
2025-05-03 04:25:12,346 - main - INFO - Running configuration 476/756:
2025-05-03 04:25:12,346 - main - INFO - Model: DenseNet121
2025-05-03 04:25:12,346 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:25:12,346 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:25:12,346 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:25:12,346 - main - INFO - ==================================================
2025-05-03 04:25:12,346 - training.model_DenseNet121_opt_Adam_lr_0.01_id_476 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_476
2025-05-03 04:25:12,346 - training.model_DenseNet121_opt_Adam_lr_0.01_id_476 - INFO - Config: {
  "id": 476,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:25:12,488 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:25:12,489 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 476,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:25:12,489 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:25:12,660 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:25:12,662 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:25:12,666 - training.model_DenseNet121_opt_Adam_lr_0.01_id_476 - INFO - Starting model evaluation
2025-05-03 04:25:24,781 - training.model_DenseNet121_opt_Adam_lr_0.01_id_476 - INFO - Evaluation metrics:
  Accuracy:  0.8533
  Precision: 0.7054
  Recall:    0.8287
  F1 Score:  0.7621
  IoU:       0.6156
  mAP:       0.7787
  AUC:       0.9014
2025-05-03 04:25:24,783 - training.model_DenseNet121_opt_Adam_lr_0.01_id_476 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_476/final_results.json
2025-05-03 04:25:24,784 - training.model_DenseNet121_opt_Adam_lr_0.01_id_476 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_476/final_results.json
2025-05-03 04:25:24,784 - main - INFO - 
Summary for configuration 476:
2025-05-03 04:25:24,784 - main - INFO - Accuracy: 0.8533
2025-05-03 04:25:24,784 - main - INFO - Precision: 0.7054
2025-05-03 04:25:24,784 - main - INFO - Recall: 0.8287
2025-05-03 04:25:24,784 - main - INFO - F1 Score: 0.7621
2025-05-03 04:25:24,784 - main - INFO - IoU: 0.6156
2025-05-03 04:25:24,784 - main - INFO - mAP: 0.7787
2025-05-03 04:25:24,784 - main - INFO - AUC: 0.9014
2025-05-03 04:25:24,784 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:25:24,784 - main - INFO - 
==================================================
2025-05-03 04:25:24,784 - main - INFO - Running configuration 477/756:
2025-05-03 04:25:24,784 - main - INFO - Model: DenseNet121
2025-05-03 04:25:24,784 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:25:24,784 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:25:24,784 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:25:24,784 - main - INFO - ==================================================
2025-05-03 04:25:24,785 - training.model_DenseNet121_opt_Adam_lr_0.01_id_477 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_477
2025-05-03 04:25:24,785 - training.model_DenseNet121_opt_Adam_lr_0.01_id_477 - INFO - Config: {
  "id": 477,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:25:24,916 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:25:24,916 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 477,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:25:24,916 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:25:25,088 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:25:25,089 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:25:25,094 - training.model_DenseNet121_opt_Adam_lr_0.01_id_477 - INFO - Starting model evaluation
2025-05-03 04:25:37,028 - training.model_DenseNet121_opt_Adam_lr_0.01_id_477 - INFO - Evaluation metrics:
  Accuracy:  0.8494
  Precision: 0.6959
  Recall:    0.8322
  F1 Score:  0.7580
  IoU:       0.6103
  mAP:       0.7645
  AUC:       0.8983
2025-05-03 04:25:37,029 - training.model_DenseNet121_opt_Adam_lr_0.01_id_477 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_477/final_results.json
2025-05-03 04:25:37,031 - training.model_DenseNet121_opt_Adam_lr_0.01_id_477 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_477/final_results.json
2025-05-03 04:25:37,031 - main - INFO - 
Summary for configuration 477:
2025-05-03 04:25:37,031 - main - INFO - Accuracy: 0.8494
2025-05-03 04:25:37,031 - main - INFO - Precision: 0.6959
2025-05-03 04:25:37,031 - main - INFO - Recall: 0.8322
2025-05-03 04:25:37,031 - main - INFO - F1 Score: 0.7580
2025-05-03 04:25:37,031 - main - INFO - IoU: 0.6103
2025-05-03 04:25:37,031 - main - INFO - mAP: 0.7645
2025-05-03 04:25:37,031 - main - INFO - AUC: 0.8983
2025-05-03 04:25:37,031 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:25:37,031 - main - INFO - 
==================================================
2025-05-03 04:25:37,031 - main - INFO - Running configuration 478/756:
2025-05-03 04:25:37,031 - main - INFO - Model: DenseNet121
2025-05-03 04:25:37,031 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:25:37,031 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:25:37,031 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:25:37,031 - main - INFO - ==================================================
2025-05-03 04:25:37,031 - training.model_DenseNet121_opt_Adam_lr_0.01_id_478 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_478
2025-05-03 04:25:37,031 - training.model_DenseNet121_opt_Adam_lr_0.01_id_478 - INFO - Config: {
  "id": 478,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:25:37,162 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:25:37,162 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 478,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:25:37,162 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:25:37,549 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:25:37,551 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:25:37,555 - training.model_DenseNet121_opt_Adam_lr_0.01_id_478 - INFO - Starting model evaluation
2025-05-03 04:25:49,212 - training.model_DenseNet121_opt_Adam_lr_0.01_id_478 - INFO - Evaluation metrics:
  Accuracy:  0.8434
  Precision: 0.6860
  Recall:    0.8252
  F1 Score:  0.7492
  IoU:       0.5990
  mAP:       0.7634
  AUC:       0.8943
2025-05-03 04:25:49,214 - training.model_DenseNet121_opt_Adam_lr_0.01_id_478 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_478/final_results.json
2025-05-03 04:25:49,215 - training.model_DenseNet121_opt_Adam_lr_0.01_id_478 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_478/final_results.json
2025-05-03 04:25:49,215 - main - INFO - 
Summary for configuration 478:
2025-05-03 04:25:49,215 - main - INFO - Accuracy: 0.8434
2025-05-03 04:25:49,215 - main - INFO - Precision: 0.6860
2025-05-03 04:25:49,215 - main - INFO - Recall: 0.8252
2025-05-03 04:25:49,215 - main - INFO - F1 Score: 0.7492
2025-05-03 04:25:49,215 - main - INFO - IoU: 0.5990
2025-05-03 04:25:49,215 - main - INFO - mAP: 0.7634
2025-05-03 04:25:49,215 - main - INFO - AUC: 0.8943
2025-05-03 04:25:49,215 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:25:49,215 - main - INFO - 
==================================================
2025-05-03 04:25:49,215 - main - INFO - Running configuration 479/756:
2025-05-03 04:25:49,215 - main - INFO - Model: DenseNet121
2025-05-03 04:25:49,215 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:25:49,216 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:25:49,216 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:25:49,216 - main - INFO - ==================================================
2025-05-03 04:25:49,216 - training.model_DenseNet121_opt_Adam_lr_0.01_id_479 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_479
2025-05-03 04:25:49,216 - training.model_DenseNet121_opt_Adam_lr_0.01_id_479 - INFO - Config: {
  "id": 479,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:25:49,348 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:25:49,349 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 479,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:25:49,349 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:25:49,520 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:25:49,522 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:25:49,526 - training.model_DenseNet121_opt_Adam_lr_0.01_id_479 - INFO - Starting model evaluation
2025-05-03 04:26:01,059 - training.model_DenseNet121_opt_Adam_lr_0.01_id_479 - INFO - Evaluation metrics:
  Accuracy:  0.8434
  Precision: 0.6905
  Recall:    0.8112
  F1 Score:  0.7460
  IoU:       0.5949
  mAP:       0.7710
  AUC:       0.9016
2025-05-03 04:26:01,060 - training.model_DenseNet121_opt_Adam_lr_0.01_id_479 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_479/final_results.json
2025-05-03 04:26:01,062 - training.model_DenseNet121_opt_Adam_lr_0.01_id_479 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_479/final_results.json
2025-05-03 04:26:01,062 - main - INFO - 
Summary for configuration 479:
2025-05-03 04:26:01,062 - main - INFO - Accuracy: 0.8434
2025-05-03 04:26:01,062 - main - INFO - Precision: 0.6905
2025-05-03 04:26:01,062 - main - INFO - Recall: 0.8112
2025-05-03 04:26:01,062 - main - INFO - F1 Score: 0.7460
2025-05-03 04:26:01,062 - main - INFO - IoU: 0.5949
2025-05-03 04:26:01,062 - main - INFO - mAP: 0.7710
2025-05-03 04:26:01,062 - main - INFO - AUC: 0.9016
2025-05-03 04:26:01,062 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:26:01,062 - main - INFO - 
==================================================
2025-05-03 04:26:01,062 - main - INFO - Running configuration 480/756:
2025-05-03 04:26:01,062 - main - INFO - Model: DenseNet121
2025-05-03 04:26:01,062 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 04:26:01,062 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:26:01,062 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:26:01,062 - main - INFO - ==================================================
2025-05-03 04:26:01,062 - training.model_DenseNet121_opt_Adam_lr_0.01_id_480 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01_id_480
2025-05-03 04:26:01,062 - training.model_DenseNet121_opt_Adam_lr_0.01_id_480 - INFO - Config: {
  "id": 480,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:26:01,193 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.01
2025-05-03 04:26:01,193 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 480,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:26:01,193 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 04:26:01,364 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.8705
2025-05-03 04:26:01,366 - training.model_DenseNet121_opt_Adam_lr_0.01 - INFO - Training completed after 1122.42 seconds
2025-05-03 04:26:01,370 - training.model_DenseNet121_opt_Adam_lr_0.01_id_480 - INFO - Starting model evaluation
2025-05-03 04:26:12,860 - training.model_DenseNet121_opt_Adam_lr_0.01_id_480 - INFO - Evaluation metrics:
  Accuracy:  0.8404
  Precision: 0.6855
  Recall:    0.8077
  F1 Score:  0.7416
  IoU:       0.5893
  mAP:       0.7717
  AUC:       0.8939
2025-05-03 04:26:12,861 - training.model_DenseNet121_opt_Adam_lr_0.01_id_480 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_480/final_results.json
2025-05-03 04:26:12,863 - training.model_DenseNet121_opt_Adam_lr_0.01_id_480 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.01_id_480/final_results.json
2025-05-03 04:26:12,863 - main - INFO - 
Summary for configuration 480:
2025-05-03 04:26:12,863 - main - INFO - Accuracy: 0.8404
2025-05-03 04:26:12,863 - main - INFO - Precision: 0.6855
2025-05-03 04:26:12,863 - main - INFO - Recall: 0.8077
2025-05-03 04:26:12,863 - main - INFO - F1 Score: 0.7416
2025-05-03 04:26:12,863 - main - INFO - IoU: 0.5893
2025-05-03 04:26:12,863 - main - INFO - mAP: 0.7717
2025-05-03 04:26:12,863 - main - INFO - AUC: 0.8939
2025-05-03 04:26:12,863 - main - INFO - Training time: 1122.42 seconds
2025-05-03 04:26:12,863 - main - INFO - 
==================================================
2025-05-03 04:26:12,863 - main - INFO - Running configuration 481/756:
2025-05-03 04:26:12,863 - main - INFO - Model: DenseNet121
2025-05-03 04:26:12,863 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:26:12,863 - main - INFO - Scheduler: StepLR
2025-05-03 04:26:12,863 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:26:12,863 - main - INFO - ==================================================
2025-05-03 04:26:12,863 - training.model_DenseNet121_opt_Adam_lr_0.001_id_481 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_481
2025-05-03 04:26:12,863 - training.model_DenseNet121_opt_Adam_lr_0.001_id_481 - INFO - Config: {
  "id": 481,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:26:13,252 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:26:13,252 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 481,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:26:13,252 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-03 04:26:13,253 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-03 04:26:37,345 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 04:27:09,126 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.7899
2025-05-03 04:27:09,156 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 1 completed in 55.90s - Train Loss: 0.5422, Train Acc: 0.7533, Val Loss: 0.5154, Val Acc: 0.7899
2025-05-03 04:27:09,258 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 04:27:09,259 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 2/20
2025-05-03 04:27:33,345 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 04:28:04,799 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.7899 to 0.8276
2025-05-03 04:28:04,838 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 2 completed in 55.58s - Train Loss: 0.5215, Train Acc: 0.7804, Val Loss: 0.4775, Val Acc: 0.8276
2025-05-03 04:28:04,935 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 04:28:04,936 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 3/20
2025-05-03 04:28:28,103 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 04:29:01,302 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8276 to 0.8619
2025-05-03 04:29:01,341 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 3 completed in 56.41s - Train Loss: 0.4815, Train Acc: 0.8226, Val Loss: 0.4437, Val Acc: 0.8619
2025-05-03 04:29:01,442 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 04:29:01,442 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 4/20
2025-05-03 04:29:25,378 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 04:29:57,100 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation loss improved from inf to 0.4968
2025-05-03 04:29:57,226 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 4 completed in 55.78s - Train Loss: 0.4669, Train Acc: 0.8415, Val Loss: 0.4968, Val Acc: 0.8105
2025-05-03 04:29:57,322 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 04:29:57,323 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 5/20
2025-05-03 04:30:21,822 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 04:30:54,025 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8619 to 0.8748
2025-05-03 04:30:54,065 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 5 completed in 56.74s - Train Loss: 0.4625, Train Acc: 0.8436, Val Loss: 0.4334, Val Acc: 0.8748
2025-05-03 04:30:54,161 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 04:30:54,161 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 6/20
2025-05-03 04:31:17,829 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 04:31:51,901 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4968 to 0.4517
2025-05-03 04:31:51,941 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 6 completed in 57.78s - Train Loss: 0.4503, Train Acc: 0.8565, Val Loss: 0.4517, Val Acc: 0.8499
2025-05-03 04:31:52,038 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 04:31:52,039 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 7/20
2025-05-03 04:32:16,419 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 04:32:48,510 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 7 completed in 56.47s - Train Loss: 0.4588, Train Acc: 0.8456, Val Loss: 0.4809, Val Acc: 0.8250
2025-05-03 04:32:48,614 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 04:32:48,615 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 8/20
2025-05-03 04:33:13,069 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 04:33:44,031 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4517 to 0.4484
2025-05-03 04:33:44,072 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 8 completed in 55.46s - Train Loss: 0.4567, Train Acc: 0.8505, Val Loss: 0.4484, Val Acc: 0.8611
2025-05-03 04:33:44,178 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 04:33:44,179 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 9/20
2025-05-03 04:34:08,757 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 04:34:40,517 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8748 to 0.8765
2025-05-03 04:34:40,558 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 9 completed in 56.38s - Train Loss: 0.4304, Train Acc: 0.8764, Val Loss: 0.4335, Val Acc: 0.8765
2025-05-03 04:34:40,653 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 04:34:40,653 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 10/20
2025-05-03 04:35:05,086 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 04:35:36,626 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4484 to 0.4384
2025-05-03 04:35:36,672 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 10 completed in 56.02s - Train Loss: 0.4193, Train Acc: 0.8938, Val Loss: 0.4384, Val Acc: 0.8731
2025-05-03 04:35:36,774 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 04:35:36,774 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 11/20
2025-05-03 04:35:59,451 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 04:36:31,415 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.8765 to 0.9211
2025-05-03 04:36:31,458 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 11 completed in 54.68s - Train Loss: 0.3964, Train Acc: 0.9159, Val Loss: 0.3882, Val Acc: 0.9211
2025-05-03 04:36:31,556 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 04:36:31,557 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 12/20
2025-05-03 04:36:55,931 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 04:37:27,646 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9211 to 0.9391
2025-05-03 04:37:27,696 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 12 completed in 56.14s - Train Loss: 0.3716, Train Acc: 0.9425, Val Loss: 0.3728, Val Acc: 0.9391
2025-05-03 04:37:27,802 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 04:37:27,803 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 04:37:51,885 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 04:38:23,741 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4384 to 0.3732
2025-05-03 04:38:23,782 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 13 completed in 55.98s - Train Loss: 0.3651, Train Acc: 0.9502, Val Loss: 0.3732, Val Acc: 0.9383
2025-05-03 04:38:23,880 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 04:38:23,880 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 04:38:47,464 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 04:39:19,967 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 14 completed in 56.09s - Train Loss: 0.3587, Train Acc: 0.9539, Val Loss: 0.3734, Val Acc: 0.9383
2025-05-03 04:39:20,070 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 04:39:20,070 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 04:39:43,437 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 04:40:14,857 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9391 to 0.9545
2025-05-03 04:40:14,897 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 15 completed in 54.83s - Train Loss: 0.3562, Train Acc: 0.9590, Val Loss: 0.3595, Val Acc: 0.9545
2025-05-03 04:40:15,013 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 04:40:15,013 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 04:40:38,520 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 04:41:10,384 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3732 to 0.3577
2025-05-03 04:41:10,426 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 16 completed in 55.41s - Train Loss: 0.3475, Train Acc: 0.9665, Val Loss: 0.3577, Val Acc: 0.9520
2025-05-03 04:41:10,522 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 04:41:10,523 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 04:41:33,180 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 04:42:06,397 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 17 completed in 55.87s - Train Loss: 0.3503, Train Acc: 0.9631, Val Loss: 0.3592, Val Acc: 0.9511
2025-05-03 04:42:06,499 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 04:42:06,500 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 04:42:30,637 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 04:43:02,969 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9545 to 0.9648
2025-05-03 04:43:03,010 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 18 completed in 56.51s - Train Loss: 0.3458, Train Acc: 0.9687, Val Loss: 0.3478, Val Acc: 0.9648
2025-05-03 04:43:03,109 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 04:43:03,109 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 04:43:26,822 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 04:43:59,092 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3577 to 0.3522
2025-05-03 04:43:59,132 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 19 completed in 56.02s - Train Loss: 0.3457, Train Acc: 0.9685, Val Loss: 0.3522, Val Acc: 0.9605
2025-05-03 04:43:59,230 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 04:43:59,230 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 04:44:22,950 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 04:44:55,674 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3522 to 0.3503
2025-05-03 04:44:55,716 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Epoch 20 completed in 56.49s - Train Loss: 0.3410, Train Acc: 0.9738, Val Loss: 0.3503, Val Acc: 0.9614
2025-05-03 04:44:55,813 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 04:44:55,814 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:44:55,819 - training.model_DenseNet121_opt_Adam_lr_0.001_id_481 - INFO - Starting model evaluation
2025-05-03 04:45:07,214 - training.model_DenseNet121_opt_Adam_lr_0.001_id_481 - INFO - Evaluation metrics:
  Accuracy:  0.9495
  Precision: 0.9123
  Recall:    0.9091
  F1 Score:  0.9107
  IoU:       0.8360
  mAP:       0.9662
  AUC:       0.9851
2025-05-03 04:45:07,216 - training.model_DenseNet121_opt_Adam_lr_0.001_id_481 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_481/final_results.json
2025-05-03 04:45:07,218 - training.model_DenseNet121_opt_Adam_lr_0.001_id_481 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_481/final_results.json
2025-05-03 04:45:07,218 - main - INFO - 
Summary for configuration 481:
2025-05-03 04:45:07,218 - main - INFO - Accuracy: 0.9495
2025-05-03 04:45:07,218 - main - INFO - Precision: 0.9123
2025-05-03 04:45:07,218 - main - INFO - Recall: 0.9091
2025-05-03 04:45:07,218 - main - INFO - F1 Score: 0.9107
2025-05-03 04:45:07,218 - main - INFO - IoU: 0.8360
2025-05-03 04:45:07,218 - main - INFO - mAP: 0.9662
2025-05-03 04:45:07,218 - main - INFO - AUC: 0.9851
2025-05-03 04:45:07,218 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:45:07,218 - main - INFO - 
==================================================
2025-05-03 04:45:07,218 - main - INFO - Running configuration 482/756:
2025-05-03 04:45:07,218 - main - INFO - Model: DenseNet121
2025-05-03 04:45:07,218 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:45:07,218 - main - INFO - Scheduler: StepLR
2025-05-03 04:45:07,218 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:45:07,218 - main - INFO - ==================================================
2025-05-03 04:45:07,218 - training.model_DenseNet121_opt_Adam_lr_0.001_id_482 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_482
2025-05-03 04:45:07,218 - training.model_DenseNet121_opt_Adam_lr_0.001_id_482 - INFO - Config: {
  "id": 482,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:45:07,348 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:45:07,348 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 482,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:45:07,348 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:45:07,521 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:45:07,523 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:45:07,527 - training.model_DenseNet121_opt_Adam_lr_0.001_id_482 - INFO - Starting model evaluation
2025-05-03 04:45:19,122 - training.model_DenseNet121_opt_Adam_lr_0.001_id_482 - INFO - Evaluation metrics:
  Accuracy:  0.9455
  Precision: 0.8969
  Recall:    0.9126
  F1 Score:  0.9047
  IoU:       0.8259
  mAP:       0.9572
  AUC:       0.9816
2025-05-03 04:45:19,123 - training.model_DenseNet121_opt_Adam_lr_0.001_id_482 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_482/final_results.json
2025-05-03 04:45:19,125 - training.model_DenseNet121_opt_Adam_lr_0.001_id_482 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_482/final_results.json
2025-05-03 04:45:19,125 - main - INFO - 
Summary for configuration 482:
2025-05-03 04:45:19,125 - main - INFO - Accuracy: 0.9455
2025-05-03 04:45:19,125 - main - INFO - Precision: 0.8969
2025-05-03 04:45:19,125 - main - INFO - Recall: 0.9126
2025-05-03 04:45:19,125 - main - INFO - F1 Score: 0.9047
2025-05-03 04:45:19,125 - main - INFO - IoU: 0.8259
2025-05-03 04:45:19,125 - main - INFO - mAP: 0.9572
2025-05-03 04:45:19,125 - main - INFO - AUC: 0.9816
2025-05-03 04:45:19,125 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:45:19,125 - main - INFO - 
==================================================
2025-05-03 04:45:19,125 - main - INFO - Running configuration 483/756:
2025-05-03 04:45:19,125 - main - INFO - Model: DenseNet121
2025-05-03 04:45:19,125 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:45:19,125 - main - INFO - Scheduler: StepLR
2025-05-03 04:45:19,125 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:45:19,125 - main - INFO - ==================================================
2025-05-03 04:45:19,125 - training.model_DenseNet121_opt_Adam_lr_0.001_id_483 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_483
2025-05-03 04:45:19,125 - training.model_DenseNet121_opt_Adam_lr_0.001_id_483 - INFO - Config: {
  "id": 483,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:45:19,257 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:45:19,258 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 483,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:45:19,258 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:45:19,431 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:45:19,433 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:45:19,437 - training.model_DenseNet121_opt_Adam_lr_0.001_id_483 - INFO - Starting model evaluation
2025-05-03 04:45:31,127 - training.model_DenseNet121_opt_Adam_lr_0.001_id_483 - INFO - Evaluation metrics:
  Accuracy:  0.9455
  Precision: 0.9053
  Recall:    0.9021
  F1 Score:  0.9037
  IoU:       0.8243
  mAP:       0.9644
  AUC:       0.9827
2025-05-03 04:45:31,130 - training.model_DenseNet121_opt_Adam_lr_0.001_id_483 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_483/final_results.json
2025-05-03 04:45:31,133 - training.model_DenseNet121_opt_Adam_lr_0.001_id_483 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_483/final_results.json
2025-05-03 04:45:31,133 - main - INFO - 
Summary for configuration 483:
2025-05-03 04:45:31,133 - main - INFO - Accuracy: 0.9455
2025-05-03 04:45:31,133 - main - INFO - Precision: 0.9053
2025-05-03 04:45:31,133 - main - INFO - Recall: 0.9021
2025-05-03 04:45:31,133 - main - INFO - F1 Score: 0.9037
2025-05-03 04:45:31,133 - main - INFO - IoU: 0.8243
2025-05-03 04:45:31,133 - main - INFO - mAP: 0.9644
2025-05-03 04:45:31,133 - main - INFO - AUC: 0.9827
2025-05-03 04:45:31,133 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:45:31,133 - main - INFO - 
==================================================
2025-05-03 04:45:31,133 - main - INFO - Running configuration 484/756:
2025-05-03 04:45:31,133 - main - INFO - Model: DenseNet121
2025-05-03 04:45:31,133 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:45:31,133 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:45:31,133 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:45:31,133 - main - INFO - ==================================================
2025-05-03 04:45:31,133 - training.model_DenseNet121_opt_Adam_lr_0.001_id_484 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_484
2025-05-03 04:45:31,133 - training.model_DenseNet121_opt_Adam_lr_0.001_id_484 - INFO - Config: {
  "id": 484,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:45:31,265 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:45:31,265 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 484,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:45:31,265 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:45:31,437 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:45:31,439 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:45:31,443 - training.model_DenseNet121_opt_Adam_lr_0.001_id_484 - INFO - Starting model evaluation
2025-05-03 04:45:43,156 - training.model_DenseNet121_opt_Adam_lr_0.001_id_484 - INFO - Evaluation metrics:
  Accuracy:  0.9455
  Precision: 0.8969
  Recall:    0.9126
  F1 Score:  0.9047
  IoU:       0.8259
  mAP:       0.9561
  AUC:       0.9800
2025-05-03 04:45:43,157 - training.model_DenseNet121_opt_Adam_lr_0.001_id_484 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_484/final_results.json
2025-05-03 04:45:43,159 - training.model_DenseNet121_opt_Adam_lr_0.001_id_484 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_484/final_results.json
2025-05-03 04:45:43,159 - main - INFO - 
Summary for configuration 484:
2025-05-03 04:45:43,159 - main - INFO - Accuracy: 0.9455
2025-05-03 04:45:43,159 - main - INFO - Precision: 0.8969
2025-05-03 04:45:43,159 - main - INFO - Recall: 0.9126
2025-05-03 04:45:43,159 - main - INFO - F1 Score: 0.9047
2025-05-03 04:45:43,159 - main - INFO - IoU: 0.8259
2025-05-03 04:45:43,159 - main - INFO - mAP: 0.9561
2025-05-03 04:45:43,159 - main - INFO - AUC: 0.9800
2025-05-03 04:45:43,159 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:45:43,159 - main - INFO - 
==================================================
2025-05-03 04:45:43,159 - main - INFO - Running configuration 485/756:
2025-05-03 04:45:43,159 - main - INFO - Model: DenseNet121
2025-05-03 04:45:43,159 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:45:43,159 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:45:43,159 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:45:43,159 - main - INFO - ==================================================
2025-05-03 04:45:43,159 - training.model_DenseNet121_opt_Adam_lr_0.001_id_485 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_485
2025-05-03 04:45:43,159 - training.model_DenseNet121_opt_Adam_lr_0.001_id_485 - INFO - Config: {
  "id": 485,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:45:43,529 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:45:43,529 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 485,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:45:43,529 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:45:43,701 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:45:43,703 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:45:43,708 - training.model_DenseNet121_opt_Adam_lr_0.001_id_485 - INFO - Starting model evaluation
2025-05-03 04:45:55,404 - training.model_DenseNet121_opt_Adam_lr_0.001_id_485 - INFO - Evaluation metrics:
  Accuracy:  0.9435
  Precision: 0.9046
  Recall:    0.8951
  F1 Score:  0.8998
  IoU:       0.8179
  mAP:       0.9684
  AUC:       0.9853
2025-05-03 04:45:55,406 - training.model_DenseNet121_opt_Adam_lr_0.001_id_485 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_485/final_results.json
2025-05-03 04:45:55,408 - training.model_DenseNet121_opt_Adam_lr_0.001_id_485 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_485/final_results.json
2025-05-03 04:45:55,408 - main - INFO - 
Summary for configuration 485:
2025-05-03 04:45:55,408 - main - INFO - Accuracy: 0.9435
2025-05-03 04:45:55,408 - main - INFO - Precision: 0.9046
2025-05-03 04:45:55,408 - main - INFO - Recall: 0.8951
2025-05-03 04:45:55,408 - main - INFO - F1 Score: 0.8998
2025-05-03 04:45:55,408 - main - INFO - IoU: 0.8179
2025-05-03 04:45:55,408 - main - INFO - mAP: 0.9684
2025-05-03 04:45:55,408 - main - INFO - AUC: 0.9853
2025-05-03 04:45:55,408 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:45:55,408 - main - INFO - 
==================================================
2025-05-03 04:45:55,408 - main - INFO - Running configuration 486/756:
2025-05-03 04:45:55,408 - main - INFO - Model: DenseNet121
2025-05-03 04:45:55,408 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:45:55,408 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 04:45:55,408 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:45:55,408 - main - INFO - ==================================================
2025-05-03 04:45:55,408 - training.model_DenseNet121_opt_Adam_lr_0.001_id_486 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_486
2025-05-03 04:45:55,408 - training.model_DenseNet121_opt_Adam_lr_0.001_id_486 - INFO - Config: {
  "id": 486,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:45:55,540 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:45:55,540 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 486,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:45:55,540 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:45:55,713 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:45:55,714 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:45:55,719 - training.model_DenseNet121_opt_Adam_lr_0.001_id_486 - INFO - Starting model evaluation
2025-05-03 04:46:07,338 - training.model_DenseNet121_opt_Adam_lr_0.001_id_486 - INFO - Evaluation metrics:
  Accuracy:  0.9445
  Precision: 0.9049
  Recall:    0.8986
  F1 Score:  0.9018
  IoU:       0.8211
  mAP:       0.9649
  AUC:       0.9814
2025-05-03 04:46:07,340 - training.model_DenseNet121_opt_Adam_lr_0.001_id_486 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_486/final_results.json
2025-05-03 04:46:07,341 - training.model_DenseNet121_opt_Adam_lr_0.001_id_486 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_486/final_results.json
2025-05-03 04:46:07,341 - main - INFO - 
Summary for configuration 486:
2025-05-03 04:46:07,341 - main - INFO - Accuracy: 0.9445
2025-05-03 04:46:07,341 - main - INFO - Precision: 0.9049
2025-05-03 04:46:07,341 - main - INFO - Recall: 0.8986
2025-05-03 04:46:07,341 - main - INFO - F1 Score: 0.9018
2025-05-03 04:46:07,341 - main - INFO - IoU: 0.8211
2025-05-03 04:46:07,341 - main - INFO - mAP: 0.9649
2025-05-03 04:46:07,341 - main - INFO - AUC: 0.9814
2025-05-03 04:46:07,341 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:46:07,341 - main - INFO - 
==================================================
2025-05-03 04:46:07,341 - main - INFO - Running configuration 487/756:
2025-05-03 04:46:07,341 - main - INFO - Model: DenseNet121
2025-05-03 04:46:07,341 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:46:07,341 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:46:07,341 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:46:07,341 - main - INFO - ==================================================
2025-05-03 04:46:07,342 - training.model_DenseNet121_opt_Adam_lr_0.001_id_487 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_487
2025-05-03 04:46:07,342 - training.model_DenseNet121_opt_Adam_lr_0.001_id_487 - INFO - Config: {
  "id": 487,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:46:07,555 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:46:07,555 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 487,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:46:07,555 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:46:07,728 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:46:07,729 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:46:07,734 - training.model_DenseNet121_opt_Adam_lr_0.001_id_487 - INFO - Starting model evaluation
2025-05-03 04:46:19,597 - training.model_DenseNet121_opt_Adam_lr_0.001_id_487 - INFO - Evaluation metrics:
  Accuracy:  0.9465
  Precision: 0.8946
  Recall:    0.9196
  F1 Score:  0.9069
  IoU:       0.8297
  mAP:       0.9623
  AUC:       0.9854
2025-05-03 04:46:19,599 - training.model_DenseNet121_opt_Adam_lr_0.001_id_487 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_487/final_results.json
2025-05-03 04:46:19,601 - training.model_DenseNet121_opt_Adam_lr_0.001_id_487 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_487/final_results.json
2025-05-03 04:46:19,601 - main - INFO - 
Summary for configuration 487:
2025-05-03 04:46:19,601 - main - INFO - Accuracy: 0.9465
2025-05-03 04:46:19,601 - main - INFO - Precision: 0.8946
2025-05-03 04:46:19,601 - main - INFO - Recall: 0.9196
2025-05-03 04:46:19,601 - main - INFO - F1 Score: 0.9069
2025-05-03 04:46:19,601 - main - INFO - IoU: 0.8297
2025-05-03 04:46:19,601 - main - INFO - mAP: 0.9623
2025-05-03 04:46:19,601 - main - INFO - AUC: 0.9854
2025-05-03 04:46:19,601 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:46:19,601 - main - INFO - 
==================================================
2025-05-03 04:46:19,601 - main - INFO - Running configuration 488/756:
2025-05-03 04:46:19,601 - main - INFO - Model: DenseNet121
2025-05-03 04:46:19,601 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:46:19,601 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:46:19,601 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:46:19,601 - main - INFO - ==================================================
2025-05-03 04:46:19,601 - training.model_DenseNet121_opt_Adam_lr_0.001_id_488 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_488
2025-05-03 04:46:19,601 - training.model_DenseNet121_opt_Adam_lr_0.001_id_488 - INFO - Config: {
  "id": 488,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:46:19,881 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:46:19,881 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 488,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:46:19,881 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:46:20,054 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:46:20,055 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:46:20,060 - training.model_DenseNet121_opt_Adam_lr_0.001_id_488 - INFO - Starting model evaluation
2025-05-03 04:46:31,707 - training.model_DenseNet121_opt_Adam_lr_0.001_id_488 - INFO - Evaluation metrics:
  Accuracy:  0.9534
  Precision: 0.9223
  Recall:    0.9126
  F1 Score:  0.9174
  IoU:       0.8474
  mAP:       0.9647
  AUC:       0.9823
2025-05-03 04:46:31,708 - training.model_DenseNet121_opt_Adam_lr_0.001_id_488 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_488/final_results.json
2025-05-03 04:46:31,710 - training.model_DenseNet121_opt_Adam_lr_0.001_id_488 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_488/final_results.json
2025-05-03 04:46:31,710 - main - INFO - 
Summary for configuration 488:
2025-05-03 04:46:31,710 - main - INFO - Accuracy: 0.9534
2025-05-03 04:46:31,710 - main - INFO - Precision: 0.9223
2025-05-03 04:46:31,710 - main - INFO - Recall: 0.9126
2025-05-03 04:46:31,710 - main - INFO - F1 Score: 0.9174
2025-05-03 04:46:31,710 - main - INFO - IoU: 0.8474
2025-05-03 04:46:31,710 - main - INFO - mAP: 0.9647
2025-05-03 04:46:31,710 - main - INFO - AUC: 0.9823
2025-05-03 04:46:31,710 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:46:31,710 - main - INFO - 
==================================================
2025-05-03 04:46:31,710 - main - INFO - Running configuration 489/756:
2025-05-03 04:46:31,710 - main - INFO - Model: DenseNet121
2025-05-03 04:46:31,710 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:46:31,710 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 04:46:31,710 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:46:31,710 - main - INFO - ==================================================
2025-05-03 04:46:31,710 - training.model_DenseNet121_opt_Adam_lr_0.001_id_489 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_489
2025-05-03 04:46:31,710 - training.model_DenseNet121_opt_Adam_lr_0.001_id_489 - INFO - Config: {
  "id": 489,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:46:32,087 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:46:32,087 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 489,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:46:32,087 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:46:32,259 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:46:32,261 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:46:32,265 - training.model_DenseNet121_opt_Adam_lr_0.001_id_489 - INFO - Starting model evaluation
2025-05-03 04:46:43,896 - training.model_DenseNet121_opt_Adam_lr_0.001_id_489 - INFO - Evaluation metrics:
  Accuracy:  0.9485
  Precision: 0.9091
  Recall:    0.9091
  F1 Score:  0.9091
  IoU:       0.8333
  mAP:       0.9586
  AUC:       0.9834
2025-05-03 04:46:43,898 - training.model_DenseNet121_opt_Adam_lr_0.001_id_489 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_489/final_results.json
2025-05-03 04:46:43,900 - training.model_DenseNet121_opt_Adam_lr_0.001_id_489 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_489/final_results.json
2025-05-03 04:46:43,900 - main - INFO - 
Summary for configuration 489:
2025-05-03 04:46:43,900 - main - INFO - Accuracy: 0.9485
2025-05-03 04:46:43,900 - main - INFO - Precision: 0.9091
2025-05-03 04:46:43,900 - main - INFO - Recall: 0.9091
2025-05-03 04:46:43,900 - main - INFO - F1 Score: 0.9091
2025-05-03 04:46:43,900 - main - INFO - IoU: 0.8333
2025-05-03 04:46:43,900 - main - INFO - mAP: 0.9586
2025-05-03 04:46:43,900 - main - INFO - AUC: 0.9834
2025-05-03 04:46:43,900 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:46:43,900 - main - INFO - 
==================================================
2025-05-03 04:46:43,900 - main - INFO - Running configuration 490/756:
2025-05-03 04:46:43,900 - main - INFO - Model: DenseNet121
2025-05-03 04:46:43,900 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:46:43,900 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:46:43,900 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:46:43,900 - main - INFO - ==================================================
2025-05-03 04:46:43,900 - training.model_DenseNet121_opt_Adam_lr_0.001_id_490 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_490
2025-05-03 04:46:43,900 - training.model_DenseNet121_opt_Adam_lr_0.001_id_490 - INFO - Config: {
  "id": 490,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:46:44,031 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:46:44,031 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 490,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:46:44,031 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:46:44,203 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:46:44,204 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:46:44,209 - training.model_DenseNet121_opt_Adam_lr_0.001_id_490 - INFO - Starting model evaluation
2025-05-03 04:46:55,901 - training.model_DenseNet121_opt_Adam_lr_0.001_id_490 - INFO - Evaluation metrics:
  Accuracy:  0.9514
  Precision: 0.9129
  Recall:    0.9161
  F1 Score:  0.9145
  IoU:       0.8424
  mAP:       0.9634
  AUC:       0.9861
2025-05-03 04:46:55,903 - training.model_DenseNet121_opt_Adam_lr_0.001_id_490 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_490/final_results.json
2025-05-03 04:46:55,904 - training.model_DenseNet121_opt_Adam_lr_0.001_id_490 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_490/final_results.json
2025-05-03 04:46:55,905 - main - INFO - 
Summary for configuration 490:
2025-05-03 04:46:55,905 - main - INFO - Accuracy: 0.9514
2025-05-03 04:46:55,905 - main - INFO - Precision: 0.9129
2025-05-03 04:46:55,905 - main - INFO - Recall: 0.9161
2025-05-03 04:46:55,905 - main - INFO - F1 Score: 0.9145
2025-05-03 04:46:55,905 - main - INFO - IoU: 0.8424
2025-05-03 04:46:55,905 - main - INFO - mAP: 0.9634
2025-05-03 04:46:55,905 - main - INFO - AUC: 0.9861
2025-05-03 04:46:55,905 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:46:55,905 - main - INFO - 
==================================================
2025-05-03 04:46:55,905 - main - INFO - Running configuration 491/756:
2025-05-03 04:46:55,905 - main - INFO - Model: DenseNet121
2025-05-03 04:46:55,905 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:46:55,905 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:46:55,905 - main - INFO - Loss Function: FocalLoss
2025-05-03 04:46:55,905 - main - INFO - ==================================================
2025-05-03 04:46:55,905 - training.model_DenseNet121_opt_Adam_lr_0.001_id_491 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_491
2025-05-03 04:46:55,905 - training.model_DenseNet121_opt_Adam_lr_0.001_id_491 - INFO - Config: {
  "id": 491,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:46:56,071 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:46:56,071 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 491,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 04:46:56,071 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:46:56,243 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:46:56,244 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:46:56,249 - training.model_DenseNet121_opt_Adam_lr_0.001_id_491 - INFO - Starting model evaluation
2025-05-03 04:47:07,901 - training.model_DenseNet121_opt_Adam_lr_0.001_id_491 - INFO - Evaluation metrics:
  Accuracy:  0.9514
  Precision: 0.9072
  Recall:    0.9231
  F1 Score:  0.9151
  IoU:       0.8435
  mAP:       0.9700
  AUC:       0.9877
2025-05-03 04:47:07,903 - training.model_DenseNet121_opt_Adam_lr_0.001_id_491 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_491/final_results.json
2025-05-03 04:47:07,904 - training.model_DenseNet121_opt_Adam_lr_0.001_id_491 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_491/final_results.json
2025-05-03 04:47:07,904 - main - INFO - 
Summary for configuration 491:
2025-05-03 04:47:07,904 - main - INFO - Accuracy: 0.9514
2025-05-03 04:47:07,904 - main - INFO - Precision: 0.9072
2025-05-03 04:47:07,904 - main - INFO - Recall: 0.9231
2025-05-03 04:47:07,904 - main - INFO - F1 Score: 0.9151
2025-05-03 04:47:07,904 - main - INFO - IoU: 0.8435
2025-05-03 04:47:07,904 - main - INFO - mAP: 0.9700
2025-05-03 04:47:07,904 - main - INFO - AUC: 0.9877
2025-05-03 04:47:07,904 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:47:07,904 - main - INFO - 
==================================================
2025-05-03 04:47:07,904 - main - INFO - Running configuration 492/756:
2025-05-03 04:47:07,904 - main - INFO - Model: DenseNet121
2025-05-03 04:47:07,904 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 04:47:07,905 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 04:47:07,905 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 04:47:07,905 - main - INFO - ==================================================
2025-05-03 04:47:07,905 - training.model_DenseNet121_opt_Adam_lr_0.001_id_492 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001_id_492
2025-05-03 04:47:07,905 - training.model_DenseNet121_opt_Adam_lr_0.001_id_492 - INFO - Config: {
  "id": 492,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:47:08,367 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.001
2025-05-03 04:47:08,367 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 492,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 04:47:08,368 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 04:47:08,544 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9648
2025-05-03 04:47:08,545 - training.model_DenseNet121_opt_Adam_lr_0.001 - INFO - Training completed after 1122.46 seconds
2025-05-03 04:47:08,550 - training.model_DenseNet121_opt_Adam_lr_0.001_id_492 - INFO - Starting model evaluation
2025-05-03 04:47:20,065 - training.model_DenseNet121_opt_Adam_lr_0.001_id_492 - INFO - Evaluation metrics:
  Accuracy:  0.9455
  Precision: 0.9024
  Recall:    0.9056
  F1 Score:  0.9040
  IoU:       0.8248
  mAP:       0.9580
  AUC:       0.9803
2025-05-03 04:47:20,068 - training.model_DenseNet121_opt_Adam_lr_0.001_id_492 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_492/final_results.json
2025-05-03 04:47:20,070 - training.model_DenseNet121_opt_Adam_lr_0.001_id_492 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.001_id_492/final_results.json
2025-05-03 04:47:20,070 - main - INFO - 
Summary for configuration 492:
2025-05-03 04:47:20,070 - main - INFO - Accuracy: 0.9455
2025-05-03 04:47:20,070 - main - INFO - Precision: 0.9024
2025-05-03 04:47:20,070 - main - INFO - Recall: 0.9056
2025-05-03 04:47:20,070 - main - INFO - F1 Score: 0.9040
2025-05-03 04:47:20,070 - main - INFO - IoU: 0.8248
2025-05-03 04:47:20,070 - main - INFO - mAP: 0.9580
2025-05-03 04:47:20,070 - main - INFO - AUC: 0.9803
2025-05-03 04:47:20,070 - main - INFO - Training time: 1122.46 seconds
2025-05-03 04:47:20,070 - main - INFO - 
==================================================
2025-05-03 04:47:20,070 - main - INFO - Running configuration 493/756:
2025-05-03 04:47:20,070 - main - INFO - Model: DenseNet121
2025-05-03 04:47:20,070 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 04:47:20,070 - main - INFO - Scheduler: StepLR
2025-05-03 04:47:20,070 - main - INFO - Loss Function: CrossEntropy
2025-05-03 04:47:20,070 - main - INFO - ==================================================
2025-05-03 04:47:20,070 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_493 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_493
2025-05-03 04:47:20,070 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_493 - INFO - Config: {
  "id": 493,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:47:20,200 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 04:47:20,200 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 493,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 04:47:20,200 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-03 04:47:20,201 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-03 04:47:43,427 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 04:48:16,458 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9451
2025-05-03 04:48:16,490 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 1 completed in 56.29s - Train Loss: 0.4210, Train Acc: 0.8968, Val Loss: 0.3677, Val Acc: 0.9451
2025-05-03 04:48:16,586 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 04:48:16,587 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-03 04:48:39,796 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 04:49:11,537 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9451 to 0.9648
2025-05-03 04:49:11,578 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 2 completed in 54.99s - Train Loss: 0.3570, Train Acc: 0.9577, Val Loss: 0.3471, Val Acc: 0.9648
2025-05-03 04:49:11,676 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 04:49:11,677 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-03 04:49:35,584 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 04:50:06,985 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9648 to 0.9666
2025-05-03 04:50:07,027 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 3 completed in 55.35s - Train Loss: 0.3460, Train Acc: 0.9689, Val Loss: 0.3450, Val Acc: 0.9666
2025-05-03 04:50:07,121 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 04:50:07,121 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-03 04:50:30,213 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 04:51:01,421 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9666 to 0.9726
2025-05-03 04:51:01,462 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 4 completed in 54.34s - Train Loss: 0.3392, Train Acc: 0.9773, Val Loss: 0.3372, Val Acc: 0.9726
2025-05-03 04:51:01,560 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 04:51:01,561 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 04:51:25,439 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 04:51:57,449 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation loss improved from inf to 0.3393
2025-05-03 04:51:57,517 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 5 completed in 55.96s - Train Loss: 0.3373, Train Acc: 0.9773, Val Loss: 0.3393, Val Acc: 0.9726
2025-05-03 04:51:57,618 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 04:51:57,618 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 04:52:21,102 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 04:52:53,437 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 6 completed in 55.82s - Train Loss: 0.3382, Train Acc: 0.9753, Val Loss: 0.3529, Val Acc: 0.9605
2025-05-03 04:52:53,543 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 04:52:53,543 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 04:53:17,005 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 04:53:48,321 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9726 to 0.9811
2025-05-03 04:53:48,370 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 7 completed in 54.83s - Train Loss: 0.3318, Train Acc: 0.9820, Val Loss: 0.3336, Val Acc: 0.9811
2025-05-03 04:53:48,474 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 04:53:48,474 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 04:54:11,974 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 04:54:43,979 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 8 completed in 55.50s - Train Loss: 0.3268, Train Acc: 0.9871, Val Loss: 0.3418, Val Acc: 0.9717
2025-05-03 04:54:44,082 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 04:54:44,083 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 04:55:07,270 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 04:55:39,108 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 9 completed in 55.03s - Train Loss: 0.3304, Train Acc: 0.9843, Val Loss: 0.3394, Val Acc: 0.9743
2025-05-03 04:55:39,207 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 04:55:39,207 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 04:56:03,055 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 04:56:35,502 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 10 completed in 56.30s - Train Loss: 0.3245, Train Acc: 0.9897, Val Loss: 0.3422, Val Acc: 0.9700
2025-05-03 04:56:35,612 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 04:56:35,612 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 04:56:59,977 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 04:57:32,295 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3393 to 0.3322
2025-05-03 04:57:32,336 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 11 completed in 56.72s - Train Loss: 0.3207, Train Acc: 0.9931, Val Loss: 0.3322, Val Acc: 0.9811
2025-05-03 04:57:32,438 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 04:57:32,438 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 04:57:55,414 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 04:58:27,251 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9811 to 0.9828
2025-05-03 04:58:27,292 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 12 completed in 54.85s - Train Loss: 0.3189, Train Acc: 0.9949, Val Loss: 0.3292, Val Acc: 0.9828
2025-05-03 04:58:27,390 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 04:58:27,391 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 04:58:50,750 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 04:59:22,050 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9828 to 0.9863
2025-05-03 04:59:22,109 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 13 completed in 54.72s - Train Loss: 0.3170, Train Acc: 0.9964, Val Loss: 0.3265, Val Acc: 0.9863
2025-05-03 04:59:22,206 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 04:59:22,207 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 04:59:45,246 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 05:00:17,953 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3322 to 0.3275
2025-05-03 05:00:17,995 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 14 completed in 55.79s - Train Loss: 0.3170, Train Acc: 0.9966, Val Loss: 0.3275, Val Acc: 0.9854
2025-05-03 05:00:18,092 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 05:00:18,092 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 05:00:41,765 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 05:01:13,848 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9863 to 0.9897
2025-05-03 05:01:13,894 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 15 completed in 55.80s - Train Loss: 0.3171, Train Acc: 0.9964, Val Loss: 0.3248, Val Acc: 0.9897
2025-05-03 05:01:13,993 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 05:01:13,994 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 05:01:36,715 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 05:02:09,867 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3275 to 0.3249
2025-05-03 05:02:09,907 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 16 completed in 55.91s - Train Loss: 0.3155, Train Acc: 0.9981, Val Loss: 0.3249, Val Acc: 0.9854
2025-05-03 05:02:10,004 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 05:02:10,004 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 05:02:33,960 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 05:03:06,703 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3249 to 0.3246
2025-05-03 05:03:06,741 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 17 completed in 56.74s - Train Loss: 0.3156, Train Acc: 0.9981, Val Loss: 0.3246, Val Acc: 0.9889
2025-05-03 05:03:06,834 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 05:03:06,835 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 05:03:30,269 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 05:04:02,723 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 18 completed in 55.89s - Train Loss: 0.3181, Train Acc: 0.9972, Val Loss: 0.3267, Val Acc: 0.9863
2025-05-03 05:04:02,832 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 05:04:02,832 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 05:04:25,880 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 05:04:58,987 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 19 completed in 56.15s - Train Loss: 0.3153, Train Acc: 0.9983, Val Loss: 0.3246, Val Acc: 0.9889
2025-05-03 05:04:59,089 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 05:04:59,089 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 05:05:23,078 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 05:05:55,919 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Epoch 20 completed in 56.83s - Train Loss: 0.3153, Train Acc: 0.9981, Val Loss: 0.3262, Val Acc: 0.9863
2025-05-03 05:05:56,019 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 05:05:56,020 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:05:56,025 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_493 - INFO - Starting model evaluation
2025-05-03 05:06:08,167 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_493 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9249
  Recall:    0.9476
  F1 Score:  0.9361
  IoU:       0.8799
  mAP:       0.9769
  AUC:       0.9877
2025-05-03 05:06:08,169 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_493 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_493/final_results.json
2025-05-03 05:06:08,170 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_493 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_493/final_results.json
2025-05-03 05:06:08,170 - main - INFO - 
Summary for configuration 493:
2025-05-03 05:06:08,170 - main - INFO - Accuracy: 0.9633
2025-05-03 05:06:08,170 - main - INFO - Precision: 0.9249
2025-05-03 05:06:08,170 - main - INFO - Recall: 0.9476
2025-05-03 05:06:08,170 - main - INFO - F1 Score: 0.9361
2025-05-03 05:06:08,170 - main - INFO - IoU: 0.8799
2025-05-03 05:06:08,170 - main - INFO - mAP: 0.9769
2025-05-03 05:06:08,170 - main - INFO - AUC: 0.9877
2025-05-03 05:06:08,170 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:06:08,170 - main - INFO - 
==================================================
2025-05-03 05:06:08,170 - main - INFO - Running configuration 494/756:
2025-05-03 05:06:08,170 - main - INFO - Model: DenseNet121
2025-05-03 05:06:08,170 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:06:08,170 - main - INFO - Scheduler: StepLR
2025-05-03 05:06:08,170 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:06:08,170 - main - INFO - ==================================================
2025-05-03 05:06:08,171 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_494 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_494
2025-05-03 05:06:08,171 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_494 - INFO - Config: {
  "id": 494,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:06:08,302 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:06:08,302 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 494,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:06:08,302 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:06:08,474 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:06:08,476 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:06:08,480 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_494 - INFO - Starting model evaluation
2025-05-03 05:06:20,435 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_494 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9215
  Recall:    0.9441
  F1 Score:  0.9326
  IoU:       0.8738
  mAP:       0.9783
  AUC:       0.9883
2025-05-03 05:06:20,436 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_494 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_494/final_results.json
2025-05-03 05:06:20,438 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_494 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_494/final_results.json
2025-05-03 05:06:20,438 - main - INFO - 
Summary for configuration 494:
2025-05-03 05:06:20,438 - main - INFO - Accuracy: 0.9613
2025-05-03 05:06:20,438 - main - INFO - Precision: 0.9215
2025-05-03 05:06:20,438 - main - INFO - Recall: 0.9441
2025-05-03 05:06:20,438 - main - INFO - F1 Score: 0.9326
2025-05-03 05:06:20,438 - main - INFO - IoU: 0.8738
2025-05-03 05:06:20,438 - main - INFO - mAP: 0.9783
2025-05-03 05:06:20,438 - main - INFO - AUC: 0.9883
2025-05-03 05:06:20,438 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:06:20,438 - main - INFO - 
==================================================
2025-05-03 05:06:20,438 - main - INFO - Running configuration 495/756:
2025-05-03 05:06:20,438 - main - INFO - Model: DenseNet121
2025-05-03 05:06:20,438 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:06:20,438 - main - INFO - Scheduler: StepLR
2025-05-03 05:06:20,438 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:06:20,438 - main - INFO - ==================================================
2025-05-03 05:06:20,438 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_495 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_495
2025-05-03 05:06:20,438 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_495 - INFO - Config: {
  "id": 495,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:06:20,569 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:06:20,569 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 495,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:06:20,569 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:06:20,742 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:06:20,743 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:06:20,748 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_495 - INFO - Starting model evaluation
2025-05-03 05:06:32,692 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_495 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9128
  Recall:    0.9510
  F1 Score:  0.9315
  IoU:       0.8718
  mAP:       0.9807
  AUC:       0.9897
2025-05-03 05:06:32,694 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_495 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_495/final_results.json
2025-05-03 05:06:32,696 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_495 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_495/final_results.json
2025-05-03 05:06:32,696 - main - INFO - 
Summary for configuration 495:
2025-05-03 05:06:32,696 - main - INFO - Accuracy: 0.9604
2025-05-03 05:06:32,696 - main - INFO - Precision: 0.9128
2025-05-03 05:06:32,696 - main - INFO - Recall: 0.9510
2025-05-03 05:06:32,696 - main - INFO - F1 Score: 0.9315
2025-05-03 05:06:32,696 - main - INFO - IoU: 0.8718
2025-05-03 05:06:32,696 - main - INFO - mAP: 0.9807
2025-05-03 05:06:32,696 - main - INFO - AUC: 0.9897
2025-05-03 05:06:32,696 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:06:32,696 - main - INFO - 
==================================================
2025-05-03 05:06:32,696 - main - INFO - Running configuration 496/756:
2025-05-03 05:06:32,696 - main - INFO - Model: DenseNet121
2025-05-03 05:06:32,696 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:06:32,696 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:06:32,696 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:06:32,696 - main - INFO - ==================================================
2025-05-03 05:06:32,696 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_496 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_496
2025-05-03 05:06:32,696 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_496 - INFO - Config: {
  "id": 496,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:06:33,084 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:06:33,084 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 496,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:06:33,084 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:06:33,257 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:06:33,258 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:06:33,263 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_496 - INFO - Starting model evaluation
2025-05-03 05:06:44,990 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_496 - INFO - Evaluation metrics:
  Accuracy:  0.9584
  Precision: 0.9150
  Recall:    0.9406
  F1 Score:  0.9276
  IoU:       0.8650
  mAP:       0.9752
  AUC:       0.9868
2025-05-03 05:06:44,992 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_496 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_496/final_results.json
2025-05-03 05:06:44,994 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_496 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_496/final_results.json
2025-05-03 05:06:44,994 - main - INFO - 
Summary for configuration 496:
2025-05-03 05:06:44,994 - main - INFO - Accuracy: 0.9584
2025-05-03 05:06:44,994 - main - INFO - Precision: 0.9150
2025-05-03 05:06:44,994 - main - INFO - Recall: 0.9406
2025-05-03 05:06:44,994 - main - INFO - F1 Score: 0.9276
2025-05-03 05:06:44,994 - main - INFO - IoU: 0.8650
2025-05-03 05:06:44,994 - main - INFO - mAP: 0.9752
2025-05-03 05:06:44,994 - main - INFO - AUC: 0.9868
2025-05-03 05:06:44,994 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:06:44,994 - main - INFO - 
==================================================
2025-05-03 05:06:44,994 - main - INFO - Running configuration 497/756:
2025-05-03 05:06:44,994 - main - INFO - Model: DenseNet121
2025-05-03 05:06:44,994 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:06:44,994 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:06:44,994 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:06:44,994 - main - INFO - ==================================================
2025-05-03 05:06:44,994 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_497 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_497
2025-05-03 05:06:44,994 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_497 - INFO - Config: {
  "id": 497,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:06:45,288 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:06:45,288 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 497,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:06:45,288 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:06:45,461 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:06:45,462 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:06:45,467 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_497 - INFO - Starting model evaluation
2025-05-03 05:06:57,554 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_497 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9281
  Recall:    0.9476
  F1 Score:  0.9377
  IoU:       0.8827
  mAP:       0.9777
  AUC:       0.9880
2025-05-03 05:06:57,556 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_497 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_497/final_results.json
2025-05-03 05:06:57,557 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_497 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_497/final_results.json
2025-05-03 05:06:57,557 - main - INFO - 
Summary for configuration 497:
2025-05-03 05:06:57,557 - main - INFO - Accuracy: 0.9643
2025-05-03 05:06:57,557 - main - INFO - Precision: 0.9281
2025-05-03 05:06:57,557 - main - INFO - Recall: 0.9476
2025-05-03 05:06:57,557 - main - INFO - F1 Score: 0.9377
2025-05-03 05:06:57,557 - main - INFO - IoU: 0.8827
2025-05-03 05:06:57,558 - main - INFO - mAP: 0.9777
2025-05-03 05:06:57,558 - main - INFO - AUC: 0.9880
2025-05-03 05:06:57,558 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:06:57,558 - main - INFO - 
==================================================
2025-05-03 05:06:57,558 - main - INFO - Running configuration 498/756:
2025-05-03 05:06:57,558 - main - INFO - Model: DenseNet121
2025-05-03 05:06:57,558 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:06:57,558 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:06:57,558 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:06:57,558 - main - INFO - ==================================================
2025-05-03 05:06:57,558 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_498 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_498
2025-05-03 05:06:57,558 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_498 - INFO - Config: {
  "id": 498,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:06:57,689 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:06:57,689 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 498,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:06:57,689 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:06:57,950 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:06:57,952 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:06:57,956 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_498 - INFO - Starting model evaluation
2025-05-03 05:07:09,658 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_498 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9125
  Recall:    0.9476
  F1 Score:  0.9297
  IoU:       0.8686
  mAP:       0.9783
  AUC:       0.9882
2025-05-03 05:07:09,660 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_498 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_498/final_results.json
2025-05-03 05:07:09,661 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_498 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_498/final_results.json
2025-05-03 05:07:09,661 - main - INFO - 
Summary for configuration 498:
2025-05-03 05:07:09,661 - main - INFO - Accuracy: 0.9594
2025-05-03 05:07:09,661 - main - INFO - Precision: 0.9125
2025-05-03 05:07:09,661 - main - INFO - Recall: 0.9476
2025-05-03 05:07:09,661 - main - INFO - F1 Score: 0.9297
2025-05-03 05:07:09,661 - main - INFO - IoU: 0.8686
2025-05-03 05:07:09,661 - main - INFO - mAP: 0.9783
2025-05-03 05:07:09,661 - main - INFO - AUC: 0.9882
2025-05-03 05:07:09,661 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:07:09,661 - main - INFO - 
==================================================
2025-05-03 05:07:09,661 - main - INFO - Running configuration 499/756:
2025-05-03 05:07:09,661 - main - INFO - Model: DenseNet121
2025-05-03 05:07:09,661 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:07:09,661 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:07:09,661 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:07:09,661 - main - INFO - ==================================================
2025-05-03 05:07:09,662 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_499 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_499
2025-05-03 05:07:09,662 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_499 - INFO - Config: {
  "id": 499,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:07:09,809 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:07:09,809 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 499,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:07:09,809 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:07:09,981 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:07:09,983 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:07:09,987 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_499 - INFO - Starting model evaluation
2025-05-03 05:07:21,485 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_499 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9218
  Recall:    0.9476
  F1 Score:  0.9345
  IoU:       0.8770
  mAP:       0.9778
  AUC:       0.9883
2025-05-03 05:07:21,487 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_499 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_499/final_results.json
2025-05-03 05:07:21,489 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_499 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_499/final_results.json
2025-05-03 05:07:21,489 - main - INFO - 
Summary for configuration 499:
2025-05-03 05:07:21,489 - main - INFO - Accuracy: 0.9623
2025-05-03 05:07:21,489 - main - INFO - Precision: 0.9218
2025-05-03 05:07:21,489 - main - INFO - Recall: 0.9476
2025-05-03 05:07:21,489 - main - INFO - F1 Score: 0.9345
2025-05-03 05:07:21,489 - main - INFO - IoU: 0.8770
2025-05-03 05:07:21,489 - main - INFO - mAP: 0.9778
2025-05-03 05:07:21,489 - main - INFO - AUC: 0.9883
2025-05-03 05:07:21,489 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:07:21,489 - main - INFO - 
==================================================
2025-05-03 05:07:21,489 - main - INFO - Running configuration 500/756:
2025-05-03 05:07:21,489 - main - INFO - Model: DenseNet121
2025-05-03 05:07:21,489 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:07:21,489 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:07:21,489 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:07:21,489 - main - INFO - ==================================================
2025-05-03 05:07:21,489 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_500 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_500
2025-05-03 05:07:21,489 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_500 - INFO - Config: {
  "id": 500,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:07:21,620 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:07:21,620 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 500,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:07:21,620 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:07:21,793 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:07:21,794 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:07:21,799 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_500 - INFO - Starting model evaluation
2025-05-03 05:07:33,365 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_500 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9218
  Recall:    0.9476
  F1 Score:  0.9345
  IoU:       0.8770
  mAP:       0.9766
  AUC:       0.9883
2025-05-03 05:07:33,367 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_500 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_500/final_results.json
2025-05-03 05:07:33,368 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_500 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_500/final_results.json
2025-05-03 05:07:33,368 - main - INFO - 
Summary for configuration 500:
2025-05-03 05:07:33,368 - main - INFO - Accuracy: 0.9623
2025-05-03 05:07:33,368 - main - INFO - Precision: 0.9218
2025-05-03 05:07:33,368 - main - INFO - Recall: 0.9476
2025-05-03 05:07:33,368 - main - INFO - F1 Score: 0.9345
2025-05-03 05:07:33,368 - main - INFO - IoU: 0.8770
2025-05-03 05:07:33,368 - main - INFO - mAP: 0.9766
2025-05-03 05:07:33,368 - main - INFO - AUC: 0.9883
2025-05-03 05:07:33,368 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:07:33,368 - main - INFO - 
==================================================
2025-05-03 05:07:33,368 - main - INFO - Running configuration 501/756:
2025-05-03 05:07:33,368 - main - INFO - Model: DenseNet121
2025-05-03 05:07:33,368 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:07:33,368 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:07:33,368 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:07:33,368 - main - INFO - ==================================================
2025-05-03 05:07:33,369 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_501 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_501
2025-05-03 05:07:33,369 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_501 - INFO - Config: {
  "id": 501,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:07:33,496 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:07:33,496 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 501,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:07:33,497 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:07:33,942 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:07:33,944 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:07:33,948 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_501 - INFO - Starting model evaluation
2025-05-03 05:07:45,709 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_501 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9215
  Recall:    0.9441
  F1 Score:  0.9326
  IoU:       0.8738
  mAP:       0.9779
  AUC:       0.9876
2025-05-03 05:07:45,711 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_501 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_501/final_results.json
2025-05-03 05:07:45,712 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_501 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_501/final_results.json
2025-05-03 05:07:45,712 - main - INFO - 
Summary for configuration 501:
2025-05-03 05:07:45,712 - main - INFO - Accuracy: 0.9613
2025-05-03 05:07:45,712 - main - INFO - Precision: 0.9215
2025-05-03 05:07:45,712 - main - INFO - Recall: 0.9441
2025-05-03 05:07:45,712 - main - INFO - F1 Score: 0.9326
2025-05-03 05:07:45,712 - main - INFO - IoU: 0.8738
2025-05-03 05:07:45,712 - main - INFO - mAP: 0.9779
2025-05-03 05:07:45,712 - main - INFO - AUC: 0.9876
2025-05-03 05:07:45,712 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:07:45,712 - main - INFO - 
==================================================
2025-05-03 05:07:45,712 - main - INFO - Running configuration 502/756:
2025-05-03 05:07:45,712 - main - INFO - Model: DenseNet121
2025-05-03 05:07:45,712 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:07:45,712 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:07:45,712 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:07:45,712 - main - INFO - ==================================================
2025-05-03 05:07:45,713 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_502 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_502
2025-05-03 05:07:45,713 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_502 - INFO - Config: {
  "id": 502,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:07:45,847 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:07:45,847 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 502,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:07:45,847 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:07:46,019 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:07:46,021 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:07:46,025 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_502 - INFO - Starting model evaluation
2025-05-03 05:07:57,881 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_502 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9345
  Recall:    0.9476
  F1 Score:  0.9410
  IoU:       0.8885
  mAP:       0.9769
  AUC:       0.9866
2025-05-03 05:07:57,883 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_502 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_502/final_results.json
2025-05-03 05:07:57,885 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_502 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_502/final_results.json
2025-05-03 05:07:57,885 - main - INFO - 
Summary for configuration 502:
2025-05-03 05:07:57,885 - main - INFO - Accuracy: 0.9663
2025-05-03 05:07:57,885 - main - INFO - Precision: 0.9345
2025-05-03 05:07:57,885 - main - INFO - Recall: 0.9476
2025-05-03 05:07:57,885 - main - INFO - F1 Score: 0.9410
2025-05-03 05:07:57,885 - main - INFO - IoU: 0.8885
2025-05-03 05:07:57,885 - main - INFO - mAP: 0.9769
2025-05-03 05:07:57,885 - main - INFO - AUC: 0.9866
2025-05-03 05:07:57,885 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:07:57,885 - main - INFO - 
==================================================
2025-05-03 05:07:57,885 - main - INFO - Running configuration 503/756:
2025-05-03 05:07:57,885 - main - INFO - Model: DenseNet121
2025-05-03 05:07:57,885 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:07:57,885 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:07:57,885 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:07:57,885 - main - INFO - ==================================================
2025-05-03 05:07:57,885 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_503 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_503
2025-05-03 05:07:57,885 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_503 - INFO - Config: {
  "id": 503,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:07:58,013 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:07:58,013 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 503,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:07:58,013 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:07:58,185 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:07:58,187 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:07:58,192 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_503 - INFO - Starting model evaluation
2025-05-03 05:08:10,162 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_503 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9313
  Recall:    0.9476
  F1 Score:  0.9393
  IoU:       0.8856
  mAP:       0.9806
  AUC:       0.9895
2025-05-03 05:08:10,163 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_503 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_503/final_results.json
2025-05-03 05:08:10,165 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_503 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_503/final_results.json
2025-05-03 05:08:10,165 - main - INFO - 
Summary for configuration 503:
2025-05-03 05:08:10,165 - main - INFO - Accuracy: 0.9653
2025-05-03 05:08:10,165 - main - INFO - Precision: 0.9313
2025-05-03 05:08:10,165 - main - INFO - Recall: 0.9476
2025-05-03 05:08:10,165 - main - INFO - F1 Score: 0.9393
2025-05-03 05:08:10,165 - main - INFO - IoU: 0.8856
2025-05-03 05:08:10,165 - main - INFO - mAP: 0.9806
2025-05-03 05:08:10,165 - main - INFO - AUC: 0.9895
2025-05-03 05:08:10,165 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:08:10,165 - main - INFO - 
==================================================
2025-05-03 05:08:10,165 - main - INFO - Running configuration 504/756:
2025-05-03 05:08:10,165 - main - INFO - Model: DenseNet121
2025-05-03 05:08:10,165 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 05:08:10,165 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:08:10,165 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:08:10,165 - main - INFO - ==================================================
2025-05-03 05:08:10,166 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_504 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001_id_504
2025-05-03 05:08:10,166 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_504 - INFO - Config: {
  "id": 504,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:08:10,298 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_Adam_lr_0.0001
2025-05-03 05:08:10,302 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 504,
  "model_name": "DenseNet121",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:08:10,302 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 05:08:10,473 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 05:08:10,475 - training.model_DenseNet121_opt_Adam_lr_0.0001 - INFO - Training completed after 1115.72 seconds
2025-05-03 05:08:10,479 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_504 - INFO - Starting model evaluation
2025-05-03 05:08:22,032 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_504 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9244
  Recall:    0.9406
  F1 Score:  0.9324
  IoU:       0.8734
  mAP:       0.9795
  AUC:       0.9889
2025-05-03 05:08:22,033 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_504 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_504/final_results.json
2025-05-03 05:08:22,035 - training.model_DenseNet121_opt_Adam_lr_0.0001_id_504 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_Adam_lr_0.0001_id_504/final_results.json
2025-05-03 05:08:22,035 - main - INFO - 
Summary for configuration 504:
2025-05-03 05:08:22,035 - main - INFO - Accuracy: 0.9613
2025-05-03 05:08:22,035 - main - INFO - Precision: 0.9244
2025-05-03 05:08:22,035 - main - INFO - Recall: 0.9406
2025-05-03 05:08:22,035 - main - INFO - F1 Score: 0.9324
2025-05-03 05:08:22,035 - main - INFO - IoU: 0.8734
2025-05-03 05:08:22,035 - main - INFO - mAP: 0.9795
2025-05-03 05:08:22,035 - main - INFO - AUC: 0.9889
2025-05-03 05:08:22,035 - main - INFO - Training time: 1115.72 seconds
2025-05-03 05:08:22,035 - main - INFO - 
==================================================
2025-05-03 05:08:22,035 - main - INFO - Running configuration 505/756:
2025-05-03 05:08:22,035 - main - INFO - Model: DenseNet121
2025-05-03 05:08:22,035 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:08:22,035 - main - INFO - Scheduler: StepLR
2025-05-03 05:08:22,035 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:08:22,035 - main - INFO - ==================================================
2025-05-03 05:08:22,035 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_505 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_505
2025-05-03 05:08:22,036 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_505 - INFO - Config: {
  "id": 505,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:08:22,382 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:08:22,382 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 505,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:08:22,382 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-03 05:08:22,383 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 1/20
2025-05-03 05:08:46,007 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 05:09:18,063 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.5043
2025-05-03 05:09:18,095 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 1 completed in 55.71s - Train Loss: 0.8148, Train Acc: 0.4981, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:09:18,193 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 05:09:18,193 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 2/20
2025-05-03 05:09:42,244 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 05:10:14,386 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Validation loss improved from inf to 0.8096
2025-05-03 05:10:14,426 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 2 completed in 56.23s - Train Loss: 0.8143, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:10:14,527 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 05:10:14,527 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 3/20
2025-05-03 05:10:38,736 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 05:11:11,113 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8096 to 0.8096
2025-05-03 05:11:11,169 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 3 completed in 56.64s - Train Loss: 0.8143, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:11:11,273 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 05:11:11,274 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 4/20
2025-05-03 05:11:34,769 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 05:12:07,272 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8096 to 0.8096
2025-05-03 05:12:07,312 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 4 completed in 56.04s - Train Loss: 0.8150, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:12:07,415 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 05:12:07,416 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 5/20
2025-05-03 05:12:31,145 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 05:13:03,106 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 5 completed in 55.69s - Train Loss: 0.8140, Train Acc: 0.4983, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:13:03,211 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 05:13:03,212 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 6/20
2025-05-03 05:13:27,719 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 05:13:59,448 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 6 completed in 56.24s - Train Loss: 0.8143, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:13:59,547 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 05:13:59,548 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 7/20
2025-05-03 05:14:23,650 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 05:14:56,468 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 7 completed in 56.92s - Train Loss: 0.8137, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:14:56,654 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 05:14:56,655 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 8/20
2025-05-03 05:15:20,362 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 05:15:51,796 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 8 completed in 55.14s - Train Loss: 0.8150, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:15:51,894 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 05:15:51,895 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 9/20
2025-05-03 05:16:16,462 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 05:16:48,725 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 9 completed in 56.83s - Train Loss: 0.8143, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:16:48,824 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 05:16:48,824 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 9 epochs
2025-05-03 05:16:48,824 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 506.34 seconds
2025-05-03 05:16:48,829 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_505 - INFO - Starting model evaluation
2025-05-03 05:17:00,807 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_505 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2778
  AUC:       0.5137
2025-05-03 05:17:00,809 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_505 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_505/final_results.json
2025-05-03 05:17:00,811 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_505 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_505/final_results.json
2025-05-03 05:17:00,811 - main - INFO - 
Summary for configuration 505:
2025-05-03 05:17:00,811 - main - INFO - Accuracy: 0.7166
2025-05-03 05:17:00,811 - main - INFO - Precision: 0.0000
2025-05-03 05:17:00,811 - main - INFO - Recall: 0.0000
2025-05-03 05:17:00,811 - main - INFO - F1 Score: 0.0000
2025-05-03 05:17:00,811 - main - INFO - IoU: 0.0000
2025-05-03 05:17:00,811 - main - INFO - mAP: 0.2778
2025-05-03 05:17:00,811 - main - INFO - AUC: 0.5137
2025-05-03 05:17:00,811 - main - INFO - Training time: 506.34 seconds
2025-05-03 05:17:00,811 - main - INFO - 
==================================================
2025-05-03 05:17:00,811 - main - INFO - Running configuration 506/756:
2025-05-03 05:17:00,811 - main - INFO - Model: DenseNet121
2025-05-03 05:17:00,811 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:17:00,811 - main - INFO - Scheduler: StepLR
2025-05-03 05:17:00,811 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:17:00,811 - main - INFO - ==================================================
2025-05-03 05:17:00,811 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_506 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_506
2025-05-03 05:17:00,811 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_506 - INFO - Config: {
  "id": 506,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:17:00,942 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:17:00,942 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 506,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:17:00,942 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:17:01,115 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 9, best validation accuracy: 0.5043
2025-05-03 05:17:01,116 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 10/20
2025-05-03 05:17:25,117 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 05:17:56,276 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8096 to 0.0899
2025-05-03 05:17:56,316 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 10 completed in 55.20s - Train Loss: 0.0909, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 05:17:56,415 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 05:17:56,416 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 11/20
2025-05-03 05:18:20,849 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 05:18:53,283 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 11 completed in 56.87s - Train Loss: 0.0909, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 05:18:53,389 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 05:18:53,390 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 12/20
2025-05-03 05:19:17,112 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 05:19:49,940 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 12 completed in 56.55s - Train Loss: 0.0906, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 05:19:50,046 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 05:19:50,046 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 05:20:14,338 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 05:20:47,689 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 13 completed in 57.64s - Train Loss: 0.0909, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 05:20:47,789 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 05:20:47,789 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 05:21:12,234 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 05:21:45,386 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 14 completed in 57.60s - Train Loss: 0.0910, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 05:21:45,498 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 05:21:45,498 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 05:22:09,032 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 05:22:42,216 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 15 completed in 56.72s - Train Loss: 0.0906, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 05:22:42,312 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 05:22:42,313 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 15 epochs
2025-05-03 05:22:42,313 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 847.44 seconds
2025-05-03 05:22:42,319 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_506 - INFO - Starting model evaluation
2025-05-03 05:22:54,314 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_506 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2780
  AUC:       0.5158
2025-05-03 05:22:54,316 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_506 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_506/final_results.json
2025-05-03 05:22:54,318 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_506 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_506/final_results.json
2025-05-03 05:22:54,318 - main - INFO - 
Summary for configuration 506:
2025-05-03 05:22:54,318 - main - INFO - Accuracy: 0.7166
2025-05-03 05:22:54,318 - main - INFO - Precision: 0.0000
2025-05-03 05:22:54,318 - main - INFO - Recall: 0.0000
2025-05-03 05:22:54,318 - main - INFO - F1 Score: 0.0000
2025-05-03 05:22:54,318 - main - INFO - IoU: 0.0000
2025-05-03 05:22:54,318 - main - INFO - mAP: 0.2780
2025-05-03 05:22:54,318 - main - INFO - AUC: 0.5158
2025-05-03 05:22:54,318 - main - INFO - Training time: 847.44 seconds
2025-05-03 05:22:54,318 - main - INFO - 
==================================================
2025-05-03 05:22:54,318 - main - INFO - Running configuration 507/756:
2025-05-03 05:22:54,318 - main - INFO - Model: DenseNet121
2025-05-03 05:22:54,318 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:22:54,318 - main - INFO - Scheduler: StepLR
2025-05-03 05:22:54,318 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:22:54,318 - main - INFO - ==================================================
2025-05-03 05:22:54,318 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_507 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_507
2025-05-03 05:22:54,318 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_507 - INFO - Config: {
  "id": 507,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:22:54,450 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:22:54,450 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 507,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:22:54,450 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:22:54,623 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 15, best validation accuracy: 0.5043
2025-05-03 05:22:54,624 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 05:23:19,693 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 05:23:52,273 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 16 completed in 57.65s - Train Loss: 0.8136, Train Acc: 0.4989, Val Loss: 0.8103, Val Acc: 0.5043
2025-05-03 05:23:52,373 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 05:23:52,373 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 16 epochs
2025-05-03 05:23:52,373 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 905.09 seconds
2025-05-03 05:23:52,378 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_507 - INFO - Starting model evaluation
2025-05-03 05:24:03,909 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_507 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2788
  AUC:       0.5131
2025-05-03 05:24:03,911 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_507 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_507/final_results.json
2025-05-03 05:24:03,914 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_507 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_507/final_results.json
2025-05-03 05:24:03,914 - main - INFO - 
Summary for configuration 507:
2025-05-03 05:24:03,914 - main - INFO - Accuracy: 0.7166
2025-05-03 05:24:03,914 - main - INFO - Precision: 0.0000
2025-05-03 05:24:03,914 - main - INFO - Recall: 0.0000
2025-05-03 05:24:03,914 - main - INFO - F1 Score: 0.0000
2025-05-03 05:24:03,914 - main - INFO - IoU: 0.0000
2025-05-03 05:24:03,914 - main - INFO - mAP: 0.2788
2025-05-03 05:24:03,914 - main - INFO - AUC: 0.5131
2025-05-03 05:24:03,914 - main - INFO - Training time: 905.09 seconds
2025-05-03 05:24:03,914 - main - INFO - 
==================================================
2025-05-03 05:24:03,914 - main - INFO - Running configuration 508/756:
2025-05-03 05:24:03,914 - main - INFO - Model: DenseNet121
2025-05-03 05:24:03,914 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:24:03,914 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:24:03,914 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:24:03,914 - main - INFO - ==================================================
2025-05-03 05:24:03,914 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_508 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_508
2025-05-03 05:24:03,914 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_508 - INFO - Config: {
  "id": 508,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:24:04,040 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:24:04,041 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 508,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:24:04,041 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:24:04,212 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 16, best validation accuracy: 0.5043
2025-05-03 05:24:04,214 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 05:24:28,162 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 05:25:00,123 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 17 completed in 55.91s - Train Loss: 0.8143, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:25:00,222 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 05:25:00,223 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 17 epochs
2025-05-03 05:25:00,223 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 961.00 seconds
2025-05-03 05:25:00,228 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_508 - INFO - Starting model evaluation
2025-05-03 05:25:11,809 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_508 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2652
  AUC:       0.4940
2025-05-03 05:25:11,811 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_508 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_508/final_results.json
2025-05-03 05:25:11,813 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_508 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_508/final_results.json
2025-05-03 05:25:11,813 - main - INFO - 
Summary for configuration 508:
2025-05-03 05:25:11,813 - main - INFO - Accuracy: 0.7166
2025-05-03 05:25:11,813 - main - INFO - Precision: 0.0000
2025-05-03 05:25:11,813 - main - INFO - Recall: 0.0000
2025-05-03 05:25:11,813 - main - INFO - F1 Score: 0.0000
2025-05-03 05:25:11,813 - main - INFO - IoU: 0.0000
2025-05-03 05:25:11,813 - main - INFO - mAP: 0.2652
2025-05-03 05:25:11,813 - main - INFO - AUC: 0.4940
2025-05-03 05:25:11,813 - main - INFO - Training time: 961.00 seconds
2025-05-03 05:25:11,813 - main - INFO - 
==================================================
2025-05-03 05:25:11,813 - main - INFO - Running configuration 509/756:
2025-05-03 05:25:11,813 - main - INFO - Model: DenseNet121
2025-05-03 05:25:11,813 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:25:11,813 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:25:11,813 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:25:11,813 - main - INFO - ==================================================
2025-05-03 05:25:11,813 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_509 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_509
2025-05-03 05:25:11,813 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_509 - INFO - Config: {
  "id": 509,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:25:12,092 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:25:12,093 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 509,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:25:12,093 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:25:12,267 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 17, best validation accuracy: 0.5043
2025-05-03 05:25:12,268 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 05:25:36,469 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 05:26:08,686 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 18 completed in 56.42s - Train Loss: 0.0905, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 05:26:08,792 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 05:26:08,792 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 18 epochs
2025-05-03 05:26:08,792 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 1017.42 seconds
2025-05-03 05:26:08,798 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_509 - INFO - Starting model evaluation
2025-05-03 05:26:20,557 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_509 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2804
  AUC:       0.5159
2025-05-03 05:26:20,559 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_509 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_509/final_results.json
2025-05-03 05:26:20,561 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_509 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_509/final_results.json
2025-05-03 05:26:20,561 - main - INFO - 
Summary for configuration 509:
2025-05-03 05:26:20,561 - main - INFO - Accuracy: 0.7166
2025-05-03 05:26:20,561 - main - INFO - Precision: 0.0000
2025-05-03 05:26:20,561 - main - INFO - Recall: 0.0000
2025-05-03 05:26:20,561 - main - INFO - F1 Score: 0.0000
2025-05-03 05:26:20,561 - main - INFO - IoU: 0.0000
2025-05-03 05:26:20,561 - main - INFO - mAP: 0.2804
2025-05-03 05:26:20,561 - main - INFO - AUC: 0.5159
2025-05-03 05:26:20,561 - main - INFO - Training time: 1017.42 seconds
2025-05-03 05:26:20,561 - main - INFO - 
==================================================
2025-05-03 05:26:20,561 - main - INFO - Running configuration 510/756:
2025-05-03 05:26:20,561 - main - INFO - Model: DenseNet121
2025-05-03 05:26:20,561 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:26:20,561 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:26:20,561 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:26:20,561 - main - INFO - ==================================================
2025-05-03 05:26:20,562 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_510 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_510
2025-05-03 05:26:20,562 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_510 - INFO - Config: {
  "id": 510,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:26:20,703 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:26:20,704 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 510,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:26:20,704 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:26:20,876 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 18, best validation accuracy: 0.5043
2025-05-03 05:26:20,878 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 05:26:44,857 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 05:27:17,085 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 19 completed in 56.21s - Train Loss: 0.8131, Train Acc: 0.4989, Val Loss: 0.8103, Val Acc: 0.5043
2025-05-03 05:27:17,193 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 05:27:17,194 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 19 epochs
2025-05-03 05:27:17,194 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 1073.62 seconds
2025-05-03 05:27:17,199 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_510 - INFO - Starting model evaluation
2025-05-03 05:27:28,694 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_510 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2759
  AUC:       0.5157
2025-05-03 05:27:28,696 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_510 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_510/final_results.json
2025-05-03 05:27:28,698 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_510 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_510/final_results.json
2025-05-03 05:27:28,698 - main - INFO - 
Summary for configuration 510:
2025-05-03 05:27:28,698 - main - INFO - Accuracy: 0.7166
2025-05-03 05:27:28,698 - main - INFO - Precision: 0.0000
2025-05-03 05:27:28,698 - main - INFO - Recall: 0.0000
2025-05-03 05:27:28,698 - main - INFO - F1 Score: 0.0000
2025-05-03 05:27:28,698 - main - INFO - IoU: 0.0000
2025-05-03 05:27:28,698 - main - INFO - mAP: 0.2759
2025-05-03 05:27:28,698 - main - INFO - AUC: 0.5157
2025-05-03 05:27:28,698 - main - INFO - Training time: 1073.62 seconds
2025-05-03 05:27:28,698 - main - INFO - 
==================================================
2025-05-03 05:27:28,698 - main - INFO - Running configuration 511/756:
2025-05-03 05:27:28,698 - main - INFO - Model: DenseNet121
2025-05-03 05:27:28,698 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:27:28,698 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:27:28,698 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:27:28,698 - main - INFO - ==================================================
2025-05-03 05:27:28,699 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_511 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_511
2025-05-03 05:27:28,699 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_511 - INFO - Config: {
  "id": 511,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:27:28,890 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:27:28,890 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 511,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:27:28,890 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:27:29,064 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 19, best validation accuracy: 0.5043
2025-05-03 05:27:29,065 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 05:27:52,478 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 05:28:23,992 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Epoch 20 completed in 54.93s - Train Loss: 0.8150, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 05:28:24,097 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 05:28:24,097 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 20 epochs
2025-05-03 05:28:24,097 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 1128.55 seconds
2025-05-03 05:28:24,103 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_511 - INFO - Starting model evaluation
2025-05-03 05:28:35,422 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_511 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2844
  AUC:       0.5154
2025-05-03 05:28:35,425 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_511 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_511/final_results.json
2025-05-03 05:28:35,428 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_511 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_511/final_results.json
2025-05-03 05:28:35,428 - main - INFO - 
Summary for configuration 511:
2025-05-03 05:28:35,428 - main - INFO - Accuracy: 0.7166
2025-05-03 05:28:35,428 - main - INFO - Precision: 0.0000
2025-05-03 05:28:35,428 - main - INFO - Recall: 0.0000
2025-05-03 05:28:35,428 - main - INFO - F1 Score: 0.0000
2025-05-03 05:28:35,428 - main - INFO - IoU: 0.0000
2025-05-03 05:28:35,428 - main - INFO - mAP: 0.2844
2025-05-03 05:28:35,428 - main - INFO - AUC: 0.5154
2025-05-03 05:28:35,428 - main - INFO - Training time: 1128.55 seconds
2025-05-03 05:28:35,428 - main - INFO - 
==================================================
2025-05-03 05:28:35,428 - main - INFO - Running configuration 512/756:
2025-05-03 05:28:35,428 - main - INFO - Model: DenseNet121
2025-05-03 05:28:35,428 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:28:35,428 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:28:35,428 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:28:35,428 - main - INFO - ==================================================
2025-05-03 05:28:35,429 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_512 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_512
2025-05-03 05:28:35,429 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_512 - INFO - Config: {
  "id": 512,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:28:35,568 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:28:35,568 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 512,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:28:35,568 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:28:35,741 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 05:28:35,742 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 1128.55 seconds
2025-05-03 05:28:35,747 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_512 - INFO - Starting model evaluation
2025-05-03 05:28:47,616 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_512 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2835
  AUC:       0.5098
2025-05-03 05:28:47,618 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_512 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_512/final_results.json
2025-05-03 05:28:47,619 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_512 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_512/final_results.json
2025-05-03 05:28:47,619 - main - INFO - 
Summary for configuration 512:
2025-05-03 05:28:47,619 - main - INFO - Accuracy: 0.7166
2025-05-03 05:28:47,619 - main - INFO - Precision: 0.0000
2025-05-03 05:28:47,619 - main - INFO - Recall: 0.0000
2025-05-03 05:28:47,619 - main - INFO - F1 Score: 0.0000
2025-05-03 05:28:47,620 - main - INFO - IoU: 0.0000
2025-05-03 05:28:47,620 - main - INFO - mAP: 0.2835
2025-05-03 05:28:47,620 - main - INFO - AUC: 0.5098
2025-05-03 05:28:47,620 - main - INFO - Training time: 1128.55 seconds
2025-05-03 05:28:47,620 - main - INFO - 
==================================================
2025-05-03 05:28:47,620 - main - INFO - Running configuration 513/756:
2025-05-03 05:28:47,620 - main - INFO - Model: DenseNet121
2025-05-03 05:28:47,620 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:28:47,620 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:28:47,620 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:28:47,620 - main - INFO - ==================================================
2025-05-03 05:28:47,620 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_513 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_513
2025-05-03 05:28:47,620 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_513 - INFO - Config: {
  "id": 513,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:28:47,751 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:28:47,751 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 513,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:28:47,751 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:28:47,924 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 05:28:47,926 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 1128.55 seconds
2025-05-03 05:28:47,931 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_513 - INFO - Starting model evaluation
2025-05-03 05:28:59,676 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_513 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2866
  AUC:       0.5176
2025-05-03 05:28:59,679 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_513 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_513/final_results.json
2025-05-03 05:28:59,682 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_513 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_513/final_results.json
2025-05-03 05:28:59,682 - main - INFO - 
Summary for configuration 513:
2025-05-03 05:28:59,682 - main - INFO - Accuracy: 0.7166
2025-05-03 05:28:59,682 - main - INFO - Precision: 0.0000
2025-05-03 05:28:59,682 - main - INFO - Recall: 0.0000
2025-05-03 05:28:59,682 - main - INFO - F1 Score: 0.0000
2025-05-03 05:28:59,682 - main - INFO - IoU: 0.0000
2025-05-03 05:28:59,682 - main - INFO - mAP: 0.2866
2025-05-03 05:28:59,682 - main - INFO - AUC: 0.5176
2025-05-03 05:28:59,682 - main - INFO - Training time: 1128.55 seconds
2025-05-03 05:28:59,682 - main - INFO - 
==================================================
2025-05-03 05:28:59,682 - main - INFO - Running configuration 514/756:
2025-05-03 05:28:59,682 - main - INFO - Model: DenseNet121
2025-05-03 05:28:59,683 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:28:59,683 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:28:59,683 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:28:59,683 - main - INFO - ==================================================
2025-05-03 05:28:59,683 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_514 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_514
2025-05-03 05:28:59,683 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_514 - INFO - Config: {
  "id": 514,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:28:59,818 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:28:59,821 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 514,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:28:59,821 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:29:00,348 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 05:29:00,350 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 1128.55 seconds
2025-05-03 05:29:00,354 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_514 - INFO - Starting model evaluation
2025-05-03 05:29:12,037 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_514 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2797
  AUC:       0.5090
2025-05-03 05:29:12,039 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_514 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_514/final_results.json
2025-05-03 05:29:12,040 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_514 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_514/final_results.json
2025-05-03 05:29:12,040 - main - INFO - 
Summary for configuration 514:
2025-05-03 05:29:12,041 - main - INFO - Accuracy: 0.7166
2025-05-03 05:29:12,041 - main - INFO - Precision: 0.0000
2025-05-03 05:29:12,041 - main - INFO - Recall: 0.0000
2025-05-03 05:29:12,041 - main - INFO - F1 Score: 0.0000
2025-05-03 05:29:12,041 - main - INFO - IoU: 0.0000
2025-05-03 05:29:12,041 - main - INFO - mAP: 0.2797
2025-05-03 05:29:12,041 - main - INFO - AUC: 0.5090
2025-05-03 05:29:12,041 - main - INFO - Training time: 1128.55 seconds
2025-05-03 05:29:12,041 - main - INFO - 
==================================================
2025-05-03 05:29:12,041 - main - INFO - Running configuration 515/756:
2025-05-03 05:29:12,041 - main - INFO - Model: DenseNet121
2025-05-03 05:29:12,041 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:29:12,041 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:29:12,041 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:29:12,041 - main - INFO - ==================================================
2025-05-03 05:29:12,041 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_515 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_515
2025-05-03 05:29:12,041 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_515 - INFO - Config: {
  "id": 515,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:29:12,410 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:29:12,410 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 515,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:29:12,410 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:29:12,583 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 05:29:12,585 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 1128.55 seconds
2025-05-03 05:29:12,589 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_515 - INFO - Starting model evaluation
2025-05-03 05:29:24,286 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_515 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2867
  AUC:       0.5181
2025-05-03 05:29:24,288 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_515 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_515/final_results.json
2025-05-03 05:29:24,290 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_515 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_515/final_results.json
2025-05-03 05:29:24,290 - main - INFO - 
Summary for configuration 515:
2025-05-03 05:29:24,290 - main - INFO - Accuracy: 0.7166
2025-05-03 05:29:24,290 - main - INFO - Precision: 0.0000
2025-05-03 05:29:24,290 - main - INFO - Recall: 0.0000
2025-05-03 05:29:24,290 - main - INFO - F1 Score: 0.0000
2025-05-03 05:29:24,290 - main - INFO - IoU: 0.0000
2025-05-03 05:29:24,290 - main - INFO - mAP: 0.2867
2025-05-03 05:29:24,290 - main - INFO - AUC: 0.5181
2025-05-03 05:29:24,290 - main - INFO - Training time: 1128.55 seconds
2025-05-03 05:29:24,290 - main - INFO - 
==================================================
2025-05-03 05:29:24,290 - main - INFO - Running configuration 516/756:
2025-05-03 05:29:24,290 - main - INFO - Model: DenseNet121
2025-05-03 05:29:24,290 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 05:29:24,290 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:29:24,290 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:29:24,290 - main - INFO - ==================================================
2025-05-03 05:29:24,290 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_516 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01_id_516
2025-05-03 05:29:24,290 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_516 - INFO - Config: {
  "id": 516,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:29:24,420 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.01
2025-05-03 05:29:24,420 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 516,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:29:24,420 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 05:29:24,593 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 05:29:24,594 - training.model_DenseNet121_opt_AdamW_lr_0.01 - INFO - Training completed after 1128.55 seconds
2025-05-03 05:29:24,599 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_516 - INFO - Starting model evaluation
2025-05-03 05:29:35,810 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_516 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2847
  AUC:       0.5136
2025-05-03 05:29:35,812 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_516 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_516/final_results.json
2025-05-03 05:29:35,813 - training.model_DenseNet121_opt_AdamW_lr_0.01_id_516 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.01_id_516/final_results.json
2025-05-03 05:29:35,813 - main - INFO - 
Summary for configuration 516:
2025-05-03 05:29:35,813 - main - INFO - Accuracy: 0.7166
2025-05-03 05:29:35,813 - main - INFO - Precision: 0.0000
2025-05-03 05:29:35,813 - main - INFO - Recall: 0.0000
2025-05-03 05:29:35,813 - main - INFO - F1 Score: 0.0000
2025-05-03 05:29:35,813 - main - INFO - IoU: 0.0000
2025-05-03 05:29:35,813 - main - INFO - mAP: 0.2847
2025-05-03 05:29:35,813 - main - INFO - AUC: 0.5136
2025-05-03 05:29:35,813 - main - INFO - Training time: 1128.55 seconds
2025-05-03 05:29:35,814 - main - INFO - 
==================================================
2025-05-03 05:29:35,814 - main - INFO - Running configuration 517/756:
2025-05-03 05:29:35,814 - main - INFO - Model: DenseNet121
2025-05-03 05:29:35,814 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:29:35,814 - main - INFO - Scheduler: StepLR
2025-05-03 05:29:35,814 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:29:35,814 - main - INFO - ==================================================
2025-05-03 05:29:35,814 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_517 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_517
2025-05-03 05:29:35,814 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_517 - INFO - Config: {
  "id": 517,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:29:35,945 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:29:35,945 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 517,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:29:35,945 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-03 05:29:35,946 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 1/20
2025-05-03 05:30:00,004 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 05:30:32,487 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.7058
2025-05-03 05:30:32,520 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 1 completed in 56.57s - Train Loss: 0.5483, Train Acc: 0.7492, Val Loss: 0.5939, Val Acc: 0.7058
2025-05-03 05:30:32,627 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 05:30:32,627 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 2/20
2025-05-03 05:30:56,389 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 05:31:29,018 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation loss improved from inf to 0.6975
2025-05-03 05:31:29,065 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 2 completed in 56.44s - Train Loss: 0.5012, Train Acc: 0.8050, Val Loss: 0.6975, Val Acc: 0.6012
2025-05-03 05:31:29,167 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 05:31:29,168 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 3/20
2025-05-03 05:31:53,120 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 05:32:24,906 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.7058 to 0.7350
2025-05-03 05:32:24,944 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 3 completed in 55.78s - Train Loss: 0.4830, Train Acc: 0.8237, Val Loss: 0.5611, Val Acc: 0.7350
2025-05-03 05:32:25,038 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 05:32:25,038 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 4/20
2025-05-03 05:32:48,828 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 05:33:19,735 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.7350 to 0.8396
2025-05-03 05:33:19,777 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 4 completed in 54.74s - Train Loss: 0.4760, Train Acc: 0.8295, Val Loss: 0.4709, Val Acc: 0.8396
2025-05-03 05:33:19,876 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 05:33:19,877 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 5/20
2025-05-03 05:33:44,188 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 05:34:16,831 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8396 to 0.8679
2025-05-03 05:34:16,871 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 5 completed in 56.99s - Train Loss: 0.4503, Train Acc: 0.8574, Val Loss: 0.4412, Val Acc: 0.8679
2025-05-03 05:34:16,970 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 05:34:16,970 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 6/20
2025-05-03 05:34:40,638 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 05:35:13,108 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.6975 to 0.4601
2025-05-03 05:35:13,148 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 6 completed in 56.18s - Train Loss: 0.4692, Train Acc: 0.8381, Val Loss: 0.4601, Val Acc: 0.8456
2025-05-03 05:35:13,241 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 05:35:13,242 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 7/20
2025-05-03 05:35:37,554 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 05:36:09,283 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8679 to 0.8842
2025-05-03 05:36:09,324 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 7 completed in 56.08s - Train Loss: 0.4551, Train Acc: 0.8539, Val Loss: 0.4250, Val Acc: 0.8842
2025-05-03 05:36:09,432 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 05:36:09,432 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 8/20
2025-05-03 05:36:33,460 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 05:37:04,421 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.4601 to 0.4447
2025-05-03 05:37:04,463 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 8 completed in 55.03s - Train Loss: 0.4498, Train Acc: 0.8593, Val Loss: 0.4447, Val Acc: 0.8593
2025-05-03 05:37:04,563 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 05:37:04,564 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 9/20
2025-05-03 05:37:28,529 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 05:38:01,311 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 9 completed in 56.75s - Train Loss: 0.4421, Train Acc: 0.8664, Val Loss: 0.4946, Val Acc: 0.8036
2025-05-03 05:38:01,411 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 05:38:01,411 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 10/20
2025-05-03 05:38:24,692 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 05:38:57,005 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8842 to 0.8945
2025-05-03 05:38:57,046 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 10 completed in 55.64s - Train Loss: 0.4236, Train Acc: 0.8844, Val Loss: 0.4135, Val Acc: 0.8945
2025-05-03 05:38:57,147 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 05:38:57,147 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 11/20
2025-05-03 05:39:21,647 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 05:39:52,924 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8945 to 0.9142
2025-05-03 05:39:52,965 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 11 completed in 55.82s - Train Loss: 0.3955, Train Acc: 0.9163, Val Loss: 0.3920, Val Acc: 0.9142
2025-05-03 05:39:53,067 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 05:39:53,068 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 12/20
2025-05-03 05:40:17,433 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 05:40:48,758 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9142 to 0.9280
2025-05-03 05:40:48,796 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 12 completed in 55.73s - Train Loss: 0.3780, Train Acc: 0.9346, Val Loss: 0.3847, Val Acc: 0.9280
2025-05-03 05:40:48,895 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 05:40:48,895 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 05:41:12,832 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 05:41:44,765 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9280 to 0.9322
2025-05-03 05:41:44,808 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 13 completed in 55.91s - Train Loss: 0.3718, Train Acc: 0.9389, Val Loss: 0.3762, Val Acc: 0.9322
2025-05-03 05:41:44,906 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 05:41:44,907 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 05:42:07,790 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 05:42:41,463 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9322 to 0.9434
2025-05-03 05:42:41,504 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 14 completed in 56.60s - Train Loss: 0.3680, Train Acc: 0.9423, Val Loss: 0.3686, Val Acc: 0.9434
2025-05-03 05:42:41,609 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 05:42:41,610 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 05:43:05,473 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 05:43:37,151 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9434 to 0.9443
2025-05-03 05:43:37,192 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 15 completed in 55.58s - Train Loss: 0.3624, Train Acc: 0.9511, Val Loss: 0.3679, Val Acc: 0.9443
2025-05-03 05:43:37,287 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 05:43:37,287 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 05:44:02,139 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 05:44:33,967 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9443 to 0.9485
2025-05-03 05:44:34,006 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 16 completed in 56.72s - Train Loss: 0.3593, Train Acc: 0.9537, Val Loss: 0.3638, Val Acc: 0.9485
2025-05-03 05:44:34,114 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 05:44:34,114 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 05:44:58,988 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 05:45:31,058 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9485 to 0.9503
2025-05-03 05:45:31,099 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 17 completed in 56.98s - Train Loss: 0.3554, Train Acc: 0.9592, Val Loss: 0.3626, Val Acc: 0.9503
2025-05-03 05:45:31,198 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 05:45:31,198 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 05:45:55,737 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 05:46:27,830 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9503 to 0.9511
2025-05-03 05:46:27,872 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 18 completed in 56.67s - Train Loss: 0.3525, Train Acc: 0.9605, Val Loss: 0.3607, Val Acc: 0.9511
2025-05-03 05:46:27,969 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 05:46:27,970 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 05:46:52,482 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 05:47:25,558 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9511 to 0.9528
2025-05-03 05:47:25,600 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 19 completed in 57.63s - Train Loss: 0.3514, Train Acc: 0.9622, Val Loss: 0.3619, Val Acc: 0.9528
2025-05-03 05:47:25,698 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 05:47:25,699 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 05:47:49,726 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 05:48:22,235 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9528 to 0.9597
2025-05-03 05:48:22,280 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Epoch 20 completed in 56.58s - Train Loss: 0.3514, Train Acc: 0.9629, Val Loss: 0.3555, Val Acc: 0.9597
2025-05-03 05:48:22,377 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 05:48:22,378 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:48:22,383 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_517 - INFO - Starting model evaluation
2025-05-03 05:48:34,272 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_517 - INFO - Evaluation metrics:
  Accuracy:  0.9326
  Precision: 0.8658
  Recall:    0.9021
  F1 Score:  0.8836
  IoU:       0.7914
  mAP:       0.9300
  AUC:       0.9651
2025-05-03 05:48:34,273 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_517 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_517/final_results.json
2025-05-03 05:48:34,275 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_517 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_517/final_results.json
2025-05-03 05:48:34,275 - main - INFO - 
Summary for configuration 517:
2025-05-03 05:48:34,275 - main - INFO - Accuracy: 0.9326
2025-05-03 05:48:34,275 - main - INFO - Precision: 0.8658
2025-05-03 05:48:34,275 - main - INFO - Recall: 0.9021
2025-05-03 05:48:34,275 - main - INFO - F1 Score: 0.8836
2025-05-03 05:48:34,275 - main - INFO - IoU: 0.7914
2025-05-03 05:48:34,275 - main - INFO - mAP: 0.9300
2025-05-03 05:48:34,275 - main - INFO - AUC: 0.9651
2025-05-03 05:48:34,275 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:48:34,275 - main - INFO - 
==================================================
2025-05-03 05:48:34,275 - main - INFO - Running configuration 518/756:
2025-05-03 05:48:34,275 - main - INFO - Model: DenseNet121
2025-05-03 05:48:34,275 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:48:34,275 - main - INFO - Scheduler: StepLR
2025-05-03 05:48:34,275 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:48:34,275 - main - INFO - ==================================================
2025-05-03 05:48:34,275 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_518 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_518
2025-05-03 05:48:34,275 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_518 - INFO - Config: {
  "id": 518,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:48:34,518 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:48:34,518 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 518,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:48:34,518 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:48:34,692 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:48:34,694 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:48:34,699 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_518 - INFO - Starting model evaluation
2025-05-03 05:48:46,925 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_518 - INFO - Evaluation metrics:
  Accuracy:  0.9366
  Precision: 0.8801
  Recall:    0.8986
  F1 Score:  0.8893
  IoU:       0.8006
  mAP:       0.9381
  AUC:       0.9665
2025-05-03 05:48:46,927 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_518 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_518/final_results.json
2025-05-03 05:48:46,928 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_518 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_518/final_results.json
2025-05-03 05:48:46,928 - main - INFO - 
Summary for configuration 518:
2025-05-03 05:48:46,929 - main - INFO - Accuracy: 0.9366
2025-05-03 05:48:46,929 - main - INFO - Precision: 0.8801
2025-05-03 05:48:46,929 - main - INFO - Recall: 0.8986
2025-05-03 05:48:46,929 - main - INFO - F1 Score: 0.8893
2025-05-03 05:48:46,929 - main - INFO - IoU: 0.8006
2025-05-03 05:48:46,929 - main - INFO - mAP: 0.9381
2025-05-03 05:48:46,929 - main - INFO - AUC: 0.9665
2025-05-03 05:48:46,929 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:48:46,929 - main - INFO - 
==================================================
2025-05-03 05:48:46,929 - main - INFO - Running configuration 519/756:
2025-05-03 05:48:46,929 - main - INFO - Model: DenseNet121
2025-05-03 05:48:46,929 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:48:46,929 - main - INFO - Scheduler: StepLR
2025-05-03 05:48:46,929 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:48:46,929 - main - INFO - ==================================================
2025-05-03 05:48:46,929 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_519 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_519
2025-05-03 05:48:46,929 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_519 - INFO - Config: {
  "id": 519,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:48:47,092 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:48:47,092 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 519,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:48:47,092 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:48:47,265 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:48:47,267 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:48:47,271 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_519 - INFO - Starting model evaluation
2025-05-03 05:48:59,153 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_519 - INFO - Evaluation metrics:
  Accuracy:  0.9356
  Precision: 0.8824
  Recall:    0.8916
  F1 Score:  0.8870
  IoU:       0.7969
  mAP:       0.9320
  AUC:       0.9642
2025-05-03 05:48:59,155 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_519 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_519/final_results.json
2025-05-03 05:48:59,157 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_519 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_519/final_results.json
2025-05-03 05:48:59,157 - main - INFO - 
Summary for configuration 519:
2025-05-03 05:48:59,157 - main - INFO - Accuracy: 0.9356
2025-05-03 05:48:59,157 - main - INFO - Precision: 0.8824
2025-05-03 05:48:59,157 - main - INFO - Recall: 0.8916
2025-05-03 05:48:59,157 - main - INFO - F1 Score: 0.8870
2025-05-03 05:48:59,157 - main - INFO - IoU: 0.7969
2025-05-03 05:48:59,157 - main - INFO - mAP: 0.9320
2025-05-03 05:48:59,157 - main - INFO - AUC: 0.9642
2025-05-03 05:48:59,157 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:48:59,157 - main - INFO - 
==================================================
2025-05-03 05:48:59,157 - main - INFO - Running configuration 520/756:
2025-05-03 05:48:59,157 - main - INFO - Model: DenseNet121
2025-05-03 05:48:59,157 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:48:59,157 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:48:59,157 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:48:59,157 - main - INFO - ==================================================
2025-05-03 05:48:59,157 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_520 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_520
2025-05-03 05:48:59,157 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_520 - INFO - Config: {
  "id": 520,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:48:59,376 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:48:59,376 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 520,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:48:59,376 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:48:59,548 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:48:59,550 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:48:59,555 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_520 - INFO - Starting model evaluation
2025-05-03 05:49:11,222 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_520 - INFO - Evaluation metrics:
  Accuracy:  0.9366
  Precision: 0.8776
  Recall:    0.9021
  F1 Score:  0.8897
  IoU:       0.8012
  mAP:       0.9402
  AUC:       0.9676
2025-05-03 05:49:11,225 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_520 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_520/final_results.json
2025-05-03 05:49:11,227 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_520 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_520/final_results.json
2025-05-03 05:49:11,227 - main - INFO - 
Summary for configuration 520:
2025-05-03 05:49:11,227 - main - INFO - Accuracy: 0.9366
2025-05-03 05:49:11,227 - main - INFO - Precision: 0.8776
2025-05-03 05:49:11,227 - main - INFO - Recall: 0.9021
2025-05-03 05:49:11,227 - main - INFO - F1 Score: 0.8897
2025-05-03 05:49:11,227 - main - INFO - IoU: 0.8012
2025-05-03 05:49:11,227 - main - INFO - mAP: 0.9402
2025-05-03 05:49:11,227 - main - INFO - AUC: 0.9676
2025-05-03 05:49:11,227 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:49:11,227 - main - INFO - 
==================================================
2025-05-03 05:49:11,227 - main - INFO - Running configuration 521/756:
2025-05-03 05:49:11,227 - main - INFO - Model: DenseNet121
2025-05-03 05:49:11,227 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:49:11,227 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:49:11,227 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:49:11,228 - main - INFO - ==================================================
2025-05-03 05:49:11,228 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_521 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_521
2025-05-03 05:49:11,228 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_521 - INFO - Config: {
  "id": 521,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:49:11,369 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:49:11,369 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 521,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:49:11,369 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:49:11,894 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:49:11,895 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:49:11,900 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_521 - INFO - Starting model evaluation
2025-05-03 05:49:23,690 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_521 - INFO - Evaluation metrics:
  Accuracy:  0.9326
  Precision: 0.8759
  Recall:    0.8881
  F1 Score:  0.8819
  IoU:       0.7888
  mAP:       0.9350
  AUC:       0.9648
2025-05-03 05:49:23,692 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_521 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_521/final_results.json
2025-05-03 05:49:23,693 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_521 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_521/final_results.json
2025-05-03 05:49:23,693 - main - INFO - 
Summary for configuration 521:
2025-05-03 05:49:23,693 - main - INFO - Accuracy: 0.9326
2025-05-03 05:49:23,693 - main - INFO - Precision: 0.8759
2025-05-03 05:49:23,693 - main - INFO - Recall: 0.8881
2025-05-03 05:49:23,693 - main - INFO - F1 Score: 0.8819
2025-05-03 05:49:23,693 - main - INFO - IoU: 0.7888
2025-05-03 05:49:23,693 - main - INFO - mAP: 0.9350
2025-05-03 05:49:23,693 - main - INFO - AUC: 0.9648
2025-05-03 05:49:23,693 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:49:23,693 - main - INFO - 
==================================================
2025-05-03 05:49:23,693 - main - INFO - Running configuration 522/756:
2025-05-03 05:49:23,693 - main - INFO - Model: DenseNet121
2025-05-03 05:49:23,693 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:49:23,693 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 05:49:23,694 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:49:23,694 - main - INFO - ==================================================
2025-05-03 05:49:23,694 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_522 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_522
2025-05-03 05:49:23,694 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_522 - INFO - Config: {
  "id": 522,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:49:23,825 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:49:23,825 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 522,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:49:23,825 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:49:24,261 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:49:24,263 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:49:24,267 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_522 - INFO - Starting model evaluation
2025-05-03 05:49:36,347 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_522 - INFO - Evaluation metrics:
  Accuracy:  0.9306
  Precision: 0.8673
  Recall:    0.8916
  F1 Score:  0.8793
  IoU:       0.7846
  mAP:       0.9288
  AUC:       0.9638
2025-05-03 05:49:36,349 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_522 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_522/final_results.json
2025-05-03 05:49:36,351 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_522 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_522/final_results.json
2025-05-03 05:49:36,351 - main - INFO - 
Summary for configuration 522:
2025-05-03 05:49:36,351 - main - INFO - Accuracy: 0.9306
2025-05-03 05:49:36,351 - main - INFO - Precision: 0.8673
2025-05-03 05:49:36,351 - main - INFO - Recall: 0.8916
2025-05-03 05:49:36,351 - main - INFO - F1 Score: 0.8793
2025-05-03 05:49:36,351 - main - INFO - IoU: 0.7846
2025-05-03 05:49:36,351 - main - INFO - mAP: 0.9288
2025-05-03 05:49:36,351 - main - INFO - AUC: 0.9638
2025-05-03 05:49:36,351 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:49:36,351 - main - INFO - 
==================================================
2025-05-03 05:49:36,351 - main - INFO - Running configuration 523/756:
2025-05-03 05:49:36,351 - main - INFO - Model: DenseNet121
2025-05-03 05:49:36,351 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:49:36,351 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:49:36,351 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:49:36,351 - main - INFO - ==================================================
2025-05-03 05:49:36,351 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_523 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_523
2025-05-03 05:49:36,351 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_523 - INFO - Config: {
  "id": 523,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:49:36,547 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:49:36,547 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 523,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:49:36,547 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:49:36,719 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:49:36,721 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:49:36,726 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_523 - INFO - Starting model evaluation
2025-05-03 05:49:48,197 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_523 - INFO - Evaluation metrics:
  Accuracy:  0.9356
  Precision: 0.8771
  Recall:    0.8986
  F1 Score:  0.8877
  IoU:       0.7981
  mAP:       0.9350
  AUC:       0.9655
2025-05-03 05:49:48,199 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_523 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_523/final_results.json
2025-05-03 05:49:48,200 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_523 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_523/final_results.json
2025-05-03 05:49:48,200 - main - INFO - 
Summary for configuration 523:
2025-05-03 05:49:48,201 - main - INFO - Accuracy: 0.9356
2025-05-03 05:49:48,201 - main - INFO - Precision: 0.8771
2025-05-03 05:49:48,201 - main - INFO - Recall: 0.8986
2025-05-03 05:49:48,201 - main - INFO - F1 Score: 0.8877
2025-05-03 05:49:48,201 - main - INFO - IoU: 0.7981
2025-05-03 05:49:48,201 - main - INFO - mAP: 0.9350
2025-05-03 05:49:48,201 - main - INFO - AUC: 0.9655
2025-05-03 05:49:48,201 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:49:48,201 - main - INFO - 
==================================================
2025-05-03 05:49:48,201 - main - INFO - Running configuration 524/756:
2025-05-03 05:49:48,201 - main - INFO - Model: DenseNet121
2025-05-03 05:49:48,201 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:49:48,201 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:49:48,201 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:49:48,201 - main - INFO - ==================================================
2025-05-03 05:49:48,201 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_524 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_524
2025-05-03 05:49:48,201 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_524 - INFO - Config: {
  "id": 524,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:49:48,356 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:49:48,356 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 524,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:49:48,356 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:49:48,529 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:49:48,531 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:49:48,535 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_524 - INFO - Starting model evaluation
2025-05-03 05:49:59,863 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_524 - INFO - Evaluation metrics:
  Accuracy:  0.9316
  Precision: 0.8729
  Recall:    0.8881
  F1 Score:  0.8804
  IoU:       0.7864
  mAP:       0.9298
  AUC:       0.9652
2025-05-03 05:49:59,867 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_524 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_524/final_results.json
2025-05-03 05:49:59,869 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_524 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_524/final_results.json
2025-05-03 05:49:59,869 - main - INFO - 
Summary for configuration 524:
2025-05-03 05:49:59,869 - main - INFO - Accuracy: 0.9316
2025-05-03 05:49:59,869 - main - INFO - Precision: 0.8729
2025-05-03 05:49:59,869 - main - INFO - Recall: 0.8881
2025-05-03 05:49:59,869 - main - INFO - F1 Score: 0.8804
2025-05-03 05:49:59,869 - main - INFO - IoU: 0.7864
2025-05-03 05:49:59,869 - main - INFO - mAP: 0.9298
2025-05-03 05:49:59,869 - main - INFO - AUC: 0.9652
2025-05-03 05:49:59,869 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:49:59,869 - main - INFO - 
==================================================
2025-05-03 05:49:59,869 - main - INFO - Running configuration 525/756:
2025-05-03 05:49:59,869 - main - INFO - Model: DenseNet121
2025-05-03 05:49:59,869 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:49:59,869 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 05:49:59,869 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:49:59,869 - main - INFO - ==================================================
2025-05-03 05:49:59,869 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_525 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_525
2025-05-03 05:49:59,869 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_525 - INFO - Config: {
  "id": 525,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:50:00,002 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:50:00,002 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 525,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:50:00,002 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:50:00,175 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:50:00,177 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:50:00,182 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_525 - INFO - Starting model evaluation
2025-05-03 05:50:12,062 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_525 - INFO - Evaluation metrics:
  Accuracy:  0.9296
  Precision: 0.8644
  Recall:    0.8916
  F1 Score:  0.8778
  IoU:       0.7822
  mAP:       0.9347
  AUC:       0.9644
2025-05-03 05:50:12,063 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_525 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_525/final_results.json
2025-05-03 05:50:12,065 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_525 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_525/final_results.json
2025-05-03 05:50:12,065 - main - INFO - 
Summary for configuration 525:
2025-05-03 05:50:12,065 - main - INFO - Accuracy: 0.9296
2025-05-03 05:50:12,065 - main - INFO - Precision: 0.8644
2025-05-03 05:50:12,065 - main - INFO - Recall: 0.8916
2025-05-03 05:50:12,065 - main - INFO - F1 Score: 0.8778
2025-05-03 05:50:12,065 - main - INFO - IoU: 0.7822
2025-05-03 05:50:12,065 - main - INFO - mAP: 0.9347
2025-05-03 05:50:12,065 - main - INFO - AUC: 0.9644
2025-05-03 05:50:12,065 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:50:12,065 - main - INFO - 
==================================================
2025-05-03 05:50:12,065 - main - INFO - Running configuration 526/756:
2025-05-03 05:50:12,065 - main - INFO - Model: DenseNet121
2025-05-03 05:50:12,065 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:50:12,065 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:50:12,065 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:50:12,065 - main - INFO - ==================================================
2025-05-03 05:50:12,065 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_526 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_526
2025-05-03 05:50:12,065 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_526 - INFO - Config: {
  "id": 526,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:50:12,205 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:50:12,205 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 526,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:50:12,205 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:50:12,377 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:50:12,379 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:50:12,384 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_526 - INFO - Starting model evaluation
2025-05-03 05:50:24,275 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_526 - INFO - Evaluation metrics:
  Accuracy:  0.9277
  Precision: 0.8635
  Recall:    0.8846
  F1 Score:  0.8739
  IoU:       0.7761
  mAP:       0.9284
  AUC:       0.9634
2025-05-03 05:50:24,277 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_526 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_526/final_results.json
2025-05-03 05:50:24,278 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_526 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_526/final_results.json
2025-05-03 05:50:24,278 - main - INFO - 
Summary for configuration 526:
2025-05-03 05:50:24,278 - main - INFO - Accuracy: 0.9277
2025-05-03 05:50:24,278 - main - INFO - Precision: 0.8635
2025-05-03 05:50:24,278 - main - INFO - Recall: 0.8846
2025-05-03 05:50:24,279 - main - INFO - F1 Score: 0.8739
2025-05-03 05:50:24,279 - main - INFO - IoU: 0.7761
2025-05-03 05:50:24,279 - main - INFO - mAP: 0.9284
2025-05-03 05:50:24,279 - main - INFO - AUC: 0.9634
2025-05-03 05:50:24,279 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:50:24,279 - main - INFO - 
==================================================
2025-05-03 05:50:24,279 - main - INFO - Running configuration 527/756:
2025-05-03 05:50:24,279 - main - INFO - Model: DenseNet121
2025-05-03 05:50:24,279 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:50:24,279 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:50:24,279 - main - INFO - Loss Function: FocalLoss
2025-05-03 05:50:24,279 - main - INFO - ==================================================
2025-05-03 05:50:24,279 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_527 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_527
2025-05-03 05:50:24,279 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_527 - INFO - Config: {
  "id": 527,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:50:24,500 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:50:24,500 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 527,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 05:50:24,500 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:50:24,672 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:50:24,674 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:50:24,679 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_527 - INFO - Starting model evaluation
2025-05-03 05:50:36,204 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_527 - INFO - Evaluation metrics:
  Accuracy:  0.9376
  Precision: 0.8832
  Recall:    0.8986
  F1 Score:  0.8908
  IoU:       0.8031
  mAP:       0.9331
  AUC:       0.9671
2025-05-03 05:50:36,206 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_527 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_527/final_results.json
2025-05-03 05:50:36,207 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_527 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_527/final_results.json
2025-05-03 05:50:36,207 - main - INFO - 
Summary for configuration 527:
2025-05-03 05:50:36,207 - main - INFO - Accuracy: 0.9376
2025-05-03 05:50:36,207 - main - INFO - Precision: 0.8832
2025-05-03 05:50:36,207 - main - INFO - Recall: 0.8986
2025-05-03 05:50:36,207 - main - INFO - F1 Score: 0.8908
2025-05-03 05:50:36,207 - main - INFO - IoU: 0.8031
2025-05-03 05:50:36,207 - main - INFO - mAP: 0.9331
2025-05-03 05:50:36,207 - main - INFO - AUC: 0.9671
2025-05-03 05:50:36,207 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:50:36,207 - main - INFO - 
==================================================
2025-05-03 05:50:36,207 - main - INFO - Running configuration 528/756:
2025-05-03 05:50:36,207 - main - INFO - Model: DenseNet121
2025-05-03 05:50:36,207 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 05:50:36,207 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 05:50:36,207 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 05:50:36,207 - main - INFO - ==================================================
2025-05-03 05:50:36,208 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_528 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001_id_528
2025-05-03 05:50:36,208 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_528 - INFO - Config: {
  "id": 528,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:50:36,338 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.001
2025-05-03 05:50:36,338 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 528,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 05:50:36,339 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 05:50:36,593 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9597
2025-05-03 05:50:36,596 - training.model_DenseNet121_opt_AdamW_lr_0.001 - INFO - Training completed after 1126.33 seconds
2025-05-03 05:50:36,601 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_528 - INFO - Starting model evaluation
2025-05-03 05:50:48,571 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_528 - INFO - Evaluation metrics:
  Accuracy:  0.9306
  Precision: 0.8600
  Recall:    0.9021
  F1 Score:  0.8805
  IoU:       0.7866
  mAP:       0.9351
  AUC:       0.9657
2025-05-03 05:50:48,573 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_528 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_528/final_results.json
2025-05-03 05:50:48,575 - training.model_DenseNet121_opt_AdamW_lr_0.001_id_528 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.001_id_528/final_results.json
2025-05-03 05:50:48,575 - main - INFO - 
Summary for configuration 528:
2025-05-03 05:50:48,575 - main - INFO - Accuracy: 0.9306
2025-05-03 05:50:48,575 - main - INFO - Precision: 0.8600
2025-05-03 05:50:48,575 - main - INFO - Recall: 0.9021
2025-05-03 05:50:48,575 - main - INFO - F1 Score: 0.8805
2025-05-03 05:50:48,575 - main - INFO - IoU: 0.7866
2025-05-03 05:50:48,575 - main - INFO - mAP: 0.9351
2025-05-03 05:50:48,575 - main - INFO - AUC: 0.9657
2025-05-03 05:50:48,575 - main - INFO - Training time: 1126.33 seconds
2025-05-03 05:50:48,575 - main - INFO - 
==================================================
2025-05-03 05:50:48,575 - main - INFO - Running configuration 529/756:
2025-05-03 05:50:48,575 - main - INFO - Model: DenseNet121
2025-05-03 05:50:48,575 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 05:50:48,575 - main - INFO - Scheduler: StepLR
2025-05-03 05:50:48,575 - main - INFO - Loss Function: CrossEntropy
2025-05-03 05:50:48,575 - main - INFO - ==================================================
2025-05-03 05:50:48,575 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_529 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_529
2025-05-03 05:50:48,575 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_529 - INFO - Config: {
  "id": 529,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:50:48,798 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 05:50:48,798 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 529,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 05:50:48,798 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-03 05:50:48,799 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-03 05:51:12,963 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 05:51:43,941 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9425
2025-05-03 05:51:43,973 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 1 completed in 55.17s - Train Loss: 0.4276, Train Acc: 0.8951, Val Loss: 0.3705, Val Acc: 0.9425
2025-05-03 05:51:44,073 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 05:51:44,073 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-03 05:52:07,597 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 05:52:40,192 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9425 to 0.9674
2025-05-03 05:52:40,232 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 2 completed in 56.16s - Train Loss: 0.3546, Train Acc: 0.9599, Val Loss: 0.3473, Val Acc: 0.9674
2025-05-03 05:52:40,330 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 05:52:40,330 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-03 05:53:03,301 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 05:53:35,582 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from inf to 0.3491
2025-05-03 05:53:35,627 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 3 completed in 55.30s - Train Loss: 0.3466, Train Acc: 0.9672, Val Loss: 0.3491, Val Acc: 0.9657
2025-05-03 05:53:35,724 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 05:53:35,725 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-03 05:54:00,506 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 05:54:32,971 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 4 completed in 57.25s - Train Loss: 0.3419, Train Acc: 0.9717, Val Loss: 0.3533, Val Acc: 0.9580
2025-05-03 05:54:33,073 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 05:54:33,073 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 05:54:56,578 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 05:55:28,973 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9674 to 0.9700
2025-05-03 05:55:29,019 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 5 completed in 55.95s - Train Loss: 0.3376, Train Acc: 0.9758, Val Loss: 0.3421, Val Acc: 0.9700
2025-05-03 05:55:29,131 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 05:55:29,131 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 05:55:54,186 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 05:56:26,228 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9700 to 0.9786
2025-05-03 05:56:26,274 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 6 completed in 57.14s - Train Loss: 0.3272, Train Acc: 0.9876, Val Loss: 0.3361, Val Acc: 0.9786
2025-05-03 05:56:26,376 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 05:56:26,377 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 05:56:51,065 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 05:57:22,407 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3491 to 0.3409
2025-05-03 05:57:22,447 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 7 completed in 56.07s - Train Loss: 0.3284, Train Acc: 0.9854, Val Loss: 0.3409, Val Acc: 0.9708
2025-05-03 05:57:22,553 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 05:57:22,553 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 05:57:46,547 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 05:58:18,945 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 8 completed in 56.39s - Train Loss: 0.3300, Train Acc: 0.9822, Val Loss: 0.3413, Val Acc: 0.9734
2025-05-03 05:58:19,048 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 05:58:19,049 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 05:58:42,574 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 05:59:13,729 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9786 to 0.9863
2025-05-03 05:59:13,770 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 9 completed in 54.72s - Train Loss: 0.3320, Train Acc: 0.9811, Val Loss: 0.3278, Val Acc: 0.9863
2025-05-03 05:59:13,879 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 05:59:13,879 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 05:59:37,709 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 06:00:09,948 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 10 completed in 56.07s - Train Loss: 0.3305, Train Acc: 0.9841, Val Loss: 0.3573, Val Acc: 0.9545
2025-05-03 06:00:10,051 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 06:00:10,051 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 06:00:34,486 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 06:01:06,442 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3409 to 0.3316
2025-05-03 06:01:06,483 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 11 completed in 56.43s - Train Loss: 0.3247, Train Acc: 0.9891, Val Loss: 0.3316, Val Acc: 0.9811
2025-05-03 06:01:06,585 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 06:01:06,585 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 06:01:30,139 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 06:02:02,452 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3316 to 0.3279
2025-05-03 06:02:02,495 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 12 completed in 55.91s - Train Loss: 0.3201, Train Acc: 0.9936, Val Loss: 0.3279, Val Acc: 0.9837
2025-05-03 06:02:02,688 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 06:02:02,689 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 06:02:27,046 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 06:02:58,140 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 13 completed in 55.45s - Train Loss: 0.3185, Train Acc: 0.9953, Val Loss: 0.3300, Val Acc: 0.9837
2025-05-03 06:02:58,242 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 06:02:58,243 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 06:03:21,680 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 06:03:53,727 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9863 to 0.9871
2025-05-03 06:03:53,789 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 14 completed in 55.55s - Train Loss: 0.3183, Train Acc: 0.9959, Val Loss: 0.3265, Val Acc: 0.9871
2025-05-03 06:03:53,888 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 06:03:53,888 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 06:04:17,656 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 06:04:49,707 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3279 to 0.3261
2025-05-03 06:04:49,749 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 15 completed in 55.86s - Train Loss: 0.3178, Train Acc: 0.9957, Val Loss: 0.3261, Val Acc: 0.9871
2025-05-03 06:04:49,846 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 06:04:49,846 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 06:05:13,271 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 06:05:44,678 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9871 to 0.9897
2025-05-03 06:05:44,718 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 16 completed in 54.87s - Train Loss: 0.3174, Train Acc: 0.9961, Val Loss: 0.3241, Val Acc: 0.9897
2025-05-03 06:05:44,826 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 06:05:44,826 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 06:06:07,783 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 06:06:39,650 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3261 to 0.3249
2025-05-03 06:06:39,692 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 17 completed in 54.87s - Train Loss: 0.3162, Train Acc: 0.9976, Val Loss: 0.3249, Val Acc: 0.9871
2025-05-03 06:06:39,792 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 06:06:39,793 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 06:07:03,728 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 06:07:35,545 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 18 completed in 55.75s - Train Loss: 0.3155, Train Acc: 0.9981, Val Loss: 0.3266, Val Acc: 0.9854
2025-05-03 06:07:35,643 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 06:07:35,643 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 06:07:59,534 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 06:08:31,229 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 19 completed in 55.59s - Train Loss: 0.3152, Train Acc: 0.9983, Val Loss: 0.3250, Val Acc: 0.9880
2025-05-03 06:08:31,334 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 06:08:31,335 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 06:08:54,570 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 06:09:25,847 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3249 to 0.3246
2025-05-03 06:09:25,888 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Epoch 20 completed in 54.55s - Train Loss: 0.3155, Train Acc: 0.9983, Val Loss: 0.3246, Val Acc: 0.9889
2025-05-03 06:09:25,981 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 06:09:25,982 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:09:25,987 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_529 - INFO - Starting model evaluation
2025-05-03 06:09:38,088 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_529 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9212
  Recall:    0.9406
  F1 Score:  0.9308
  IoU:       0.8706
  mAP:       0.9822
  AUC:       0.9932
2025-05-03 06:09:38,090 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_529 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_529/final_results.json
2025-05-03 06:09:38,092 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_529 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_529/final_results.json
2025-05-03 06:09:38,092 - main - INFO - 
Summary for configuration 529:
2025-05-03 06:09:38,092 - main - INFO - Accuracy: 0.9604
2025-05-03 06:09:38,092 - main - INFO - Precision: 0.9212
2025-05-03 06:09:38,092 - main - INFO - Recall: 0.9406
2025-05-03 06:09:38,092 - main - INFO - F1 Score: 0.9308
2025-05-03 06:09:38,092 - main - INFO - IoU: 0.8706
2025-05-03 06:09:38,092 - main - INFO - mAP: 0.9822
2025-05-03 06:09:38,092 - main - INFO - AUC: 0.9932
2025-05-03 06:09:38,092 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:09:38,092 - main - INFO - 
==================================================
2025-05-03 06:09:38,092 - main - INFO - Running configuration 530/756:
2025-05-03 06:09:38,092 - main - INFO - Model: DenseNet121
2025-05-03 06:09:38,092 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:09:38,092 - main - INFO - Scheduler: StepLR
2025-05-03 06:09:38,092 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:09:38,092 - main - INFO - ==================================================
2025-05-03 06:09:38,092 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_530 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_530
2025-05-03 06:09:38,092 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_530 - INFO - Config: {
  "id": 530,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:09:38,245 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:09:38,245 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 530,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:09:38,245 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:09:38,419 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:09:38,420 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:09:38,425 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_530 - INFO - Starting model evaluation
2025-05-03 06:09:50,502 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_530 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9153
  Recall:    0.9441
  F1 Score:  0.9294
  IoU:       0.8682
  mAP:       0.9831
  AUC:       0.9925
2025-05-03 06:09:50,504 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_530 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_530/final_results.json
2025-05-03 06:09:50,505 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_530 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_530/final_results.json
2025-05-03 06:09:50,506 - main - INFO - 
Summary for configuration 530:
2025-05-03 06:09:50,506 - main - INFO - Accuracy: 0.9594
2025-05-03 06:09:50,506 - main - INFO - Precision: 0.9153
2025-05-03 06:09:50,506 - main - INFO - Recall: 0.9441
2025-05-03 06:09:50,506 - main - INFO - F1 Score: 0.9294
2025-05-03 06:09:50,506 - main - INFO - IoU: 0.8682
2025-05-03 06:09:50,506 - main - INFO - mAP: 0.9831
2025-05-03 06:09:50,506 - main - INFO - AUC: 0.9925
2025-05-03 06:09:50,506 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:09:50,506 - main - INFO - 
==================================================
2025-05-03 06:09:50,506 - main - INFO - Running configuration 531/756:
2025-05-03 06:09:50,506 - main - INFO - Model: DenseNet121
2025-05-03 06:09:50,506 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:09:50,506 - main - INFO - Scheduler: StepLR
2025-05-03 06:09:50,506 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:09:50,506 - main - INFO - ==================================================
2025-05-03 06:09:50,506 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_531 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_531
2025-05-03 06:09:50,506 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_531 - INFO - Config: {
  "id": 531,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:09:50,804 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:09:50,804 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 531,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:09:50,805 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:09:50,978 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:09:50,979 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:09:50,984 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_531 - INFO - Starting model evaluation
2025-05-03 06:10:02,792 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_531 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9155
  Recall:    0.9476
  F1 Score:  0.9313
  IoU:       0.8714
  mAP:       0.9792
  AUC:       0.9912
2025-05-03 06:10:02,794 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_531 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_531/final_results.json
2025-05-03 06:10:02,796 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_531 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_531/final_results.json
2025-05-03 06:10:02,796 - main - INFO - 
Summary for configuration 531:
2025-05-03 06:10:02,796 - main - INFO - Accuracy: 0.9604
2025-05-03 06:10:02,796 - main - INFO - Precision: 0.9155
2025-05-03 06:10:02,796 - main - INFO - Recall: 0.9476
2025-05-03 06:10:02,796 - main - INFO - F1 Score: 0.9313
2025-05-03 06:10:02,796 - main - INFO - IoU: 0.8714
2025-05-03 06:10:02,796 - main - INFO - mAP: 0.9792
2025-05-03 06:10:02,796 - main - INFO - AUC: 0.9912
2025-05-03 06:10:02,796 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:10:02,796 - main - INFO - 
==================================================
2025-05-03 06:10:02,796 - main - INFO - Running configuration 532/756:
2025-05-03 06:10:02,796 - main - INFO - Model: DenseNet121
2025-05-03 06:10:02,796 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:10:02,796 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:10:02,796 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:10:02,796 - main - INFO - ==================================================
2025-05-03 06:10:02,796 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_532 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_532
2025-05-03 06:10:02,796 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_532 - INFO - Config: {
  "id": 532,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:10:02,929 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:10:02,929 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 532,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:10:02,929 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:10:03,103 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:10:03,104 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:10:03,109 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_532 - INFO - Starting model evaluation
2025-05-03 06:10:14,718 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_532 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9186
  Recall:    0.9476
  F1 Score:  0.9329
  IoU:       0.8742
  mAP:       0.9775
  AUC:       0.9916
2025-05-03 06:10:14,720 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_532 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_532/final_results.json
2025-05-03 06:10:14,721 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_532 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_532/final_results.json
2025-05-03 06:10:14,721 - main - INFO - 
Summary for configuration 532:
2025-05-03 06:10:14,721 - main - INFO - Accuracy: 0.9613
2025-05-03 06:10:14,721 - main - INFO - Precision: 0.9186
2025-05-03 06:10:14,721 - main - INFO - Recall: 0.9476
2025-05-03 06:10:14,721 - main - INFO - F1 Score: 0.9329
2025-05-03 06:10:14,721 - main - INFO - IoU: 0.8742
2025-05-03 06:10:14,721 - main - INFO - mAP: 0.9775
2025-05-03 06:10:14,721 - main - INFO - AUC: 0.9916
2025-05-03 06:10:14,721 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:10:14,721 - main - INFO - 
==================================================
2025-05-03 06:10:14,721 - main - INFO - Running configuration 533/756:
2025-05-03 06:10:14,721 - main - INFO - Model: DenseNet121
2025-05-03 06:10:14,721 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:10:14,721 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:10:14,721 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:10:14,721 - main - INFO - ==================================================
2025-05-03 06:10:14,721 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_533 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_533
2025-05-03 06:10:14,722 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_533 - INFO - Config: {
  "id": 533,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:10:15,098 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:10:15,098 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 533,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:10:15,098 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:10:15,271 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:10:15,273 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:10:15,277 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_533 - INFO - Starting model evaluation
2025-05-03 06:10:27,077 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_533 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9184
  Recall:    0.9441
  F1 Score:  0.9310
  IoU:       0.8710
  mAP:       0.9818
  AUC:       0.9914
2025-05-03 06:10:27,078 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_533 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_533/final_results.json
2025-05-03 06:10:27,080 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_533 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_533/final_results.json
2025-05-03 06:10:27,080 - main - INFO - 
Summary for configuration 533:
2025-05-03 06:10:27,080 - main - INFO - Accuracy: 0.9604
2025-05-03 06:10:27,080 - main - INFO - Precision: 0.9184
2025-05-03 06:10:27,080 - main - INFO - Recall: 0.9441
2025-05-03 06:10:27,080 - main - INFO - F1 Score: 0.9310
2025-05-03 06:10:27,080 - main - INFO - IoU: 0.8710
2025-05-03 06:10:27,080 - main - INFO - mAP: 0.9818
2025-05-03 06:10:27,080 - main - INFO - AUC: 0.9914
2025-05-03 06:10:27,080 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:10:27,080 - main - INFO - 
==================================================
2025-05-03 06:10:27,080 - main - INFO - Running configuration 534/756:
2025-05-03 06:10:27,080 - main - INFO - Model: DenseNet121
2025-05-03 06:10:27,080 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:10:27,080 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:10:27,080 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:10:27,080 - main - INFO - ==================================================
2025-05-03 06:10:27,080 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_534 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_534
2025-05-03 06:10:27,080 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_534 - INFO - Config: {
  "id": 534,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:10:27,481 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:10:27,481 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 534,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:10:27,481 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:10:27,654 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:10:27,656 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:10:27,660 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_534 - INFO - Starting model evaluation
2025-05-03 06:10:39,143 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_534 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9247
  Recall:    0.9441
  F1 Score:  0.9343
  IoU:       0.8766
  mAP:       0.9809
  AUC:       0.9927
2025-05-03 06:10:39,145 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_534 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_534/final_results.json
2025-05-03 06:10:39,147 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_534 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_534/final_results.json
2025-05-03 06:10:39,147 - main - INFO - 
Summary for configuration 534:
2025-05-03 06:10:39,147 - main - INFO - Accuracy: 0.9623
2025-05-03 06:10:39,147 - main - INFO - Precision: 0.9247
2025-05-03 06:10:39,147 - main - INFO - Recall: 0.9441
2025-05-03 06:10:39,147 - main - INFO - F1 Score: 0.9343
2025-05-03 06:10:39,147 - main - INFO - IoU: 0.8766
2025-05-03 06:10:39,147 - main - INFO - mAP: 0.9809
2025-05-03 06:10:39,147 - main - INFO - AUC: 0.9927
2025-05-03 06:10:39,147 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:10:39,147 - main - INFO - 
==================================================
2025-05-03 06:10:39,147 - main - INFO - Running configuration 535/756:
2025-05-03 06:10:39,147 - main - INFO - Model: DenseNet121
2025-05-03 06:10:39,147 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:10:39,147 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:10:39,147 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:10:39,147 - main - INFO - ==================================================
2025-05-03 06:10:39,147 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_535 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_535
2025-05-03 06:10:39,147 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_535 - INFO - Config: {
  "id": 535,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:10:39,534 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:10:39,534 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 535,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:10:39,534 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:10:39,708 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:10:39,710 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:10:39,715 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_535 - INFO - Starting model evaluation
2025-05-03 06:10:51,338 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_535 - INFO - Evaluation metrics:
  Accuracy:  0.9584
  Precision: 0.9067
  Recall:    0.9510
  F1 Score:  0.9283
  IoU:       0.8662
  mAP:       0.9805
  AUC:       0.9927
2025-05-03 06:10:51,340 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_535 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_535/final_results.json
2025-05-03 06:10:51,342 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_535 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_535/final_results.json
2025-05-03 06:10:51,342 - main - INFO - 
Summary for configuration 535:
2025-05-03 06:10:51,342 - main - INFO - Accuracy: 0.9584
2025-05-03 06:10:51,342 - main - INFO - Precision: 0.9067
2025-05-03 06:10:51,342 - main - INFO - Recall: 0.9510
2025-05-03 06:10:51,342 - main - INFO - F1 Score: 0.9283
2025-05-03 06:10:51,342 - main - INFO - IoU: 0.8662
2025-05-03 06:10:51,342 - main - INFO - mAP: 0.9805
2025-05-03 06:10:51,342 - main - INFO - AUC: 0.9927
2025-05-03 06:10:51,342 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:10:51,342 - main - INFO - 
==================================================
2025-05-03 06:10:51,342 - main - INFO - Running configuration 536/756:
2025-05-03 06:10:51,342 - main - INFO - Model: DenseNet121
2025-05-03 06:10:51,342 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:10:51,342 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:10:51,342 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:10:51,342 - main - INFO - ==================================================
2025-05-03 06:10:51,342 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_536 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_536
2025-05-03 06:10:51,342 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_536 - INFO - Config: {
  "id": 536,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:10:51,813 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:10:51,813 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 536,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:10:51,813 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:10:51,987 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:10:51,988 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:10:51,993 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_536 - INFO - Starting model evaluation
2025-05-03 06:11:03,460 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_536 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9249
  Recall:    0.9476
  F1 Score:  0.9361
  IoU:       0.8799
  mAP:       0.9849
  AUC:       0.9934
2025-05-03 06:11:03,461 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_536 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_536/final_results.json
2025-05-03 06:11:03,463 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_536 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_536/final_results.json
2025-05-03 06:11:03,463 - main - INFO - 
Summary for configuration 536:
2025-05-03 06:11:03,463 - main - INFO - Accuracy: 0.9633
2025-05-03 06:11:03,463 - main - INFO - Precision: 0.9249
2025-05-03 06:11:03,463 - main - INFO - Recall: 0.9476
2025-05-03 06:11:03,463 - main - INFO - F1 Score: 0.9361
2025-05-03 06:11:03,463 - main - INFO - IoU: 0.8799
2025-05-03 06:11:03,463 - main - INFO - mAP: 0.9849
2025-05-03 06:11:03,463 - main - INFO - AUC: 0.9934
2025-05-03 06:11:03,463 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:11:03,463 - main - INFO - 
==================================================
2025-05-03 06:11:03,463 - main - INFO - Running configuration 537/756:
2025-05-03 06:11:03,463 - main - INFO - Model: DenseNet121
2025-05-03 06:11:03,463 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:11:03,463 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:11:03,463 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:11:03,463 - main - INFO - ==================================================
2025-05-03 06:11:03,463 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_537 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_537
2025-05-03 06:11:03,463 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_537 - INFO - Config: {
  "id": 537,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:11:03,594 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:11:03,595 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 537,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:11:03,595 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:11:03,767 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:11:03,769 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:11:03,773 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_537 - INFO - Starting model evaluation
2025-05-03 06:11:15,690 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_537 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.9091
  Recall:    0.9441
  F1 Score:  0.9262
  IoU:       0.8626
  mAP:       0.9796
  AUC:       0.9918
2025-05-03 06:11:15,692 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_537 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_537/final_results.json
2025-05-03 06:11:15,694 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_537 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_537/final_results.json
2025-05-03 06:11:15,694 - main - INFO - 
Summary for configuration 537:
2025-05-03 06:11:15,694 - main - INFO - Accuracy: 0.9574
2025-05-03 06:11:15,694 - main - INFO - Precision: 0.9091
2025-05-03 06:11:15,694 - main - INFO - Recall: 0.9441
2025-05-03 06:11:15,694 - main - INFO - F1 Score: 0.9262
2025-05-03 06:11:15,694 - main - INFO - IoU: 0.8626
2025-05-03 06:11:15,694 - main - INFO - mAP: 0.9796
2025-05-03 06:11:15,694 - main - INFO - AUC: 0.9918
2025-05-03 06:11:15,694 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:11:15,694 - main - INFO - 
==================================================
2025-05-03 06:11:15,694 - main - INFO - Running configuration 538/756:
2025-05-03 06:11:15,694 - main - INFO - Model: DenseNet121
2025-05-03 06:11:15,694 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:11:15,694 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:11:15,694 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:11:15,694 - main - INFO - ==================================================
2025-05-03 06:11:15,694 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_538 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_538
2025-05-03 06:11:15,694 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_538 - INFO - Config: {
  "id": 538,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:11:15,933 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:11:15,933 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 538,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:11:15,933 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:11:16,105 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:11:16,107 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:11:16,111 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_538 - INFO - Starting model evaluation
2025-05-03 06:11:27,459 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_538 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9218
  Recall:    0.9476
  F1 Score:  0.9345
  IoU:       0.8770
  mAP:       0.9825
  AUC:       0.9914
2025-05-03 06:11:27,460 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_538 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_538/final_results.json
2025-05-03 06:11:27,462 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_538 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_538/final_results.json
2025-05-03 06:11:27,462 - main - INFO - 
Summary for configuration 538:
2025-05-03 06:11:27,462 - main - INFO - Accuracy: 0.9623
2025-05-03 06:11:27,462 - main - INFO - Precision: 0.9218
2025-05-03 06:11:27,462 - main - INFO - Recall: 0.9476
2025-05-03 06:11:27,462 - main - INFO - F1 Score: 0.9345
2025-05-03 06:11:27,462 - main - INFO - IoU: 0.8770
2025-05-03 06:11:27,462 - main - INFO - mAP: 0.9825
2025-05-03 06:11:27,462 - main - INFO - AUC: 0.9914
2025-05-03 06:11:27,462 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:11:27,462 - main - INFO - 
==================================================
2025-05-03 06:11:27,462 - main - INFO - Running configuration 539/756:
2025-05-03 06:11:27,462 - main - INFO - Model: DenseNet121
2025-05-03 06:11:27,462 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:11:27,462 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:11:27,462 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:11:27,462 - main - INFO - ==================================================
2025-05-03 06:11:27,462 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_539 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_539
2025-05-03 06:11:27,462 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_539 - INFO - Config: {
  "id": 539,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:11:27,593 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:11:27,593 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 539,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:11:27,593 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:11:27,766 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:11:27,768 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:11:27,772 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_539 - INFO - Starting model evaluation
2025-05-03 06:11:39,455 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_539 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9184
  Recall:    0.9441
  F1 Score:  0.9310
  IoU:       0.8710
  mAP:       0.9793
  AUC:       0.9919
2025-05-03 06:11:39,457 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_539 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_539/final_results.json
2025-05-03 06:11:39,458 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_539 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_539/final_results.json
2025-05-03 06:11:39,458 - main - INFO - 
Summary for configuration 539:
2025-05-03 06:11:39,458 - main - INFO - Accuracy: 0.9604
2025-05-03 06:11:39,458 - main - INFO - Precision: 0.9184
2025-05-03 06:11:39,458 - main - INFO - Recall: 0.9441
2025-05-03 06:11:39,458 - main - INFO - F1 Score: 0.9310
2025-05-03 06:11:39,458 - main - INFO - IoU: 0.8710
2025-05-03 06:11:39,458 - main - INFO - mAP: 0.9793
2025-05-03 06:11:39,458 - main - INFO - AUC: 0.9919
2025-05-03 06:11:39,458 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:11:39,458 - main - INFO - 
==================================================
2025-05-03 06:11:39,458 - main - INFO - Running configuration 540/756:
2025-05-03 06:11:39,458 - main - INFO - Model: DenseNet121
2025-05-03 06:11:39,458 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 06:11:39,458 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:11:39,458 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:11:39,458 - main - INFO - ==================================================
2025-05-03 06:11:39,459 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_540 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001_id_540
2025-05-03 06:11:39,459 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_540 - INFO - Config: {
  "id": 540,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:11:39,852 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_DenseNet121_opt_AdamW_lr_0.0001
2025-05-03 06:11:39,853 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 540,
  "model_name": "DenseNet121",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:11:39,853 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 06:11:40,025 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9897
2025-05-03 06:11:40,027 - training.model_DenseNet121_opt_AdamW_lr_0.0001 - INFO - Training completed after 1117.09 seconds
2025-05-03 06:11:40,031 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_540 - INFO - Starting model evaluation
2025-05-03 06:11:51,646 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_540 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9184
  Recall:    0.9441
  F1 Score:  0.9310
  IoU:       0.8710
  mAP:       0.9796
  AUC:       0.9907
2025-05-03 06:11:51,647 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_540 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_540/final_results.json
2025-05-03 06:11:51,649 - training.model_DenseNet121_opt_AdamW_lr_0.0001_id_540 - INFO - Final results saved to model_results_v2/model_DenseNet121_opt_AdamW_lr_0.0001_id_540/final_results.json
2025-05-03 06:11:51,649 - main - INFO - 
Summary for configuration 540:
2025-05-03 06:11:51,649 - main - INFO - Accuracy: 0.9604
2025-05-03 06:11:51,649 - main - INFO - Precision: 0.9184
2025-05-03 06:11:51,649 - main - INFO - Recall: 0.9441
2025-05-03 06:11:51,649 - main - INFO - F1 Score: 0.9310
2025-05-03 06:11:51,649 - main - INFO - IoU: 0.8710
2025-05-03 06:11:51,649 - main - INFO - mAP: 0.9796
2025-05-03 06:11:51,649 - main - INFO - AUC: 0.9907
2025-05-03 06:11:51,649 - main - INFO - Training time: 1117.09 seconds
2025-05-03 06:11:51,649 - main - INFO - 
==================================================
2025-05-03 06:11:51,649 - main - INFO - Running configuration 541/756:
2025-05-03 06:11:51,649 - main - INFO - Model: MobileNetV3
2025-05-03 06:11:51,649 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:11:51,649 - main - INFO - Scheduler: StepLR
2025-05-03 06:11:51,649 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:11:51,649 - main - INFO - ==================================================
2025-05-03 06:11:51,649 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_541 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_541
2025-05-03 06:11:51,649 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_541 - INFO - Config: {
  "id": 541,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:11:51,970 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:11:51,970 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 541,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:11:51,970 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-03 06:11:51,971 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 1/20
2025-05-03 06:12:11,528 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 06:12:39,937 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.9245
2025-05-03 06:12:39,953 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 1 completed in 47.98s - Train Loss: 0.4644, Train Acc: 0.8526, Val Loss: 0.3940, Val Acc: 0.9245
2025-05-03 06:12:39,991 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 06:12:39,991 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 2/20
2025-05-03 06:12:59,724 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 06:13:28,133 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9245 to 0.9485
2025-05-03 06:13:28,156 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 2 completed in 48.17s - Train Loss: 0.3675, Train Acc: 0.9459, Val Loss: 0.3633, Val Acc: 0.9485
2025-05-03 06:13:28,191 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 06:13:28,191 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 3/20
2025-05-03 06:13:48,286 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 06:14:16,202 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from inf to 0.3688
2025-05-03 06:14:16,225 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 3 completed in 48.03s - Train Loss: 0.3506, Train Acc: 0.9625, Val Loss: 0.3688, Val Acc: 0.9400
2025-05-03 06:14:16,262 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 06:14:16,262 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 4/20
2025-05-03 06:14:37,065 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 06:15:05,191 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9485 to 0.9554
2025-05-03 06:15:05,215 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 4 completed in 48.95s - Train Loss: 0.3446, Train Acc: 0.9691, Val Loss: 0.3575, Val Acc: 0.9554
2025-05-03 06:15:05,250 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 06:15:05,251 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 5/20
2025-05-03 06:15:26,108 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 06:15:54,262 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3688 to 0.3657
2025-05-03 06:15:54,286 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 5 completed in 49.03s - Train Loss: 0.3386, Train Acc: 0.9749, Val Loss: 0.3657, Val Acc: 0.9468
2025-05-03 06:15:54,321 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 06:15:54,321 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 6/20
2025-05-03 06:16:15,547 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 06:16:43,690 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 6 completed in 49.37s - Train Loss: 0.3316, Train Acc: 0.9828, Val Loss: 0.3687, Val Acc: 0.9434
2025-05-03 06:16:43,730 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 06:16:43,730 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 7/20
2025-05-03 06:17:03,929 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 06:17:32,802 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9554 to 0.9571
2025-05-03 06:17:32,824 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 7 completed in 49.09s - Train Loss: 0.3271, Train Acc: 0.9863, Val Loss: 0.3578, Val Acc: 0.9571
2025-05-03 06:17:32,862 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 06:17:32,863 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 8/20
2025-05-03 06:17:52,639 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 06:18:21,041 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3657 to 0.3608
2025-05-03 06:18:21,062 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 8 completed in 48.20s - Train Loss: 0.3292, Train Acc: 0.9841, Val Loss: 0.3608, Val Acc: 0.9528
2025-05-03 06:18:21,100 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 06:18:21,100 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 9/20
2025-05-03 06:18:41,599 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 06:19:09,453 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3608 to 0.3562
2025-05-03 06:19:09,480 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 9 completed in 48.38s - Train Loss: 0.3282, Train Acc: 0.9846, Val Loss: 0.3562, Val Acc: 0.9554
2025-05-03 06:19:09,516 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 06:19:09,517 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 10/20
2025-05-03 06:19:28,089 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 06:19:57,287 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9571 to 0.9708
2025-05-03 06:19:57,311 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 10 completed in 47.79s - Train Loss: 0.3264, Train Acc: 0.9871, Val Loss: 0.3428, Val Acc: 0.9708
2025-05-03 06:19:57,347 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 06:19:57,347 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 11/20
2025-05-03 06:20:17,273 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 06:20:45,851 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9708 to 0.9837
2025-05-03 06:20:45,874 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 11 completed in 48.53s - Train Loss: 0.3222, Train Acc: 0.9918, Val Loss: 0.3305, Val Acc: 0.9837
2025-05-03 06:20:45,912 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 06:20:45,913 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 12/20
2025-05-03 06:21:06,433 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 06:21:35,249 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3562 to 0.3312
2025-05-03 06:21:35,276 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 12 completed in 49.36s - Train Loss: 0.3192, Train Acc: 0.9944, Val Loss: 0.3312, Val Acc: 0.9811
2025-05-03 06:21:35,311 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 06:21:35,312 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 06:21:55,796 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 06:22:23,178 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9837 to 0.9846
2025-05-03 06:22:23,198 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 13 completed in 47.89s - Train Loss: 0.3190, Train Acc: 0.9940, Val Loss: 0.3276, Val Acc: 0.9846
2025-05-03 06:22:23,233 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 06:22:23,233 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 06:22:43,120 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 06:23:11,189 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3312 to 0.3294
2025-05-03 06:23:11,213 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 14 completed in 47.98s - Train Loss: 0.3173, Train Acc: 0.9959, Val Loss: 0.3294, Val Acc: 0.9828
2025-05-03 06:23:11,249 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 06:23:11,250 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 06:23:31,271 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 06:23:58,977 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9846 to 0.9863
2025-05-03 06:23:59,012 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 15 completed in 47.76s - Train Loss: 0.3169, Train Acc: 0.9974, Val Loss: 0.3275, Val Acc: 0.9863
2025-05-03 06:23:59,051 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 06:23:59,052 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 06:24:19,185 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 06:24:46,288 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3294 to 0.3289
2025-05-03 06:24:46,308 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 16 completed in 47.26s - Train Loss: 0.3174, Train Acc: 0.9966, Val Loss: 0.3289, Val Acc: 0.9828
2025-05-03 06:24:46,344 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 06:24:46,344 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 06:25:05,991 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 06:25:33,441 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3289 to 0.3274
2025-05-03 06:25:33,464 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 17 completed in 47.12s - Train Loss: 0.3162, Train Acc: 0.9976, Val Loss: 0.3274, Val Acc: 0.9837
2025-05-03 06:25:33,501 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 06:25:33,501 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 06:25:53,293 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 06:26:20,401 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3274 to 0.3264
2025-05-03 06:26:20,424 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 18 completed in 46.92s - Train Loss: 0.3169, Train Acc: 0.9961, Val Loss: 0.3264, Val Acc: 0.9854
2025-05-03 06:26:20,462 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 06:26:20,462 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 06:26:39,952 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 06:27:08,112 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 19 completed in 47.65s - Train Loss: 0.3160, Train Acc: 0.9979, Val Loss: 0.3271, Val Acc: 0.9863
2025-05-03 06:27:08,156 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 06:27:08,156 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 06:27:28,656 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 06:27:56,975 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Epoch 20 completed in 48.82s - Train Loss: 0.3159, Train Acc: 0.9979, Val Loss: 0.3278, Val Acc: 0.9846
2025-05-03 06:27:57,014 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 06:27:57,015 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:27:57,018 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_541 - INFO - Starting model evaluation
2025-05-03 06:28:05,810 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_541 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9406
  Recall:    0.9406
  F1 Score:  0.9406
  IoU:       0.8878
  mAP:       0.9828
  AUC:       0.9935
2025-05-03 06:28:05,811 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_541 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_541/final_results.json
2025-05-03 06:28:05,813 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_541 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_541/final_results.json
2025-05-03 06:28:05,813 - main - INFO - 
Summary for configuration 541:
2025-05-03 06:28:05,813 - main - INFO - Accuracy: 0.9663
2025-05-03 06:28:05,813 - main - INFO - Precision: 0.9406
2025-05-03 06:28:05,813 - main - INFO - Recall: 0.9406
2025-05-03 06:28:05,813 - main - INFO - F1 Score: 0.9406
2025-05-03 06:28:05,813 - main - INFO - IoU: 0.8878
2025-05-03 06:28:05,813 - main - INFO - mAP: 0.9828
2025-05-03 06:28:05,813 - main - INFO - AUC: 0.9935
2025-05-03 06:28:05,813 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:28:05,813 - main - INFO - 
==================================================
2025-05-03 06:28:05,813 - main - INFO - Running configuration 542/756:
2025-05-03 06:28:05,813 - main - INFO - Model: MobileNetV3
2025-05-03 06:28:05,813 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:28:05,813 - main - INFO - Scheduler: StepLR
2025-05-03 06:28:05,813 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:28:05,813 - main - INFO - ==================================================
2025-05-03 06:28:05,813 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_542 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_542
2025-05-03 06:28:05,813 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_542 - INFO - Config: {
  "id": 542,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:28:05,889 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:28:05,889 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 542,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:28:05,889 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:28:05,953 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:28:05,954 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:28:05,956 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_542 - INFO - Starting model evaluation
2025-05-03 06:28:14,733 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_542 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9404
  Recall:    0.9371
  F1 Score:  0.9387
  IoU:       0.8845
  mAP:       0.9818
  AUC:       0.9928
2025-05-03 06:28:14,735 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_542 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_542/final_results.json
2025-05-03 06:28:14,736 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_542 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_542/final_results.json
2025-05-03 06:28:14,736 - main - INFO - 
Summary for configuration 542:
2025-05-03 06:28:14,736 - main - INFO - Accuracy: 0.9653
2025-05-03 06:28:14,737 - main - INFO - Precision: 0.9404
2025-05-03 06:28:14,737 - main - INFO - Recall: 0.9371
2025-05-03 06:28:14,737 - main - INFO - F1 Score: 0.9387
2025-05-03 06:28:14,737 - main - INFO - IoU: 0.8845
2025-05-03 06:28:14,737 - main - INFO - mAP: 0.9818
2025-05-03 06:28:14,737 - main - INFO - AUC: 0.9928
2025-05-03 06:28:14,737 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:28:14,737 - main - INFO - 
==================================================
2025-05-03 06:28:14,737 - main - INFO - Running configuration 543/756:
2025-05-03 06:28:14,737 - main - INFO - Model: MobileNetV3
2025-05-03 06:28:14,737 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:28:14,737 - main - INFO - Scheduler: StepLR
2025-05-03 06:28:14,737 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:28:14,737 - main - INFO - ==================================================
2025-05-03 06:28:14,737 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_543 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_543
2025-05-03 06:28:14,737 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_543 - INFO - Config: {
  "id": 543,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:28:14,813 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:28:14,813 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 543,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:28:14,813 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:28:14,876 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:28:14,877 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:28:14,879 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_543 - INFO - Starting model evaluation
2025-05-03 06:28:24,001 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_543 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9401
  Recall:    0.9336
  F1 Score:  0.9368
  IoU:       0.8812
  mAP:       0.9812
  AUC:       0.9922
2025-05-03 06:28:24,003 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_543 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_543/final_results.json
2025-05-03 06:28:24,004 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_543 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_543/final_results.json
2025-05-03 06:28:24,004 - main - INFO - 
Summary for configuration 543:
2025-05-03 06:28:24,004 - main - INFO - Accuracy: 0.9643
2025-05-03 06:28:24,004 - main - INFO - Precision: 0.9401
2025-05-03 06:28:24,004 - main - INFO - Recall: 0.9336
2025-05-03 06:28:24,004 - main - INFO - F1 Score: 0.9368
2025-05-03 06:28:24,004 - main - INFO - IoU: 0.8812
2025-05-03 06:28:24,004 - main - INFO - mAP: 0.9812
2025-05-03 06:28:24,004 - main - INFO - AUC: 0.9922
2025-05-03 06:28:24,004 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:28:24,004 - main - INFO - 
==================================================
2025-05-03 06:28:24,004 - main - INFO - Running configuration 544/756:
2025-05-03 06:28:24,004 - main - INFO - Model: MobileNetV3
2025-05-03 06:28:24,004 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:28:24,004 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:28:24,004 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:28:24,004 - main - INFO - ==================================================
2025-05-03 06:28:24,005 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_544 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_544
2025-05-03 06:28:24,005 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_544 - INFO - Config: {
  "id": 544,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:28:24,080 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:28:24,080 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 544,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:28:24,080 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:28:24,144 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:28:24,144 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:28:24,147 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_544 - INFO - Starting model evaluation
2025-05-03 06:28:33,376 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_544 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9336
  Recall:    0.9336
  F1 Score:  0.9336
  IoU:       0.8754
  mAP:       0.9800
  AUC:       0.9919
2025-05-03 06:28:33,377 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_544 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_544/final_results.json
2025-05-03 06:28:33,379 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_544 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_544/final_results.json
2025-05-03 06:28:33,379 - main - INFO - 
Summary for configuration 544:
2025-05-03 06:28:33,379 - main - INFO - Accuracy: 0.9623
2025-05-03 06:28:33,379 - main - INFO - Precision: 0.9336
2025-05-03 06:28:33,379 - main - INFO - Recall: 0.9336
2025-05-03 06:28:33,379 - main - INFO - F1 Score: 0.9336
2025-05-03 06:28:33,379 - main - INFO - IoU: 0.8754
2025-05-03 06:28:33,379 - main - INFO - mAP: 0.9800
2025-05-03 06:28:33,379 - main - INFO - AUC: 0.9919
2025-05-03 06:28:33,379 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:28:33,379 - main - INFO - 
==================================================
2025-05-03 06:28:33,379 - main - INFO - Running configuration 545/756:
2025-05-03 06:28:33,379 - main - INFO - Model: MobileNetV3
2025-05-03 06:28:33,379 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:28:33,379 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:28:33,379 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:28:33,379 - main - INFO - ==================================================
2025-05-03 06:28:33,379 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_545 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_545
2025-05-03 06:28:33,379 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_545 - INFO - Config: {
  "id": 545,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:28:33,535 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:28:33,536 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 545,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:28:33,536 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:28:33,599 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:28:33,600 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:28:33,602 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_545 - INFO - Starting model evaluation
2025-05-03 06:28:42,387 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_545 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9435
  Recall:    0.9336
  F1 Score:  0.9385
  IoU:       0.8841
  mAP:       0.9810
  AUC:       0.9923
2025-05-03 06:28:42,389 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_545 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_545/final_results.json
2025-05-03 06:28:42,390 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_545 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_545/final_results.json
2025-05-03 06:28:42,390 - main - INFO - 
Summary for configuration 545:
2025-05-03 06:28:42,390 - main - INFO - Accuracy: 0.9653
2025-05-03 06:28:42,390 - main - INFO - Precision: 0.9435
2025-05-03 06:28:42,390 - main - INFO - Recall: 0.9336
2025-05-03 06:28:42,390 - main - INFO - F1 Score: 0.9385
2025-05-03 06:28:42,391 - main - INFO - IoU: 0.8841
2025-05-03 06:28:42,391 - main - INFO - mAP: 0.9810
2025-05-03 06:28:42,391 - main - INFO - AUC: 0.9923
2025-05-03 06:28:42,391 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:28:42,391 - main - INFO - 
==================================================
2025-05-03 06:28:42,391 - main - INFO - Running configuration 546/756:
2025-05-03 06:28:42,391 - main - INFO - Model: MobileNetV3
2025-05-03 06:28:42,391 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:28:42,391 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:28:42,391 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:28:42,391 - main - INFO - ==================================================
2025-05-03 06:28:42,391 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_546 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_546
2025-05-03 06:28:42,391 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_546 - INFO - Config: {
  "id": 546,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:28:42,465 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:28:42,465 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 546,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:28:42,465 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:28:42,529 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:28:42,530 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:28:42,532 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_546 - INFO - Starting model evaluation
2025-05-03 06:28:51,065 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_546 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9375
  Recall:    0.9441
  F1 Score:  0.9408
  IoU:       0.8882
  mAP:       0.9809
  AUC:       0.9923
2025-05-03 06:28:51,067 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_546 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_546/final_results.json
2025-05-03 06:28:51,068 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_546 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_546/final_results.json
2025-05-03 06:28:51,068 - main - INFO - 
Summary for configuration 546:
2025-05-03 06:28:51,068 - main - INFO - Accuracy: 0.9663
2025-05-03 06:28:51,068 - main - INFO - Precision: 0.9375
2025-05-03 06:28:51,069 - main - INFO - Recall: 0.9441
2025-05-03 06:28:51,069 - main - INFO - F1 Score: 0.9408
2025-05-03 06:28:51,069 - main - INFO - IoU: 0.8882
2025-05-03 06:28:51,069 - main - INFO - mAP: 0.9809
2025-05-03 06:28:51,069 - main - INFO - AUC: 0.9923
2025-05-03 06:28:51,069 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:28:51,069 - main - INFO - 
==================================================
2025-05-03 06:28:51,069 - main - INFO - Running configuration 547/756:
2025-05-03 06:28:51,069 - main - INFO - Model: MobileNetV3
2025-05-03 06:28:51,069 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:28:51,069 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:28:51,069 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:28:51,069 - main - INFO - ==================================================
2025-05-03 06:28:51,069 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_547 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_547
2025-05-03 06:28:51,069 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_547 - INFO - Config: {
  "id": 547,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:28:51,361 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:28:51,361 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 547,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:28:51,361 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:28:51,448 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:28:51,449 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:28:51,451 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_547 - INFO - Starting model evaluation
2025-05-03 06:29:00,217 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_547 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9406
  Recall:    0.9406
  F1 Score:  0.9406
  IoU:       0.8878
  mAP:       0.9813
  AUC:       0.9925
2025-05-03 06:29:00,218 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_547 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_547/final_results.json
2025-05-03 06:29:00,220 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_547 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_547/final_results.json
2025-05-03 06:29:00,220 - main - INFO - 
Summary for configuration 547:
2025-05-03 06:29:00,220 - main - INFO - Accuracy: 0.9663
2025-05-03 06:29:00,220 - main - INFO - Precision: 0.9406
2025-05-03 06:29:00,220 - main - INFO - Recall: 0.9406
2025-05-03 06:29:00,220 - main - INFO - F1 Score: 0.9406
2025-05-03 06:29:00,220 - main - INFO - IoU: 0.8878
2025-05-03 06:29:00,220 - main - INFO - mAP: 0.9813
2025-05-03 06:29:00,220 - main - INFO - AUC: 0.9925
2025-05-03 06:29:00,220 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:29:00,220 - main - INFO - 
==================================================
2025-05-03 06:29:00,220 - main - INFO - Running configuration 548/756:
2025-05-03 06:29:00,220 - main - INFO - Model: MobileNetV3
2025-05-03 06:29:00,220 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:29:00,220 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:29:00,220 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:29:00,220 - main - INFO - ==================================================
2025-05-03 06:29:00,220 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_548 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_548
2025-05-03 06:29:00,220 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_548 - INFO - Config: {
  "id": 548,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:29:00,294 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:29:00,294 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 548,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:29:00,294 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:29:00,358 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:29:00,359 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:29:00,361 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_548 - INFO - Starting model evaluation
2025-05-03 06:29:09,758 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_548 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9444
  Recall:    0.9510
  F1 Score:  0.9477
  IoU:       0.9007
  mAP:       0.9810
  AUC:       0.9919
2025-05-03 06:29:09,759 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_548 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_548/final_results.json
2025-05-03 06:29:09,761 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_548 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_548/final_results.json
2025-05-03 06:29:09,761 - main - INFO - 
Summary for configuration 548:
2025-05-03 06:29:09,761 - main - INFO - Accuracy: 0.9703
2025-05-03 06:29:09,761 - main - INFO - Precision: 0.9444
2025-05-03 06:29:09,761 - main - INFO - Recall: 0.9510
2025-05-03 06:29:09,761 - main - INFO - F1 Score: 0.9477
2025-05-03 06:29:09,761 - main - INFO - IoU: 0.9007
2025-05-03 06:29:09,761 - main - INFO - mAP: 0.9810
2025-05-03 06:29:09,761 - main - INFO - AUC: 0.9919
2025-05-03 06:29:09,761 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:29:09,761 - main - INFO - 
==================================================
2025-05-03 06:29:09,761 - main - INFO - Running configuration 549/756:
2025-05-03 06:29:09,761 - main - INFO - Model: MobileNetV3
2025-05-03 06:29:09,761 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:29:09,761 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:29:09,761 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:29:09,761 - main - INFO - ==================================================
2025-05-03 06:29:09,761 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_549 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_549
2025-05-03 06:29:09,761 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_549 - INFO - Config: {
  "id": 549,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:29:09,836 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:29:09,836 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 549,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:29:09,836 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:29:09,900 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:29:09,901 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:29:09,903 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_549 - INFO - Starting model evaluation
2025-05-03 06:29:18,785 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_549 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9375
  Recall:    0.9441
  F1 Score:  0.9408
  IoU:       0.8882
  mAP:       0.9818
  AUC:       0.9926
2025-05-03 06:29:18,787 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_549 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_549/final_results.json
2025-05-03 06:29:18,788 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_549 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_549/final_results.json
2025-05-03 06:29:18,788 - main - INFO - 
Summary for configuration 549:
2025-05-03 06:29:18,788 - main - INFO - Accuracy: 0.9663
2025-05-03 06:29:18,788 - main - INFO - Precision: 0.9375
2025-05-03 06:29:18,788 - main - INFO - Recall: 0.9441
2025-05-03 06:29:18,788 - main - INFO - F1 Score: 0.9408
2025-05-03 06:29:18,788 - main - INFO - IoU: 0.8882
2025-05-03 06:29:18,788 - main - INFO - mAP: 0.9818
2025-05-03 06:29:18,788 - main - INFO - AUC: 0.9926
2025-05-03 06:29:18,788 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:29:18,788 - main - INFO - 
==================================================
2025-05-03 06:29:18,788 - main - INFO - Running configuration 550/756:
2025-05-03 06:29:18,788 - main - INFO - Model: MobileNetV3
2025-05-03 06:29:18,788 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:29:18,788 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:29:18,788 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:29:18,788 - main - INFO - ==================================================
2025-05-03 06:29:18,789 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_550 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_550
2025-05-03 06:29:18,789 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_550 - INFO - Config: {
  "id": 550,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:29:18,994 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:29:18,994 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 550,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:29:18,994 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:29:19,174 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:29:19,175 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:29:19,177 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_550 - INFO - Starting model evaluation
2025-05-03 06:29:27,979 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_550 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9476
  Recall:    0.9476
  F1 Score:  0.9476
  IoU:       0.9003
  mAP:       0.9817
  AUC:       0.9931
2025-05-03 06:29:27,981 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_550 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_550/final_results.json
2025-05-03 06:29:27,982 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_550 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_550/final_results.json
2025-05-03 06:29:27,982 - main - INFO - 
Summary for configuration 550:
2025-05-03 06:29:27,982 - main - INFO - Accuracy: 0.9703
2025-05-03 06:29:27,982 - main - INFO - Precision: 0.9476
2025-05-03 06:29:27,982 - main - INFO - Recall: 0.9476
2025-05-03 06:29:27,983 - main - INFO - F1 Score: 0.9476
2025-05-03 06:29:27,983 - main - INFO - IoU: 0.9003
2025-05-03 06:29:27,983 - main - INFO - mAP: 0.9817
2025-05-03 06:29:27,983 - main - INFO - AUC: 0.9931
2025-05-03 06:29:27,983 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:29:27,983 - main - INFO - 
==================================================
2025-05-03 06:29:27,983 - main - INFO - Running configuration 551/756:
2025-05-03 06:29:27,983 - main - INFO - Model: MobileNetV3
2025-05-03 06:29:27,983 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:29:27,983 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:29:27,983 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:29:27,983 - main - INFO - ==================================================
2025-05-03 06:29:27,983 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_551 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_551
2025-05-03 06:29:27,983 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_551 - INFO - Config: {
  "id": 551,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:29:28,056 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:29:28,056 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 551,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:29:28,057 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:29:28,120 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:29:28,121 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:29:28,123 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_551 - INFO - Starting model evaluation
2025-05-03 06:29:37,430 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_551 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9439
  Recall:    0.9406
  F1 Score:  0.9422
  IoU:       0.8907
  mAP:       0.9815
  AUC:       0.9926
2025-05-03 06:29:37,431 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_551 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_551/final_results.json
2025-05-03 06:29:37,433 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_551 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_551/final_results.json
2025-05-03 06:29:37,433 - main - INFO - 
Summary for configuration 551:
2025-05-03 06:29:37,433 - main - INFO - Accuracy: 0.9673
2025-05-03 06:29:37,433 - main - INFO - Precision: 0.9439
2025-05-03 06:29:37,433 - main - INFO - Recall: 0.9406
2025-05-03 06:29:37,433 - main - INFO - F1 Score: 0.9422
2025-05-03 06:29:37,433 - main - INFO - IoU: 0.8907
2025-05-03 06:29:37,433 - main - INFO - mAP: 0.9815
2025-05-03 06:29:37,433 - main - INFO - AUC: 0.9926
2025-05-03 06:29:37,433 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:29:37,433 - main - INFO - 
==================================================
2025-05-03 06:29:37,433 - main - INFO - Running configuration 552/756:
2025-05-03 06:29:37,433 - main - INFO - Model: MobileNetV3
2025-05-03 06:29:37,433 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 06:29:37,433 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:29:37,433 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:29:37,433 - main - INFO - ==================================================
2025-05-03 06:29:37,433 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_552 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01_id_552
2025-05-03 06:29:37,433 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_552 - INFO - Config: {
  "id": 552,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:29:37,507 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.01
2025-05-03 06:29:37,507 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 552,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:29:37,507 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 06:29:37,571 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 06:29:37,572 - training.model_MobileNetV3_opt_SGD_lr_0.01 - INFO - Training completed after 965.00 seconds
2025-05-03 06:29:37,574 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_552 - INFO - Starting model evaluation
2025-05-03 06:29:46,572 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_552 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9406
  Recall:    0.9406
  F1 Score:  0.9406
  IoU:       0.8878
  mAP:       0.9812
  AUC:       0.9922
2025-05-03 06:29:46,574 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_552 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_552/final_results.json
2025-05-03 06:29:46,575 - training.model_MobileNetV3_opt_SGD_lr_0.01_id_552 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.01_id_552/final_results.json
2025-05-03 06:29:46,575 - main - INFO - 
Summary for configuration 552:
2025-05-03 06:29:46,575 - main - INFO - Accuracy: 0.9663
2025-05-03 06:29:46,575 - main - INFO - Precision: 0.9406
2025-05-03 06:29:46,575 - main - INFO - Recall: 0.9406
2025-05-03 06:29:46,575 - main - INFO - F1 Score: 0.9406
2025-05-03 06:29:46,575 - main - INFO - IoU: 0.8878
2025-05-03 06:29:46,575 - main - INFO - mAP: 0.9812
2025-05-03 06:29:46,575 - main - INFO - AUC: 0.9922
2025-05-03 06:29:46,576 - main - INFO - Training time: 965.00 seconds
2025-05-03 06:29:46,576 - main - INFO - 
==================================================
2025-05-03 06:29:46,576 - main - INFO - Running configuration 553/756:
2025-05-03 06:29:46,576 - main - INFO - Model: MobileNetV3
2025-05-03 06:29:46,576 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:29:46,576 - main - INFO - Scheduler: StepLR
2025-05-03 06:29:46,576 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:29:46,576 - main - INFO - ==================================================
2025-05-03 06:29:46,576 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_553 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_553
2025-05-03 06:29:46,576 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_553 - INFO - Config: {
  "id": 553,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:29:46,650 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:29:46,650 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 553,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:29:46,650 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-03 06:29:46,651 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 1/20
2025-05-03 06:30:07,011 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 06:30:36,017 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.8002
2025-05-03 06:30:36,039 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 1 completed in 49.39s - Train Loss: 0.6136, Train Acc: 0.7136, Val Loss: 0.5516, Val Acc: 0.8002
2025-05-03 06:30:36,077 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 06:30:36,077 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 2/20
2025-05-03 06:30:55,902 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 06:31:25,288 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.8002 to 0.8611
2025-05-03 06:31:25,311 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 2 completed in 49.23s - Train Loss: 0.5074, Train Acc: 0.8299, Val Loss: 0.4730, Val Acc: 0.8611
2025-05-03 06:31:25,346 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 06:31:25,346 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 3/20
2025-05-03 06:31:45,579 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 06:32:14,177 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.8611 to 0.9022
2025-05-03 06:32:14,199 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 3 completed in 48.85s - Train Loss: 0.4494, Train Acc: 0.8810, Val Loss: 0.4290, Val Acc: 0.9022
2025-05-03 06:32:14,235 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 06:32:14,235 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 4/20
2025-05-03 06:32:34,255 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 06:33:02,949 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9022 to 0.9194
2025-05-03 06:33:02,986 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 4 completed in 48.75s - Train Loss: 0.4159, Train Acc: 0.9097, Val Loss: 0.4024, Val Acc: 0.9194
2025-05-03 06:33:03,022 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 06:33:03,022 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 5/20
2025-05-03 06:33:23,014 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 06:33:50,858 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9194 to 0.9468
2025-05-03 06:33:50,881 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 5 completed in 47.86s - Train Loss: 0.3922, Train Acc: 0.9346, Val Loss: 0.3837, Val Acc: 0.9468
2025-05-03 06:33:50,915 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 06:33:50,915 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 6/20
2025-05-03 06:34:11,541 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 06:34:40,309 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9468 to 0.9485
2025-05-03 06:34:40,331 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 6 completed in 49.42s - Train Loss: 0.3743, Train Acc: 0.9511, Val Loss: 0.3711, Val Acc: 0.9485
2025-05-03 06:34:40,370 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 06:34:40,371 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 7/20
2025-05-03 06:35:00,918 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 06:35:30,596 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9485 to 0.9623
2025-05-03 06:35:30,618 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 7 completed in 50.25s - Train Loss: 0.3603, Train Acc: 0.9629, Val Loss: 0.3601, Val Acc: 0.9623
2025-05-03 06:35:30,657 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 06:35:30,657 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 8/20
2025-05-03 06:35:51,798 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 06:36:21,354 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9623 to 0.9631
2025-05-03 06:36:21,386 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 8 completed in 50.73s - Train Loss: 0.3524, Train Acc: 0.9687, Val Loss: 0.3547, Val Acc: 0.9631
2025-05-03 06:36:21,425 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 06:36:21,425 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 9/20
2025-05-03 06:36:41,529 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 06:37:10,494 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9631 to 0.9666
2025-05-03 06:37:10,516 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 9 completed in 49.09s - Train Loss: 0.3480, Train Acc: 0.9730, Val Loss: 0.3472, Val Acc: 0.9666
2025-05-03 06:37:10,551 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 06:37:10,551 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 10/20
2025-05-03 06:37:31,963 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 06:38:00,031 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9666 to 0.9726
2025-05-03 06:38:00,058 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 10 completed in 49.51s - Train Loss: 0.3404, Train Acc: 0.9813, Val Loss: 0.3445, Val Acc: 0.9726
2025-05-03 06:38:00,098 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 06:38:00,098 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 11/20
2025-05-03 06:38:20,517 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 06:38:49,820 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation loss improved from inf to 0.3434
2025-05-03 06:38:49,855 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 11 completed in 49.76s - Train Loss: 0.3362, Train Acc: 0.9824, Val Loss: 0.3434, Val Acc: 0.9674
2025-05-03 06:38:49,893 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 06:38:49,893 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 12/20
2025-05-03 06:39:10,319 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 06:39:39,442 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3434 to 0.3426
2025-05-03 06:39:39,466 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 12 completed in 49.57s - Train Loss: 0.3365, Train Acc: 0.9811, Val Loss: 0.3426, Val Acc: 0.9708
2025-05-03 06:39:39,504 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 06:39:39,504 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 06:39:59,730 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 06:40:29,752 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3426 to 0.3422
2025-05-03 06:40:29,775 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 13 completed in 50.27s - Train Loss: 0.3373, Train Acc: 0.9820, Val Loss: 0.3422, Val Acc: 0.9726
2025-05-03 06:40:29,810 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 06:40:29,810 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 06:40:50,787 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 06:41:20,460 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 14 completed in 50.65s - Train Loss: 0.3355, Train Acc: 0.9841, Val Loss: 0.3435, Val Acc: 0.9683
2025-05-03 06:41:20,506 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 06:41:20,507 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 06:41:41,053 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 06:42:10,643 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9726 to 0.9743
2025-05-03 06:42:10,667 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 15 completed in 50.16s - Train Loss: 0.3345, Train Acc: 0.9841, Val Loss: 0.3392, Val Acc: 0.9743
2025-05-03 06:42:10,704 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 06:42:10,705 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 06:42:31,780 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 06:43:01,960 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3422 to 0.3392
2025-05-03 06:43:01,984 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 16 completed in 51.28s - Train Loss: 0.3335, Train Acc: 0.9856, Val Loss: 0.3392, Val Acc: 0.9743
2025-05-03 06:43:02,024 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 06:43:02,025 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 06:43:23,303 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 06:43:53,115 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 17 completed in 51.09s - Train Loss: 0.3334, Train Acc: 0.9861, Val Loss: 0.3400, Val Acc: 0.9734
2025-05-03 06:43:53,156 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 06:43:53,157 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 06:44:14,339 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 06:44:44,581 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9743 to 0.9760
2025-05-03 06:44:44,602 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 18 completed in 51.45s - Train Loss: 0.3319, Train Acc: 0.9869, Val Loss: 0.3377, Val Acc: 0.9760
2025-05-03 06:44:44,642 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 06:44:44,642 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 06:45:05,077 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 06:45:34,048 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 19 completed in 49.41s - Train Loss: 0.3331, Train Acc: 0.9852, Val Loss: 0.3392, Val Acc: 0.9751
2025-05-03 06:45:34,086 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 06:45:34,087 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 06:45:54,768 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 06:46:24,290 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3392 to 0.3389
2025-05-03 06:46:24,311 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Epoch 20 completed in 50.22s - Train Loss: 0.3344, Train Acc: 0.9835, Val Loss: 0.3389, Val Acc: 0.9760
2025-05-03 06:46:24,348 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 06:46:24,348 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:46:24,351 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_553 - INFO - Starting model evaluation
2025-05-03 06:46:33,479 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_553 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.9091
  Recall:    0.9441
  F1 Score:  0.9262
  IoU:       0.8626
  mAP:       0.9807
  AUC:       0.9895
2025-05-03 06:46:33,481 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_553 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_553/final_results.json
2025-05-03 06:46:33,482 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_553 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_553/final_results.json
2025-05-03 06:46:33,482 - main - INFO - 
Summary for configuration 553:
2025-05-03 06:46:33,482 - main - INFO - Accuracy: 0.9574
2025-05-03 06:46:33,482 - main - INFO - Precision: 0.9091
2025-05-03 06:46:33,482 - main - INFO - Recall: 0.9441
2025-05-03 06:46:33,482 - main - INFO - F1 Score: 0.9262
2025-05-03 06:46:33,482 - main - INFO - IoU: 0.8626
2025-05-03 06:46:33,482 - main - INFO - mAP: 0.9807
2025-05-03 06:46:33,482 - main - INFO - AUC: 0.9895
2025-05-03 06:46:33,482 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:46:33,482 - main - INFO - 
==================================================
2025-05-03 06:46:33,482 - main - INFO - Running configuration 554/756:
2025-05-03 06:46:33,482 - main - INFO - Model: MobileNetV3
2025-05-03 06:46:33,482 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:46:33,482 - main - INFO - Scheduler: StepLR
2025-05-03 06:46:33,482 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:46:33,482 - main - INFO - ==================================================
2025-05-03 06:46:33,483 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_554 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_554
2025-05-03 06:46:33,483 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_554 - INFO - Config: {
  "id": 554,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:46:33,573 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:46:33,573 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 554,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:46:33,573 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:46:33,637 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:46:33,638 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:46:33,641 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_554 - INFO - Starting model evaluation
2025-05-03 06:46:42,570 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_554 - INFO - Evaluation metrics:
  Accuracy:  0.9504
  Precision: 0.8960
  Recall:    0.9336
  F1 Score:  0.9144
  IoU:       0.8423
  mAP:       0.9768
  AUC:       0.9866
2025-05-03 06:46:42,572 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_554 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_554/final_results.json
2025-05-03 06:46:42,573 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_554 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_554/final_results.json
2025-05-03 06:46:42,574 - main - INFO - 
Summary for configuration 554:
2025-05-03 06:46:42,574 - main - INFO - Accuracy: 0.9504
2025-05-03 06:46:42,574 - main - INFO - Precision: 0.8960
2025-05-03 06:46:42,574 - main - INFO - Recall: 0.9336
2025-05-03 06:46:42,574 - main - INFO - F1 Score: 0.9144
2025-05-03 06:46:42,574 - main - INFO - IoU: 0.8423
2025-05-03 06:46:42,574 - main - INFO - mAP: 0.9768
2025-05-03 06:46:42,574 - main - INFO - AUC: 0.9866
2025-05-03 06:46:42,574 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:46:42,574 - main - INFO - 
==================================================
2025-05-03 06:46:42,574 - main - INFO - Running configuration 555/756:
2025-05-03 06:46:42,574 - main - INFO - Model: MobileNetV3
2025-05-03 06:46:42,574 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:46:42,574 - main - INFO - Scheduler: StepLR
2025-05-03 06:46:42,574 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:46:42,574 - main - INFO - ==================================================
2025-05-03 06:46:42,574 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_555 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_555
2025-05-03 06:46:42,574 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_555 - INFO - Config: {
  "id": 555,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:46:42,825 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:46:42,825 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 555,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:46:42,826 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:46:42,889 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:46:42,890 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:46:42,892 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_555 - INFO - Starting model evaluation
2025-05-03 06:46:52,071 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_555 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9054
  Recall:    0.9371
  F1 Score:  0.9210
  IoU:       0.8535
  mAP:       0.9782
  AUC:       0.9878
2025-05-03 06:46:52,075 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_555 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_555/final_results.json
2025-05-03 06:46:52,076 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_555 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_555/final_results.json
2025-05-03 06:46:52,076 - main - INFO - 
Summary for configuration 555:
2025-05-03 06:46:52,076 - main - INFO - Accuracy: 0.9544
2025-05-03 06:46:52,076 - main - INFO - Precision: 0.9054
2025-05-03 06:46:52,076 - main - INFO - Recall: 0.9371
2025-05-03 06:46:52,076 - main - INFO - F1 Score: 0.9210
2025-05-03 06:46:52,076 - main - INFO - IoU: 0.8535
2025-05-03 06:46:52,077 - main - INFO - mAP: 0.9782
2025-05-03 06:46:52,077 - main - INFO - AUC: 0.9878
2025-05-03 06:46:52,077 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:46:52,077 - main - INFO - 
==================================================
2025-05-03 06:46:52,077 - main - INFO - Running configuration 556/756:
2025-05-03 06:46:52,077 - main - INFO - Model: MobileNetV3
2025-05-03 06:46:52,077 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:46:52,077 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:46:52,077 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:46:52,077 - main - INFO - ==================================================
2025-05-03 06:46:52,077 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_556 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_556
2025-05-03 06:46:52,077 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_556 - INFO - Config: {
  "id": 556,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:46:52,313 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:46:52,313 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 556,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:46:52,313 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:46:52,377 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:46:52,378 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:46:52,380 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_556 - INFO - Starting model evaluation
2025-05-03 06:47:01,796 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_556 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.9020
  Recall:    0.9336
  F1 Score:  0.9175
  IoU:       0.8476
  mAP:       0.9780
  AUC:       0.9873
2025-05-03 06:47:01,798 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_556 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_556/final_results.json
2025-05-03 06:47:01,800 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_556 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_556/final_results.json
2025-05-03 06:47:01,800 - main - INFO - 
Summary for configuration 556:
2025-05-03 06:47:01,800 - main - INFO - Accuracy: 0.9524
2025-05-03 06:47:01,800 - main - INFO - Precision: 0.9020
2025-05-03 06:47:01,800 - main - INFO - Recall: 0.9336
2025-05-03 06:47:01,800 - main - INFO - F1 Score: 0.9175
2025-05-03 06:47:01,800 - main - INFO - IoU: 0.8476
2025-05-03 06:47:01,800 - main - INFO - mAP: 0.9780
2025-05-03 06:47:01,800 - main - INFO - AUC: 0.9873
2025-05-03 06:47:01,800 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:47:01,800 - main - INFO - 
==================================================
2025-05-03 06:47:01,800 - main - INFO - Running configuration 557/756:
2025-05-03 06:47:01,800 - main - INFO - Model: MobileNetV3
2025-05-03 06:47:01,800 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:47:01,800 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:47:01,800 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:47:01,800 - main - INFO - ==================================================
2025-05-03 06:47:01,800 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_557 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_557
2025-05-03 06:47:01,800 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_557 - INFO - Config: {
  "id": 557,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:47:01,964 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:47:01,964 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 557,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:47:01,964 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:47:02,028 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:47:02,029 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:47:02,031 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_557 - INFO - Starting model evaluation
2025-05-03 06:47:11,150 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_557 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.9085
  Recall:    0.9371
  F1 Score:  0.9225
  IoU:       0.8562
  mAP:       0.9804
  AUC:       0.9890
2025-05-03 06:47:11,152 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_557 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_557/final_results.json
2025-05-03 06:47:11,153 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_557 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_557/final_results.json
2025-05-03 06:47:11,153 - main - INFO - 
Summary for configuration 557:
2025-05-03 06:47:11,153 - main - INFO - Accuracy: 0.9554
2025-05-03 06:47:11,153 - main - INFO - Precision: 0.9085
2025-05-03 06:47:11,153 - main - INFO - Recall: 0.9371
2025-05-03 06:47:11,153 - main - INFO - F1 Score: 0.9225
2025-05-03 06:47:11,154 - main - INFO - IoU: 0.8562
2025-05-03 06:47:11,154 - main - INFO - mAP: 0.9804
2025-05-03 06:47:11,154 - main - INFO - AUC: 0.9890
2025-05-03 06:47:11,154 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:47:11,154 - main - INFO - 
==================================================
2025-05-03 06:47:11,154 - main - INFO - Running configuration 558/756:
2025-05-03 06:47:11,154 - main - INFO - Model: MobileNetV3
2025-05-03 06:47:11,154 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:47:11,154 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 06:47:11,154 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:47:11,154 - main - INFO - ==================================================
2025-05-03 06:47:11,154 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_558 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_558
2025-05-03 06:47:11,154 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_558 - INFO - Config: {
  "id": 558,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:47:11,323 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:47:11,323 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 558,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:47:11,324 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:47:11,387 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:47:11,388 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:47:11,390 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_558 - INFO - Starting model evaluation
2025-05-03 06:47:20,570 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_558 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9027
  Recall:    0.9406
  F1 Score:  0.9212
  IoU:       0.8540
  mAP:       0.9776
  AUC:       0.9876
2025-05-03 06:47:20,572 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_558 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_558/final_results.json
2025-05-03 06:47:20,574 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_558 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_558/final_results.json
2025-05-03 06:47:20,574 - main - INFO - 
Summary for configuration 558:
2025-05-03 06:47:20,574 - main - INFO - Accuracy: 0.9544
2025-05-03 06:47:20,574 - main - INFO - Precision: 0.9027
2025-05-03 06:47:20,574 - main - INFO - Recall: 0.9406
2025-05-03 06:47:20,574 - main - INFO - F1 Score: 0.9212
2025-05-03 06:47:20,574 - main - INFO - IoU: 0.8540
2025-05-03 06:47:20,574 - main - INFO - mAP: 0.9776
2025-05-03 06:47:20,574 - main - INFO - AUC: 0.9876
2025-05-03 06:47:20,574 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:47:20,574 - main - INFO - 
==================================================
2025-05-03 06:47:20,574 - main - INFO - Running configuration 559/756:
2025-05-03 06:47:20,574 - main - INFO - Model: MobileNetV3
2025-05-03 06:47:20,574 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:47:20,574 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:47:20,574 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:47:20,574 - main - INFO - ==================================================
2025-05-03 06:47:20,574 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_559 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_559
2025-05-03 06:47:20,574 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_559 - INFO - Config: {
  "id": 559,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:47:20,820 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:47:20,820 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 559,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:47:20,820 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:47:20,883 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:47:20,884 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:47:20,886 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_559 - INFO - Starting model evaluation
2025-05-03 06:47:30,298 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_559 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.9085
  Recall:    0.9371
  F1 Score:  0.9225
  IoU:       0.8562
  mAP:       0.9800
  AUC:       0.9891
2025-05-03 06:47:30,300 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_559 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_559/final_results.json
2025-05-03 06:47:30,301 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_559 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_559/final_results.json
2025-05-03 06:47:30,301 - main - INFO - 
Summary for configuration 559:
2025-05-03 06:47:30,301 - main - INFO - Accuracy: 0.9554
2025-05-03 06:47:30,301 - main - INFO - Precision: 0.9085
2025-05-03 06:47:30,301 - main - INFO - Recall: 0.9371
2025-05-03 06:47:30,301 - main - INFO - F1 Score: 0.9225
2025-05-03 06:47:30,301 - main - INFO - IoU: 0.8562
2025-05-03 06:47:30,301 - main - INFO - mAP: 0.9800
2025-05-03 06:47:30,301 - main - INFO - AUC: 0.9891
2025-05-03 06:47:30,301 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:47:30,301 - main - INFO - 
==================================================
2025-05-03 06:47:30,301 - main - INFO - Running configuration 560/756:
2025-05-03 06:47:30,301 - main - INFO - Model: MobileNetV3
2025-05-03 06:47:30,301 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:47:30,301 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:47:30,301 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:47:30,301 - main - INFO - ==================================================
2025-05-03 06:47:30,302 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_560 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_560
2025-05-03 06:47:30,302 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_560 - INFO - Config: {
  "id": 560,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:47:30,560 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:47:30,560 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 560,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:47:30,560 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:47:30,624 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:47:30,625 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:47:30,627 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_560 - INFO - Starting model evaluation
2025-05-03 06:47:39,797 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_560 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.9085
  Recall:    0.9371
  F1 Score:  0.9225
  IoU:       0.8562
  mAP:       0.9765
  AUC:       0.9862
2025-05-03 06:47:39,798 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_560 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_560/final_results.json
2025-05-03 06:47:39,800 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_560 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_560/final_results.json
2025-05-03 06:47:39,800 - main - INFO - 
Summary for configuration 560:
2025-05-03 06:47:39,800 - main - INFO - Accuracy: 0.9554
2025-05-03 06:47:39,800 - main - INFO - Precision: 0.9085
2025-05-03 06:47:39,800 - main - INFO - Recall: 0.9371
2025-05-03 06:47:39,800 - main - INFO - F1 Score: 0.9225
2025-05-03 06:47:39,800 - main - INFO - IoU: 0.8562
2025-05-03 06:47:39,800 - main - INFO - mAP: 0.9765
2025-05-03 06:47:39,800 - main - INFO - AUC: 0.9862
2025-05-03 06:47:39,800 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:47:39,800 - main - INFO - 
==================================================
2025-05-03 06:47:39,800 - main - INFO - Running configuration 561/756:
2025-05-03 06:47:39,800 - main - INFO - Model: MobileNetV3
2025-05-03 06:47:39,800 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:47:39,800 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 06:47:39,800 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:47:39,800 - main - INFO - ==================================================
2025-05-03 06:47:39,800 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_561 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_561
2025-05-03 06:47:39,800 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_561 - INFO - Config: {
  "id": 561,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:47:39,995 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:47:39,995 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 561,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:47:39,995 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:47:40,059 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:47:40,060 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:47:40,062 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_561 - INFO - Starting model evaluation
2025-05-03 06:47:49,511 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_561 - INFO - Evaluation metrics:
  Accuracy:  0.9534
  Precision: 0.9078
  Recall:    0.9301
  F1 Score:  0.9188
  IoU:       0.8498
  mAP:       0.9791
  AUC:       0.9894
2025-05-03 06:47:49,513 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_561 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_561/final_results.json
2025-05-03 06:47:49,515 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_561 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_561/final_results.json
2025-05-03 06:47:49,515 - main - INFO - 
Summary for configuration 561:
2025-05-03 06:47:49,515 - main - INFO - Accuracy: 0.9534
2025-05-03 06:47:49,515 - main - INFO - Precision: 0.9078
2025-05-03 06:47:49,515 - main - INFO - Recall: 0.9301
2025-05-03 06:47:49,515 - main - INFO - F1 Score: 0.9188
2025-05-03 06:47:49,515 - main - INFO - IoU: 0.8498
2025-05-03 06:47:49,515 - main - INFO - mAP: 0.9791
2025-05-03 06:47:49,515 - main - INFO - AUC: 0.9894
2025-05-03 06:47:49,515 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:47:49,515 - main - INFO - 
==================================================
2025-05-03 06:47:49,515 - main - INFO - Running configuration 562/756:
2025-05-03 06:47:49,515 - main - INFO - Model: MobileNetV3
2025-05-03 06:47:49,515 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:47:49,515 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:47:49,515 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:47:49,515 - main - INFO - ==================================================
2025-05-03 06:47:49,515 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_562 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_562
2025-05-03 06:47:49,515 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_562 - INFO - Config: {
  "id": 562,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:47:49,589 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:47:49,590 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 562,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:47:49,590 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:47:49,909 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:47:49,910 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:47:49,914 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_562 - INFO - Starting model evaluation
2025-05-03 06:47:58,987 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_562 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.9048
  Recall:    0.9301
  F1 Score:  0.9172
  IoU:       0.8471
  mAP:       0.9781
  AUC:       0.9885
2025-05-03 06:47:58,988 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_562 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_562/final_results.json
2025-05-03 06:47:58,990 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_562 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_562/final_results.json
2025-05-03 06:47:58,990 - main - INFO - 
Summary for configuration 562:
2025-05-03 06:47:58,990 - main - INFO - Accuracy: 0.9524
2025-05-03 06:47:58,990 - main - INFO - Precision: 0.9048
2025-05-03 06:47:58,990 - main - INFO - Recall: 0.9301
2025-05-03 06:47:58,990 - main - INFO - F1 Score: 0.9172
2025-05-03 06:47:58,990 - main - INFO - IoU: 0.8471
2025-05-03 06:47:58,990 - main - INFO - mAP: 0.9781
2025-05-03 06:47:58,990 - main - INFO - AUC: 0.9885
2025-05-03 06:47:58,990 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:47:58,990 - main - INFO - 
==================================================
2025-05-03 06:47:58,990 - main - INFO - Running configuration 563/756:
2025-05-03 06:47:58,990 - main - INFO - Model: MobileNetV3
2025-05-03 06:47:58,990 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:47:58,990 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:47:58,990 - main - INFO - Loss Function: FocalLoss
2025-05-03 06:47:58,990 - main - INFO - ==================================================
2025-05-03 06:47:58,990 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_563 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_563
2025-05-03 06:47:58,990 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_563 - INFO - Config: {
  "id": 563,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:47:59,112 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:47:59,112 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 563,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 06:47:59,112 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:47:59,175 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:47:59,176 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:47:59,178 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_563 - INFO - Starting model evaluation
2025-05-03 06:48:08,483 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_563 - INFO - Evaluation metrics:
  Accuracy:  0.9514
  Precision: 0.9072
  Recall:    0.9231
  F1 Score:  0.9151
  IoU:       0.8435
  mAP:       0.9767
  AUC:       0.9871
2025-05-03 06:48:08,485 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_563 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_563/final_results.json
2025-05-03 06:48:08,487 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_563 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_563/final_results.json
2025-05-03 06:48:08,487 - main - INFO - 
Summary for configuration 563:
2025-05-03 06:48:08,487 - main - INFO - Accuracy: 0.9514
2025-05-03 06:48:08,487 - main - INFO - Precision: 0.9072
2025-05-03 06:48:08,487 - main - INFO - Recall: 0.9231
2025-05-03 06:48:08,487 - main - INFO - F1 Score: 0.9151
2025-05-03 06:48:08,487 - main - INFO - IoU: 0.8435
2025-05-03 06:48:08,487 - main - INFO - mAP: 0.9767
2025-05-03 06:48:08,487 - main - INFO - AUC: 0.9871
2025-05-03 06:48:08,487 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:48:08,487 - main - INFO - 
==================================================
2025-05-03 06:48:08,487 - main - INFO - Running configuration 564/756:
2025-05-03 06:48:08,487 - main - INFO - Model: MobileNetV3
2025-05-03 06:48:08,487 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 06:48:08,487 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 06:48:08,487 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 06:48:08,487 - main - INFO - ==================================================
2025-05-03 06:48:08,487 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_564 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001_id_564
2025-05-03 06:48:08,487 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_564 - INFO - Config: {
  "id": 564,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:48:08,647 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.001
2025-05-03 06:48:08,647 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 564,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 06:48:08,647 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 06:48:08,710 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9760
2025-05-03 06:48:08,711 - training.model_MobileNetV3_opt_SGD_lr_0.001 - INFO - Training completed after 997.66 seconds
2025-05-03 06:48:08,713 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_564 - INFO - Starting model evaluation
2025-05-03 06:48:17,859 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_564 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.9030
  Recall:    0.9441
  F1 Score:  0.9231
  IoU:       0.8571
  mAP:       0.9798
  AUC:       0.9894
2025-05-03 06:48:17,861 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_564 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_564/final_results.json
2025-05-03 06:48:17,862 - training.model_MobileNetV3_opt_SGD_lr_0.001_id_564 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.001_id_564/final_results.json
2025-05-03 06:48:17,862 - main - INFO - 
Summary for configuration 564:
2025-05-03 06:48:17,862 - main - INFO - Accuracy: 0.9554
2025-05-03 06:48:17,862 - main - INFO - Precision: 0.9030
2025-05-03 06:48:17,862 - main - INFO - Recall: 0.9441
2025-05-03 06:48:17,862 - main - INFO - F1 Score: 0.9231
2025-05-03 06:48:17,862 - main - INFO - IoU: 0.8571
2025-05-03 06:48:17,862 - main - INFO - mAP: 0.9798
2025-05-03 06:48:17,862 - main - INFO - AUC: 0.9894
2025-05-03 06:48:17,862 - main - INFO - Training time: 997.66 seconds
2025-05-03 06:48:17,862 - main - INFO - 
==================================================
2025-05-03 06:48:17,862 - main - INFO - Running configuration 565/756:
2025-05-03 06:48:17,862 - main - INFO - Model: MobileNetV3
2025-05-03 06:48:17,862 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 06:48:17,862 - main - INFO - Scheduler: StepLR
2025-05-03 06:48:17,862 - main - INFO - Loss Function: CrossEntropy
2025-05-03 06:48:17,862 - main - INFO - ==================================================
2025-05-03 06:48:17,863 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_565 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_565
2025-05-03 06:48:17,863 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_565 - INFO - Config: {
  "id": 565,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:48:17,937 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 06:48:17,937 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 565,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 06:48:17,937 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-03 06:48:17,938 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-03 06:48:39,896 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 06:49:08,667 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.6955
2025-05-03 06:49:08,685 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 1 completed in 50.75s - Train Loss: 0.6811, Train Acc: 0.5899, Val Loss: 0.6651, Val Acc: 0.6955
2025-05-03 06:49:08,726 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 06:49:08,726 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-03 06:49:30,888 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 06:50:01,205 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.6955 to 0.7376
2025-05-03 06:50:01,227 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 2 completed in 52.50s - Train Loss: 0.6578, Train Acc: 0.6748, Val Loss: 0.6415, Val Acc: 0.7376
2025-05-03 06:50:01,264 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 06:50:01,264 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-03 06:50:22,084 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 06:50:51,622 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7376 to 0.7633
2025-05-03 06:50:51,644 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 3 completed in 50.38s - Train Loss: 0.6375, Train Acc: 0.7272, Val Loss: 0.6223, Val Acc: 0.7633
2025-05-03 06:50:51,681 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 06:50:51,682 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-03 06:51:11,119 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 06:51:39,995 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7633 to 0.7762
2025-05-03 06:51:40,019 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 4 completed in 48.34s - Train Loss: 0.6185, Train Acc: 0.7587, Val Loss: 0.6058, Val Acc: 0.7762
2025-05-03 06:51:40,055 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 06:51:40,055 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 06:52:00,658 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 06:52:29,427 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7762 to 0.7873
2025-05-03 06:52:29,450 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 5 completed in 49.39s - Train Loss: 0.6053, Train Acc: 0.7675, Val Loss: 0.5912, Val Acc: 0.7873
2025-05-03 06:52:29,486 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 06:52:29,487 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 06:52:49,741 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 06:53:19,058 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7873 to 0.7933
2025-05-03 06:53:19,080 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 6 completed in 49.59s - Train Loss: 0.5909, Train Acc: 0.7808, Val Loss: 0.5768, Val Acc: 0.7933
2025-05-03 06:53:19,117 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 06:53:19,118 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 06:53:40,423 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 06:54:09,264 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.7933 to 0.8053
2025-05-03 06:54:09,291 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 7 completed in 50.17s - Train Loss: 0.5779, Train Acc: 0.7819, Val Loss: 0.5658, Val Acc: 0.8053
2025-05-03 06:54:09,327 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 06:54:09,328 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 06:54:30,237 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 06:54:59,193 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8053 to 0.8087
2025-05-03 06:54:59,229 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 8 completed in 49.90s - Train Loss: 0.5659, Train Acc: 0.7926, Val Loss: 0.5536, Val Acc: 0.8087
2025-05-03 06:54:59,266 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 06:54:59,266 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 06:55:20,468 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 06:55:50,318 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8087 to 0.8130
2025-05-03 06:55:50,342 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 9 completed in 51.08s - Train Loss: 0.5543, Train Acc: 0.8052, Val Loss: 0.5436, Val Acc: 0.8130
2025-05-03 06:55:50,381 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 06:55:50,382 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 06:56:10,288 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 06:56:41,164 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8130 to 0.8173
2025-05-03 06:56:41,197 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 10 completed in 50.82s - Train Loss: 0.5418, Train Acc: 0.8209, Val Loss: 0.5342, Val Acc: 0.8173
2025-05-03 06:56:41,233 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 06:56:41,233 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 06:57:03,135 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 06:57:33,242 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8173 to 0.8276
2025-05-03 06:57:33,263 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 11 completed in 52.03s - Train Loss: 0.5388, Train Acc: 0.8170, Val Loss: 0.5311, Val Acc: 0.8276
2025-05-03 06:57:33,298 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 06:57:33,299 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 06:57:54,493 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 06:58:24,001 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation loss improved from inf to 0.5316
2025-05-03 06:58:24,024 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 12 completed in 50.73s - Train Loss: 0.5374, Train Acc: 0.8192, Val Loss: 0.5316, Val Acc: 0.8208
2025-05-03 06:58:24,061 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 06:58:24,062 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 06:58:44,552 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 06:59:14,939 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5316 to 0.5305
2025-05-03 06:59:14,962 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 13 completed in 50.90s - Train Loss: 0.5367, Train Acc: 0.8192, Val Loss: 0.5305, Val Acc: 0.8225
2025-05-03 06:59:14,997 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 06:59:14,997 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 06:59:36,362 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 07:00:06,803 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5305 to 0.5301
2025-05-03 07:00:06,824 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 14 completed in 51.83s - Train Loss: 0.5353, Train Acc: 0.8222, Val Loss: 0.5301, Val Acc: 0.8233
2025-05-03 07:00:06,861 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 07:00:06,862 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 07:00:27,297 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 07:00:55,394 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5301 to 0.5286
2025-05-03 07:00:55,419 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 15 completed in 48.56s - Train Loss: 0.5339, Train Acc: 0.8177, Val Loss: 0.5286, Val Acc: 0.8199
2025-05-03 07:00:55,459 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 07:00:55,459 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 07:01:15,899 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 07:01:42,852 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5286 to 0.5284
2025-05-03 07:01:42,877 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 16 completed in 47.42s - Train Loss: 0.5326, Train Acc: 0.8202, Val Loss: 0.5284, Val Acc: 0.8250
2025-05-03 07:01:42,913 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 07:01:42,914 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 07:02:05,921 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 07:02:34,473 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8276 to 0.8328
2025-05-03 07:02:34,496 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 17 completed in 51.58s - Train Loss: 0.5328, Train Acc: 0.8207, Val Loss: 0.5262, Val Acc: 0.8328
2025-05-03 07:02:34,532 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 07:02:34,533 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 07:02:56,005 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 07:03:26,009 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5284 to 0.5265
2025-05-03 07:03:26,031 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 18 completed in 51.50s - Train Loss: 0.5313, Train Acc: 0.8250, Val Loss: 0.5265, Val Acc: 0.8233
2025-05-03 07:03:26,066 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 07:03:26,066 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 07:03:47,138 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 07:04:15,252 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5265 to 0.5242
2025-05-03 07:04:15,276 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 19 completed in 49.21s - Train Loss: 0.5325, Train Acc: 0.8179, Val Loss: 0.5242, Val Acc: 0.8285
2025-05-03 07:04:15,318 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 07:04:15,318 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 07:04:35,961 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 07:05:03,799 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.5242 to 0.5236
2025-05-03 07:05:03,820 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Epoch 20 completed in 48.50s - Train Loss: 0.5298, Train Acc: 0.8239, Val Loss: 0.5236, Val Acc: 0.8293
2025-05-03 07:05:03,860 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 07:05:03,860 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:05:03,863 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_565 - INFO - Starting model evaluation
2025-05-03 07:05:13,153 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_565 - INFO - Evaluation metrics:
  Accuracy:  0.8712
  Precision: 0.7671
  Recall:    0.7832
  F1 Score:  0.7751
  IoU:       0.6328
  mAP:       0.8703
  AUC:       0.9238
2025-05-03 07:05:13,154 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_565 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_565/final_results.json
2025-05-03 07:05:13,156 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_565 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_565/final_results.json
2025-05-03 07:05:13,156 - main - INFO - 
Summary for configuration 565:
2025-05-03 07:05:13,156 - main - INFO - Accuracy: 0.8712
2025-05-03 07:05:13,156 - main - INFO - Precision: 0.7671
2025-05-03 07:05:13,156 - main - INFO - Recall: 0.7832
2025-05-03 07:05:13,156 - main - INFO - F1 Score: 0.7751
2025-05-03 07:05:13,156 - main - INFO - IoU: 0.6328
2025-05-03 07:05:13,156 - main - INFO - mAP: 0.8703
2025-05-03 07:05:13,156 - main - INFO - AUC: 0.9238
2025-05-03 07:05:13,156 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:05:13,156 - main - INFO - 
==================================================
2025-05-03 07:05:13,156 - main - INFO - Running configuration 566/756:
2025-05-03 07:05:13,156 - main - INFO - Model: MobileNetV3
2025-05-03 07:05:13,156 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:05:13,156 - main - INFO - Scheduler: StepLR
2025-05-03 07:05:13,156 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:05:13,156 - main - INFO - ==================================================
2025-05-03 07:05:13,156 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_566 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_566
2025-05-03 07:05:13,156 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_566 - INFO - Config: {
  "id": 566,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:05:13,476 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:05:13,476 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 566,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:05:13,476 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:05:13,541 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:05:13,542 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:05:13,544 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_566 - INFO - Starting model evaluation
2025-05-03 07:05:22,945 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_566 - INFO - Evaluation metrics:
  Accuracy:  0.8682
  Precision: 0.7647
  Recall:    0.7727
  F1 Score:  0.7687
  IoU:       0.6243
  mAP:       0.8653
  AUC:       0.9193
2025-05-03 07:05:22,947 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_566 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_566/final_results.json
2025-05-03 07:05:22,948 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_566 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_566/final_results.json
2025-05-03 07:05:22,948 - main - INFO - 
Summary for configuration 566:
2025-05-03 07:05:22,948 - main - INFO - Accuracy: 0.8682
2025-05-03 07:05:22,948 - main - INFO - Precision: 0.7647
2025-05-03 07:05:22,948 - main - INFO - Recall: 0.7727
2025-05-03 07:05:22,948 - main - INFO - F1 Score: 0.7687
2025-05-03 07:05:22,948 - main - INFO - IoU: 0.6243
2025-05-03 07:05:22,948 - main - INFO - mAP: 0.8653
2025-05-03 07:05:22,948 - main - INFO - AUC: 0.9193
2025-05-03 07:05:22,948 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:05:22,948 - main - INFO - 
==================================================
2025-05-03 07:05:22,948 - main - INFO - Running configuration 567/756:
2025-05-03 07:05:22,948 - main - INFO - Model: MobileNetV3
2025-05-03 07:05:22,948 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:05:22,948 - main - INFO - Scheduler: StepLR
2025-05-03 07:05:22,948 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:05:22,948 - main - INFO - ==================================================
2025-05-03 07:05:22,949 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_567 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_567
2025-05-03 07:05:22,949 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_567 - INFO - Config: {
  "id": 567,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:05:23,234 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:05:23,234 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 567,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:05:23,234 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:05:23,298 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:05:23,298 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:05:23,300 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_567 - INFO - Starting model evaluation
2025-05-03 07:05:32,520 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_567 - INFO - Evaluation metrics:
  Accuracy:  0.8722
  Precision: 0.7698
  Recall:    0.7832
  F1 Score:  0.7764
  IoU:       0.6346
  mAP:       0.8642
  AUC:       0.9197
2025-05-03 07:05:32,522 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_567 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_567/final_results.json
2025-05-03 07:05:32,523 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_567 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_567/final_results.json
2025-05-03 07:05:32,523 - main - INFO - 
Summary for configuration 567:
2025-05-03 07:05:32,523 - main - INFO - Accuracy: 0.8722
2025-05-03 07:05:32,523 - main - INFO - Precision: 0.7698
2025-05-03 07:05:32,523 - main - INFO - Recall: 0.7832
2025-05-03 07:05:32,523 - main - INFO - F1 Score: 0.7764
2025-05-03 07:05:32,523 - main - INFO - IoU: 0.6346
2025-05-03 07:05:32,523 - main - INFO - mAP: 0.8642
2025-05-03 07:05:32,523 - main - INFO - AUC: 0.9197
2025-05-03 07:05:32,523 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:05:32,523 - main - INFO - 
==================================================
2025-05-03 07:05:32,523 - main - INFO - Running configuration 568/756:
2025-05-03 07:05:32,523 - main - INFO - Model: MobileNetV3
2025-05-03 07:05:32,523 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:05:32,523 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:05:32,523 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:05:32,523 - main - INFO - ==================================================
2025-05-03 07:05:32,524 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_568 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_568
2025-05-03 07:05:32,524 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_568 - INFO - Config: {
  "id": 568,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:05:32,597 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:05:32,598 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 568,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:05:32,598 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:05:32,661 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:05:32,662 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:05:32,664 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_568 - INFO - Starting model evaluation
2025-05-03 07:05:41,877 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_568 - INFO - Evaluation metrics:
  Accuracy:  0.8702
  Precision: 0.7663
  Recall:    0.7797
  F1 Score:  0.7730
  IoU:       0.6299
  mAP:       0.8677
  AUC:       0.9196
2025-05-03 07:05:41,879 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_568 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_568/final_results.json
2025-05-03 07:05:41,880 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_568 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_568/final_results.json
2025-05-03 07:05:41,880 - main - INFO - 
Summary for configuration 568:
2025-05-03 07:05:41,880 - main - INFO - Accuracy: 0.8702
2025-05-03 07:05:41,880 - main - INFO - Precision: 0.7663
2025-05-03 07:05:41,880 - main - INFO - Recall: 0.7797
2025-05-03 07:05:41,880 - main - INFO - F1 Score: 0.7730
2025-05-03 07:05:41,880 - main - INFO - IoU: 0.6299
2025-05-03 07:05:41,880 - main - INFO - mAP: 0.8677
2025-05-03 07:05:41,880 - main - INFO - AUC: 0.9196
2025-05-03 07:05:41,880 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:05:41,880 - main - INFO - 
==================================================
2025-05-03 07:05:41,880 - main - INFO - Running configuration 569/756:
2025-05-03 07:05:41,880 - main - INFO - Model: MobileNetV3
2025-05-03 07:05:41,880 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:05:41,880 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:05:41,880 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:05:41,880 - main - INFO - ==================================================
2025-05-03 07:05:41,881 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_569 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_569
2025-05-03 07:05:41,881 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_569 - INFO - Config: {
  "id": 569,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:05:41,955 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:05:41,956 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 569,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:05:41,956 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:05:42,019 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:05:42,020 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:05:42,022 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_569 - INFO - Starting model evaluation
2025-05-03 07:05:51,528 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_569 - INFO - Evaluation metrics:
  Accuracy:  0.8652
  Precision: 0.7604
  Recall:    0.7657
  F1 Score:  0.7631
  IoU:       0.6169
  mAP:       0.8646
  AUC:       0.9192
2025-05-03 07:05:51,529 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_569 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_569/final_results.json
2025-05-03 07:05:51,531 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_569 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_569/final_results.json
2025-05-03 07:05:51,531 - main - INFO - 
Summary for configuration 569:
2025-05-03 07:05:51,531 - main - INFO - Accuracy: 0.8652
2025-05-03 07:05:51,531 - main - INFO - Precision: 0.7604
2025-05-03 07:05:51,531 - main - INFO - Recall: 0.7657
2025-05-03 07:05:51,531 - main - INFO - F1 Score: 0.7631
2025-05-03 07:05:51,531 - main - INFO - IoU: 0.6169
2025-05-03 07:05:51,531 - main - INFO - mAP: 0.8646
2025-05-03 07:05:51,531 - main - INFO - AUC: 0.9192
2025-05-03 07:05:51,531 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:05:51,531 - main - INFO - 
==================================================
2025-05-03 07:05:51,531 - main - INFO - Running configuration 570/756:
2025-05-03 07:05:51,531 - main - INFO - Model: MobileNetV3
2025-05-03 07:05:51,531 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:05:51,531 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:05:51,531 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:05:51,531 - main - INFO - ==================================================
2025-05-03 07:05:51,531 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_570 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_570
2025-05-03 07:05:51,531 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_570 - INFO - Config: {
  "id": 570,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:05:51,726 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:05:51,726 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 570,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:05:51,727 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:05:51,790 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:05:51,791 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:05:51,793 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_570 - INFO - Starting model evaluation
2025-05-03 07:06:01,131 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_570 - INFO - Evaluation metrics:
  Accuracy:  0.8662
  Precision: 0.7595
  Recall:    0.7727
  F1 Score:  0.7660
  IoU:       0.6208
  mAP:       0.8650
  AUC:       0.9206
2025-05-03 07:06:01,132 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_570 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_570/final_results.json
2025-05-03 07:06:01,134 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_570 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_570/final_results.json
2025-05-03 07:06:01,134 - main - INFO - 
Summary for configuration 570:
2025-05-03 07:06:01,134 - main - INFO - Accuracy: 0.8662
2025-05-03 07:06:01,134 - main - INFO - Precision: 0.7595
2025-05-03 07:06:01,134 - main - INFO - Recall: 0.7727
2025-05-03 07:06:01,134 - main - INFO - F1 Score: 0.7660
2025-05-03 07:06:01,134 - main - INFO - IoU: 0.6208
2025-05-03 07:06:01,134 - main - INFO - mAP: 0.8650
2025-05-03 07:06:01,134 - main - INFO - AUC: 0.9206
2025-05-03 07:06:01,134 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:06:01,134 - main - INFO - 
==================================================
2025-05-03 07:06:01,134 - main - INFO - Running configuration 571/756:
2025-05-03 07:06:01,134 - main - INFO - Model: MobileNetV3
2025-05-03 07:06:01,134 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:06:01,134 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:06:01,134 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:06:01,134 - main - INFO - ==================================================
2025-05-03 07:06:01,134 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_571 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_571
2025-05-03 07:06:01,134 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_571 - INFO - Config: {
  "id": 571,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:06:01,209 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:06:01,209 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 571,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:06:01,209 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:06:01,272 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:06:01,273 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:06:01,275 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_571 - INFO - Starting model evaluation
2025-05-03 07:06:10,499 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_571 - INFO - Evaluation metrics:
  Accuracy:  0.8632
  Precision: 0.7500
  Recall:    0.7762
  F1 Score:  0.7629
  IoU:       0.6167
  mAP:       0.8598
  AUC:       0.9177
2025-05-03 07:06:10,501 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_571 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_571/final_results.json
2025-05-03 07:06:10,502 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_571 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_571/final_results.json
2025-05-03 07:06:10,502 - main - INFO - 
Summary for configuration 571:
2025-05-03 07:06:10,502 - main - INFO - Accuracy: 0.8632
2025-05-03 07:06:10,502 - main - INFO - Precision: 0.7500
2025-05-03 07:06:10,502 - main - INFO - Recall: 0.7762
2025-05-03 07:06:10,502 - main - INFO - F1 Score: 0.7629
2025-05-03 07:06:10,502 - main - INFO - IoU: 0.6167
2025-05-03 07:06:10,502 - main - INFO - mAP: 0.8598
2025-05-03 07:06:10,502 - main - INFO - AUC: 0.9177
2025-05-03 07:06:10,502 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:06:10,502 - main - INFO - 
==================================================
2025-05-03 07:06:10,502 - main - INFO - Running configuration 572/756:
2025-05-03 07:06:10,502 - main - INFO - Model: MobileNetV3
2025-05-03 07:06:10,502 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:06:10,502 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:06:10,502 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:06:10,502 - main - INFO - ==================================================
2025-05-03 07:06:10,503 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_572 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_572
2025-05-03 07:06:10,503 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_572 - INFO - Config: {
  "id": 572,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:06:10,578 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:06:10,579 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 572,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:06:10,579 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:06:10,642 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:06:10,642 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:06:10,645 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_572 - INFO - Starting model evaluation
2025-05-03 07:06:19,880 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_572 - INFO - Evaluation metrics:
  Accuracy:  0.8692
  Precision: 0.7619
  Recall:    0.7832
  F1 Score:  0.7724
  IoU:       0.6292
  mAP:       0.8653
  AUC:       0.9206
2025-05-03 07:06:19,882 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_572 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_572/final_results.json
2025-05-03 07:06:19,883 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_572 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_572/final_results.json
2025-05-03 07:06:19,883 - main - INFO - 
Summary for configuration 572:
2025-05-03 07:06:19,883 - main - INFO - Accuracy: 0.8692
2025-05-03 07:06:19,883 - main - INFO - Precision: 0.7619
2025-05-03 07:06:19,883 - main - INFO - Recall: 0.7832
2025-05-03 07:06:19,883 - main - INFO - F1 Score: 0.7724
2025-05-03 07:06:19,883 - main - INFO - IoU: 0.6292
2025-05-03 07:06:19,883 - main - INFO - mAP: 0.8653
2025-05-03 07:06:19,883 - main - INFO - AUC: 0.9206
2025-05-03 07:06:19,883 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:06:19,883 - main - INFO - 
==================================================
2025-05-03 07:06:19,883 - main - INFO - Running configuration 573/756:
2025-05-03 07:06:19,884 - main - INFO - Model: MobileNetV3
2025-05-03 07:06:19,884 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:06:19,884 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:06:19,884 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:06:19,884 - main - INFO - ==================================================
2025-05-03 07:06:19,884 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_573 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_573
2025-05-03 07:06:19,884 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_573 - INFO - Config: {
  "id": 573,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:06:19,958 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:06:19,958 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 573,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:06:19,958 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:06:20,112 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:06:20,113 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:06:20,115 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_573 - INFO - Starting model evaluation
2025-05-03 07:06:29,510 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_573 - INFO - Evaluation metrics:
  Accuracy:  0.8692
  Precision: 0.7674
  Recall:    0.7727
  F1 Score:  0.7700
  IoU:       0.6261
  mAP:       0.8653
  AUC:       0.9216
2025-05-03 07:06:29,512 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_573 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_573/final_results.json
2025-05-03 07:06:29,513 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_573 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_573/final_results.json
2025-05-03 07:06:29,513 - main - INFO - 
Summary for configuration 573:
2025-05-03 07:06:29,513 - main - INFO - Accuracy: 0.8692
2025-05-03 07:06:29,513 - main - INFO - Precision: 0.7674
2025-05-03 07:06:29,513 - main - INFO - Recall: 0.7727
2025-05-03 07:06:29,513 - main - INFO - F1 Score: 0.7700
2025-05-03 07:06:29,513 - main - INFO - IoU: 0.6261
2025-05-03 07:06:29,513 - main - INFO - mAP: 0.8653
2025-05-03 07:06:29,513 - main - INFO - AUC: 0.9216
2025-05-03 07:06:29,513 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:06:29,513 - main - INFO - 
==================================================
2025-05-03 07:06:29,513 - main - INFO - Running configuration 574/756:
2025-05-03 07:06:29,513 - main - INFO - Model: MobileNetV3
2025-05-03 07:06:29,513 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:06:29,513 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:06:29,513 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:06:29,513 - main - INFO - ==================================================
2025-05-03 07:06:29,514 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_574 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_574
2025-05-03 07:06:29,514 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_574 - INFO - Config: {
  "id": 574,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:06:29,784 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:06:29,784 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 574,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:06:29,784 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:06:29,848 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:06:29,849 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:06:29,851 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_574 - INFO - Starting model evaluation
2025-05-03 07:06:39,108 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_574 - INFO - Evaluation metrics:
  Accuracy:  0.8583
  Precision: 0.7457
  Recall:    0.7587
  F1 Score:  0.7522
  IoU:       0.6028
  mAP:       0.8612
  AUC:       0.9173
2025-05-03 07:06:39,110 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_574 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_574/final_results.json
2025-05-03 07:06:39,111 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_574 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_574/final_results.json
2025-05-03 07:06:39,111 - main - INFO - 
Summary for configuration 574:
2025-05-03 07:06:39,111 - main - INFO - Accuracy: 0.8583
2025-05-03 07:06:39,111 - main - INFO - Precision: 0.7457
2025-05-03 07:06:39,111 - main - INFO - Recall: 0.7587
2025-05-03 07:06:39,111 - main - INFO - F1 Score: 0.7522
2025-05-03 07:06:39,111 - main - INFO - IoU: 0.6028
2025-05-03 07:06:39,111 - main - INFO - mAP: 0.8612
2025-05-03 07:06:39,111 - main - INFO - AUC: 0.9173
2025-05-03 07:06:39,111 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:06:39,111 - main - INFO - 
==================================================
2025-05-03 07:06:39,111 - main - INFO - Running configuration 575/756:
2025-05-03 07:06:39,111 - main - INFO - Model: MobileNetV3
2025-05-03 07:06:39,111 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:06:39,111 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:06:39,111 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:06:39,111 - main - INFO - ==================================================
2025-05-03 07:06:39,112 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_575 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_575
2025-05-03 07:06:39,112 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_575 - INFO - Config: {
  "id": 575,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:06:39,186 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:06:39,186 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 575,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:06:39,186 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:06:39,249 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:06:39,250 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:06:39,252 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_575 - INFO - Starting model evaluation
2025-05-03 07:06:48,663 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_575 - INFO - Evaluation metrics:
  Accuracy:  0.8662
  Precision: 0.7631
  Recall:    0.7657
  F1 Score:  0.7644
  IoU:       0.6186
  mAP:       0.8649
  AUC:       0.9190
2025-05-03 07:06:48,665 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_575 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_575/final_results.json
2025-05-03 07:06:48,666 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_575 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_575/final_results.json
2025-05-03 07:06:48,666 - main - INFO - 
Summary for configuration 575:
2025-05-03 07:06:48,667 - main - INFO - Accuracy: 0.8662
2025-05-03 07:06:48,667 - main - INFO - Precision: 0.7631
2025-05-03 07:06:48,667 - main - INFO - Recall: 0.7657
2025-05-03 07:06:48,667 - main - INFO - F1 Score: 0.7644
2025-05-03 07:06:48,667 - main - INFO - IoU: 0.6186
2025-05-03 07:06:48,667 - main - INFO - mAP: 0.8649
2025-05-03 07:06:48,667 - main - INFO - AUC: 0.9190
2025-05-03 07:06:48,667 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:06:48,667 - main - INFO - 
==================================================
2025-05-03 07:06:48,667 - main - INFO - Running configuration 576/756:
2025-05-03 07:06:48,667 - main - INFO - Model: MobileNetV3
2025-05-03 07:06:48,667 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 07:06:48,667 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:06:48,667 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:06:48,667 - main - INFO - ==================================================
2025-05-03 07:06:48,667 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_576 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001_id_576
2025-05-03 07:06:48,667 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_576 - INFO - Config: {
  "id": 576,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:06:48,742 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_SGD_lr_0.0001
2025-05-03 07:06:48,743 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 576,
  "model_name": "MobileNetV3",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:06:48,743 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 07:06:48,806 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.8328
2025-05-03 07:06:48,807 - training.model_MobileNetV3_opt_SGD_lr_0.0001 - INFO - Training completed after 1005.88 seconds
2025-05-03 07:06:48,809 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_576 - INFO - Starting model evaluation
2025-05-03 07:06:57,816 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_576 - INFO - Evaluation metrics:
  Accuracy:  0.8662
  Precision: 0.7687
  Recall:    0.7552
  F1 Score:  0.7619
  IoU:       0.6154
  mAP:       0.8676
  AUC:       0.9206
2025-05-03 07:06:57,817 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_576 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_576/final_results.json
2025-05-03 07:06:57,819 - training.model_MobileNetV3_opt_SGD_lr_0.0001_id_576 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_SGD_lr_0.0001_id_576/final_results.json
2025-05-03 07:06:57,819 - main - INFO - 
Summary for configuration 576:
2025-05-03 07:06:57,819 - main - INFO - Accuracy: 0.8662
2025-05-03 07:06:57,819 - main - INFO - Precision: 0.7687
2025-05-03 07:06:57,819 - main - INFO - Recall: 0.7552
2025-05-03 07:06:57,819 - main - INFO - F1 Score: 0.7619
2025-05-03 07:06:57,819 - main - INFO - IoU: 0.6154
2025-05-03 07:06:57,819 - main - INFO - mAP: 0.8676
2025-05-03 07:06:57,819 - main - INFO - AUC: 0.9206
2025-05-03 07:06:57,819 - main - INFO - Training time: 1005.88 seconds
2025-05-03 07:06:57,819 - main - INFO - 
==================================================
2025-05-03 07:06:57,819 - main - INFO - Running configuration 577/756:
2025-05-03 07:06:57,819 - main - INFO - Model: MobileNetV3
2025-05-03 07:06:57,819 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:06:57,819 - main - INFO - Scheduler: StepLR
2025-05-03 07:06:57,819 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:06:57,819 - main - INFO - ==================================================
2025-05-03 07:06:57,819 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_577 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_577
2025-05-03 07:06:57,819 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_577 - INFO - Config: {
  "id": 577,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:06:57,894 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:06:57,894 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 577,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:06:57,894 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-03 07:06:57,895 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 1/20
2025-05-03 07:07:18,130 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 07:07:47,229 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.5094
2025-05-03 07:07:47,246 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 1 completed in 49.35s - Train Loss: 0.8159, Train Acc: 0.4964, Val Loss: 0.8022, Val Acc: 0.5094
2025-05-03 07:07:47,303 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 07:07:47,303 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 2/20
2025-05-03 07:08:07,420 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 07:08:36,315 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation loss improved from inf to 0.8169
2025-05-03 07:08:36,338 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 2 completed in 49.03s - Train Loss: 0.8195, Train Acc: 0.4938, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 07:08:36,392 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 07:08:36,393 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 3/20
2025-05-03 07:08:56,455 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 07:09:24,508 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.8169 to 0.8096
2025-05-03 07:09:24,532 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 3 completed in 48.14s - Train Loss: 0.8114, Train Acc: 0.5026, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 07:09:24,588 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 07:09:24,588 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 4/20
2025-05-03 07:09:44,221 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 07:10:12,747 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.8096 to 0.8094
2025-05-03 07:10:12,772 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 4 completed in 48.18s - Train Loss: 0.7983, Train Acc: 0.5139, Val Loss: 0.8094, Val Acc: 0.5043
2025-05-03 07:10:12,831 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 07:10:12,831 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 5/20
2025-05-03 07:10:32,686 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 07:11:00,816 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 5 completed in 47.98s - Train Loss: 0.8158, Train Acc: 0.4979, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 07:11:00,875 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 07:11:00,876 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 6/20
2025-05-03 07:11:22,195 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 07:11:50,159 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 6 completed in 49.28s - Train Loss: 0.8183, Train Acc: 0.4946, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 07:11:50,222 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 07:11:50,223 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 7/20
2025-05-03 07:12:11,109 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 07:12:39,692 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 7 completed in 49.47s - Train Loss: 0.8143, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 07:12:39,749 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 07:12:39,749 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 8/20
2025-05-03 07:13:00,784 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 07:13:28,732 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 8 completed in 48.98s - Train Loss: 0.8137, Train Acc: 0.4989, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 07:13:28,789 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 07:13:28,790 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 9/20
2025-05-03 07:13:49,571 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 07:14:18,033 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 9 completed in 49.24s - Train Loss: 0.8170, Train Acc: 0.4944, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 07:14:18,097 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 07:14:18,097 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Early stopping triggered after 9 epochs
2025-05-03 07:14:18,097 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 440.14 seconds
2025-05-03 07:14:18,100 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_577 - INFO - Starting model evaluation
2025-05-03 07:14:27,294 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_577 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2840
  AUC:       0.5014
2025-05-03 07:14:27,295 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_577 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_577/final_results.json
2025-05-03 07:14:27,296 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_577 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_577/final_results.json
2025-05-03 07:14:27,296 - main - INFO - 
Summary for configuration 577:
2025-05-03 07:14:27,296 - main - INFO - Accuracy: 0.2834
2025-05-03 07:14:27,296 - main - INFO - Precision: 0.2834
2025-05-03 07:14:27,296 - main - INFO - Recall: 1.0000
2025-05-03 07:14:27,296 - main - INFO - F1 Score: 0.4417
2025-05-03 07:14:27,296 - main - INFO - IoU: 0.2834
2025-05-03 07:14:27,296 - main - INFO - mAP: 0.2840
2025-05-03 07:14:27,296 - main - INFO - AUC: 0.5014
2025-05-03 07:14:27,296 - main - INFO - Training time: 440.14 seconds
2025-05-03 07:14:27,296 - main - INFO - 
==================================================
2025-05-03 07:14:27,296 - main - INFO - Running configuration 578/756:
2025-05-03 07:14:27,296 - main - INFO - Model: MobileNetV3
2025-05-03 07:14:27,296 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:14:27,296 - main - INFO - Scheduler: StepLR
2025-05-03 07:14:27,296 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:14:27,296 - main - INFO - ==================================================
2025-05-03 07:14:27,296 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_578 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_578
2025-05-03 07:14:27,296 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_578 - INFO - Config: {
  "id": 578,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:14:27,504 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:14:27,505 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 578,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:14:27,505 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:14:27,705 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 9, best validation accuracy: 0.5094
2025-05-03 07:14:27,706 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 10/20
2025-05-03 07:14:48,138 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 07:15:16,940 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.8094 to 0.0912
2025-05-03 07:15:16,963 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 10 completed in 49.26s - Train Loss: 0.0901, Train Acc: 0.5015, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 07:15:17,029 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 07:15:17,030 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 11/20
2025-05-03 07:15:38,664 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 07:16:07,032 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5094 to 0.5326
2025-05-03 07:16:07,056 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 11 completed in 50.03s - Train Loss: 0.0636, Train Acc: 0.5502, Val Loss: 0.0432, Val Acc: 0.5326
2025-05-03 07:16:07,107 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 07:16:07,108 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 12/20
2025-05-03 07:16:27,209 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 07:16:55,659 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5326 to 0.5369
2025-05-03 07:16:55,682 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 12 completed in 48.57s - Train Loss: 0.0394, Train Acc: 0.6480, Val Loss: 0.0419, Val Acc: 0.5369
2025-05-03 07:16:55,741 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 07:16:55,741 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 07:17:15,978 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 07:17:44,689 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.5369 to 0.6072
2025-05-03 07:17:44,725 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 13 completed in 48.98s - Train Loss: 0.0357, Train Acc: 0.6954, Val Loss: 0.0421, Val Acc: 0.6072
2025-05-03 07:17:44,781 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 07:17:44,782 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 07:18:04,856 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 07:18:32,275 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.6072 to 0.7290
2025-05-03 07:18:32,299 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 14 completed in 47.52s - Train Loss: 0.0338, Train Acc: 0.7119, Val Loss: 0.0363, Val Acc: 0.7290
2025-05-03 07:18:32,352 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 07:18:32,352 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 07:18:53,205 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 07:19:21,193 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.0912 to 0.0377
2025-05-03 07:19:21,216 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 15 completed in 48.86s - Train Loss: 0.0328, Train Acc: 0.7319, Val Loss: 0.0377, Val Acc: 0.6209
2025-05-03 07:19:21,279 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 07:19:21,280 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 07:19:42,330 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 07:20:10,576 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7290 to 0.7521
2025-05-03 07:20:10,601 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 16 completed in 49.32s - Train Loss: 0.0313, Train Acc: 0.7495, Val Loss: 0.0341, Val Acc: 0.7521
2025-05-03 07:20:10,657 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 07:20:10,658 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 07:20:30,692 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 07:20:58,563 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 17 completed in 47.91s - Train Loss: 0.0300, Train Acc: 0.7585, Val Loss: 0.0404, Val Acc: 0.5961
2025-05-03 07:20:58,729 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 07:20:58,729 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 07:21:20,143 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 07:21:48,521 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7521 to 0.7907
2025-05-03 07:21:48,542 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 18 completed in 49.81s - Train Loss: 0.0289, Train Acc: 0.7891, Val Loss: 0.0301, Val Acc: 0.7907
2025-05-03 07:21:48,605 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 07:21:48,606 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 07:22:08,573 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 07:22:37,580 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.0377 to 0.0307
2025-05-03 07:22:37,604 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 19 completed in 49.00s - Train Loss: 0.0267, Train Acc: 0.8102, Val Loss: 0.0307, Val Acc: 0.7659
2025-05-03 07:22:37,659 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 07:22:37,660 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 07:22:58,164 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 07:23:26,614 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.7907 to 0.7950
2025-05-03 07:23:26,634 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Epoch 20 completed in 48.97s - Train Loss: 0.0260, Train Acc: 0.8228, Val Loss: 0.0297, Val Acc: 0.7950
2025-05-03 07:23:26,691 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 07:23:26,691 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:23:26,694 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_578 - INFO - Starting model evaluation
2025-05-03 07:23:35,703 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_578 - INFO - Evaluation metrics:
  Accuracy:  0.8454
  Precision: 0.7754
  Recall:    0.6399
  F1 Score:  0.7011
  IoU:       0.5398
  mAP:       0.7964
  AUC:       0.8709
2025-05-03 07:23:35,704 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_578 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_578/final_results.json
2025-05-03 07:23:35,706 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_578 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_578/final_results.json
2025-05-03 07:23:35,706 - main - INFO - 
Summary for configuration 578:
2025-05-03 07:23:35,706 - main - INFO - Accuracy: 0.8454
2025-05-03 07:23:35,706 - main - INFO - Precision: 0.7754
2025-05-03 07:23:35,706 - main - INFO - Recall: 0.6399
2025-05-03 07:23:35,706 - main - INFO - F1 Score: 0.7011
2025-05-03 07:23:35,706 - main - INFO - IoU: 0.5398
2025-05-03 07:23:35,706 - main - INFO - mAP: 0.7964
2025-05-03 07:23:35,706 - main - INFO - AUC: 0.8709
2025-05-03 07:23:35,706 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:23:35,706 - main - INFO - 
==================================================
2025-05-03 07:23:35,706 - main - INFO - Running configuration 579/756:
2025-05-03 07:23:35,706 - main - INFO - Model: MobileNetV3
2025-05-03 07:23:35,706 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:23:35,706 - main - INFO - Scheduler: StepLR
2025-05-03 07:23:35,706 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:23:35,706 - main - INFO - ==================================================
2025-05-03 07:23:35,706 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_579 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_579
2025-05-03 07:23:35,706 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_579 - INFO - Config: {
  "id": 579,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:23:35,782 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:23:35,782 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 579,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:23:35,782 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:23:35,870 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:23:35,871 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:23:35,873 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_579 - INFO - Starting model evaluation
2025-05-03 07:23:45,111 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_579 - INFO - Evaluation metrics:
  Accuracy:  0.8474
  Precision: 0.7821
  Recall:    0.6399
  F1 Score:  0.7038
  IoU:       0.5430
  mAP:       0.7975
  AUC:       0.8714
2025-05-03 07:23:45,113 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_579 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_579/final_results.json
2025-05-03 07:23:45,114 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_579 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_579/final_results.json
2025-05-03 07:23:45,114 - main - INFO - 
Summary for configuration 579:
2025-05-03 07:23:45,114 - main - INFO - Accuracy: 0.8474
2025-05-03 07:23:45,114 - main - INFO - Precision: 0.7821
2025-05-03 07:23:45,114 - main - INFO - Recall: 0.6399
2025-05-03 07:23:45,114 - main - INFO - F1 Score: 0.7038
2025-05-03 07:23:45,114 - main - INFO - IoU: 0.5430
2025-05-03 07:23:45,114 - main - INFO - mAP: 0.7975
2025-05-03 07:23:45,114 - main - INFO - AUC: 0.8714
2025-05-03 07:23:45,114 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:23:45,115 - main - INFO - 
==================================================
2025-05-03 07:23:45,115 - main - INFO - Running configuration 580/756:
2025-05-03 07:23:45,115 - main - INFO - Model: MobileNetV3
2025-05-03 07:23:45,115 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:23:45,115 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:23:45,115 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:23:45,115 - main - INFO - ==================================================
2025-05-03 07:23:45,115 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_580 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_580
2025-05-03 07:23:45,115 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_580 - INFO - Config: {
  "id": 580,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:23:45,190 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:23:45,190 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 580,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:23:45,190 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:23:45,277 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:23:45,278 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:23:45,280 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_580 - INFO - Starting model evaluation
2025-05-03 07:23:54,420 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_580 - INFO - Evaluation metrics:
  Accuracy:  0.8434
  Precision: 0.7735
  Recall:    0.6329
  F1 Score:  0.6962
  IoU:       0.5339
  mAP:       0.7987
  AUC:       0.8723
2025-05-03 07:23:54,422 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_580 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_580/final_results.json
2025-05-03 07:23:54,423 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_580 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_580/final_results.json
2025-05-03 07:23:54,423 - main - INFO - 
Summary for configuration 580:
2025-05-03 07:23:54,423 - main - INFO - Accuracy: 0.8434
2025-05-03 07:23:54,423 - main - INFO - Precision: 0.7735
2025-05-03 07:23:54,423 - main - INFO - Recall: 0.6329
2025-05-03 07:23:54,423 - main - INFO - F1 Score: 0.6962
2025-05-03 07:23:54,423 - main - INFO - IoU: 0.5339
2025-05-03 07:23:54,423 - main - INFO - mAP: 0.7987
2025-05-03 07:23:54,423 - main - INFO - AUC: 0.8723
2025-05-03 07:23:54,423 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:23:54,424 - main - INFO - 
==================================================
2025-05-03 07:23:54,424 - main - INFO - Running configuration 581/756:
2025-05-03 07:23:54,424 - main - INFO - Model: MobileNetV3
2025-05-03 07:23:54,424 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:23:54,424 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:23:54,424 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:23:54,424 - main - INFO - ==================================================
2025-05-03 07:23:54,424 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_581 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_581
2025-05-03 07:23:54,424 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_581 - INFO - Config: {
  "id": 581,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:23:54,498 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:23:54,499 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 581,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:23:54,499 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:23:54,586 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:23:54,587 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:23:54,589 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_581 - INFO - Starting model evaluation
2025-05-03 07:24:03,634 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_581 - INFO - Evaluation metrics:
  Accuracy:  0.8355
  Precision: 0.7500
  Recall:    0.6294
  F1 Score:  0.6844
  IoU:       0.5202
  mAP:       0.7914
  AUC:       0.8667
2025-05-03 07:24:03,637 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_581 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_581/final_results.json
2025-05-03 07:24:03,639 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_581 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_581/final_results.json
2025-05-03 07:24:03,639 - main - INFO - 
Summary for configuration 581:
2025-05-03 07:24:03,639 - main - INFO - Accuracy: 0.8355
2025-05-03 07:24:03,639 - main - INFO - Precision: 0.7500
2025-05-03 07:24:03,639 - main - INFO - Recall: 0.6294
2025-05-03 07:24:03,639 - main - INFO - F1 Score: 0.6844
2025-05-03 07:24:03,639 - main - INFO - IoU: 0.5202
2025-05-03 07:24:03,640 - main - INFO - mAP: 0.7914
2025-05-03 07:24:03,640 - main - INFO - AUC: 0.8667
2025-05-03 07:24:03,640 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:24:03,640 - main - INFO - 
==================================================
2025-05-03 07:24:03,640 - main - INFO - Running configuration 582/756:
2025-05-03 07:24:03,640 - main - INFO - Model: MobileNetV3
2025-05-03 07:24:03,640 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:24:03,640 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:24:03,640 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:24:03,640 - main - INFO - ==================================================
2025-05-03 07:24:03,640 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_582 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_582
2025-05-03 07:24:03,640 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_582 - INFO - Config: {
  "id": 582,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:24:03,716 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:24:03,716 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 582,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:24:03,716 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:24:03,803 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:24:03,804 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:24:03,806 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_582 - INFO - Starting model evaluation
2025-05-03 07:24:13,180 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_582 - INFO - Evaluation metrics:
  Accuracy:  0.8365
  Precision: 0.7553
  Recall:    0.6259
  F1 Score:  0.6845
  IoU:       0.5203
  mAP:       0.7935
  AUC:       0.8687
2025-05-03 07:24:13,182 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_582 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_582/final_results.json
2025-05-03 07:24:13,183 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_582 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_582/final_results.json
2025-05-03 07:24:13,183 - main - INFO - 
Summary for configuration 582:
2025-05-03 07:24:13,184 - main - INFO - Accuracy: 0.8365
2025-05-03 07:24:13,184 - main - INFO - Precision: 0.7553
2025-05-03 07:24:13,184 - main - INFO - Recall: 0.6259
2025-05-03 07:24:13,184 - main - INFO - F1 Score: 0.6845
2025-05-03 07:24:13,184 - main - INFO - IoU: 0.5203
2025-05-03 07:24:13,184 - main - INFO - mAP: 0.7935
2025-05-03 07:24:13,184 - main - INFO - AUC: 0.8687
2025-05-03 07:24:13,184 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:24:13,184 - main - INFO - 
==================================================
2025-05-03 07:24:13,184 - main - INFO - Running configuration 583/756:
2025-05-03 07:24:13,184 - main - INFO - Model: MobileNetV3
2025-05-03 07:24:13,184 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:24:13,184 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:24:13,184 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:24:13,184 - main - INFO - ==================================================
2025-05-03 07:24:13,184 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_583 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_583
2025-05-03 07:24:13,184 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_583 - INFO - Config: {
  "id": 583,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:24:13,508 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:24:13,508 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 583,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:24:13,508 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:24:13,598 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:24:13,599 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:24:13,603 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_583 - INFO - Starting model evaluation
2025-05-03 07:24:22,495 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_583 - INFO - Evaluation metrics:
  Accuracy:  0.8464
  Precision: 0.7695
  Recall:    0.6538
  F1 Score:  0.7070
  IoU:       0.5468
  mAP:       0.7957
  AUC:       0.8699
2025-05-03 07:24:22,497 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_583 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_583/final_results.json
2025-05-03 07:24:22,498 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_583 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_583/final_results.json
2025-05-03 07:24:22,498 - main - INFO - 
Summary for configuration 583:
2025-05-03 07:24:22,498 - main - INFO - Accuracy: 0.8464
2025-05-03 07:24:22,498 - main - INFO - Precision: 0.7695
2025-05-03 07:24:22,498 - main - INFO - Recall: 0.6538
2025-05-03 07:24:22,498 - main - INFO - F1 Score: 0.7070
2025-05-03 07:24:22,498 - main - INFO - IoU: 0.5468
2025-05-03 07:24:22,498 - main - INFO - mAP: 0.7957
2025-05-03 07:24:22,499 - main - INFO - AUC: 0.8699
2025-05-03 07:24:22,499 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:24:22,499 - main - INFO - 
==================================================
2025-05-03 07:24:22,499 - main - INFO - Running configuration 584/756:
2025-05-03 07:24:22,499 - main - INFO - Model: MobileNetV3
2025-05-03 07:24:22,499 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:24:22,499 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:24:22,499 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:24:22,499 - main - INFO - ==================================================
2025-05-03 07:24:22,499 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_584 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_584
2025-05-03 07:24:22,499 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_584 - INFO - Config: {
  "id": 584,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:24:22,804 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:24:22,804 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 584,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:24:22,804 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:24:22,900 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:24:22,902 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:24:22,906 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_584 - INFO - Starting model evaluation
2025-05-03 07:24:32,260 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_584 - INFO - Evaluation metrics:
  Accuracy:  0.8444
  Precision: 0.7768
  Recall:    0.6329
  F1 Score:  0.6975
  IoU:       0.5355
  mAP:       0.7966
  AUC:       0.8717
2025-05-03 07:24:32,261 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_584 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_584/final_results.json
2025-05-03 07:24:32,263 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_584 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_584/final_results.json
2025-05-03 07:24:32,263 - main - INFO - 
Summary for configuration 584:
2025-05-03 07:24:32,263 - main - INFO - Accuracy: 0.8444
2025-05-03 07:24:32,263 - main - INFO - Precision: 0.7768
2025-05-03 07:24:32,263 - main - INFO - Recall: 0.6329
2025-05-03 07:24:32,263 - main - INFO - F1 Score: 0.6975
2025-05-03 07:24:32,263 - main - INFO - IoU: 0.5355
2025-05-03 07:24:32,263 - main - INFO - mAP: 0.7966
2025-05-03 07:24:32,263 - main - INFO - AUC: 0.8717
2025-05-03 07:24:32,263 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:24:32,263 - main - INFO - 
==================================================
2025-05-03 07:24:32,263 - main - INFO - Running configuration 585/756:
2025-05-03 07:24:32,263 - main - INFO - Model: MobileNetV3
2025-05-03 07:24:32,263 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:24:32,263 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:24:32,263 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:24:32,263 - main - INFO - ==================================================
2025-05-03 07:24:32,263 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_585 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_585
2025-05-03 07:24:32,263 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_585 - INFO - Config: {
  "id": 585,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:24:32,338 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:24:32,338 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 585,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:24:32,338 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:24:32,424 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:24:32,425 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:24:32,427 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_585 - INFO - Starting model evaluation
2025-05-03 07:24:41,761 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_585 - INFO - Evaluation metrics:
  Accuracy:  0.8414
  Precision: 0.7603
  Recall:    0.6434
  F1 Score:  0.6970
  IoU:       0.5349
  mAP:       0.7971
  AUC:       0.8705
2025-05-03 07:24:41,763 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_585 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_585/final_results.json
2025-05-03 07:24:41,764 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_585 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_585/final_results.json
2025-05-03 07:24:41,764 - main - INFO - 
Summary for configuration 585:
2025-05-03 07:24:41,764 - main - INFO - Accuracy: 0.8414
2025-05-03 07:24:41,764 - main - INFO - Precision: 0.7603
2025-05-03 07:24:41,764 - main - INFO - Recall: 0.6434
2025-05-03 07:24:41,764 - main - INFO - F1 Score: 0.6970
2025-05-03 07:24:41,764 - main - INFO - IoU: 0.5349
2025-05-03 07:24:41,764 - main - INFO - mAP: 0.7971
2025-05-03 07:24:41,764 - main - INFO - AUC: 0.8705
2025-05-03 07:24:41,764 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:24:41,765 - main - INFO - 
==================================================
2025-05-03 07:24:41,765 - main - INFO - Running configuration 586/756:
2025-05-03 07:24:41,765 - main - INFO - Model: MobileNetV3
2025-05-03 07:24:41,765 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:24:41,765 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:24:41,765 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:24:41,765 - main - INFO - ==================================================
2025-05-03 07:24:41,765 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_586 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_586
2025-05-03 07:24:41,765 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_586 - INFO - Config: {
  "id": 586,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:24:42,046 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:24:42,046 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 586,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:24:42,046 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:24:42,133 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:24:42,134 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:24:42,136 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_586 - INFO - Starting model evaluation
2025-05-03 07:24:51,283 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_586 - INFO - Evaluation metrics:
  Accuracy:  0.8454
  Precision: 0.7802
  Recall:    0.6329
  F1 Score:  0.6988
  IoU:       0.5371
  mAP:       0.7961
  AUC:       0.8709
2025-05-03 07:24:51,285 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_586 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_586/final_results.json
2025-05-03 07:24:51,286 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_586 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_586/final_results.json
2025-05-03 07:24:51,286 - main - INFO - 
Summary for configuration 586:
2025-05-03 07:24:51,286 - main - INFO - Accuracy: 0.8454
2025-05-03 07:24:51,286 - main - INFO - Precision: 0.7802
2025-05-03 07:24:51,286 - main - INFO - Recall: 0.6329
2025-05-03 07:24:51,286 - main - INFO - F1 Score: 0.6988
2025-05-03 07:24:51,286 - main - INFO - IoU: 0.5371
2025-05-03 07:24:51,286 - main - INFO - mAP: 0.7961
2025-05-03 07:24:51,286 - main - INFO - AUC: 0.8709
2025-05-03 07:24:51,286 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:24:51,286 - main - INFO - 
==================================================
2025-05-03 07:24:51,286 - main - INFO - Running configuration 587/756:
2025-05-03 07:24:51,286 - main - INFO - Model: MobileNetV3
2025-05-03 07:24:51,286 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:24:51,286 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:24:51,287 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:24:51,287 - main - INFO - ==================================================
2025-05-03 07:24:51,287 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_587 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_587
2025-05-03 07:24:51,287 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_587 - INFO - Config: {
  "id": 587,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:24:51,615 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:24:51,615 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 587,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:24:51,615 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:24:51,706 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:24:51,707 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:24:51,709 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_587 - INFO - Starting model evaluation
2025-05-03 07:25:00,933 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_587 - INFO - Evaluation metrics:
  Accuracy:  0.8414
  Precision: 0.7582
  Recall:    0.6469
  F1 Score:  0.6981
  IoU:       0.5362
  mAP:       0.8021
  AUC:       0.8730
2025-05-03 07:25:00,935 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_587 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_587/final_results.json
2025-05-03 07:25:00,937 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_587 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_587/final_results.json
2025-05-03 07:25:00,937 - main - INFO - 
Summary for configuration 587:
2025-05-03 07:25:00,937 - main - INFO - Accuracy: 0.8414
2025-05-03 07:25:00,937 - main - INFO - Precision: 0.7582
2025-05-03 07:25:00,937 - main - INFO - Recall: 0.6469
2025-05-03 07:25:00,937 - main - INFO - F1 Score: 0.6981
2025-05-03 07:25:00,937 - main - INFO - IoU: 0.5362
2025-05-03 07:25:00,937 - main - INFO - mAP: 0.8021
2025-05-03 07:25:00,937 - main - INFO - AUC: 0.8730
2025-05-03 07:25:00,937 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:25:00,937 - main - INFO - 
==================================================
2025-05-03 07:25:00,937 - main - INFO - Running configuration 588/756:
2025-05-03 07:25:00,937 - main - INFO - Model: MobileNetV3
2025-05-03 07:25:00,937 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 07:25:00,937 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:25:00,937 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:25:00,937 - main - INFO - ==================================================
2025-05-03 07:25:00,937 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_588 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01_id_588
2025-05-03 07:25:00,937 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_588 - INFO - Config: {
  "id": 588,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:25:01,107 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.01
2025-05-03 07:25:01,107 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 588,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:25:01,107 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 07:25:01,195 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.7950
2025-05-03 07:25:01,196 - training.model_MobileNetV3_opt_Adam_lr_0.01 - INFO - Training completed after 979.07 seconds
2025-05-03 07:25:01,199 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_588 - INFO - Starting model evaluation
2025-05-03 07:25:10,333 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_588 - INFO - Evaluation metrics:
  Accuracy:  0.8454
  Precision: 0.7708
  Recall:    0.6469
  F1 Score:  0.7034
  IoU:       0.5425
  mAP:       0.8019
  AUC:       0.8719
2025-05-03 07:25:10,334 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_588 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_588/final_results.json
2025-05-03 07:25:10,336 - training.model_MobileNetV3_opt_Adam_lr_0.01_id_588 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.01_id_588/final_results.json
2025-05-03 07:25:10,336 - main - INFO - 
Summary for configuration 588:
2025-05-03 07:25:10,336 - main - INFO - Accuracy: 0.8454
2025-05-03 07:25:10,336 - main - INFO - Precision: 0.7708
2025-05-03 07:25:10,336 - main - INFO - Recall: 0.6469
2025-05-03 07:25:10,336 - main - INFO - F1 Score: 0.7034
2025-05-03 07:25:10,336 - main - INFO - IoU: 0.5425
2025-05-03 07:25:10,336 - main - INFO - mAP: 0.8019
2025-05-03 07:25:10,336 - main - INFO - AUC: 0.8719
2025-05-03 07:25:10,336 - main - INFO - Training time: 979.07 seconds
2025-05-03 07:25:10,336 - main - INFO - 
==================================================
2025-05-03 07:25:10,336 - main - INFO - Running configuration 589/756:
2025-05-03 07:25:10,336 - main - INFO - Model: MobileNetV3
2025-05-03 07:25:10,336 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:25:10,336 - main - INFO - Scheduler: StepLR
2025-05-03 07:25:10,336 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:25:10,336 - main - INFO - ==================================================
2025-05-03 07:25:10,336 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_589 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_589
2025-05-03 07:25:10,336 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_589 - INFO - Config: {
  "id": 589,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:25:10,537 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:25:10,537 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 589,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:25:10,537 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-03 07:25:10,538 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-03 07:25:31,464 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 07:26:00,534 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.7024
2025-05-03 07:26:00,552 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 1 completed in 50.01s - Train Loss: 0.4796, Train Acc: 0.8237, Val Loss: 0.6042, Val Acc: 0.7024
2025-05-03 07:26:00,609 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 07:26:00,609 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 2/20
2025-05-03 07:26:20,199 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 07:26:49,272 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.7024 to 0.9031
2025-05-03 07:26:49,295 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 2 completed in 48.69s - Train Loss: 0.4445, Train Acc: 0.8621, Val Loss: 0.4095, Val Acc: 0.9031
2025-05-03 07:26:49,351 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 07:26:49,351 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 3/20
2025-05-03 07:27:09,948 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 07:27:38,047 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation loss improved from inf to 0.4758
2025-05-03 07:27:38,069 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 3 completed in 48.72s - Train Loss: 0.4214, Train Acc: 0.8880, Val Loss: 0.4758, Val Acc: 0.8242
2025-05-03 07:27:38,129 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 07:27:38,130 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 4/20
2025-05-03 07:27:57,843 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 07:28:26,071 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 4 completed in 47.94s - Train Loss: 0.3992, Train Acc: 0.9131, Val Loss: 0.5209, Val Acc: 0.7822
2025-05-03 07:28:26,126 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 07:28:26,127 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 5/20
2025-05-03 07:28:46,618 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 07:29:15,266 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 5 completed in 49.14s - Train Loss: 0.3949, Train Acc: 0.9163, Val Loss: 0.5217, Val Acc: 0.7942
2025-05-03 07:29:15,332 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 07:29:15,332 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 6/20
2025-05-03 07:29:35,689 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 07:30:04,482 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9031 to 0.9245
2025-05-03 07:30:04,503 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 6 completed in 49.17s - Train Loss: 0.3997, Train Acc: 0.9116, Val Loss: 0.3872, Val Acc: 0.9245
2025-05-03 07:30:04,558 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 07:30:04,559 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 7/20
2025-05-03 07:30:24,348 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 07:30:53,217 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4758 to 0.4232
2025-05-03 07:30:53,240 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 7 completed in 48.68s - Train Loss: 0.3881, Train Acc: 0.9228, Val Loss: 0.4232, Val Acc: 0.8868
2025-05-03 07:30:53,295 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 07:30:53,295 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 8/20
2025-05-03 07:31:14,034 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 07:31:43,451 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.4232 to 0.3911
2025-05-03 07:31:43,473 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 8 completed in 50.18s - Train Loss: 0.3803, Train Acc: 0.9318, Val Loss: 0.3911, Val Acc: 0.9194
2025-05-03 07:31:43,532 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 07:31:43,532 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 9/20
2025-05-03 07:32:03,488 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 07:32:31,188 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 9 completed in 47.66s - Train Loss: 0.3757, Train Acc: 0.9365, Val Loss: 0.4121, Val Acc: 0.8937
2025-05-03 07:32:31,244 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 07:32:31,245 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 10/20
2025-05-03 07:32:51,490 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 07:33:20,393 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9245 to 0.9357
2025-05-03 07:33:20,414 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 10 completed in 49.17s - Train Loss: 0.3590, Train Acc: 0.9530, Val Loss: 0.3774, Val Acc: 0.9357
2025-05-03 07:33:20,468 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 07:33:20,468 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 11/20
2025-05-03 07:33:40,194 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 07:34:09,133 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9357 to 0.9554
2025-05-03 07:34:09,155 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 11 completed in 48.69s - Train Loss: 0.3493, Train Acc: 0.9625, Val Loss: 0.3553, Val Acc: 0.9554
2025-05-03 07:34:09,215 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 07:34:09,216 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 12/20
2025-05-03 07:34:29,551 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 07:34:57,724 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9554 to 0.9726
2025-05-03 07:34:57,746 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 12 completed in 48.53s - Train Loss: 0.3430, Train Acc: 0.9704, Val Loss: 0.3428, Val Acc: 0.9726
2025-05-03 07:34:57,805 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 07:34:57,805 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 07:35:17,703 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 07:35:46,154 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3911 to 0.3490
2025-05-03 07:35:46,178 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 13 completed in 48.37s - Train Loss: 0.3351, Train Acc: 0.9779, Val Loss: 0.3490, Val Acc: 0.9623
2025-05-03 07:35:46,233 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 07:35:46,234 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 07:36:06,666 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 07:36:35,422 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3490 to 0.3408
2025-05-03 07:36:35,445 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 14 completed in 49.21s - Train Loss: 0.3325, Train Acc: 0.9794, Val Loss: 0.3408, Val Acc: 0.9717
2025-05-03 07:36:35,497 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 07:36:35,497 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 07:36:55,708 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 07:37:24,487 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 15 completed in 48.99s - Train Loss: 0.3296, Train Acc: 0.9839, Val Loss: 0.3477, Val Acc: 0.9648
2025-05-03 07:37:24,546 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 07:37:24,547 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 07:37:45,401 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 07:38:14,202 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3408 to 0.3404
2025-05-03 07:38:14,226 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 16 completed in 49.68s - Train Loss: 0.3271, Train Acc: 0.9867, Val Loss: 0.3404, Val Acc: 0.9726
2025-05-03 07:38:14,280 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 07:38:14,281 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 07:38:34,289 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 07:39:03,273 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.9726 to 0.9777
2025-05-03 07:39:03,306 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 17 completed in 49.03s - Train Loss: 0.3272, Train Acc: 0.9856, Val Loss: 0.3372, Val Acc: 0.9777
2025-05-03 07:39:03,364 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 07:39:03,365 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 07:39:25,033 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 07:39:54,206 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3404 to 0.3399
2025-05-03 07:39:54,229 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 18 completed in 50.86s - Train Loss: 0.3255, Train Acc: 0.9867, Val Loss: 0.3399, Val Acc: 0.9717
2025-05-03 07:39:54,290 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 07:39:54,291 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 07:40:13,453 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 07:40:42,186 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.3399 to 0.3381
2025-05-03 07:40:42,223 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 19 completed in 47.93s - Train Loss: 0.3258, Train Acc: 0.9871, Val Loss: 0.3381, Val Acc: 0.9743
2025-05-03 07:40:42,276 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 07:40:42,277 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 07:41:02,249 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 07:41:30,387 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Epoch 20 completed in 48.11s - Train Loss: 0.3256, Train Acc: 0.9880, Val Loss: 0.3387, Val Acc: 0.9734
2025-05-03 07:41:30,443 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 07:41:30,444 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:41:30,446 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_589 - INFO - Starting model evaluation
2025-05-03 07:41:39,396 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_589 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.8925
  Recall:    0.9580
  F1 Score:  0.9241
  IoU:       0.8589
  mAP:       0.9665
  AUC:       0.9870
2025-05-03 07:41:39,397 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_589 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_589/final_results.json
2025-05-03 07:41:39,399 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_589 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_589/final_results.json
2025-05-03 07:41:39,399 - main - INFO - 
Summary for configuration 589:
2025-05-03 07:41:39,399 - main - INFO - Accuracy: 0.9554
2025-05-03 07:41:39,399 - main - INFO - Precision: 0.8925
2025-05-03 07:41:39,399 - main - INFO - Recall: 0.9580
2025-05-03 07:41:39,399 - main - INFO - F1 Score: 0.9241
2025-05-03 07:41:39,399 - main - INFO - IoU: 0.8589
2025-05-03 07:41:39,399 - main - INFO - mAP: 0.9665
2025-05-03 07:41:39,399 - main - INFO - AUC: 0.9870
2025-05-03 07:41:39,399 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:41:39,399 - main - INFO - 
==================================================
2025-05-03 07:41:39,399 - main - INFO - Running configuration 590/756:
2025-05-03 07:41:39,399 - main - INFO - Model: MobileNetV3
2025-05-03 07:41:39,399 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:41:39,399 - main - INFO - Scheduler: StepLR
2025-05-03 07:41:39,399 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:41:39,399 - main - INFO - ==================================================
2025-05-03 07:41:39,399 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_590 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_590
2025-05-03 07:41:39,399 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_590 - INFO - Config: {
  "id": 590,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:41:39,475 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:41:39,475 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 590,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:41:39,475 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:41:39,563 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:41:39,564 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:41:39,566 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_590 - INFO - Starting model evaluation
2025-05-03 07:41:48,823 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_590 - INFO - Evaluation metrics:
  Accuracy:  0.9534
  Precision: 0.8918
  Recall:    0.9510
  F1 Score:  0.9205
  IoU:       0.8527
  mAP:       0.9675
  AUC:       0.9874
2025-05-03 07:41:48,828 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_590 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_590/final_results.json
2025-05-03 07:41:48,830 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_590 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_590/final_results.json
2025-05-03 07:41:48,830 - main - INFO - 
Summary for configuration 590:
2025-05-03 07:41:48,830 - main - INFO - Accuracy: 0.9534
2025-05-03 07:41:48,830 - main - INFO - Precision: 0.8918
2025-05-03 07:41:48,830 - main - INFO - Recall: 0.9510
2025-05-03 07:41:48,830 - main - INFO - F1 Score: 0.9205
2025-05-03 07:41:48,830 - main - INFO - IoU: 0.8527
2025-05-03 07:41:48,830 - main - INFO - mAP: 0.9675
2025-05-03 07:41:48,830 - main - INFO - AUC: 0.9874
2025-05-03 07:41:48,830 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:41:48,830 - main - INFO - 
==================================================
2025-05-03 07:41:48,830 - main - INFO - Running configuration 591/756:
2025-05-03 07:41:48,830 - main - INFO - Model: MobileNetV3
2025-05-03 07:41:48,830 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:41:48,830 - main - INFO - Scheduler: StepLR
2025-05-03 07:41:48,830 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:41:48,830 - main - INFO - ==================================================
2025-05-03 07:41:48,830 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_591 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_591
2025-05-03 07:41:48,830 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_591 - INFO - Config: {
  "id": 591,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:41:48,905 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:41:48,905 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 591,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:41:48,905 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:41:48,992 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:41:48,993 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:41:48,995 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_591 - INFO - Starting model evaluation
2025-05-03 07:41:58,154 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_591 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.8980
  Recall:    0.9545
  F1 Score:  0.9254
  IoU:       0.8612
  mAP:       0.9684
  AUC:       0.9874
2025-05-03 07:41:58,156 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_591 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_591/final_results.json
2025-05-03 07:41:58,157 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_591 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_591/final_results.json
2025-05-03 07:41:58,157 - main - INFO - 
Summary for configuration 591:
2025-05-03 07:41:58,157 - main - INFO - Accuracy: 0.9564
2025-05-03 07:41:58,158 - main - INFO - Precision: 0.8980
2025-05-03 07:41:58,158 - main - INFO - Recall: 0.9545
2025-05-03 07:41:58,158 - main - INFO - F1 Score: 0.9254
2025-05-03 07:41:58,158 - main - INFO - IoU: 0.8612
2025-05-03 07:41:58,158 - main - INFO - mAP: 0.9684
2025-05-03 07:41:58,158 - main - INFO - AUC: 0.9874
2025-05-03 07:41:58,158 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:41:58,158 - main - INFO - 
==================================================
2025-05-03 07:41:58,158 - main - INFO - Running configuration 592/756:
2025-05-03 07:41:58,158 - main - INFO - Model: MobileNetV3
2025-05-03 07:41:58,158 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:41:58,158 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:41:58,158 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:41:58,158 - main - INFO - ==================================================
2025-05-03 07:41:58,158 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_592 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_592
2025-05-03 07:41:58,158 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_592 - INFO - Config: {
  "id": 592,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:41:58,359 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:41:58,360 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 592,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:41:58,360 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:41:58,448 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:41:58,449 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:41:58,451 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_592 - INFO - Starting model evaluation
2025-05-03 07:42:07,813 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_592 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.9037
  Recall:    0.9510
  F1 Score:  0.9267
  IoU:       0.8635
  mAP:       0.9706
  AUC:       0.9873
2025-05-03 07:42:07,816 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_592 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_592/final_results.json
2025-05-03 07:42:07,819 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_592 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_592/final_results.json
2025-05-03 07:42:07,819 - main - INFO - 
Summary for configuration 592:
2025-05-03 07:42:07,819 - main - INFO - Accuracy: 0.9574
2025-05-03 07:42:07,819 - main - INFO - Precision: 0.9037
2025-05-03 07:42:07,819 - main - INFO - Recall: 0.9510
2025-05-03 07:42:07,819 - main - INFO - F1 Score: 0.9267
2025-05-03 07:42:07,819 - main - INFO - IoU: 0.8635
2025-05-03 07:42:07,819 - main - INFO - mAP: 0.9706
2025-05-03 07:42:07,819 - main - INFO - AUC: 0.9873
2025-05-03 07:42:07,819 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:42:07,819 - main - INFO - 
==================================================
2025-05-03 07:42:07,819 - main - INFO - Running configuration 593/756:
2025-05-03 07:42:07,819 - main - INFO - Model: MobileNetV3
2025-05-03 07:42:07,819 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:42:07,819 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:42:07,819 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:42:07,819 - main - INFO - ==================================================
2025-05-03 07:42:07,819 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_593 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_593
2025-05-03 07:42:07,819 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_593 - INFO - Config: {
  "id": 593,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:42:07,991 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:42:07,991 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 593,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:42:07,991 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:42:08,080 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:42:08,081 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:42:08,083 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_593 - INFO - Starting model evaluation
2025-05-03 07:42:17,263 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_593 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.8951
  Recall:    0.9545
  F1 Score:  0.9239
  IoU:       0.8585
  mAP:       0.9669
  AUC:       0.9868
2025-05-03 07:42:17,266 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_593 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_593/final_results.json
2025-05-03 07:42:17,267 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_593 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_593/final_results.json
2025-05-03 07:42:17,267 - main - INFO - 
Summary for configuration 593:
2025-05-03 07:42:17,267 - main - INFO - Accuracy: 0.9554
2025-05-03 07:42:17,267 - main - INFO - Precision: 0.8951
2025-05-03 07:42:17,267 - main - INFO - Recall: 0.9545
2025-05-03 07:42:17,267 - main - INFO - F1 Score: 0.9239
2025-05-03 07:42:17,267 - main - INFO - IoU: 0.8585
2025-05-03 07:42:17,267 - main - INFO - mAP: 0.9669
2025-05-03 07:42:17,267 - main - INFO - AUC: 0.9868
2025-05-03 07:42:17,267 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:42:17,267 - main - INFO - 
==================================================
2025-05-03 07:42:17,267 - main - INFO - Running configuration 594/756:
2025-05-03 07:42:17,268 - main - INFO - Model: MobileNetV3
2025-05-03 07:42:17,268 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:42:17,268 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 07:42:17,268 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:42:17,268 - main - INFO - ==================================================
2025-05-03 07:42:17,268 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_594 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_594
2025-05-03 07:42:17,268 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_594 - INFO - Config: {
  "id": 594,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:42:17,342 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:42:17,342 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 594,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:42:17,342 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:42:17,430 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:42:17,431 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:42:17,433 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_594 - INFO - Starting model evaluation
2025-05-03 07:42:26,979 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_594 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.9003
  Recall:    0.9476
  F1 Score:  0.9233
  IoU:       0.8576
  mAP:       0.9716
  AUC:       0.9873
2025-05-03 07:42:26,982 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_594 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_594/final_results.json
2025-05-03 07:42:26,984 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_594 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_594/final_results.json
2025-05-03 07:42:26,985 - main - INFO - 
Summary for configuration 594:
2025-05-03 07:42:26,985 - main - INFO - Accuracy: 0.9554
2025-05-03 07:42:26,985 - main - INFO - Precision: 0.9003
2025-05-03 07:42:26,985 - main - INFO - Recall: 0.9476
2025-05-03 07:42:26,985 - main - INFO - F1 Score: 0.9233
2025-05-03 07:42:26,985 - main - INFO - IoU: 0.8576
2025-05-03 07:42:26,985 - main - INFO - mAP: 0.9716
2025-05-03 07:42:26,985 - main - INFO - AUC: 0.9873
2025-05-03 07:42:26,985 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:42:26,985 - main - INFO - 
==================================================
2025-05-03 07:42:26,985 - main - INFO - Running configuration 595/756:
2025-05-03 07:42:26,985 - main - INFO - Model: MobileNetV3
2025-05-03 07:42:26,985 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:42:26,985 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:42:26,985 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:42:26,985 - main - INFO - ==================================================
2025-05-03 07:42:26,985 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_595 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_595
2025-05-03 07:42:26,985 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_595 - INFO - Config: {
  "id": 595,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:42:27,059 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:42:27,059 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 595,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:42:27,059 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:42:27,146 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:42:27,147 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:42:27,149 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_595 - INFO - Starting model evaluation
2025-05-03 07:42:36,443 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_595 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.8947
  Recall:    0.9510
  F1 Score:  0.9220
  IoU:       0.8553
  mAP:       0.9666
  AUC:       0.9864
2025-05-03 07:42:36,445 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_595 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_595/final_results.json
2025-05-03 07:42:36,447 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_595 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_595/final_results.json
2025-05-03 07:42:36,447 - main - INFO - 
Summary for configuration 595:
2025-05-03 07:42:36,447 - main - INFO - Accuracy: 0.9544
2025-05-03 07:42:36,447 - main - INFO - Precision: 0.8947
2025-05-03 07:42:36,447 - main - INFO - Recall: 0.9510
2025-05-03 07:42:36,447 - main - INFO - F1 Score: 0.9220
2025-05-03 07:42:36,447 - main - INFO - IoU: 0.8553
2025-05-03 07:42:36,447 - main - INFO - mAP: 0.9666
2025-05-03 07:42:36,447 - main - INFO - AUC: 0.9864
2025-05-03 07:42:36,447 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:42:36,447 - main - INFO - 
==================================================
2025-05-03 07:42:36,447 - main - INFO - Running configuration 596/756:
2025-05-03 07:42:36,447 - main - INFO - Model: MobileNetV3
2025-05-03 07:42:36,447 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:42:36,447 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:42:36,447 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:42:36,447 - main - INFO - ==================================================
2025-05-03 07:42:36,447 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_596 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_596
2025-05-03 07:42:36,447 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_596 - INFO - Config: {
  "id": 596,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:42:36,639 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:42:36,640 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 596,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:42:36,640 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:42:36,727 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:42:36,727 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:42:36,730 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_596 - INFO - Starting model evaluation
2025-05-03 07:42:46,258 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_596 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9103
  Recall:    0.9580
  F1 Score:  0.9336
  IoU:       0.8754
  mAP:       0.9731
  AUC:       0.9882
2025-05-03 07:42:46,260 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_596 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_596/final_results.json
2025-05-03 07:42:46,261 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_596 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_596/final_results.json
2025-05-03 07:42:46,261 - main - INFO - 
Summary for configuration 596:
2025-05-03 07:42:46,261 - main - INFO - Accuracy: 0.9613
2025-05-03 07:42:46,261 - main - INFO - Precision: 0.9103
2025-05-03 07:42:46,261 - main - INFO - Recall: 0.9580
2025-05-03 07:42:46,261 - main - INFO - F1 Score: 0.9336
2025-05-03 07:42:46,261 - main - INFO - IoU: 0.8754
2025-05-03 07:42:46,261 - main - INFO - mAP: 0.9731
2025-05-03 07:42:46,261 - main - INFO - AUC: 0.9882
2025-05-03 07:42:46,261 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:42:46,261 - main - INFO - 
==================================================
2025-05-03 07:42:46,261 - main - INFO - Running configuration 597/756:
2025-05-03 07:42:46,261 - main - INFO - Model: MobileNetV3
2025-05-03 07:42:46,261 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:42:46,261 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 07:42:46,261 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:42:46,261 - main - INFO - ==================================================
2025-05-03 07:42:46,262 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_597 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_597
2025-05-03 07:42:46,262 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_597 - INFO - Config: {
  "id": 597,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:42:46,529 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:42:46,529 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 597,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:42:46,529 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:42:46,618 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:42:46,619 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:42:46,623 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_597 - INFO - Starting model evaluation
2025-05-03 07:42:55,946 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_597 - INFO - Evaluation metrics:
  Accuracy:  0.9554
  Precision: 0.8951
  Recall:    0.9545
  F1 Score:  0.9239
  IoU:       0.8585
  mAP:       0.9664
  AUC:       0.9865
2025-05-03 07:42:55,948 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_597 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_597/final_results.json
2025-05-03 07:42:55,950 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_597 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_597/final_results.json
2025-05-03 07:42:55,950 - main - INFO - 
Summary for configuration 597:
2025-05-03 07:42:55,950 - main - INFO - Accuracy: 0.9554
2025-05-03 07:42:55,950 - main - INFO - Precision: 0.8951
2025-05-03 07:42:55,950 - main - INFO - Recall: 0.9545
2025-05-03 07:42:55,950 - main - INFO - F1 Score: 0.9239
2025-05-03 07:42:55,950 - main - INFO - IoU: 0.8585
2025-05-03 07:42:55,950 - main - INFO - mAP: 0.9664
2025-05-03 07:42:55,950 - main - INFO - AUC: 0.9865
2025-05-03 07:42:55,950 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:42:55,950 - main - INFO - 
==================================================
2025-05-03 07:42:55,950 - main - INFO - Running configuration 598/756:
2025-05-03 07:42:55,950 - main - INFO - Model: MobileNetV3
2025-05-03 07:42:55,950 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:42:55,950 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:42:55,950 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:42:55,950 - main - INFO - ==================================================
2025-05-03 07:42:55,950 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_598 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_598
2025-05-03 07:42:55,950 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_598 - INFO - Config: {
  "id": 598,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:42:56,244 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:42:56,245 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 598,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:42:56,245 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:42:56,332 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:42:56,333 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:42:56,335 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_598 - INFO - Starting model evaluation
2025-05-03 07:43:05,661 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_598 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9076
  Recall:    0.9615
  F1 Score:  0.9338
  IoU:       0.8758
  mAP:       0.9764
  AUC:       0.9901
2025-05-03 07:43:05,662 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_598 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_598/final_results.json
2025-05-03 07:43:05,664 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_598 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_598/final_results.json
2025-05-03 07:43:05,664 - main - INFO - 
Summary for configuration 598:
2025-05-03 07:43:05,664 - main - INFO - Accuracy: 0.9613
2025-05-03 07:43:05,664 - main - INFO - Precision: 0.9076
2025-05-03 07:43:05,664 - main - INFO - Recall: 0.9615
2025-05-03 07:43:05,664 - main - INFO - F1 Score: 0.9338
2025-05-03 07:43:05,664 - main - INFO - IoU: 0.8758
2025-05-03 07:43:05,664 - main - INFO - mAP: 0.9764
2025-05-03 07:43:05,664 - main - INFO - AUC: 0.9901
2025-05-03 07:43:05,664 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:43:05,664 - main - INFO - 
==================================================
2025-05-03 07:43:05,664 - main - INFO - Running configuration 599/756:
2025-05-03 07:43:05,664 - main - INFO - Model: MobileNetV3
2025-05-03 07:43:05,664 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:43:05,664 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:43:05,664 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:43:05,664 - main - INFO - ==================================================
2025-05-03 07:43:05,665 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_599 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_599
2025-05-03 07:43:05,665 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_599 - INFO - Config: {
  "id": 599,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:43:05,887 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:43:05,888 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 599,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:43:05,888 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:43:06,082 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:43:06,082 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:43:06,085 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_599 - INFO - Starting model evaluation
2025-05-03 07:43:15,692 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_599 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9043
  Recall:    0.9580
  F1 Score:  0.9304
  IoU:       0.8698
  mAP:       0.9732
  AUC:       0.9889
2025-05-03 07:43:15,695 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_599 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_599/final_results.json
2025-05-03 07:43:15,697 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_599 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_599/final_results.json
2025-05-03 07:43:15,697 - main - INFO - 
Summary for configuration 599:
2025-05-03 07:43:15,697 - main - INFO - Accuracy: 0.9594
2025-05-03 07:43:15,697 - main - INFO - Precision: 0.9043
2025-05-03 07:43:15,697 - main - INFO - Recall: 0.9580
2025-05-03 07:43:15,697 - main - INFO - F1 Score: 0.9304
2025-05-03 07:43:15,697 - main - INFO - IoU: 0.8698
2025-05-03 07:43:15,697 - main - INFO - mAP: 0.9732
2025-05-03 07:43:15,697 - main - INFO - AUC: 0.9889
2025-05-03 07:43:15,697 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:43:15,697 - main - INFO - 
==================================================
2025-05-03 07:43:15,697 - main - INFO - Running configuration 600/756:
2025-05-03 07:43:15,697 - main - INFO - Model: MobileNetV3
2025-05-03 07:43:15,697 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 07:43:15,697 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 07:43:15,697 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 07:43:15,697 - main - INFO - ==================================================
2025-05-03 07:43:15,698 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_600 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001_id_600
2025-05-03 07:43:15,698 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_600 - INFO - Config: {
  "id": 600,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:43:15,779 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.001
2025-05-03 07:43:15,779 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 600,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 07:43:15,779 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 07:43:15,867 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9777
2025-05-03 07:43:15,868 - training.model_MobileNetV3_opt_Adam_lr_0.001 - INFO - Training completed after 979.85 seconds
2025-05-03 07:43:15,870 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_600 - INFO - Starting model evaluation
2025-05-03 07:43:25,462 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_600 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.8958
  Recall:    0.9615
  F1 Score:  0.9275
  IoU:       0.8648
  mAP:       0.9684
  AUC:       0.9871
2025-05-03 07:43:25,463 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_600 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_600/final_results.json
2025-05-03 07:43:25,465 - training.model_MobileNetV3_opt_Adam_lr_0.001_id_600 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.001_id_600/final_results.json
2025-05-03 07:43:25,465 - main - INFO - 
Summary for configuration 600:
2025-05-03 07:43:25,465 - main - INFO - Accuracy: 0.9574
2025-05-03 07:43:25,465 - main - INFO - Precision: 0.8958
2025-05-03 07:43:25,465 - main - INFO - Recall: 0.9615
2025-05-03 07:43:25,465 - main - INFO - F1 Score: 0.9275
2025-05-03 07:43:25,465 - main - INFO - IoU: 0.8648
2025-05-03 07:43:25,465 - main - INFO - mAP: 0.9684
2025-05-03 07:43:25,465 - main - INFO - AUC: 0.9871
2025-05-03 07:43:25,465 - main - INFO - Training time: 979.85 seconds
2025-05-03 07:43:25,465 - main - INFO - 
==================================================
2025-05-03 07:43:25,465 - main - INFO - Running configuration 601/756:
2025-05-03 07:43:25,465 - main - INFO - Model: MobileNetV3
2025-05-03 07:43:25,465 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 07:43:25,465 - main - INFO - Scheduler: StepLR
2025-05-03 07:43:25,465 - main - INFO - Loss Function: CrossEntropy
2025-05-03 07:43:25,465 - main - INFO - ==================================================
2025-05-03 07:43:25,465 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_601 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_601
2025-05-03 07:43:25,465 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_601 - INFO - Config: {
  "id": 601,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:43:25,750 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 07:43:25,750 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 601,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 07:43:25,750 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-03 07:43:25,751 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-03 07:43:45,747 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 07:44:14,825 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9297
2025-05-03 07:44:14,846 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 1 completed in 49.10s - Train Loss: 0.4572, Train Acc: 0.8561, Val Loss: 0.3809, Val Acc: 0.9297
2025-05-03 07:44:14,902 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 07:44:14,902 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-03 07:44:35,094 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 07:45:03,499 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9297 to 0.9666
2025-05-03 07:45:03,521 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 2 completed in 48.62s - Train Loss: 0.3516, Train Acc: 0.9640, Val Loss: 0.3448, Val Acc: 0.9666
2025-05-03 07:45:03,576 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 07:45:03,576 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-03 07:45:24,303 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 07:45:52,900 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9666 to 0.9674
2025-05-03 07:45:52,922 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 3 completed in 49.35s - Train Loss: 0.3383, Train Acc: 0.9764, Val Loss: 0.3478, Val Acc: 0.9674
2025-05-03 07:45:52,980 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 07:45:52,980 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-03 07:46:12,311 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 07:46:41,857 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9674 to 0.9708
2025-05-03 07:46:41,881 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 4 completed in 48.90s - Train Loss: 0.3378, Train Acc: 0.9762, Val Loss: 0.3414, Val Acc: 0.9708
2025-05-03 07:46:41,942 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 07:46:41,943 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 07:47:01,704 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 07:47:30,445 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9708 to 0.9743
2025-05-03 07:47:30,467 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 5 completed in 48.52s - Train Loss: 0.3304, Train Acc: 0.9835, Val Loss: 0.3397, Val Acc: 0.9743
2025-05-03 07:47:30,521 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 07:47:30,521 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 07:47:50,168 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 07:48:19,801 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9743 to 0.9760
2025-05-03 07:48:19,824 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 6 completed in 49.30s - Train Loss: 0.3310, Train Acc: 0.9811, Val Loss: 0.3385, Val Acc: 0.9760
2025-05-03 07:48:19,880 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 07:48:19,880 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 07:48:38,799 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 07:49:07,569 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9760 to 0.9768
2025-05-03 07:49:07,591 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 7 completed in 47.71s - Train Loss: 0.3283, Train Acc: 0.9850, Val Loss: 0.3361, Val Acc: 0.9768
2025-05-03 07:49:07,654 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 07:49:07,654 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 07:49:27,500 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 07:49:56,369 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9768 to 0.9786
2025-05-03 07:49:56,392 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 8 completed in 48.74s - Train Loss: 0.3255, Train Acc: 0.9880, Val Loss: 0.3322, Val Acc: 0.9786
2025-05-03 07:49:56,454 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 07:49:56,455 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 07:50:16,550 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 07:50:45,925 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9786 to 0.9794
2025-05-03 07:50:45,949 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 9 completed in 49.49s - Train Loss: 0.3223, Train Acc: 0.9912, Val Loss: 0.3319, Val Acc: 0.9794
2025-05-03 07:50:46,008 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 07:50:46,009 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 07:51:05,404 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 07:51:33,518 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation loss improved from inf to 0.3327
2025-05-03 07:51:33,542 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 10 completed in 47.53s - Train Loss: 0.3231, Train Acc: 0.9899, Val Loss: 0.3327, Val Acc: 0.9786
2025-05-03 07:51:33,599 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 07:51:33,599 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 07:51:55,255 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 07:52:25,031 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 11 completed in 51.43s - Train Loss: 0.3202, Train Acc: 0.9934, Val Loss: 0.3342, Val Acc: 0.9794
2025-05-03 07:52:25,086 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 07:52:25,087 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 07:52:46,240 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 07:53:14,135 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9794 to 0.9811
2025-05-03 07:53:14,159 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 12 completed in 49.07s - Train Loss: 0.3189, Train Acc: 0.9942, Val Loss: 0.3316, Val Acc: 0.9811
2025-05-03 07:53:14,224 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 07:53:14,224 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 07:53:34,723 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 07:54:03,691 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9811 to 0.9820
2025-05-03 07:54:03,714 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 13 completed in 49.49s - Train Loss: 0.3173, Train Acc: 0.9955, Val Loss: 0.3308, Val Acc: 0.9820
2025-05-03 07:54:03,775 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 07:54:03,775 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 07:54:24,718 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 07:54:53,691 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3327 to 0.3323
2025-05-03 07:54:53,714 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 14 completed in 49.94s - Train Loss: 0.3164, Train Acc: 0.9970, Val Loss: 0.3323, Val Acc: 0.9786
2025-05-03 07:54:53,773 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 07:54:53,773 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 07:55:13,620 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 07:55:43,678 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3323 to 0.3320
2025-05-03 07:55:43,702 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 15 completed in 49.93s - Train Loss: 0.3172, Train Acc: 0.9964, Val Loss: 0.3320, Val Acc: 0.9811
2025-05-03 07:55:43,755 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 07:55:43,756 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 07:56:04,000 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 07:56:32,596 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3320 to 0.3304
2025-05-03 07:56:32,619 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 16 completed in 48.86s - Train Loss: 0.3154, Train Acc: 0.9981, Val Loss: 0.3304, Val Acc: 0.9820
2025-05-03 07:56:32,677 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 07:56:32,678 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 07:56:52,229 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 07:57:21,617 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 17 completed in 48.94s - Train Loss: 0.3152, Train Acc: 0.9983, Val Loss: 0.3309, Val Acc: 0.9811
2025-05-03 07:57:21,687 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 07:57:21,687 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 07:57:41,996 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 07:58:11,597 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9820 to 0.9828
2025-05-03 07:58:11,620 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 18 completed in 49.93s - Train Loss: 0.3152, Train Acc: 0.9983, Val Loss: 0.3305, Val Acc: 0.9828
2025-05-03 07:58:11,676 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 07:58:11,676 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 07:58:31,637 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 07:59:00,257 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9828 to 0.9846
2025-05-03 07:59:00,279 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 19 completed in 48.60s - Train Loss: 0.3147, Train Acc: 0.9989, Val Loss: 0.3284, Val Acc: 0.9846
2025-05-03 07:59:00,337 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 07:59:00,337 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 07:59:21,010 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 07:59:49,048 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3304 to 0.3289
2025-05-03 07:59:49,071 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Epoch 20 completed in 48.73s - Train Loss: 0.3165, Train Acc: 0.9981, Val Loss: 0.3289, Val Acc: 0.9846
2025-05-03 07:59:49,127 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 07:59:49,128 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 07:59:49,130 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_601 - INFO - Starting model evaluation
2025-05-03 07:59:58,621 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_601 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9381
  Recall:    0.9545
  F1 Score:  0.9463
  IoU:       0.8980
  mAP:       0.9796
  AUC:       0.9913
2025-05-03 07:59:58,623 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_601 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_601/final_results.json
2025-05-03 07:59:58,625 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_601 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_601/final_results.json
2025-05-03 07:59:58,625 - main - INFO - 
Summary for configuration 601:
2025-05-03 07:59:58,625 - main - INFO - Accuracy: 0.9693
2025-05-03 07:59:58,625 - main - INFO - Precision: 0.9381
2025-05-03 07:59:58,625 - main - INFO - Recall: 0.9545
2025-05-03 07:59:58,625 - main - INFO - F1 Score: 0.9463
2025-05-03 07:59:58,625 - main - INFO - IoU: 0.8980
2025-05-03 07:59:58,625 - main - INFO - mAP: 0.9796
2025-05-03 07:59:58,625 - main - INFO - AUC: 0.9913
2025-05-03 07:59:58,625 - main - INFO - Training time: 983.32 seconds
2025-05-03 07:59:58,625 - main - INFO - 
==================================================
2025-05-03 07:59:58,625 - main - INFO - Running configuration 602/756:
2025-05-03 07:59:58,625 - main - INFO - Model: MobileNetV3
2025-05-03 07:59:58,625 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 07:59:58,625 - main - INFO - Scheduler: StepLR
2025-05-03 07:59:58,625 - main - INFO - Loss Function: FocalLoss
2025-05-03 07:59:58,625 - main - INFO - ==================================================
2025-05-03 07:59:58,625 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_602 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_602
2025-05-03 07:59:58,625 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_602 - INFO - Config: {
  "id": 602,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:59:58,848 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 07:59:58,848 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 602,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 07:59:58,848 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 07:59:58,936 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 07:59:58,937 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 07:59:58,939 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_602 - INFO - Starting model evaluation
2025-05-03 08:00:08,236 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_602 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9412
  Recall:    0.9510
  F1 Score:  0.9461
  IoU:       0.8977
  mAP:       0.9787
  AUC:       0.9897
2025-05-03 08:00:08,238 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_602 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_602/final_results.json
2025-05-03 08:00:08,239 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_602 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_602/final_results.json
2025-05-03 08:00:08,239 - main - INFO - 
Summary for configuration 602:
2025-05-03 08:00:08,239 - main - INFO - Accuracy: 0.9693
2025-05-03 08:00:08,239 - main - INFO - Precision: 0.9412
2025-05-03 08:00:08,239 - main - INFO - Recall: 0.9510
2025-05-03 08:00:08,239 - main - INFO - F1 Score: 0.9461
2025-05-03 08:00:08,239 - main - INFO - IoU: 0.8977
2025-05-03 08:00:08,239 - main - INFO - mAP: 0.9787
2025-05-03 08:00:08,239 - main - INFO - AUC: 0.9897
2025-05-03 08:00:08,239 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:00:08,239 - main - INFO - 
==================================================
2025-05-03 08:00:08,239 - main - INFO - Running configuration 603/756:
2025-05-03 08:00:08,239 - main - INFO - Model: MobileNetV3
2025-05-03 08:00:08,239 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:00:08,239 - main - INFO - Scheduler: StepLR
2025-05-03 08:00:08,239 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:00:08,239 - main - INFO - ==================================================
2025-05-03 08:00:08,240 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_603 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_603
2025-05-03 08:00:08,240 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_603 - INFO - Config: {
  "id": 603,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:00:08,348 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:00:08,348 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 603,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:00:08,348 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:00:08,436 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:00:08,437 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:00:08,439 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_603 - INFO - Starting model evaluation
2025-05-03 08:00:17,551 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_603 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9375
  Recall:    0.9441
  F1 Score:  0.9408
  IoU:       0.8882
  mAP:       0.9784
  AUC:       0.9905
2025-05-03 08:00:17,553 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_603 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_603/final_results.json
2025-05-03 08:00:17,555 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_603 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_603/final_results.json
2025-05-03 08:00:17,555 - main - INFO - 
Summary for configuration 603:
2025-05-03 08:00:17,555 - main - INFO - Accuracy: 0.9663
2025-05-03 08:00:17,555 - main - INFO - Precision: 0.9375
2025-05-03 08:00:17,555 - main - INFO - Recall: 0.9441
2025-05-03 08:00:17,555 - main - INFO - F1 Score: 0.9408
2025-05-03 08:00:17,555 - main - INFO - IoU: 0.8882
2025-05-03 08:00:17,555 - main - INFO - mAP: 0.9784
2025-05-03 08:00:17,555 - main - INFO - AUC: 0.9905
2025-05-03 08:00:17,555 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:00:17,555 - main - INFO - 
==================================================
2025-05-03 08:00:17,555 - main - INFO - Running configuration 604/756:
2025-05-03 08:00:17,555 - main - INFO - Model: MobileNetV3
2025-05-03 08:00:17,555 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:00:17,555 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:00:17,555 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:00:17,555 - main - INFO - ==================================================
2025-05-03 08:00:17,555 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_604 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_604
2025-05-03 08:00:17,555 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_604 - INFO - Config: {
  "id": 604,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:00:17,857 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:00:17,857 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 604,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:00:17,857 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:00:17,945 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:00:17,946 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:00:17,948 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_604 - INFO - Starting model evaluation
2025-05-03 08:00:27,466 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_604 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9283
  Recall:    0.9510
  F1 Score:  0.9396
  IoU:       0.8860
  mAP:       0.9777
  AUC:       0.9902
2025-05-03 08:00:27,468 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_604 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_604/final_results.json
2025-05-03 08:00:27,469 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_604 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_604/final_results.json
2025-05-03 08:00:27,470 - main - INFO - 
Summary for configuration 604:
2025-05-03 08:00:27,470 - main - INFO - Accuracy: 0.9653
2025-05-03 08:00:27,470 - main - INFO - Precision: 0.9283
2025-05-03 08:00:27,470 - main - INFO - Recall: 0.9510
2025-05-03 08:00:27,470 - main - INFO - F1 Score: 0.9396
2025-05-03 08:00:27,470 - main - INFO - IoU: 0.8860
2025-05-03 08:00:27,470 - main - INFO - mAP: 0.9777
2025-05-03 08:00:27,470 - main - INFO - AUC: 0.9902
2025-05-03 08:00:27,470 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:00:27,470 - main - INFO - 
==================================================
2025-05-03 08:00:27,470 - main - INFO - Running configuration 605/756:
2025-05-03 08:00:27,470 - main - INFO - Model: MobileNetV3
2025-05-03 08:00:27,470 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:00:27,470 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:00:27,470 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:00:27,470 - main - INFO - ==================================================
2025-05-03 08:00:27,470 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_605 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_605
2025-05-03 08:00:27,470 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_605 - INFO - Config: {
  "id": 605,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:00:27,688 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:00:27,689 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 605,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:00:27,689 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:00:27,963 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:00:27,964 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:00:27,966 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_605 - INFO - Starting model evaluation
2025-05-03 08:00:37,494 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_605 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9244
  Recall:    0.9406
  F1 Score:  0.9324
  IoU:       0.8734
  mAP:       0.9785
  AUC:       0.9902
2025-05-03 08:00:37,496 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_605 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_605/final_results.json
2025-05-03 08:00:37,498 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_605 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_605/final_results.json
2025-05-03 08:00:37,498 - main - INFO - 
Summary for configuration 605:
2025-05-03 08:00:37,498 - main - INFO - Accuracy: 0.9613
2025-05-03 08:00:37,498 - main - INFO - Precision: 0.9244
2025-05-03 08:00:37,498 - main - INFO - Recall: 0.9406
2025-05-03 08:00:37,498 - main - INFO - F1 Score: 0.9324
2025-05-03 08:00:37,498 - main - INFO - IoU: 0.8734
2025-05-03 08:00:37,498 - main - INFO - mAP: 0.9785
2025-05-03 08:00:37,498 - main - INFO - AUC: 0.9902
2025-05-03 08:00:37,498 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:00:37,498 - main - INFO - 
==================================================
2025-05-03 08:00:37,498 - main - INFO - Running configuration 606/756:
2025-05-03 08:00:37,498 - main - INFO - Model: MobileNetV3
2025-05-03 08:00:37,498 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:00:37,498 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:00:37,498 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:00:37,498 - main - INFO - ==================================================
2025-05-03 08:00:37,498 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_606 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_606
2025-05-03 08:00:37,498 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_606 - INFO - Config: {
  "id": 606,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:00:37,574 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:00:37,574 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 606,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:00:37,574 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:00:37,661 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:00:37,662 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:00:37,664 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_606 - INFO - Starting model evaluation
2025-05-03 08:00:47,046 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_606 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9308
  Recall:    0.9406
  F1 Score:  0.9357
  IoU:       0.8791
  mAP:       0.9788
  AUC:       0.9905
2025-05-03 08:00:47,049 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_606 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_606/final_results.json
2025-05-03 08:00:47,051 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_606 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_606/final_results.json
2025-05-03 08:00:47,051 - main - INFO - 
Summary for configuration 606:
2025-05-03 08:00:47,051 - main - INFO - Accuracy: 0.9633
2025-05-03 08:00:47,052 - main - INFO - Precision: 0.9308
2025-05-03 08:00:47,052 - main - INFO - Recall: 0.9406
2025-05-03 08:00:47,052 - main - INFO - F1 Score: 0.9357
2025-05-03 08:00:47,052 - main - INFO - IoU: 0.8791
2025-05-03 08:00:47,052 - main - INFO - mAP: 0.9788
2025-05-03 08:00:47,052 - main - INFO - AUC: 0.9905
2025-05-03 08:00:47,052 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:00:47,052 - main - INFO - 
==================================================
2025-05-03 08:00:47,052 - main - INFO - Running configuration 607/756:
2025-05-03 08:00:47,052 - main - INFO - Model: MobileNetV3
2025-05-03 08:00:47,052 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:00:47,052 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:00:47,052 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:00:47,052 - main - INFO - ==================================================
2025-05-03 08:00:47,052 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_607 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_607
2025-05-03 08:00:47,052 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_607 - INFO - Config: {
  "id": 607,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:00:47,129 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:00:47,129 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 607,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:00:47,129 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:00:47,222 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:00:47,223 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:00:47,226 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_607 - INFO - Starting model evaluation
2025-05-03 08:00:56,765 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_607 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9343
  Recall:    0.9441
  F1 Score:  0.9391
  IoU:       0.8852
  mAP:       0.9777
  AUC:       0.9897
2025-05-03 08:00:56,766 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_607 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_607/final_results.json
2025-05-03 08:00:56,768 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_607 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_607/final_results.json
2025-05-03 08:00:56,768 - main - INFO - 
Summary for configuration 607:
2025-05-03 08:00:56,768 - main - INFO - Accuracy: 0.9653
2025-05-03 08:00:56,768 - main - INFO - Precision: 0.9343
2025-05-03 08:00:56,768 - main - INFO - Recall: 0.9441
2025-05-03 08:00:56,768 - main - INFO - F1 Score: 0.9391
2025-05-03 08:00:56,768 - main - INFO - IoU: 0.8852
2025-05-03 08:00:56,768 - main - INFO - mAP: 0.9777
2025-05-03 08:00:56,768 - main - INFO - AUC: 0.9897
2025-05-03 08:00:56,768 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:00:56,768 - main - INFO - 
==================================================
2025-05-03 08:00:56,768 - main - INFO - Running configuration 608/756:
2025-05-03 08:00:56,768 - main - INFO - Model: MobileNetV3
2025-05-03 08:00:56,768 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:00:56,768 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:00:56,768 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:00:56,768 - main - INFO - ==================================================
2025-05-03 08:00:56,768 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_608 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_608
2025-05-03 08:00:56,768 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_608 - INFO - Config: {
  "id": 608,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:00:56,903 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:00:56,903 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 608,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:00:56,903 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:00:56,991 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:00:56,992 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:00:56,994 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_608 - INFO - Starting model evaluation
2025-05-03 08:01:06,008 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_608 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9410
  Recall:    0.9476
  F1 Score:  0.9443
  IoU:       0.8944
  mAP:       0.9792
  AUC:       0.9908
2025-05-03 08:01:06,010 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_608 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_608/final_results.json
2025-05-03 08:01:06,012 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_608 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_608/final_results.json
2025-05-03 08:01:06,012 - main - INFO - 
Summary for configuration 608:
2025-05-03 08:01:06,012 - main - INFO - Accuracy: 0.9683
2025-05-03 08:01:06,012 - main - INFO - Precision: 0.9410
2025-05-03 08:01:06,012 - main - INFO - Recall: 0.9476
2025-05-03 08:01:06,012 - main - INFO - F1 Score: 0.9443
2025-05-03 08:01:06,012 - main - INFO - IoU: 0.8944
2025-05-03 08:01:06,012 - main - INFO - mAP: 0.9792
2025-05-03 08:01:06,012 - main - INFO - AUC: 0.9908
2025-05-03 08:01:06,012 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:01:06,012 - main - INFO - 
==================================================
2025-05-03 08:01:06,012 - main - INFO - Running configuration 609/756:
2025-05-03 08:01:06,012 - main - INFO - Model: MobileNetV3
2025-05-03 08:01:06,012 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:01:06,012 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:01:06,012 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:01:06,012 - main - INFO - ==================================================
2025-05-03 08:01:06,012 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_609 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_609
2025-05-03 08:01:06,012 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_609 - INFO - Config: {
  "id": 609,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:01:06,284 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:01:06,285 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 609,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:01:06,285 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:01:06,372 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:01:06,373 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:01:06,375 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_609 - INFO - Starting model evaluation
2025-05-03 08:01:15,866 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_609 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9345
  Recall:    0.9476
  F1 Score:  0.9410
  IoU:       0.8885
  mAP:       0.9802
  AUC:       0.9918
2025-05-03 08:01:15,868 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_609 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_609/final_results.json
2025-05-03 08:01:15,871 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_609 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_609/final_results.json
2025-05-03 08:01:15,871 - main - INFO - 
Summary for configuration 609:
2025-05-03 08:01:15,871 - main - INFO - Accuracy: 0.9663
2025-05-03 08:01:15,871 - main - INFO - Precision: 0.9345
2025-05-03 08:01:15,871 - main - INFO - Recall: 0.9476
2025-05-03 08:01:15,871 - main - INFO - F1 Score: 0.9410
2025-05-03 08:01:15,871 - main - INFO - IoU: 0.8885
2025-05-03 08:01:15,871 - main - INFO - mAP: 0.9802
2025-05-03 08:01:15,871 - main - INFO - AUC: 0.9918
2025-05-03 08:01:15,871 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:01:15,871 - main - INFO - 
==================================================
2025-05-03 08:01:15,871 - main - INFO - Running configuration 610/756:
2025-05-03 08:01:15,871 - main - INFO - Model: MobileNetV3
2025-05-03 08:01:15,871 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:01:15,871 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:01:15,871 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:01:15,871 - main - INFO - ==================================================
2025-05-03 08:01:15,871 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_610 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_610
2025-05-03 08:01:15,872 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_610 - INFO - Config: {
  "id": 610,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:01:15,955 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:01:15,956 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 610,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:01:15,956 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:01:16,310 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:01:16,311 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:01:16,314 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_610 - INFO - Starting model evaluation
2025-05-03 08:01:26,007 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_610 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9220
  Recall:    0.9510
  F1 Score:  0.9363
  IoU:       0.8803
  mAP:       0.9794
  AUC:       0.9912
2025-05-03 08:01:26,009 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_610 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_610/final_results.json
2025-05-03 08:01:26,010 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_610 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_610/final_results.json
2025-05-03 08:01:26,010 - main - INFO - 
Summary for configuration 610:
2025-05-03 08:01:26,010 - main - INFO - Accuracy: 0.9633
2025-05-03 08:01:26,011 - main - INFO - Precision: 0.9220
2025-05-03 08:01:26,011 - main - INFO - Recall: 0.9510
2025-05-03 08:01:26,011 - main - INFO - F1 Score: 0.9363
2025-05-03 08:01:26,011 - main - INFO - IoU: 0.8803
2025-05-03 08:01:26,011 - main - INFO - mAP: 0.9794
2025-05-03 08:01:26,011 - main - INFO - AUC: 0.9912
2025-05-03 08:01:26,011 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:01:26,011 - main - INFO - 
==================================================
2025-05-03 08:01:26,011 - main - INFO - Running configuration 611/756:
2025-05-03 08:01:26,011 - main - INFO - Model: MobileNetV3
2025-05-03 08:01:26,011 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:01:26,011 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:01:26,011 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:01:26,011 - main - INFO - ==================================================
2025-05-03 08:01:26,011 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_611 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_611
2025-05-03 08:01:26,011 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_611 - INFO - Config: {
  "id": 611,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:01:26,086 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:01:26,086 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 611,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:01:26,086 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:01:26,173 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:01:26,174 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:01:26,176 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_611 - INFO - Starting model evaluation
2025-05-03 08:01:35,328 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_611 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9347
  Recall:    0.9510
  F1 Score:  0.9428
  IoU:       0.8918
  mAP:       0.9808
  AUC:       0.9921
2025-05-03 08:01:35,329 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_611 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_611/final_results.json
2025-05-03 08:01:35,331 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_611 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_611/final_results.json
2025-05-03 08:01:35,331 - main - INFO - 
Summary for configuration 611:
2025-05-03 08:01:35,331 - main - INFO - Accuracy: 0.9673
2025-05-03 08:01:35,331 - main - INFO - Precision: 0.9347
2025-05-03 08:01:35,331 - main - INFO - Recall: 0.9510
2025-05-03 08:01:35,331 - main - INFO - F1 Score: 0.9428
2025-05-03 08:01:35,331 - main - INFO - IoU: 0.8918
2025-05-03 08:01:35,331 - main - INFO - mAP: 0.9808
2025-05-03 08:01:35,331 - main - INFO - AUC: 0.9921
2025-05-03 08:01:35,331 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:01:35,331 - main - INFO - 
==================================================
2025-05-03 08:01:35,331 - main - INFO - Running configuration 612/756:
2025-05-03 08:01:35,331 - main - INFO - Model: MobileNetV3
2025-05-03 08:01:35,331 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 08:01:35,331 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:01:35,331 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:01:35,331 - main - INFO - ==================================================
2025-05-03 08:01:35,331 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_612 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001_id_612
2025-05-03 08:01:35,332 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_612 - INFO - Config: {
  "id": 612,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:01:35,549 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_Adam_lr_0.0001
2025-05-03 08:01:35,550 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 612,
  "model_name": "MobileNetV3",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:01:35,550 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 08:01:35,636 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9846
2025-05-03 08:01:35,637 - training.model_MobileNetV3_opt_Adam_lr_0.0001 - INFO - Training completed after 983.32 seconds
2025-05-03 08:01:35,639 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_612 - INFO - Starting model evaluation
2025-05-03 08:01:45,044 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_612 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9281
  Recall:    0.9476
  F1 Score:  0.9377
  IoU:       0.8827
  mAP:       0.9797
  AUC:       0.9919
2025-05-03 08:01:45,046 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_612 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_612/final_results.json
2025-05-03 08:01:45,048 - training.model_MobileNetV3_opt_Adam_lr_0.0001_id_612 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_Adam_lr_0.0001_id_612/final_results.json
2025-05-03 08:01:45,048 - main - INFO - 
Summary for configuration 612:
2025-05-03 08:01:45,048 - main - INFO - Accuracy: 0.9643
2025-05-03 08:01:45,048 - main - INFO - Precision: 0.9281
2025-05-03 08:01:45,048 - main - INFO - Recall: 0.9476
2025-05-03 08:01:45,048 - main - INFO - F1 Score: 0.9377
2025-05-03 08:01:45,048 - main - INFO - IoU: 0.8827
2025-05-03 08:01:45,048 - main - INFO - mAP: 0.9797
2025-05-03 08:01:45,048 - main - INFO - AUC: 0.9919
2025-05-03 08:01:45,048 - main - INFO - Training time: 983.32 seconds
2025-05-03 08:01:45,048 - main - INFO - 
==================================================
2025-05-03 08:01:45,048 - main - INFO - Running configuration 613/756:
2025-05-03 08:01:45,048 - main - INFO - Model: MobileNetV3
2025-05-03 08:01:45,048 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:01:45,048 - main - INFO - Scheduler: StepLR
2025-05-03 08:01:45,048 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:01:45,048 - main - INFO - ==================================================
2025-05-03 08:01:45,048 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_613 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_613
2025-05-03 08:01:45,048 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_613 - INFO - Config: {
  "id": 613,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:01:45,339 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:01:45,339 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 613,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:01:45,339 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-03 08:01:45,340 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 1/20
2025-05-03 08:02:05,504 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 08:02:33,995 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.4957
2025-05-03 08:02:34,026 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 1 completed in 48.69s - Train Loss: 0.8120, Train Acc: 0.5006, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:02:34,088 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 08:02:34,088 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 2/20
2025-05-03 08:02:54,474 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 08:03:23,841 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Validation loss improved from inf to 0.8169
2025-05-03 08:03:23,865 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 2 completed in 49.78s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:03:23,920 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 08:03:23,920 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 3/20
2025-05-03 08:03:44,957 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 08:04:15,414 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 3 completed in 51.49s - Train Loss: 0.8128, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:04:15,482 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 08:04:15,482 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 4/20
2025-05-03 08:04:35,937 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 08:05:04,238 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 4 completed in 48.76s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:05:04,300 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 08:05:04,301 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 5/20
2025-05-03 08:05:24,175 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 08:05:52,541 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 5 completed in 48.24s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:05:52,598 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 08:05:52,598 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 6/20
2025-05-03 08:06:13,175 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 08:06:40,589 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 6 completed in 47.99s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:06:40,646 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 08:06:40,646 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 7/20
2025-05-03 08:07:02,349 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 08:07:31,068 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 7 completed in 50.42s - Train Loss: 0.8109, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:07:31,126 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 08:07:31,127 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 7 epochs
2025-05-03 08:07:31,127 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 345.73 seconds
2025-05-03 08:07:31,129 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_613 - INFO - Starting model evaluation
2025-05-03 08:07:40,278 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_613 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:07:40,279 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_613 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_613/final_results.json
2025-05-03 08:07:40,280 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_613 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_613/final_results.json
2025-05-03 08:07:40,280 - main - INFO - 
Summary for configuration 613:
2025-05-03 08:07:40,280 - main - INFO - Accuracy: 0.2834
2025-05-03 08:07:40,280 - main - INFO - Precision: 0.2834
2025-05-03 08:07:40,280 - main - INFO - Recall: 1.0000
2025-05-03 08:07:40,280 - main - INFO - F1 Score: 0.4417
2025-05-03 08:07:40,280 - main - INFO - IoU: 0.2834
2025-05-03 08:07:40,281 - main - INFO - mAP: 0.2834
2025-05-03 08:07:40,281 - main - INFO - AUC: 0.5000
2025-05-03 08:07:40,281 - main - INFO - Training time: 345.73 seconds
2025-05-03 08:07:40,281 - main - INFO - 
==================================================
2025-05-03 08:07:40,281 - main - INFO - Running configuration 614/756:
2025-05-03 08:07:40,281 - main - INFO - Model: MobileNetV3
2025-05-03 08:07:40,281 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:07:40,281 - main - INFO - Scheduler: StepLR
2025-05-03 08:07:40,281 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:07:40,281 - main - INFO - ==================================================
2025-05-03 08:07:40,281 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_614 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_614
2025-05-03 08:07:40,281 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_614 - INFO - Config: {
  "id": 614,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:07:40,355 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:07:40,355 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 614,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:07:40,356 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:07:40,443 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 7, best validation accuracy: 0.4957
2025-05-03 08:07:40,444 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 8/20
2025-05-03 08:08:00,923 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 08:08:28,663 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8169 to 0.0912
2025-05-03 08:08:28,688 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 8 completed in 48.24s - Train Loss: 0.0905, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 08:08:28,745 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 08:08:28,745 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 9/20
2025-05-03 08:08:49,101 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 08:09:17,722 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 9 completed in 48.98s - Train Loss: 0.0903, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 08:09:17,780 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 08:09:17,781 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 10/20
2025-05-03 08:09:38,405 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 08:10:07,522 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 10 completed in 49.74s - Train Loss: 0.0905, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 08:10:07,585 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 08:10:07,585 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 11/20
2025-05-03 08:10:28,581 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 08:10:55,917 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 11 completed in 48.33s - Train Loss: 0.0906, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 08:10:55,977 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 08:10:55,977 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 12/20
2025-05-03 08:11:16,765 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 08:11:45,439 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 12 completed in 49.46s - Train Loss: 0.0905, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 08:11:45,521 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 08:11:45,522 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 08:12:04,602 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 08:12:32,778 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 13 completed in 47.26s - Train Loss: 0.0903, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 08:12:32,835 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 08:12:32,836 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 13 epochs
2025-05-03 08:12:32,836 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 638.06 seconds
2025-05-03 08:12:32,838 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_614 - INFO - Starting model evaluation
2025-05-03 08:12:42,411 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_614 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:12:42,413 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_614 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_614/final_results.json
2025-05-03 08:12:42,414 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_614 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_614/final_results.json
2025-05-03 08:12:42,414 - main - INFO - 
Summary for configuration 614:
2025-05-03 08:12:42,414 - main - INFO - Accuracy: 0.2834
2025-05-03 08:12:42,414 - main - INFO - Precision: 0.2834
2025-05-03 08:12:42,414 - main - INFO - Recall: 1.0000
2025-05-03 08:12:42,414 - main - INFO - F1 Score: 0.4417
2025-05-03 08:12:42,414 - main - INFO - IoU: 0.2834
2025-05-03 08:12:42,414 - main - INFO - mAP: 0.2834
2025-05-03 08:12:42,414 - main - INFO - AUC: 0.5000
2025-05-03 08:12:42,414 - main - INFO - Training time: 638.06 seconds
2025-05-03 08:12:42,414 - main - INFO - 
==================================================
2025-05-03 08:12:42,414 - main - INFO - Running configuration 615/756:
2025-05-03 08:12:42,414 - main - INFO - Model: MobileNetV3
2025-05-03 08:12:42,414 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:12:42,414 - main - INFO - Scheduler: StepLR
2025-05-03 08:12:42,414 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:12:42,414 - main - INFO - ==================================================
2025-05-03 08:12:42,415 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_615 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_615
2025-05-03 08:12:42,415 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_615 - INFO - Config: {
  "id": 615,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:12:42,697 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:12:42,697 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 615,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:12:42,697 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:12:42,785 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 13, best validation accuracy: 0.4957
2025-05-03 08:12:42,786 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 08:13:03,219 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 08:13:31,794 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 14 completed in 49.01s - Train Loss: 0.8134, Train Acc: 0.5011, Val Loss: 0.8162, Val Acc: 0.4957
2025-05-03 08:13:31,855 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 08:13:31,856 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 14 epochs
2025-05-03 08:13:31,856 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 687.07 seconds
2025-05-03 08:13:31,859 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_615 - INFO - Starting model evaluation
2025-05-03 08:13:41,218 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_615 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:13:41,220 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_615 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_615/final_results.json
2025-05-03 08:13:41,222 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_615 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_615/final_results.json
2025-05-03 08:13:41,222 - main - INFO - 
Summary for configuration 615:
2025-05-03 08:13:41,222 - main - INFO - Accuracy: 0.2834
2025-05-03 08:13:41,222 - main - INFO - Precision: 0.2834
2025-05-03 08:13:41,222 - main - INFO - Recall: 1.0000
2025-05-03 08:13:41,222 - main - INFO - F1 Score: 0.4417
2025-05-03 08:13:41,222 - main - INFO - IoU: 0.2834
2025-05-03 08:13:41,222 - main - INFO - mAP: 0.2834
2025-05-03 08:13:41,222 - main - INFO - AUC: 0.5000
2025-05-03 08:13:41,222 - main - INFO - Training time: 687.07 seconds
2025-05-03 08:13:41,222 - main - INFO - 
==================================================
2025-05-03 08:13:41,222 - main - INFO - Running configuration 616/756:
2025-05-03 08:13:41,222 - main - INFO - Model: MobileNetV3
2025-05-03 08:13:41,222 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:13:41,222 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:13:41,222 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:13:41,222 - main - INFO - ==================================================
2025-05-03 08:13:41,223 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_616 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_616
2025-05-03 08:13:41,223 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_616 - INFO - Config: {
  "id": 616,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:13:41,416 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:13:41,416 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 616,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:13:41,416 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:13:41,503 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 14, best validation accuracy: 0.4957
2025-05-03 08:13:41,504 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 08:14:01,932 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 08:14:30,419 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 15 completed in 48.91s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:14:30,484 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 08:14:30,485 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 15 epochs
2025-05-03 08:14:30,485 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 735.99 seconds
2025-05-03 08:14:30,487 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_616 - INFO - Starting model evaluation
2025-05-03 08:14:39,663 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_616 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:14:39,664 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_616 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_616/final_results.json
2025-05-03 08:14:39,665 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_616 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_616/final_results.json
2025-05-03 08:14:39,665 - main - INFO - 
Summary for configuration 616:
2025-05-03 08:14:39,665 - main - INFO - Accuracy: 0.2834
2025-05-03 08:14:39,665 - main - INFO - Precision: 0.2834
2025-05-03 08:14:39,665 - main - INFO - Recall: 1.0000
2025-05-03 08:14:39,665 - main - INFO - F1 Score: 0.4417
2025-05-03 08:14:39,665 - main - INFO - IoU: 0.2834
2025-05-03 08:14:39,665 - main - INFO - mAP: 0.2834
2025-05-03 08:14:39,665 - main - INFO - AUC: 0.5000
2025-05-03 08:14:39,665 - main - INFO - Training time: 735.99 seconds
2025-05-03 08:14:39,665 - main - INFO - 
==================================================
2025-05-03 08:14:39,665 - main - INFO - Running configuration 617/756:
2025-05-03 08:14:39,665 - main - INFO - Model: MobileNetV3
2025-05-03 08:14:39,665 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:14:39,665 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:14:39,665 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:14:39,665 - main - INFO - ==================================================
2025-05-03 08:14:39,665 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_617 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_617
2025-05-03 08:14:39,665 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_617 - INFO - Config: {
  "id": 617,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:14:39,975 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:14:39,976 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 617,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:14:39,976 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:14:40,064 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 15, best validation accuracy: 0.4957
2025-05-03 08:14:40,065 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 08:15:00,467 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 08:15:29,347 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 16 completed in 49.28s - Train Loss: 0.0903, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 08:15:29,407 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 08:15:29,407 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 16 epochs
2025-05-03 08:15:29,407 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 785.27 seconds
2025-05-03 08:15:29,410 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_617 - INFO - Starting model evaluation
2025-05-03 08:15:38,955 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_617 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:15:38,957 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_617 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_617/final_results.json
2025-05-03 08:15:38,958 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_617 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_617/final_results.json
2025-05-03 08:15:38,958 - main - INFO - 
Summary for configuration 617:
2025-05-03 08:15:38,958 - main - INFO - Accuracy: 0.2834
2025-05-03 08:15:38,958 - main - INFO - Precision: 0.2834
2025-05-03 08:15:38,958 - main - INFO - Recall: 1.0000
2025-05-03 08:15:38,958 - main - INFO - F1 Score: 0.4417
2025-05-03 08:15:38,958 - main - INFO - IoU: 0.2834
2025-05-03 08:15:38,958 - main - INFO - mAP: 0.2834
2025-05-03 08:15:38,958 - main - INFO - AUC: 0.5000
2025-05-03 08:15:38,958 - main - INFO - Training time: 785.27 seconds
2025-05-03 08:15:38,958 - main - INFO - 
==================================================
2025-05-03 08:15:38,958 - main - INFO - Running configuration 618/756:
2025-05-03 08:15:38,958 - main - INFO - Model: MobileNetV3
2025-05-03 08:15:38,958 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:15:38,958 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:15:38,958 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:15:38,958 - main - INFO - ==================================================
2025-05-03 08:15:38,958 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_618 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_618
2025-05-03 08:15:38,958 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_618 - INFO - Config: {
  "id": 618,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:15:39,264 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:15:39,264 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 618,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:15:39,264 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:15:39,352 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 16, best validation accuracy: 0.4957
2025-05-03 08:15:39,353 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 08:16:00,859 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 08:16:29,671 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 17 completed in 50.32s - Train Loss: 0.8124, Train Acc: 0.5011, Val Loss: 0.8162, Val Acc: 0.4957
2025-05-03 08:16:29,728 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 08:16:29,728 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 17 epochs
2025-05-03 08:16:29,728 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 835.59 seconds
2025-05-03 08:16:29,731 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_618 - INFO - Starting model evaluation
2025-05-03 08:16:38,773 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_618 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:16:38,774 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_618 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_618/final_results.json
2025-05-03 08:16:38,775 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_618 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_618/final_results.json
2025-05-03 08:16:38,775 - main - INFO - 
Summary for configuration 618:
2025-05-03 08:16:38,775 - main - INFO - Accuracy: 0.2834
2025-05-03 08:16:38,775 - main - INFO - Precision: 0.2834
2025-05-03 08:16:38,775 - main - INFO - Recall: 1.0000
2025-05-03 08:16:38,775 - main - INFO - F1 Score: 0.4417
2025-05-03 08:16:38,775 - main - INFO - IoU: 0.2834
2025-05-03 08:16:38,775 - main - INFO - mAP: 0.2834
2025-05-03 08:16:38,775 - main - INFO - AUC: 0.5000
2025-05-03 08:16:38,775 - main - INFO - Training time: 835.59 seconds
2025-05-03 08:16:38,775 - main - INFO - 
==================================================
2025-05-03 08:16:38,775 - main - INFO - Running configuration 619/756:
2025-05-03 08:16:38,775 - main - INFO - Model: MobileNetV3
2025-05-03 08:16:38,775 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:16:38,775 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:16:38,775 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:16:38,775 - main - INFO - ==================================================
2025-05-03 08:16:38,775 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_619 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_619
2025-05-03 08:16:38,775 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_619 - INFO - Config: {
  "id": 619,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:16:39,106 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:16:39,106 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 619,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:16:39,106 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:16:39,195 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 17, best validation accuracy: 0.4957
2025-05-03 08:16:39,195 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 08:16:59,666 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 08:17:27,145 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 18 completed in 47.95s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 08:17:27,203 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 08:17:27,203 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 18 epochs
2025-05-03 08:17:27,203 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 883.54 seconds
2025-05-03 08:17:27,206 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_619 - INFO - Starting model evaluation
2025-05-03 08:17:36,767 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_619 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:17:36,768 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_619 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_619/final_results.json
2025-05-03 08:17:36,769 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_619 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_619/final_results.json
2025-05-03 08:17:36,770 - main - INFO - 
Summary for configuration 619:
2025-05-03 08:17:36,770 - main - INFO - Accuracy: 0.2834
2025-05-03 08:17:36,770 - main - INFO - Precision: 0.2834
2025-05-03 08:17:36,770 - main - INFO - Recall: 1.0000
2025-05-03 08:17:36,770 - main - INFO - F1 Score: 0.4417
2025-05-03 08:17:36,770 - main - INFO - IoU: 0.2834
2025-05-03 08:17:36,770 - main - INFO - mAP: 0.2834
2025-05-03 08:17:36,770 - main - INFO - AUC: 0.5000
2025-05-03 08:17:36,770 - main - INFO - Training time: 883.54 seconds
2025-05-03 08:17:36,770 - main - INFO - 
==================================================
2025-05-03 08:17:36,770 - main - INFO - Running configuration 620/756:
2025-05-03 08:17:36,770 - main - INFO - Model: MobileNetV3
2025-05-03 08:17:36,770 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:17:36,770 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:17:36,770 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:17:36,770 - main - INFO - ==================================================
2025-05-03 08:17:36,770 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_620 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_620
2025-05-03 08:17:36,770 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_620 - INFO - Config: {
  "id": 620,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:17:37,099 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:17:37,100 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 620,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:17:37,100 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:17:37,188 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 18, best validation accuracy: 0.4957
2025-05-03 08:17:37,189 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 08:17:56,938 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 08:18:25,561 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 19 completed in 48.37s - Train Loss: 0.0905, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 08:18:25,626 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 08:18:25,627 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 19 epochs
2025-05-03 08:18:25,627 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 931.91 seconds
2025-05-03 08:18:25,629 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_620 - INFO - Starting model evaluation
2025-05-03 08:18:34,973 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_620 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:18:34,974 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_620 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_620/final_results.json
2025-05-03 08:18:34,975 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_620 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_620/final_results.json
2025-05-03 08:18:34,975 - main - INFO - 
Summary for configuration 620:
2025-05-03 08:18:34,975 - main - INFO - Accuracy: 0.2834
2025-05-03 08:18:34,975 - main - INFO - Precision: 0.2834
2025-05-03 08:18:34,975 - main - INFO - Recall: 1.0000
2025-05-03 08:18:34,975 - main - INFO - F1 Score: 0.4417
2025-05-03 08:18:34,975 - main - INFO - IoU: 0.2834
2025-05-03 08:18:34,975 - main - INFO - mAP: 0.2834
2025-05-03 08:18:34,975 - main - INFO - AUC: 0.5000
2025-05-03 08:18:34,975 - main - INFO - Training time: 931.91 seconds
2025-05-03 08:18:34,975 - main - INFO - 
==================================================
2025-05-03 08:18:34,975 - main - INFO - Running configuration 621/756:
2025-05-03 08:18:34,975 - main - INFO - Model: MobileNetV3
2025-05-03 08:18:34,975 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:18:34,975 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:18:34,975 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:18:34,975 - main - INFO - ==================================================
2025-05-03 08:18:34,975 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_621 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_621
2025-05-03 08:18:34,975 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_621 - INFO - Config: {
  "id": 621,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:18:35,088 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:18:35,088 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 621,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:18:35,088 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:18:35,178 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 19, best validation accuracy: 0.4957
2025-05-03 08:18:35,179 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 08:18:56,083 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 08:19:24,228 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Epoch 20 completed in 49.05s - Train Loss: 0.8124, Train Acc: 0.5011, Val Loss: 0.8162, Val Acc: 0.4957
2025-05-03 08:19:24,287 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 08:19:24,288 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 20 epochs
2025-05-03 08:19:24,288 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 980.96 seconds
2025-05-03 08:19:24,291 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_621 - INFO - Starting model evaluation
2025-05-03 08:19:33,303 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_621 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:19:33,304 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_621 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_621/final_results.json
2025-05-03 08:19:33,305 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_621 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_621/final_results.json
2025-05-03 08:19:33,306 - main - INFO - 
Summary for configuration 621:
2025-05-03 08:19:33,306 - main - INFO - Accuracy: 0.2834
2025-05-03 08:19:33,306 - main - INFO - Precision: 0.2834
2025-05-03 08:19:33,306 - main - INFO - Recall: 1.0000
2025-05-03 08:19:33,306 - main - INFO - F1 Score: 0.4417
2025-05-03 08:19:33,306 - main - INFO - IoU: 0.2834
2025-05-03 08:19:33,306 - main - INFO - mAP: 0.2834
2025-05-03 08:19:33,306 - main - INFO - AUC: 0.5000
2025-05-03 08:19:33,306 - main - INFO - Training time: 980.96 seconds
2025-05-03 08:19:33,306 - main - INFO - 
==================================================
2025-05-03 08:19:33,306 - main - INFO - Running configuration 622/756:
2025-05-03 08:19:33,306 - main - INFO - Model: MobileNetV3
2025-05-03 08:19:33,306 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:19:33,306 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:19:33,306 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:19:33,306 - main - INFO - ==================================================
2025-05-03 08:19:33,306 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_622 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_622
2025-05-03 08:19:33,306 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_622 - INFO - Config: {
  "id": 622,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:19:33,415 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:19:33,415 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 622,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:19:33,415 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:19:33,725 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 08:19:33,726 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 980.96 seconds
2025-05-03 08:19:33,728 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_622 - INFO - Starting model evaluation
2025-05-03 08:19:43,324 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_622 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:19:43,325 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_622 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_622/final_results.json
2025-05-03 08:19:43,326 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_622 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_622/final_results.json
2025-05-03 08:19:43,326 - main - INFO - 
Summary for configuration 622:
2025-05-03 08:19:43,326 - main - INFO - Accuracy: 0.2834
2025-05-03 08:19:43,326 - main - INFO - Precision: 0.2834
2025-05-03 08:19:43,326 - main - INFO - Recall: 1.0000
2025-05-03 08:19:43,326 - main - INFO - F1 Score: 0.4417
2025-05-03 08:19:43,326 - main - INFO - IoU: 0.2834
2025-05-03 08:19:43,326 - main - INFO - mAP: 0.2834
2025-05-03 08:19:43,326 - main - INFO - AUC: 0.5000
2025-05-03 08:19:43,326 - main - INFO - Training time: 980.96 seconds
2025-05-03 08:19:43,326 - main - INFO - 
==================================================
2025-05-03 08:19:43,326 - main - INFO - Running configuration 623/756:
2025-05-03 08:19:43,326 - main - INFO - Model: MobileNetV3
2025-05-03 08:19:43,326 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:19:43,326 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:19:43,326 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:19:43,326 - main - INFO - ==================================================
2025-05-03 08:19:43,327 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_623 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_623
2025-05-03 08:19:43,327 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_623 - INFO - Config: {
  "id": 623,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:19:43,650 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:19:43,650 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 623,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:19:43,650 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:19:43,738 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 08:19:43,739 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 980.96 seconds
2025-05-03 08:19:43,741 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_623 - INFO - Starting model evaluation
2025-05-03 08:19:52,955 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_623 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:19:52,956 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_623 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_623/final_results.json
2025-05-03 08:19:52,957 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_623 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_623/final_results.json
2025-05-03 08:19:52,957 - main - INFO - 
Summary for configuration 623:
2025-05-03 08:19:52,957 - main - INFO - Accuracy: 0.2834
2025-05-03 08:19:52,957 - main - INFO - Precision: 0.2834
2025-05-03 08:19:52,957 - main - INFO - Recall: 1.0000
2025-05-03 08:19:52,957 - main - INFO - F1 Score: 0.4417
2025-05-03 08:19:52,957 - main - INFO - IoU: 0.2834
2025-05-03 08:19:52,957 - main - INFO - mAP: 0.2834
2025-05-03 08:19:52,957 - main - INFO - AUC: 0.5000
2025-05-03 08:19:52,957 - main - INFO - Training time: 980.96 seconds
2025-05-03 08:19:52,957 - main - INFO - 
==================================================
2025-05-03 08:19:52,957 - main - INFO - Running configuration 624/756:
2025-05-03 08:19:52,957 - main - INFO - Model: MobileNetV3
2025-05-03 08:19:52,957 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 08:19:52,957 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:19:52,957 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:19:52,957 - main - INFO - ==================================================
2025-05-03 08:19:52,957 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_624 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01_id_624
2025-05-03 08:19:52,957 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_624 - INFO - Config: {
  "id": 624,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:19:53,034 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.01
2025-05-03 08:19:53,034 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 624,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:19:53,034 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 08:19:53,122 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 08:19:53,123 - training.model_MobileNetV3_opt_AdamW_lr_0.01 - INFO - Training completed after 980.96 seconds
2025-05-03 08:19:53,125 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_624 - INFO - Starting model evaluation
2025-05-03 08:20:02,540 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_624 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 08:20:02,541 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_624 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_624/final_results.json
2025-05-03 08:20:02,542 - training.model_MobileNetV3_opt_AdamW_lr_0.01_id_624 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.01_id_624/final_results.json
2025-05-03 08:20:02,542 - main - INFO - 
Summary for configuration 624:
2025-05-03 08:20:02,542 - main - INFO - Accuracy: 0.2834
2025-05-03 08:20:02,542 - main - INFO - Precision: 0.2834
2025-05-03 08:20:02,542 - main - INFO - Recall: 1.0000
2025-05-03 08:20:02,542 - main - INFO - F1 Score: 0.4417
2025-05-03 08:20:02,542 - main - INFO - IoU: 0.2834
2025-05-03 08:20:02,542 - main - INFO - mAP: 0.2834
2025-05-03 08:20:02,542 - main - INFO - AUC: 0.5000
2025-05-03 08:20:02,542 - main - INFO - Training time: 980.96 seconds
2025-05-03 08:20:02,542 - main - INFO - 
==================================================
2025-05-03 08:20:02,542 - main - INFO - Running configuration 625/756:
2025-05-03 08:20:02,542 - main - INFO - Model: MobileNetV3
2025-05-03 08:20:02,542 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:20:02,542 - main - INFO - Scheduler: StepLR
2025-05-03 08:20:02,542 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:20:02,542 - main - INFO - ==================================================
2025-05-03 08:20:02,542 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_625 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_625
2025-05-03 08:20:02,542 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_625 - INFO - Config: {
  "id": 625,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:20:02,861 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:20:02,862 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 625,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:20:02,862 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-03 08:20:02,862 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 1/20
2025-05-03 08:20:24,513 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 08:20:52,717 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.8122
2025-05-03 08:20:52,734 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 1 completed in 49.87s - Train Loss: 0.5026, Train Acc: 0.8059, Val Loss: 0.4988, Val Acc: 0.8122
2025-05-03 08:20:52,801 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 08:20:52,801 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 2/20
2025-05-03 08:21:13,825 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 08:21:41,849 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation loss improved from inf to 0.5393
2025-05-03 08:21:41,874 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 2 completed in 49.07s - Train Loss: 0.4816, Train Acc: 0.8293, Val Loss: 0.5393, Val Acc: 0.7727
2025-05-03 08:21:41,939 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 08:21:41,940 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 3/20
2025-05-03 08:22:03,211 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 08:22:30,963 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8122 to 0.8945
2025-05-03 08:22:30,992 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 3 completed in 49.05s - Train Loss: 0.4600, Train Acc: 0.8522, Val Loss: 0.4146, Val Acc: 0.8945
2025-05-03 08:22:31,050 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 08:22:31,051 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 4/20
2025-05-03 08:22:51,149 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 08:23:20,264 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.5393 to 0.4520
2025-05-03 08:23:20,290 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 4 completed in 49.24s - Train Loss: 0.4537, Train Acc: 0.8571, Val Loss: 0.4520, Val Acc: 0.8585
2025-05-03 08:23:20,347 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 08:23:20,347 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 5/20
2025-05-03 08:23:41,885 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 08:24:11,266 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 5 completed in 50.92s - Train Loss: 0.4766, Train Acc: 0.8357, Val Loss: 0.4976, Val Acc: 0.8122
2025-05-03 08:24:11,321 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 08:24:11,322 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 6/20
2025-05-03 08:24:31,985 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 08:25:01,211 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 6 completed in 49.89s - Train Loss: 0.4280, Train Acc: 0.8822, Val Loss: 0.4648, Val Acc: 0.8465
2025-05-03 08:25:01,270 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 08:25:01,271 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 7/20
2025-05-03 08:25:21,884 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 08:25:50,846 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.4520 to 0.4474
2025-05-03 08:25:50,875 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 7 completed in 49.60s - Train Loss: 0.4091, Train Acc: 0.9033, Val Loss: 0.4474, Val Acc: 0.8636
2025-05-03 08:25:50,931 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 08:25:50,931 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 8/20
2025-05-03 08:26:11,184 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 08:26:38,938 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 8 completed in 48.01s - Train Loss: 0.4023, Train Acc: 0.9095, Val Loss: 0.4646, Val Acc: 0.8431
2025-05-03 08:26:38,998 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 08:26:38,998 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 9/20
2025-05-03 08:26:59,199 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 08:27:28,712 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.4474 to 0.4348
2025-05-03 08:27:28,745 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 9 completed in 49.75s - Train Loss: 0.4241, Train Acc: 0.8876, Val Loss: 0.4348, Val Acc: 0.8765
2025-05-03 08:27:28,806 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 08:27:28,807 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 10/20
2025-05-03 08:27:48,799 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 08:28:17,181 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 10 completed in 48.37s - Train Loss: 0.4660, Train Acc: 0.8456, Val Loss: 0.5861, Val Acc: 0.7273
2025-05-03 08:28:17,236 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 08:28:17,237 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 11/20
2025-05-03 08:28:38,980 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 08:29:07,328 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 11 completed in 50.09s - Train Loss: 0.4605, Train Acc: 0.8505, Val Loss: 0.4383, Val Acc: 0.8756
2025-05-03 08:29:07,384 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 08:29:07,385 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 12/20
2025-05-03 08:29:27,601 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 08:29:56,262 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.4348 to 0.4262
2025-05-03 08:29:56,284 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 12 completed in 48.90s - Train Loss: 0.4282, Train Acc: 0.8837, Val Loss: 0.4262, Val Acc: 0.8859
2025-05-03 08:29:56,341 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 08:29:56,341 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 08:30:17,439 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 08:30:44,851 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.8945 to 0.9099
2025-05-03 08:30:44,876 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 13 completed in 48.53s - Train Loss: 0.4153, Train Acc: 0.8983, Val Loss: 0.4013, Val Acc: 0.9099
2025-05-03 08:30:44,931 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 08:30:44,931 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 08:31:05,457 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 08:31:34,479 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9099 to 0.9177
2025-05-03 08:31:34,505 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 14 completed in 49.57s - Train Loss: 0.4009, Train Acc: 0.9108, Val Loss: 0.3953, Val Acc: 0.9177
2025-05-03 08:31:34,566 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 08:31:34,566 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 08:31:55,293 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 08:32:24,178 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9177 to 0.9288
2025-05-03 08:32:24,200 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 15 completed in 49.63s - Train Loss: 0.3842, Train Acc: 0.9294, Val Loss: 0.3835, Val Acc: 0.9288
2025-05-03 08:32:24,254 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 08:32:24,254 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 08:32:43,854 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 08:33:12,205 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9288 to 0.9297
2025-05-03 08:33:12,228 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 16 completed in 47.97s - Train Loss: 0.3813, Train Acc: 0.9314, Val Loss: 0.3832, Val Acc: 0.9297
2025-05-03 08:33:12,283 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 08:33:12,283 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 08:33:33,278 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 08:34:01,422 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9297 to 0.9348
2025-05-03 08:34:01,445 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 17 completed in 49.16s - Train Loss: 0.3777, Train Acc: 0.9356, Val Loss: 0.3774, Val Acc: 0.9348
2025-05-03 08:34:01,502 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 08:34:01,502 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 08:34:21,659 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 08:34:49,869 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.4262 to 0.3799
2025-05-03 08:34:49,892 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 18 completed in 48.39s - Train Loss: 0.3777, Train Acc: 0.9354, Val Loss: 0.3799, Val Acc: 0.9314
2025-05-03 08:34:49,949 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 08:34:49,950 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 08:35:09,874 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 08:35:39,149 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.3799 to 0.3775
2025-05-03 08:35:39,172 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 19 completed in 49.22s - Train Loss: 0.3678, Train Acc: 0.9449, Val Loss: 0.3775, Val Acc: 0.9348
2025-05-03 08:35:39,227 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 08:35:39,228 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 08:35:59,378 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 08:36:27,832 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.9348 to 0.9383
2025-05-03 08:36:27,856 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Epoch 20 completed in 48.63s - Train Loss: 0.3691, Train Acc: 0.9427, Val Loss: 0.3709, Val Acc: 0.9383
2025-05-03 08:36:27,921 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 08:36:27,921 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:36:27,924 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_625 - INFO - Starting model evaluation
2025-05-03 08:36:37,285 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_625 - INFO - Evaluation metrics:
  Accuracy:  0.9257
  Precision: 0.8528
  Recall:    0.8916
  F1 Score:  0.8718
  IoU:       0.7727
  mAP:       0.8681
  AUC:       0.9519
2025-05-03 08:36:37,288 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_625 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_625/final_results.json
2025-05-03 08:36:37,290 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_625 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_625/final_results.json
2025-05-03 08:36:37,290 - main - INFO - 
Summary for configuration 625:
2025-05-03 08:36:37,290 - main - INFO - Accuracy: 0.9257
2025-05-03 08:36:37,290 - main - INFO - Precision: 0.8528
2025-05-03 08:36:37,290 - main - INFO - Recall: 0.8916
2025-05-03 08:36:37,290 - main - INFO - F1 Score: 0.8718
2025-05-03 08:36:37,290 - main - INFO - IoU: 0.7727
2025-05-03 08:36:37,290 - main - INFO - mAP: 0.8681
2025-05-03 08:36:37,290 - main - INFO - AUC: 0.9519
2025-05-03 08:36:37,290 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:36:37,290 - main - INFO - 
==================================================
2025-05-03 08:36:37,290 - main - INFO - Running configuration 626/756:
2025-05-03 08:36:37,290 - main - INFO - Model: MobileNetV3
2025-05-03 08:36:37,290 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:36:37,290 - main - INFO - Scheduler: StepLR
2025-05-03 08:36:37,290 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:36:37,290 - main - INFO - ==================================================
2025-05-03 08:36:37,291 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_626 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_626
2025-05-03 08:36:37,291 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_626 - INFO - Config: {
  "id": 626,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:36:37,372 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:36:37,373 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 626,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:36:37,373 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:36:37,461 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:36:37,461 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:36:37,464 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_626 - INFO - Starting model evaluation
2025-05-03 08:36:46,931 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_626 - INFO - Evaluation metrics:
  Accuracy:  0.9286
  Precision: 0.8591
  Recall:    0.8951
  F1 Score:  0.8767
  IoU:       0.7805
  mAP:       0.8636
  AUC:       0.9481
2025-05-03 08:36:46,932 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_626 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_626/final_results.json
2025-05-03 08:36:46,934 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_626 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_626/final_results.json
2025-05-03 08:36:46,934 - main - INFO - 
Summary for configuration 626:
2025-05-03 08:36:46,934 - main - INFO - Accuracy: 0.9286
2025-05-03 08:36:46,934 - main - INFO - Precision: 0.8591
2025-05-03 08:36:46,934 - main - INFO - Recall: 0.8951
2025-05-03 08:36:46,934 - main - INFO - F1 Score: 0.8767
2025-05-03 08:36:46,934 - main - INFO - IoU: 0.7805
2025-05-03 08:36:46,934 - main - INFO - mAP: 0.8636
2025-05-03 08:36:46,934 - main - INFO - AUC: 0.9481
2025-05-03 08:36:46,934 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:36:46,934 - main - INFO - 
==================================================
2025-05-03 08:36:46,934 - main - INFO - Running configuration 627/756:
2025-05-03 08:36:46,934 - main - INFO - Model: MobileNetV3
2025-05-03 08:36:46,934 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:36:46,934 - main - INFO - Scheduler: StepLR
2025-05-03 08:36:46,934 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:36:46,934 - main - INFO - ==================================================
2025-05-03 08:36:46,934 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_627 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_627
2025-05-03 08:36:46,934 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_627 - INFO - Config: {
  "id": 627,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:36:47,139 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:36:47,139 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 627,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:36:47,139 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:36:47,231 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:36:47,232 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:36:47,235 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_627 - INFO - Starting model evaluation
2025-05-03 08:36:56,253 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_627 - INFO - Evaluation metrics:
  Accuracy:  0.9306
  Precision: 0.8624
  Recall:    0.8986
  F1 Score:  0.8801
  IoU:       0.7859
  mAP:       0.8770
  AUC:       0.9573
2025-05-03 08:36:56,258 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_627 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_627/final_results.json
2025-05-03 08:36:56,260 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_627 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_627/final_results.json
2025-05-03 08:36:56,260 - main - INFO - 
Summary for configuration 627:
2025-05-03 08:36:56,260 - main - INFO - Accuracy: 0.9306
2025-05-03 08:36:56,260 - main - INFO - Precision: 0.8624
2025-05-03 08:36:56,260 - main - INFO - Recall: 0.8986
2025-05-03 08:36:56,260 - main - INFO - F1 Score: 0.8801
2025-05-03 08:36:56,260 - main - INFO - IoU: 0.7859
2025-05-03 08:36:56,260 - main - INFO - mAP: 0.8770
2025-05-03 08:36:56,260 - main - INFO - AUC: 0.9573
2025-05-03 08:36:56,260 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:36:56,260 - main - INFO - 
==================================================
2025-05-03 08:36:56,260 - main - INFO - Running configuration 628/756:
2025-05-03 08:36:56,260 - main - INFO - Model: MobileNetV3
2025-05-03 08:36:56,260 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:36:56,260 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:36:56,260 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:36:56,260 - main - INFO - ==================================================
2025-05-03 08:36:56,260 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_628 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_628
2025-05-03 08:36:56,260 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_628 - INFO - Config: {
  "id": 628,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:36:56,336 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:36:56,336 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 628,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:36:56,336 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:36:56,424 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:36:56,425 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:36:56,427 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_628 - INFO - Starting model evaluation
2025-05-03 08:37:05,903 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_628 - INFO - Evaluation metrics:
  Accuracy:  0.9346
  Precision: 0.8716
  Recall:    0.9021
  F1 Score:  0.8866
  IoU:       0.7963
  mAP:       0.8806
  AUC:       0.9579
2025-05-03 08:37:05,905 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_628 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_628/final_results.json
2025-05-03 08:37:05,907 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_628 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_628/final_results.json
2025-05-03 08:37:05,907 - main - INFO - 
Summary for configuration 628:
2025-05-03 08:37:05,907 - main - INFO - Accuracy: 0.9346
2025-05-03 08:37:05,907 - main - INFO - Precision: 0.8716
2025-05-03 08:37:05,907 - main - INFO - Recall: 0.9021
2025-05-03 08:37:05,907 - main - INFO - F1 Score: 0.8866
2025-05-03 08:37:05,907 - main - INFO - IoU: 0.7963
2025-05-03 08:37:05,907 - main - INFO - mAP: 0.8806
2025-05-03 08:37:05,907 - main - INFO - AUC: 0.9579
2025-05-03 08:37:05,907 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:37:05,907 - main - INFO - 
==================================================
2025-05-03 08:37:05,907 - main - INFO - Running configuration 629/756:
2025-05-03 08:37:05,907 - main - INFO - Model: MobileNetV3
2025-05-03 08:37:05,907 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:37:05,907 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:37:05,907 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:37:05,907 - main - INFO - ==================================================
2025-05-03 08:37:05,907 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_629 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_629
2025-05-03 08:37:05,907 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_629 - INFO - Config: {
  "id": 629,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:37:06,219 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:37:06,219 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 629,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:37:06,219 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:37:06,307 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:37:06,308 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:37:06,310 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_629 - INFO - Starting model evaluation
2025-05-03 08:37:15,406 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_629 - INFO - Evaluation metrics:
  Accuracy:  0.9326
  Precision: 0.8707
  Recall:    0.8951
  F1 Score:  0.8828
  IoU:       0.7901
  mAP:       0.8817
  AUC:       0.9570
2025-05-03 08:37:15,407 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_629 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_629/final_results.json
2025-05-03 08:37:15,409 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_629 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_629/final_results.json
2025-05-03 08:37:15,409 - main - INFO - 
Summary for configuration 629:
2025-05-03 08:37:15,409 - main - INFO - Accuracy: 0.9326
2025-05-03 08:37:15,409 - main - INFO - Precision: 0.8707
2025-05-03 08:37:15,409 - main - INFO - Recall: 0.8951
2025-05-03 08:37:15,409 - main - INFO - F1 Score: 0.8828
2025-05-03 08:37:15,409 - main - INFO - IoU: 0.7901
2025-05-03 08:37:15,409 - main - INFO - mAP: 0.8817
2025-05-03 08:37:15,409 - main - INFO - AUC: 0.9570
2025-05-03 08:37:15,409 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:37:15,409 - main - INFO - 
==================================================
2025-05-03 08:37:15,409 - main - INFO - Running configuration 630/756:
2025-05-03 08:37:15,409 - main - INFO - Model: MobileNetV3
2025-05-03 08:37:15,409 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:37:15,409 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:37:15,409 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:37:15,409 - main - INFO - ==================================================
2025-05-03 08:37:15,410 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_630 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_630
2025-05-03 08:37:15,410 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_630 - INFO - Config: {
  "id": 630,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:37:15,679 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:37:15,679 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 630,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:37:15,679 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:37:15,767 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:37:15,767 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:37:15,770 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_630 - INFO - Starting model evaluation
2025-05-03 08:37:25,315 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_630 - INFO - Evaluation metrics:
  Accuracy:  0.9306
  Precision: 0.8576
  Recall:    0.9056
  F1 Score:  0.8810
  IoU:       0.7872
  mAP:       0.8660
  AUC:       0.9519
2025-05-03 08:37:25,317 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_630 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_630/final_results.json
2025-05-03 08:37:25,319 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_630 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_630/final_results.json
2025-05-03 08:37:25,319 - main - INFO - 
Summary for configuration 630:
2025-05-03 08:37:25,319 - main - INFO - Accuracy: 0.9306
2025-05-03 08:37:25,319 - main - INFO - Precision: 0.8576
2025-05-03 08:37:25,319 - main - INFO - Recall: 0.9056
2025-05-03 08:37:25,319 - main - INFO - F1 Score: 0.8810
2025-05-03 08:37:25,319 - main - INFO - IoU: 0.7872
2025-05-03 08:37:25,319 - main - INFO - mAP: 0.8660
2025-05-03 08:37:25,319 - main - INFO - AUC: 0.9519
2025-05-03 08:37:25,319 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:37:25,319 - main - INFO - 
==================================================
2025-05-03 08:37:25,319 - main - INFO - Running configuration 631/756:
2025-05-03 08:37:25,319 - main - INFO - Model: MobileNetV3
2025-05-03 08:37:25,319 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:37:25,319 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:37:25,319 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:37:25,319 - main - INFO - ==================================================
2025-05-03 08:37:25,319 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_631 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_631
2025-05-03 08:37:25,319 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_631 - INFO - Config: {
  "id": 631,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:37:25,536 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:37:25,536 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 631,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:37:25,536 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:37:25,624 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:37:25,624 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:37:25,627 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_631 - INFO - Starting model evaluation
2025-05-03 08:37:34,820 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_631 - INFO - Evaluation metrics:
  Accuracy:  0.9286
  Precision: 0.8567
  Recall:    0.8986
  F1 Score:  0.8771
  IoU:       0.7812
  mAP:       0.8653
  AUC:       0.9508
2025-05-03 08:37:34,822 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_631 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_631/final_results.json
2025-05-03 08:37:34,824 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_631 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_631/final_results.json
2025-05-03 08:37:34,824 - main - INFO - 
Summary for configuration 631:
2025-05-03 08:37:34,824 - main - INFO - Accuracy: 0.9286
2025-05-03 08:37:34,824 - main - INFO - Precision: 0.8567
2025-05-03 08:37:34,824 - main - INFO - Recall: 0.8986
2025-05-03 08:37:34,824 - main - INFO - F1 Score: 0.8771
2025-05-03 08:37:34,824 - main - INFO - IoU: 0.7812
2025-05-03 08:37:34,824 - main - INFO - mAP: 0.8653
2025-05-03 08:37:34,824 - main - INFO - AUC: 0.9508
2025-05-03 08:37:34,824 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:37:34,824 - main - INFO - 
==================================================
2025-05-03 08:37:34,824 - main - INFO - Running configuration 632/756:
2025-05-03 08:37:34,824 - main - INFO - Model: MobileNetV3
2025-05-03 08:37:34,824 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:37:34,824 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:37:34,824 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:37:34,824 - main - INFO - ==================================================
2025-05-03 08:37:34,824 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_632 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_632
2025-05-03 08:37:34,824 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_632 - INFO - Config: {
  "id": 632,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:37:34,899 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:37:34,899 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 632,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:37:34,899 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:37:35,329 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:37:35,330 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:37:35,332 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_632 - INFO - Starting model evaluation
2025-05-03 08:37:44,589 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_632 - INFO - Evaluation metrics:
  Accuracy:  0.9257
  Precision: 0.8576
  Recall:    0.8846
  F1 Score:  0.8709
  IoU:       0.7713
  mAP:       0.8612
  AUC:       0.9486
2025-05-03 08:37:44,591 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_632 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_632/final_results.json
2025-05-03 08:37:44,593 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_632 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_632/final_results.json
2025-05-03 08:37:44,593 - main - INFO - 
Summary for configuration 632:
2025-05-03 08:37:44,593 - main - INFO - Accuracy: 0.9257
2025-05-03 08:37:44,593 - main - INFO - Precision: 0.8576
2025-05-03 08:37:44,593 - main - INFO - Recall: 0.8846
2025-05-03 08:37:44,593 - main - INFO - F1 Score: 0.8709
2025-05-03 08:37:44,593 - main - INFO - IoU: 0.7713
2025-05-03 08:37:44,593 - main - INFO - mAP: 0.8612
2025-05-03 08:37:44,593 - main - INFO - AUC: 0.9486
2025-05-03 08:37:44,593 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:37:44,593 - main - INFO - 
==================================================
2025-05-03 08:37:44,593 - main - INFO - Running configuration 633/756:
2025-05-03 08:37:44,593 - main - INFO - Model: MobileNetV3
2025-05-03 08:37:44,593 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:37:44,593 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:37:44,593 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:37:44,593 - main - INFO - ==================================================
2025-05-03 08:37:44,593 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_633 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_633
2025-05-03 08:37:44,593 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_633 - INFO - Config: {
  "id": 633,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:37:44,668 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:37:44,668 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 633,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:37:44,668 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:37:45,021 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:37:45,022 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:37:45,024 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_633 - INFO - Starting model evaluation
2025-05-03 08:37:54,334 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_633 - INFO - Evaluation metrics:
  Accuracy:  0.9227
  Precision: 0.8514
  Recall:    0.8811
  F1 Score:  0.8660
  IoU:       0.7636
  mAP:       0.8652
  AUC:       0.9501
2025-05-03 08:37:54,335 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_633 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_633/final_results.json
2025-05-03 08:37:54,337 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_633 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_633/final_results.json
2025-05-03 08:37:54,337 - main - INFO - 
Summary for configuration 633:
2025-05-03 08:37:54,337 - main - INFO - Accuracy: 0.9227
2025-05-03 08:37:54,337 - main - INFO - Precision: 0.8514
2025-05-03 08:37:54,337 - main - INFO - Recall: 0.8811
2025-05-03 08:37:54,337 - main - INFO - F1 Score: 0.8660
2025-05-03 08:37:54,337 - main - INFO - IoU: 0.7636
2025-05-03 08:37:54,337 - main - INFO - mAP: 0.8652
2025-05-03 08:37:54,337 - main - INFO - AUC: 0.9501
2025-05-03 08:37:54,337 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:37:54,337 - main - INFO - 
==================================================
2025-05-03 08:37:54,337 - main - INFO - Running configuration 634/756:
2025-05-03 08:37:54,337 - main - INFO - Model: MobileNetV3
2025-05-03 08:37:54,337 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:37:54,337 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:37:54,337 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:37:54,337 - main - INFO - ==================================================
2025-05-03 08:37:54,337 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_634 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_634
2025-05-03 08:37:54,338 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_634 - INFO - Config: {
  "id": 634,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:37:54,412 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:37:54,413 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 634,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:37:54,413 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:37:54,499 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:37:54,500 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:37:54,502 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_634 - INFO - Starting model evaluation
2025-05-03 08:38:03,603 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_634 - INFO - Evaluation metrics:
  Accuracy:  0.9267
  Precision: 0.8655
  Recall:    0.8776
  F1 Score:  0.8715
  IoU:       0.7723
  mAP:       0.8769
  AUC:       0.9519
2025-05-03 08:38:03,605 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_634 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_634/final_results.json
2025-05-03 08:38:03,606 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_634 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_634/final_results.json
2025-05-03 08:38:03,606 - main - INFO - 
Summary for configuration 634:
2025-05-03 08:38:03,606 - main - INFO - Accuracy: 0.9267
2025-05-03 08:38:03,606 - main - INFO - Precision: 0.8655
2025-05-03 08:38:03,606 - main - INFO - Recall: 0.8776
2025-05-03 08:38:03,606 - main - INFO - F1 Score: 0.8715
2025-05-03 08:38:03,606 - main - INFO - IoU: 0.7723
2025-05-03 08:38:03,606 - main - INFO - mAP: 0.8769
2025-05-03 08:38:03,606 - main - INFO - AUC: 0.9519
2025-05-03 08:38:03,606 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:38:03,607 - main - INFO - 
==================================================
2025-05-03 08:38:03,607 - main - INFO - Running configuration 635/756:
2025-05-03 08:38:03,607 - main - INFO - Model: MobileNetV3
2025-05-03 08:38:03,607 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:38:03,607 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:38:03,607 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:38:03,607 - main - INFO - ==================================================
2025-05-03 08:38:03,607 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_635 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_635
2025-05-03 08:38:03,607 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_635 - INFO - Config: {
  "id": 635,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:38:03,697 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:38:03,697 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 635,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:38:03,697 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:38:03,785 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:38:03,786 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:38:03,788 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_635 - INFO - Starting model evaluation
2025-05-03 08:38:13,136 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_635 - INFO - Evaluation metrics:
  Accuracy:  0.9277
  Precision: 0.8586
  Recall:    0.8916
  F1 Score:  0.8748
  IoU:       0.7774
  mAP:       0.8765
  AUC:       0.9578
2025-05-03 08:38:13,138 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_635 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_635/final_results.json
2025-05-03 08:38:13,139 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_635 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_635/final_results.json
2025-05-03 08:38:13,140 - main - INFO - 
Summary for configuration 635:
2025-05-03 08:38:13,140 - main - INFO - Accuracy: 0.9277
2025-05-03 08:38:13,140 - main - INFO - Precision: 0.8586
2025-05-03 08:38:13,140 - main - INFO - Recall: 0.8916
2025-05-03 08:38:13,140 - main - INFO - F1 Score: 0.8748
2025-05-03 08:38:13,140 - main - INFO - IoU: 0.7774
2025-05-03 08:38:13,140 - main - INFO - mAP: 0.8765
2025-05-03 08:38:13,140 - main - INFO - AUC: 0.9578
2025-05-03 08:38:13,140 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:38:13,140 - main - INFO - 
==================================================
2025-05-03 08:38:13,140 - main - INFO - Running configuration 636/756:
2025-05-03 08:38:13,140 - main - INFO - Model: MobileNetV3
2025-05-03 08:38:13,140 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 08:38:13,140 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:38:13,140 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:38:13,140 - main - INFO - ==================================================
2025-05-03 08:38:13,140 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_636 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001_id_636
2025-05-03 08:38:13,140 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_636 - INFO - Config: {
  "id": 636,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:38:13,215 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.001
2025-05-03 08:38:13,215 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 636,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:38:13,215 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 08:38:13,302 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9383
2025-05-03 08:38:13,303 - training.model_MobileNetV3_opt_AdamW_lr_0.001 - INFO - Training completed after 984.99 seconds
2025-05-03 08:38:13,305 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_636 - INFO - Starting model evaluation
2025-05-03 08:38:22,358 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_636 - INFO - Evaluation metrics:
  Accuracy:  0.9286
  Precision: 0.8615
  Recall:    0.8916
  F1 Score:  0.8763
  IoU:       0.7798
  mAP:       0.8683
  AUC:       0.9525
2025-05-03 08:38:22,360 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_636 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_636/final_results.json
2025-05-03 08:38:22,362 - training.model_MobileNetV3_opt_AdamW_lr_0.001_id_636 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.001_id_636/final_results.json
2025-05-03 08:38:22,362 - main - INFO - 
Summary for configuration 636:
2025-05-03 08:38:22,362 - main - INFO - Accuracy: 0.9286
2025-05-03 08:38:22,362 - main - INFO - Precision: 0.8615
2025-05-03 08:38:22,362 - main - INFO - Recall: 0.8916
2025-05-03 08:38:22,362 - main - INFO - F1 Score: 0.8763
2025-05-03 08:38:22,362 - main - INFO - IoU: 0.7798
2025-05-03 08:38:22,362 - main - INFO - mAP: 0.8683
2025-05-03 08:38:22,362 - main - INFO - AUC: 0.9525
2025-05-03 08:38:22,362 - main - INFO - Training time: 984.99 seconds
2025-05-03 08:38:22,362 - main - INFO - 
==================================================
2025-05-03 08:38:22,362 - main - INFO - Running configuration 637/756:
2025-05-03 08:38:22,362 - main - INFO - Model: MobileNetV3
2025-05-03 08:38:22,362 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:38:22,362 - main - INFO - Scheduler: StepLR
2025-05-03 08:38:22,362 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:38:22,362 - main - INFO - ==================================================
2025-05-03 08:38:22,362 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_637 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_637
2025-05-03 08:38:22,362 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_637 - INFO - Config: {
  "id": 637,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:38:22,686 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:38:22,686 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 637,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:38:22,686 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-03 08:38:22,687 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-03 08:38:42,698 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 08:39:12,313 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9468
2025-05-03 08:39:12,330 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 1 completed in 49.64s - Train Loss: 0.4460, Train Acc: 0.8769, Val Loss: 0.3714, Val Acc: 0.9468
2025-05-03 08:39:12,386 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 08:39:12,387 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-03 08:39:31,781 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 08:40:00,877 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9468 to 0.9657
2025-05-03 08:40:00,899 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 2 completed in 48.51s - Train Loss: 0.3531, Train Acc: 0.9605, Val Loss: 0.3453, Val Acc: 0.9657
2025-05-03 08:40:00,963 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 08:40:00,963 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-03 08:40:20,802 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 08:40:48,293 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from inf to 0.3473
2025-05-03 08:40:48,316 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 3 completed in 47.35s - Train Loss: 0.3385, Train Acc: 0.9734, Val Loss: 0.3473, Val Acc: 0.9648
2025-05-03 08:40:48,382 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 08:40:48,383 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-03 08:41:08,919 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 08:41:36,839 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9657 to 0.9794
2025-05-03 08:41:36,863 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 4 completed in 48.48s - Train Loss: 0.3319, Train Acc: 0.9831, Val Loss: 0.3353, Val Acc: 0.9794
2025-05-03 08:41:36,919 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 08:41:36,920 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 08:41:56,888 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 08:42:25,302 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3473 to 0.3349
2025-05-03 08:42:25,326 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 5 completed in 48.41s - Train Loss: 0.3327, Train Acc: 0.9809, Val Loss: 0.3349, Val Acc: 0.9786
2025-05-03 08:42:25,380 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 08:42:25,380 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 08:42:45,527 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 08:43:14,660 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 6 completed in 49.28s - Train Loss: 0.3310, Train Acc: 0.9822, Val Loss: 0.3414, Val Acc: 0.9700
2025-05-03 08:43:14,724 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 08:43:14,724 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 08:43:35,075 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 08:44:03,009 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 7 completed in 48.29s - Train Loss: 0.3269, Train Acc: 0.9865, Val Loss: 0.3413, Val Acc: 0.9717
2025-05-03 08:44:03,069 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 08:44:03,070 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 08:44:23,529 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 08:44:52,386 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 8 completed in 49.32s - Train Loss: 0.3291, Train Acc: 0.9831, Val Loss: 0.3401, Val Acc: 0.9708
2025-05-03 08:44:52,442 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 08:44:52,442 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 08:45:12,220 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 08:45:40,877 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9794 to 0.9863
2025-05-03 08:45:40,901 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 9 completed in 48.46s - Train Loss: 0.3243, Train Acc: 0.9893, Val Loss: 0.3257, Val Acc: 0.9863
2025-05-03 08:45:40,956 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 08:45:40,956 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 08:46:01,083 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 08:46:28,992 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3349 to 0.3312
2025-05-03 08:46:29,017 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 10 completed in 48.06s - Train Loss: 0.3211, Train Acc: 0.9923, Val Loss: 0.3312, Val Acc: 0.9811
2025-05-03 08:46:29,069 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 08:46:29,069 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 08:46:49,173 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 08:47:17,819 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3312 to 0.3300
2025-05-03 08:47:17,843 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 11 completed in 48.77s - Train Loss: 0.3233, Train Acc: 0.9912, Val Loss: 0.3300, Val Acc: 0.9820
2025-05-03 08:47:17,899 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 08:47:17,899 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 08:47:37,974 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 08:48:07,686 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3300 to 0.3285
2025-05-03 08:48:07,710 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 12 completed in 49.81s - Train Loss: 0.3192, Train Acc: 0.9942, Val Loss: 0.3285, Val Acc: 0.9828
2025-05-03 08:48:07,765 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 08:48:07,765 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 08:48:28,390 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 08:48:56,941 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 13 completed in 49.18s - Train Loss: 0.3190, Train Acc: 0.9944, Val Loss: 0.3292, Val Acc: 0.9837
2025-05-03 08:48:56,996 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 08:48:56,996 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 08:49:17,366 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 08:49:46,820 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 14 completed in 49.82s - Train Loss: 0.3186, Train Acc: 0.9946, Val Loss: 0.3286, Val Acc: 0.9846
2025-05-03 08:49:46,882 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 08:49:46,883 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 08:50:07,040 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 08:50:34,932 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3285 to 0.3279
2025-05-03 08:50:34,954 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 15 completed in 48.07s - Train Loss: 0.3186, Train Acc: 0.9949, Val Loss: 0.3279, Val Acc: 0.9854
2025-05-03 08:50:35,012 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 08:50:35,012 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 08:50:55,087 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 08:51:24,442 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 16 completed in 49.43s - Train Loss: 0.3179, Train Acc: 0.9957, Val Loss: 0.3279, Val Acc: 0.9854
2025-05-03 08:51:24,500 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 08:51:24,501 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 08:51:45,059 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 08:52:14,480 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3279 to 0.3274
2025-05-03 08:52:14,506 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 17 completed in 50.01s - Train Loss: 0.3179, Train Acc: 0.9957, Val Loss: 0.3274, Val Acc: 0.9854
2025-05-03 08:52:14,560 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 08:52:14,561 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 08:52:35,106 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 08:53:03,995 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 18 completed in 49.43s - Train Loss: 0.3171, Train Acc: 0.9961, Val Loss: 0.3278, Val Acc: 0.9846
2025-05-03 08:53:04,054 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 08:53:04,054 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 08:53:24,692 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 08:53:54,171 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 19 completed in 50.12s - Train Loss: 0.3164, Train Acc: 0.9972, Val Loss: 0.3283, Val Acc: 0.9846
2025-05-03 08:53:54,238 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 08:53:54,238 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 08:54:14,294 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 08:54:42,348 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Epoch 20 completed in 48.11s - Train Loss: 0.3166, Train Acc: 0.9968, Val Loss: 0.3278, Val Acc: 0.9854
2025-05-03 08:54:42,405 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 08:54:42,406 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:54:42,408 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_637 - INFO - Starting model evaluation
2025-05-03 08:54:51,482 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_637 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9310
  Recall:    0.9441
  F1 Score:  0.9375
  IoU:       0.8824
  mAP:       0.9699
  AUC:       0.9912
2025-05-03 08:54:51,484 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_637 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_637/final_results.json
2025-05-03 08:54:51,486 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_637 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_637/final_results.json
2025-05-03 08:54:51,486 - main - INFO - 
Summary for configuration 637:
2025-05-03 08:54:51,486 - main - INFO - Accuracy: 0.9643
2025-05-03 08:54:51,486 - main - INFO - Precision: 0.9310
2025-05-03 08:54:51,486 - main - INFO - Recall: 0.9441
2025-05-03 08:54:51,486 - main - INFO - F1 Score: 0.9375
2025-05-03 08:54:51,486 - main - INFO - IoU: 0.8824
2025-05-03 08:54:51,486 - main - INFO - mAP: 0.9699
2025-05-03 08:54:51,486 - main - INFO - AUC: 0.9912
2025-05-03 08:54:51,486 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:54:51,486 - main - INFO - 
==================================================
2025-05-03 08:54:51,486 - main - INFO - Running configuration 638/756:
2025-05-03 08:54:51,486 - main - INFO - Model: MobileNetV3
2025-05-03 08:54:51,486 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:54:51,486 - main - INFO - Scheduler: StepLR
2025-05-03 08:54:51,486 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:54:51,486 - main - INFO - ==================================================
2025-05-03 08:54:51,486 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_638 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_638
2025-05-03 08:54:51,486 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_638 - INFO - Config: {
  "id": 638,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:54:51,754 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:54:51,754 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 638,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:54:51,754 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:54:51,842 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:54:51,843 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:54:51,845 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_638 - INFO - Starting model evaluation
2025-05-03 08:55:00,938 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_638 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9347
  Recall:    0.9510
  F1 Score:  0.9428
  IoU:       0.8918
  mAP:       0.9762
  AUC:       0.9930
2025-05-03 08:55:00,940 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_638 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_638/final_results.json
2025-05-03 08:55:00,942 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_638 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_638/final_results.json
2025-05-03 08:55:00,942 - main - INFO - 
Summary for configuration 638:
2025-05-03 08:55:00,942 - main - INFO - Accuracy: 0.9673
2025-05-03 08:55:00,942 - main - INFO - Precision: 0.9347
2025-05-03 08:55:00,942 - main - INFO - Recall: 0.9510
2025-05-03 08:55:00,942 - main - INFO - F1 Score: 0.9428
2025-05-03 08:55:00,942 - main - INFO - IoU: 0.8918
2025-05-03 08:55:00,942 - main - INFO - mAP: 0.9762
2025-05-03 08:55:00,942 - main - INFO - AUC: 0.9930
2025-05-03 08:55:00,942 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:55:00,942 - main - INFO - 
==================================================
2025-05-03 08:55:00,942 - main - INFO - Running configuration 639/756:
2025-05-03 08:55:00,942 - main - INFO - Model: MobileNetV3
2025-05-03 08:55:00,942 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:55:00,942 - main - INFO - Scheduler: StepLR
2025-05-03 08:55:00,942 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:55:00,942 - main - INFO - ==================================================
2025-05-03 08:55:00,942 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_639 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_639
2025-05-03 08:55:00,942 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_639 - INFO - Config: {
  "id": 639,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:55:01,209 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:55:01,210 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 639,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:55:01,210 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:55:01,298 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:55:01,299 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:55:01,301 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_639 - INFO - Starting model evaluation
2025-05-03 08:55:10,469 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_639 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9347
  Recall:    0.9510
  F1 Score:  0.9428
  IoU:       0.8918
  mAP:       0.9753
  AUC:       0.9927
2025-05-03 08:55:10,470 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_639 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_639/final_results.json
2025-05-03 08:55:10,472 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_639 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_639/final_results.json
2025-05-03 08:55:10,472 - main - INFO - 
Summary for configuration 639:
2025-05-03 08:55:10,472 - main - INFO - Accuracy: 0.9673
2025-05-03 08:55:10,472 - main - INFO - Precision: 0.9347
2025-05-03 08:55:10,472 - main - INFO - Recall: 0.9510
2025-05-03 08:55:10,472 - main - INFO - F1 Score: 0.9428
2025-05-03 08:55:10,472 - main - INFO - IoU: 0.8918
2025-05-03 08:55:10,472 - main - INFO - mAP: 0.9753
2025-05-03 08:55:10,472 - main - INFO - AUC: 0.9927
2025-05-03 08:55:10,472 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:55:10,472 - main - INFO - 
==================================================
2025-05-03 08:55:10,472 - main - INFO - Running configuration 640/756:
2025-05-03 08:55:10,472 - main - INFO - Model: MobileNetV3
2025-05-03 08:55:10,472 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:55:10,472 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:55:10,472 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:55:10,472 - main - INFO - ==================================================
2025-05-03 08:55:10,472 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_640 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_640
2025-05-03 08:55:10,472 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_640 - INFO - Config: {
  "id": 640,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:55:10,548 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:55:10,548 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 640,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:55:10,548 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:55:10,635 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:55:10,636 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:55:10,638 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_640 - INFO - Starting model evaluation
2025-05-03 08:55:19,944 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_640 - INFO - Evaluation metrics:
  Accuracy:  0.9703
  Precision: 0.9414
  Recall:    0.9545
  F1 Score:  0.9479
  IoU:       0.9010
  mAP:       0.9755
  AUC:       0.9927
2025-05-03 08:55:19,947 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_640 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_640/final_results.json
2025-05-03 08:55:19,948 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_640 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_640/final_results.json
2025-05-03 08:55:19,948 - main - INFO - 
Summary for configuration 640:
2025-05-03 08:55:19,948 - main - INFO - Accuracy: 0.9703
2025-05-03 08:55:19,948 - main - INFO - Precision: 0.9414
2025-05-03 08:55:19,948 - main - INFO - Recall: 0.9545
2025-05-03 08:55:19,948 - main - INFO - F1 Score: 0.9479
2025-05-03 08:55:19,948 - main - INFO - IoU: 0.9010
2025-05-03 08:55:19,948 - main - INFO - mAP: 0.9755
2025-05-03 08:55:19,948 - main - INFO - AUC: 0.9927
2025-05-03 08:55:19,948 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:55:19,948 - main - INFO - 
==================================================
2025-05-03 08:55:19,949 - main - INFO - Running configuration 641/756:
2025-05-03 08:55:19,949 - main - INFO - Model: MobileNetV3
2025-05-03 08:55:19,949 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:55:19,949 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:55:19,949 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:55:19,949 - main - INFO - ==================================================
2025-05-03 08:55:19,949 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_641 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_641
2025-05-03 08:55:19,949 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_641 - INFO - Config: {
  "id": 641,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:55:20,056 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:55:20,056 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 641,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:55:20,056 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:55:20,146 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:55:20,147 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:55:20,149 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_641 - INFO - Starting model evaluation
2025-05-03 08:55:29,308 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_641 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9381
  Recall:    0.9545
  F1 Score:  0.9463
  IoU:       0.8980
  mAP:       0.9716
  AUC:       0.9918
2025-05-03 08:55:29,309 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_641 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_641/final_results.json
2025-05-03 08:55:29,311 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_641 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_641/final_results.json
2025-05-03 08:55:29,311 - main - INFO - 
Summary for configuration 641:
2025-05-03 08:55:29,311 - main - INFO - Accuracy: 0.9693
2025-05-03 08:55:29,311 - main - INFO - Precision: 0.9381
2025-05-03 08:55:29,311 - main - INFO - Recall: 0.9545
2025-05-03 08:55:29,311 - main - INFO - F1 Score: 0.9463
2025-05-03 08:55:29,311 - main - INFO - IoU: 0.8980
2025-05-03 08:55:29,311 - main - INFO - mAP: 0.9716
2025-05-03 08:55:29,311 - main - INFO - AUC: 0.9918
2025-05-03 08:55:29,311 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:55:29,311 - main - INFO - 
==================================================
2025-05-03 08:55:29,311 - main - INFO - Running configuration 642/756:
2025-05-03 08:55:29,311 - main - INFO - Model: MobileNetV3
2025-05-03 08:55:29,311 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:55:29,311 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 08:55:29,311 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:55:29,311 - main - INFO - ==================================================
2025-05-03 08:55:29,311 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_642 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_642
2025-05-03 08:55:29,311 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_642 - INFO - Config: {
  "id": 642,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:55:29,387 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:55:29,387 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 642,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:55:29,387 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:55:29,475 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:55:29,476 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:55:29,478 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_642 - INFO - Starting model evaluation
2025-05-03 08:55:38,431 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_642 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9313
  Recall:    0.9476
  F1 Score:  0.9393
  IoU:       0.8856
  mAP:       0.9711
  AUC:       0.9916
2025-05-03 08:55:38,432 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_642 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_642/final_results.json
2025-05-03 08:55:38,434 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_642 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_642/final_results.json
2025-05-03 08:55:38,434 - main - INFO - 
Summary for configuration 642:
2025-05-03 08:55:38,434 - main - INFO - Accuracy: 0.9653
2025-05-03 08:55:38,434 - main - INFO - Precision: 0.9313
2025-05-03 08:55:38,434 - main - INFO - Recall: 0.9476
2025-05-03 08:55:38,434 - main - INFO - F1 Score: 0.9393
2025-05-03 08:55:38,434 - main - INFO - IoU: 0.8856
2025-05-03 08:55:38,434 - main - INFO - mAP: 0.9711
2025-05-03 08:55:38,434 - main - INFO - AUC: 0.9916
2025-05-03 08:55:38,434 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:55:38,434 - main - INFO - 
==================================================
2025-05-03 08:55:38,434 - main - INFO - Running configuration 643/756:
2025-05-03 08:55:38,434 - main - INFO - Model: MobileNetV3
2025-05-03 08:55:38,434 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:55:38,434 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:55:38,434 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:55:38,434 - main - INFO - ==================================================
2025-05-03 08:55:38,434 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_643 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_643
2025-05-03 08:55:38,434 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_643 - INFO - Config: {
  "id": 643,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:55:38,510 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:55:38,510 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 643,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:55:38,511 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:55:38,598 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:55:38,599 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:55:38,601 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_643 - INFO - Starting model evaluation
2025-05-03 08:55:48,255 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_643 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9281
  Recall:    0.9476
  F1 Score:  0.9377
  IoU:       0.8827
  mAP:       0.9699
  AUC:       0.9913
2025-05-03 08:55:48,257 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_643 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_643/final_results.json
2025-05-03 08:55:48,258 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_643 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_643/final_results.json
2025-05-03 08:55:48,258 - main - INFO - 
Summary for configuration 643:
2025-05-03 08:55:48,258 - main - INFO - Accuracy: 0.9643
2025-05-03 08:55:48,258 - main - INFO - Precision: 0.9281
2025-05-03 08:55:48,258 - main - INFO - Recall: 0.9476
2025-05-03 08:55:48,258 - main - INFO - F1 Score: 0.9377
2025-05-03 08:55:48,258 - main - INFO - IoU: 0.8827
2025-05-03 08:55:48,258 - main - INFO - mAP: 0.9699
2025-05-03 08:55:48,258 - main - INFO - AUC: 0.9913
2025-05-03 08:55:48,258 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:55:48,258 - main - INFO - 
==================================================
2025-05-03 08:55:48,258 - main - INFO - Running configuration 644/756:
2025-05-03 08:55:48,258 - main - INFO - Model: MobileNetV3
2025-05-03 08:55:48,258 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:55:48,258 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:55:48,258 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:55:48,258 - main - INFO - ==================================================
2025-05-03 08:55:48,259 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_644 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_644
2025-05-03 08:55:48,259 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_644 - INFO - Config: {
  "id": 644,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:55:48,502 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:55:48,502 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 644,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:55:48,502 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:55:48,590 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:55:48,591 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:55:48,593 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_644 - INFO - Starting model evaluation
2025-05-03 08:55:57,928 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_644 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9288
  Recall:    0.9580
  F1 Score:  0.9432
  IoU:       0.8925
  mAP:       0.9715
  AUC:       0.9921
2025-05-03 08:55:57,930 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_644 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_644/final_results.json
2025-05-03 08:55:57,932 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_644 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_644/final_results.json
2025-05-03 08:55:57,932 - main - INFO - 
Summary for configuration 644:
2025-05-03 08:55:57,932 - main - INFO - Accuracy: 0.9673
2025-05-03 08:55:57,932 - main - INFO - Precision: 0.9288
2025-05-03 08:55:57,932 - main - INFO - Recall: 0.9580
2025-05-03 08:55:57,932 - main - INFO - F1 Score: 0.9432
2025-05-03 08:55:57,932 - main - INFO - IoU: 0.8925
2025-05-03 08:55:57,932 - main - INFO - mAP: 0.9715
2025-05-03 08:55:57,932 - main - INFO - AUC: 0.9921
2025-05-03 08:55:57,932 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:55:57,932 - main - INFO - 
==================================================
2025-05-03 08:55:57,932 - main - INFO - Running configuration 645/756:
2025-05-03 08:55:57,932 - main - INFO - Model: MobileNetV3
2025-05-03 08:55:57,932 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:55:57,932 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 08:55:57,932 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:55:57,932 - main - INFO - ==================================================
2025-05-03 08:55:57,932 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_645 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_645
2025-05-03 08:55:57,932 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_645 - INFO - Config: {
  "id": 645,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:55:58,007 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:55:58,008 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 645,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:55:58,008 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:55:58,095 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:55:58,096 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:55:58,098 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_645 - INFO - Starting model evaluation
2025-05-03 08:56:07,767 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_645 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9317
  Recall:    0.9545
  F1 Score:  0.9430
  IoU:       0.8922
  mAP:       0.9709
  AUC:       0.9920
2025-05-03 08:56:07,769 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_645 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_645/final_results.json
2025-05-03 08:56:07,770 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_645 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_645/final_results.json
2025-05-03 08:56:07,770 - main - INFO - 
Summary for configuration 645:
2025-05-03 08:56:07,770 - main - INFO - Accuracy: 0.9673
2025-05-03 08:56:07,770 - main - INFO - Precision: 0.9317
2025-05-03 08:56:07,770 - main - INFO - Recall: 0.9545
2025-05-03 08:56:07,770 - main - INFO - F1 Score: 0.9430
2025-05-03 08:56:07,770 - main - INFO - IoU: 0.8922
2025-05-03 08:56:07,770 - main - INFO - mAP: 0.9709
2025-05-03 08:56:07,770 - main - INFO - AUC: 0.9920
2025-05-03 08:56:07,770 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:56:07,770 - main - INFO - 
==================================================
2025-05-03 08:56:07,770 - main - INFO - Running configuration 646/756:
2025-05-03 08:56:07,770 - main - INFO - Model: MobileNetV3
2025-05-03 08:56:07,770 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:56:07,770 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:56:07,770 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:56:07,770 - main - INFO - ==================================================
2025-05-03 08:56:07,771 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_646 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_646
2025-05-03 08:56:07,771 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_646 - INFO - Config: {
  "id": 646,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:56:07,845 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:56:07,846 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 646,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:56:07,846 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:56:08,015 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:56:08,016 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:56:08,018 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_646 - INFO - Starting model evaluation
2025-05-03 08:56:17,255 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_646 - INFO - Evaluation metrics:
  Accuracy:  0.9683
  Precision: 0.9320
  Recall:    0.9580
  F1 Score:  0.9448
  IoU:       0.8954
  mAP:       0.9702
  AUC:       0.9912
2025-05-03 08:56:17,257 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_646 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_646/final_results.json
2025-05-03 08:56:17,259 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_646 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_646/final_results.json
2025-05-03 08:56:17,259 - main - INFO - 
Summary for configuration 646:
2025-05-03 08:56:17,259 - main - INFO - Accuracy: 0.9683
2025-05-03 08:56:17,259 - main - INFO - Precision: 0.9320
2025-05-03 08:56:17,259 - main - INFO - Recall: 0.9580
2025-05-03 08:56:17,259 - main - INFO - F1 Score: 0.9448
2025-05-03 08:56:17,259 - main - INFO - IoU: 0.8954
2025-05-03 08:56:17,259 - main - INFO - mAP: 0.9702
2025-05-03 08:56:17,259 - main - INFO - AUC: 0.9912
2025-05-03 08:56:17,259 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:56:17,259 - main - INFO - 
==================================================
2025-05-03 08:56:17,259 - main - INFO - Running configuration 647/756:
2025-05-03 08:56:17,259 - main - INFO - Model: MobileNetV3
2025-05-03 08:56:17,259 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:56:17,259 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:56:17,259 - main - INFO - Loss Function: FocalLoss
2025-05-03 08:56:17,259 - main - INFO - ==================================================
2025-05-03 08:56:17,259 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_647 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_647
2025-05-03 08:56:17,259 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_647 - INFO - Config: {
  "id": 647,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:56:17,587 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:56:17,587 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 647,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 08:56:17,587 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:56:17,676 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:56:17,677 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:56:17,679 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_647 - INFO - Starting model evaluation
2025-05-03 08:56:26,929 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_647 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9281
  Recall:    0.9476
  F1 Score:  0.9377
  IoU:       0.8827
  mAP:       0.9749
  AUC:       0.9924
2025-05-03 08:56:26,930 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_647 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_647/final_results.json
2025-05-03 08:56:26,932 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_647 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_647/final_results.json
2025-05-03 08:56:26,932 - main - INFO - 
Summary for configuration 647:
2025-05-03 08:56:26,932 - main - INFO - Accuracy: 0.9643
2025-05-03 08:56:26,932 - main - INFO - Precision: 0.9281
2025-05-03 08:56:26,932 - main - INFO - Recall: 0.9476
2025-05-03 08:56:26,932 - main - INFO - F1 Score: 0.9377
2025-05-03 08:56:26,932 - main - INFO - IoU: 0.8827
2025-05-03 08:56:26,932 - main - INFO - mAP: 0.9749
2025-05-03 08:56:26,932 - main - INFO - AUC: 0.9924
2025-05-03 08:56:26,932 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:56:26,932 - main - INFO - 
==================================================
2025-05-03 08:56:26,932 - main - INFO - Running configuration 648/756:
2025-05-03 08:56:26,932 - main - INFO - Model: MobileNetV3
2025-05-03 08:56:26,932 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 08:56:26,932 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 08:56:26,932 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 08:56:26,932 - main - INFO - ==================================================
2025-05-03 08:56:26,932 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_648 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001_id_648
2025-05-03 08:56:26,932 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_648 - INFO - Config: {
  "id": 648,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:56:27,060 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_MobileNetV3_opt_AdamW_lr_0.0001
2025-05-03 08:56:27,060 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 648,
  "model_name": "MobileNetV3",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 08:56:27,060 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 08:56:27,352 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9863
2025-05-03 08:56:27,353 - training.model_MobileNetV3_opt_AdamW_lr_0.0001 - INFO - Training completed after 979.66 seconds
2025-05-03 08:56:27,355 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_648 - INFO - Starting model evaluation
2025-05-03 08:56:36,921 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_648 - INFO - Evaluation metrics:
  Accuracy:  0.9673
  Precision: 0.9317
  Recall:    0.9545
  F1 Score:  0.9430
  IoU:       0.8922
  mAP:       0.9754
  AUC:       0.9925
2025-05-03 08:56:36,923 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_648 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_648/final_results.json
2025-05-03 08:56:36,925 - training.model_MobileNetV3_opt_AdamW_lr_0.0001_id_648 - INFO - Final results saved to model_results_v2/model_MobileNetV3_opt_AdamW_lr_0.0001_id_648/final_results.json
2025-05-03 08:56:36,925 - main - INFO - 
Summary for configuration 648:
2025-05-03 08:56:36,925 - main - INFO - Accuracy: 0.9673
2025-05-03 08:56:36,925 - main - INFO - Precision: 0.9317
2025-05-03 08:56:36,925 - main - INFO - Recall: 0.9545
2025-05-03 08:56:36,925 - main - INFO - F1 Score: 0.9430
2025-05-03 08:56:36,925 - main - INFO - IoU: 0.8922
2025-05-03 08:56:36,925 - main - INFO - mAP: 0.9754
2025-05-03 08:56:36,925 - main - INFO - AUC: 0.9925
2025-05-03 08:56:36,925 - main - INFO - Training time: 979.66 seconds
2025-05-03 08:56:36,925 - main - INFO - 
==================================================
2025-05-03 08:56:36,925 - main - INFO - Running configuration 649/756:
2025-05-03 08:56:36,925 - main - INFO - Model: ConvNeXt
2025-05-03 08:56:36,925 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 08:56:36,925 - main - INFO - Scheduler: StepLR
2025-05-03 08:56:36,925 - main - INFO - Loss Function: CrossEntropy
2025-05-03 08:56:36,925 - main - INFO - ==================================================
2025-05-03 08:56:36,925 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_649 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_649
2025-05-03 08:56:36,925 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_649 - INFO - Config: {
  "id": 649,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:56:37,830 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 08:56:37,830 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 649,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 08:56:37,830 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-03 08:56:37,831 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 1/20
2025-05-03 08:57:21,540 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 08:58:15,367 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.8808
2025-05-03 08:58:15,569 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 1 completed in 97.74s - Train Loss: 0.5241, Train Acc: 0.7782, Val Loss: 0.4291, Val Acc: 0.8808
2025-05-03 08:58:16,125 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 08:58:16,125 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 2/20
2025-05-03 08:59:00,856 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 08:59:52,800 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.8808 to 0.9134
2025-05-03 08:59:53,080 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 2 completed in 96.95s - Train Loss: 0.4438, Train Acc: 0.8659, Val Loss: 0.3994, Val Acc: 0.9134
2025-05-03 08:59:53,595 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 08:59:53,595 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 3/20
2025-05-03 09:00:37,297 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 09:01:30,140 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from inf to 0.4292
2025-05-03 09:01:30,409 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 3 completed in 96.81s - Train Loss: 0.4154, Train Acc: 0.8945, Val Loss: 0.4292, Val Acc: 0.8834
2025-05-03 09:01:30,939 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 09:01:30,939 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 4/20
2025-05-03 09:02:15,093 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 09:03:06,858 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.4292 to 0.4047
2025-05-03 09:03:07,135 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 4 completed in 96.20s - Train Loss: 0.3993, Train Acc: 0.9114, Val Loss: 0.4047, Val Acc: 0.9039
2025-05-03 09:03:07,643 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 09:03:07,643 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 5/20
2025-05-03 09:03:51,398 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 09:04:44,903 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9134 to 0.9545
2025-05-03 09:04:45,196 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 5 completed in 97.55s - Train Loss: 0.3801, Train Acc: 0.9288, Val Loss: 0.3562, Val Acc: 0.9545
2025-05-03 09:04:45,735 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 09:04:45,736 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 6/20
2025-05-03 09:05:32,054 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 09:06:26,505 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.4047 to 0.3951
2025-05-03 09:06:26,796 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 6 completed in 101.06s - Train Loss: 0.3664, Train Acc: 0.9453, Val Loss: 0.3951, Val Acc: 0.9134
2025-05-03 09:06:27,318 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 09:06:27,319 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 7/20
2025-05-03 09:07:11,989 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 09:08:06,100 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3951 to 0.3703
2025-05-03 09:08:06,380 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 7 completed in 99.06s - Train Loss: 0.3643, Train Acc: 0.9492, Val Loss: 0.3703, Val Acc: 0.9417
2025-05-03 09:08:06,904 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 09:08:06,904 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 8/20
2025-05-03 09:08:52,574 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 09:09:45,700 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3703 to 0.3643
2025-05-03 09:09:45,988 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 8 completed in 99.08s - Train Loss: 0.3549, Train Acc: 0.9562, Val Loss: 0.3643, Val Acc: 0.9477
2025-05-03 09:09:46,547 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 09:09:46,548 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 9/20
2025-05-03 09:10:31,308 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 09:11:25,659 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3643 to 0.3590
2025-05-03 09:11:25,929 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 9 completed in 99.38s - Train Loss: 0.3514, Train Acc: 0.9612, Val Loss: 0.3590, Val Acc: 0.9520
2025-05-03 09:11:26,433 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 09:11:26,433 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 10/20
2025-05-03 09:12:10,999 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 09:13:05,269 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 10 completed in 98.84s - Train Loss: 0.3581, Train Acc: 0.9524, Val Loss: 0.3823, Val Acc: 0.9280
2025-05-03 09:13:05,807 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 09:13:05,807 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 11/20
2025-05-03 09:13:51,959 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 09:14:46,429 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9545 to 0.9563
2025-05-03 09:14:46,720 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 11 completed in 100.91s - Train Loss: 0.3457, Train Acc: 0.9655, Val Loss: 0.3544, Val Acc: 0.9563
2025-05-03 09:14:47,245 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 09:14:47,245 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 12/20
2025-05-03 09:15:33,422 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 09:16:26,733 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3590 to 0.3563
2025-05-03 09:16:27,023 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 12 completed in 99.78s - Train Loss: 0.3399, Train Acc: 0.9728, Val Loss: 0.3563, Val Acc: 0.9554
2025-05-03 09:16:27,539 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 09:16:27,539 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 09:17:11,998 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 09:18:06,606 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9563 to 0.9631
2025-05-03 09:18:06,894 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 13 completed in 99.35s - Train Loss: 0.3348, Train Acc: 0.9785, Val Loss: 0.3491, Val Acc: 0.9631
2025-05-03 09:18:07,413 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 09:18:07,413 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 09:18:52,669 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 09:19:47,093 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3563 to 0.3484
2025-05-03 09:19:47,366 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 14 completed in 99.95s - Train Loss: 0.3334, Train Acc: 0.9801, Val Loss: 0.3484, Val Acc: 0.9631
2025-05-03 09:19:47,884 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 09:19:47,885 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 09:20:33,766 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 09:21:28,404 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 15 completed in 100.52s - Train Loss: 0.3333, Train Acc: 0.9796, Val Loss: 0.3530, Val Acc: 0.9580
2025-05-03 09:21:28,932 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 09:21:28,933 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 09:22:15,141 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 09:23:08,917 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 16 completed in 99.98s - Train Loss: 0.3306, Train Acc: 0.9828, Val Loss: 0.3498, Val Acc: 0.9623
2025-05-03 09:23:09,428 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 09:23:09,429 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 09:23:55,394 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 09:24:50,316 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 17 completed in 100.89s - Train Loss: 0.3292, Train Acc: 0.9843, Val Loss: 0.3492, Val Acc: 0.9623
2025-05-03 09:24:50,863 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 09:24:50,864 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 09:25:35,757 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 09:26:29,685 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation accuracy improved from 0.9631 to 0.9708
2025-05-03 09:26:29,982 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 18 completed in 99.12s - Train Loss: 0.3280, Train Acc: 0.9854, Val Loss: 0.3399, Val Acc: 0.9708
2025-05-03 09:26:30,503 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 09:26:30,503 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 09:27:16,616 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 09:28:11,236 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Validation loss improved from 0.3484 to 0.3442
2025-05-03 09:28:11,515 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 19 completed in 101.01s - Train Loss: 0.3283, Train Acc: 0.9848, Val Loss: 0.3442, Val Acc: 0.9666
2025-05-03 09:28:12,058 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 09:28:12,058 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 09:28:57,887 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 09:29:52,238 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Epoch 20 completed in 100.18s - Train Loss: 0.3287, Train Acc: 0.9848, Val Loss: 0.3453, Val Acc: 0.9648
2025-05-03 09:29:52,758 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 09:29:52,759 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:29:52,762 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_649 - INFO - Starting model evaluation
2025-05-03 09:30:04,194 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_649 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.8993
  Recall:    0.9371
  F1 Score:  0.9178
  IoU:       0.8481
  mAP:       0.9620
  AUC:       0.9820
2025-05-03 09:30:04,196 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_649 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_649/final_results.json
2025-05-03 09:30:04,198 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_649 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_649/final_results.json
2025-05-03 09:30:04,198 - main - INFO - 
Summary for configuration 649:
2025-05-03 09:30:04,198 - main - INFO - Accuracy: 0.9524
2025-05-03 09:30:04,198 - main - INFO - Precision: 0.8993
2025-05-03 09:30:04,198 - main - INFO - Recall: 0.9371
2025-05-03 09:30:04,198 - main - INFO - F1 Score: 0.9178
2025-05-03 09:30:04,198 - main - INFO - IoU: 0.8481
2025-05-03 09:30:04,198 - main - INFO - mAP: 0.9620
2025-05-03 09:30:04,198 - main - INFO - AUC: 0.9820
2025-05-03 09:30:04,198 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:30:04,198 - main - INFO - 
==================================================
2025-05-03 09:30:04,198 - main - INFO - Running configuration 650/756:
2025-05-03 09:30:04,198 - main - INFO - Model: ConvNeXt
2025-05-03 09:30:04,198 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:30:04,198 - main - INFO - Scheduler: StepLR
2025-05-03 09:30:04,198 - main - INFO - Loss Function: FocalLoss
2025-05-03 09:30:04,198 - main - INFO - ==================================================
2025-05-03 09:30:04,198 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_650 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_650
2025-05-03 09:30:04,198 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_650 - INFO - Config: {
  "id": 650,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 09:30:04,966 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:30:04,966 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 650,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 09:30:04,966 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:30:05,323 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:30:05,324 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:30:05,327 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_650 - INFO - Starting model evaluation
2025-05-03 09:30:17,069 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_650 - INFO - Evaluation metrics:
  Accuracy:  0.9495
  Precision: 0.8956
  Recall:    0.9301
  F1 Score:  0.9125
  IoU:       0.8391
  mAP:       0.9658
  AUC:       0.9827
2025-05-03 09:30:17,073 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_650 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_650/final_results.json
2025-05-03 09:30:17,075 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_650 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_650/final_results.json
2025-05-03 09:30:17,075 - main - INFO - 
Summary for configuration 650:
2025-05-03 09:30:17,075 - main - INFO - Accuracy: 0.9495
2025-05-03 09:30:17,075 - main - INFO - Precision: 0.8956
2025-05-03 09:30:17,075 - main - INFO - Recall: 0.9301
2025-05-03 09:30:17,075 - main - INFO - F1 Score: 0.9125
2025-05-03 09:30:17,075 - main - INFO - IoU: 0.8391
2025-05-03 09:30:17,075 - main - INFO - mAP: 0.9658
2025-05-03 09:30:17,075 - main - INFO - AUC: 0.9827
2025-05-03 09:30:17,075 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:30:17,075 - main - INFO - 
==================================================
2025-05-03 09:30:17,075 - main - INFO - Running configuration 651/756:
2025-05-03 09:30:17,075 - main - INFO - Model: ConvNeXt
2025-05-03 09:30:17,075 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:30:17,075 - main - INFO - Scheduler: StepLR
2025-05-03 09:30:17,075 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 09:30:17,075 - main - INFO - ==================================================
2025-05-03 09:30:17,075 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_651 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_651
2025-05-03 09:30:17,075 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_651 - INFO - Config: {
  "id": 651,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 09:30:17,702 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:30:17,702 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 651,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 09:30:17,702 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:30:18,056 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:30:18,058 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:30:18,060 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_651 - INFO - Starting model evaluation
2025-05-03 09:30:29,914 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_651 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9082
  Recall:    0.9336
  F1 Score:  0.9207
  IoU:       0.8530
  mAP:       0.9632
  AUC:       0.9825
2025-05-03 09:30:29,916 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_651 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_651/final_results.json
2025-05-03 09:30:29,918 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_651 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_651/final_results.json
2025-05-03 09:30:29,918 - main - INFO - 
Summary for configuration 651:
2025-05-03 09:30:29,918 - main - INFO - Accuracy: 0.9544
2025-05-03 09:30:29,918 - main - INFO - Precision: 0.9082
2025-05-03 09:30:29,918 - main - INFO - Recall: 0.9336
2025-05-03 09:30:29,918 - main - INFO - F1 Score: 0.9207
2025-05-03 09:30:29,918 - main - INFO - IoU: 0.8530
2025-05-03 09:30:29,918 - main - INFO - mAP: 0.9632
2025-05-03 09:30:29,918 - main - INFO - AUC: 0.9825
2025-05-03 09:30:29,918 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:30:29,918 - main - INFO - 
==================================================
2025-05-03 09:30:29,918 - main - INFO - Running configuration 652/756:
2025-05-03 09:30:29,918 - main - INFO - Model: ConvNeXt
2025-05-03 09:30:29,918 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:30:29,918 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 09:30:29,918 - main - INFO - Loss Function: CrossEntropy
2025-05-03 09:30:29,918 - main - INFO - ==================================================
2025-05-03 09:30:29,918 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_652 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_652
2025-05-03 09:30:29,918 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_652 - INFO - Config: {
  "id": 652,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 09:30:30,716 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:30:30,716 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 652,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 09:30:30,716 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:30:31,073 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:30:31,074 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:30:31,076 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_652 - INFO - Starting model evaluation
2025-05-03 09:30:43,034 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_652 - INFO - Evaluation metrics:
  Accuracy:  0.9524
  Precision: 0.9020
  Recall:    0.9336
  F1 Score:  0.9175
  IoU:       0.8476
  mAP:       0.9622
  AUC:       0.9816
2025-05-03 09:30:43,036 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_652 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_652/final_results.json
2025-05-03 09:30:43,037 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_652 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_652/final_results.json
2025-05-03 09:30:43,038 - main - INFO - 
Summary for configuration 652:
2025-05-03 09:30:43,038 - main - INFO - Accuracy: 0.9524
2025-05-03 09:30:43,038 - main - INFO - Precision: 0.9020
2025-05-03 09:30:43,038 - main - INFO - Recall: 0.9336
2025-05-03 09:30:43,038 - main - INFO - F1 Score: 0.9175
2025-05-03 09:30:43,038 - main - INFO - IoU: 0.8476
2025-05-03 09:30:43,038 - main - INFO - mAP: 0.9622
2025-05-03 09:30:43,038 - main - INFO - AUC: 0.9816
2025-05-03 09:30:43,038 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:30:43,038 - main - INFO - 
==================================================
2025-05-03 09:30:43,038 - main - INFO - Running configuration 653/756:
2025-05-03 09:30:43,038 - main - INFO - Model: ConvNeXt
2025-05-03 09:30:43,038 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:30:43,038 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 09:30:43,038 - main - INFO - Loss Function: FocalLoss
2025-05-03 09:30:43,038 - main - INFO - ==================================================
2025-05-03 09:30:43,038 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_653 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_653
2025-05-03 09:30:43,038 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_653 - INFO - Config: {
  "id": 653,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 09:30:43,896 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:30:43,896 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 653,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 09:30:43,896 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:30:44,254 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:30:44,255 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:30:44,257 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_653 - INFO - Starting model evaluation
2025-05-03 09:30:55,983 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_653 - INFO - Evaluation metrics:
  Accuracy:  0.9495
  Precision: 0.8983
  Recall:    0.9266
  F1 Score:  0.9122
  IoU:       0.8386
  mAP:       0.9618
  AUC:       0.9814
2025-05-03 09:30:55,985 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_653 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_653/final_results.json
2025-05-03 09:30:55,986 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_653 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_653/final_results.json
2025-05-03 09:30:55,986 - main - INFO - 
Summary for configuration 653:
2025-05-03 09:30:55,986 - main - INFO - Accuracy: 0.9495
2025-05-03 09:30:55,986 - main - INFO - Precision: 0.8983
2025-05-03 09:30:55,986 - main - INFO - Recall: 0.9266
2025-05-03 09:30:55,986 - main - INFO - F1 Score: 0.9122
2025-05-03 09:30:55,986 - main - INFO - IoU: 0.8386
2025-05-03 09:30:55,986 - main - INFO - mAP: 0.9618
2025-05-03 09:30:55,986 - main - INFO - AUC: 0.9814
2025-05-03 09:30:55,987 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:30:55,987 - main - INFO - 
==================================================
2025-05-03 09:30:55,987 - main - INFO - Running configuration 654/756:
2025-05-03 09:30:55,987 - main - INFO - Model: ConvNeXt
2025-05-03 09:30:55,987 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:30:55,987 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 09:30:55,987 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 09:30:55,987 - main - INFO - ==================================================
2025-05-03 09:30:55,987 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_654 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_654
2025-05-03 09:30:55,987 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_654 - INFO - Config: {
  "id": 654,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 09:30:56,830 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:30:56,830 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 654,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 09:30:56,830 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:30:57,193 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:30:57,194 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:30:57,197 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_654 - INFO - Starting model evaluation
2025-05-03 09:31:08,867 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_654 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9082
  Recall:    0.9336
  F1 Score:  0.9207
  IoU:       0.8530
  mAP:       0.9647
  AUC:       0.9837
2025-05-03 09:31:08,869 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_654 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_654/final_results.json
2025-05-03 09:31:08,871 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_654 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_654/final_results.json
2025-05-03 09:31:08,871 - main - INFO - 
Summary for configuration 654:
2025-05-03 09:31:08,871 - main - INFO - Accuracy: 0.9544
2025-05-03 09:31:08,871 - main - INFO - Precision: 0.9082
2025-05-03 09:31:08,871 - main - INFO - Recall: 0.9336
2025-05-03 09:31:08,871 - main - INFO - F1 Score: 0.9207
2025-05-03 09:31:08,871 - main - INFO - IoU: 0.8530
2025-05-03 09:31:08,871 - main - INFO - mAP: 0.9647
2025-05-03 09:31:08,871 - main - INFO - AUC: 0.9837
2025-05-03 09:31:08,871 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:31:08,871 - main - INFO - 
==================================================
2025-05-03 09:31:08,871 - main - INFO - Running configuration 655/756:
2025-05-03 09:31:08,871 - main - INFO - Model: ConvNeXt
2025-05-03 09:31:08,871 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:31:08,871 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 09:31:08,871 - main - INFO - Loss Function: CrossEntropy
2025-05-03 09:31:08,871 - main - INFO - ==================================================
2025-05-03 09:31:08,871 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_655 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_655
2025-05-03 09:31:08,871 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_655 - INFO - Config: {
  "id": 655,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 09:31:09,601 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:31:09,601 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 655,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 09:31:09,601 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:31:09,953 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:31:09,954 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:31:09,956 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_655 - INFO - Starting model evaluation
2025-05-03 09:31:21,642 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_655 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9027
  Recall:    0.9406
  F1 Score:  0.9212
  IoU:       0.8540
  mAP:       0.9630
  AUC:       0.9820
2025-05-03 09:31:21,643 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_655 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_655/final_results.json
2025-05-03 09:31:21,645 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_655 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_655/final_results.json
2025-05-03 09:31:21,645 - main - INFO - 
Summary for configuration 655:
2025-05-03 09:31:21,645 - main - INFO - Accuracy: 0.9544
2025-05-03 09:31:21,645 - main - INFO - Precision: 0.9027
2025-05-03 09:31:21,645 - main - INFO - Recall: 0.9406
2025-05-03 09:31:21,645 - main - INFO - F1 Score: 0.9212
2025-05-03 09:31:21,645 - main - INFO - IoU: 0.8540
2025-05-03 09:31:21,645 - main - INFO - mAP: 0.9630
2025-05-03 09:31:21,645 - main - INFO - AUC: 0.9820
2025-05-03 09:31:21,645 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:31:21,645 - main - INFO - 
==================================================
2025-05-03 09:31:21,645 - main - INFO - Running configuration 656/756:
2025-05-03 09:31:21,645 - main - INFO - Model: ConvNeXt
2025-05-03 09:31:21,645 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:31:21,645 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 09:31:21,645 - main - INFO - Loss Function: FocalLoss
2025-05-03 09:31:21,645 - main - INFO - ==================================================
2025-05-03 09:31:21,645 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_656 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_656
2025-05-03 09:31:21,645 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_656 - INFO - Config: {
  "id": 656,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 09:31:22,297 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:31:22,297 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 656,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 09:31:22,298 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:31:22,645 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:31:22,646 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:31:22,649 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_656 - INFO - Starting model evaluation
2025-05-03 09:31:34,802 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_656 - INFO - Evaluation metrics:
  Accuracy:  0.9514
  Precision: 0.8885
  Recall:    0.9476
  F1 Score:  0.9171
  IoU:       0.8469
  mAP:       0.9635
  AUC:       0.9827
2025-05-03 09:31:34,804 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_656 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_656/final_results.json
2025-05-03 09:31:34,806 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_656 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_656/final_results.json
2025-05-03 09:31:34,806 - main - INFO - 
Summary for configuration 656:
2025-05-03 09:31:34,806 - main - INFO - Accuracy: 0.9514
2025-05-03 09:31:34,806 - main - INFO - Precision: 0.8885
2025-05-03 09:31:34,806 - main - INFO - Recall: 0.9476
2025-05-03 09:31:34,806 - main - INFO - F1 Score: 0.9171
2025-05-03 09:31:34,806 - main - INFO - IoU: 0.8469
2025-05-03 09:31:34,806 - main - INFO - mAP: 0.9635
2025-05-03 09:31:34,806 - main - INFO - AUC: 0.9827
2025-05-03 09:31:34,806 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:31:34,806 - main - INFO - 
==================================================
2025-05-03 09:31:34,806 - main - INFO - Running configuration 657/756:
2025-05-03 09:31:34,806 - main - INFO - Model: ConvNeXt
2025-05-03 09:31:34,806 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:31:34,806 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 09:31:34,806 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 09:31:34,806 - main - INFO - ==================================================
2025-05-03 09:31:34,806 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_657 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_657
2025-05-03 09:31:34,806 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_657 - INFO - Config: {
  "id": 657,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 09:31:35,520 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:31:35,520 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 657,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 09:31:35,520 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:31:35,962 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:31:35,964 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:31:35,966 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_657 - INFO - Starting model evaluation
2025-05-03 09:31:47,837 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_657 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9027
  Recall:    0.9406
  F1 Score:  0.9212
  IoU:       0.8540
  mAP:       0.9622
  AUC:       0.9806
2025-05-03 09:31:47,838 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_657 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_657/final_results.json
2025-05-03 09:31:47,840 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_657 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_657/final_results.json
2025-05-03 09:31:47,840 - main - INFO - 
Summary for configuration 657:
2025-05-03 09:31:47,840 - main - INFO - Accuracy: 0.9544
2025-05-03 09:31:47,840 - main - INFO - Precision: 0.9027
2025-05-03 09:31:47,840 - main - INFO - Recall: 0.9406
2025-05-03 09:31:47,840 - main - INFO - F1 Score: 0.9212
2025-05-03 09:31:47,840 - main - INFO - IoU: 0.8540
2025-05-03 09:31:47,840 - main - INFO - mAP: 0.9622
2025-05-03 09:31:47,840 - main - INFO - AUC: 0.9806
2025-05-03 09:31:47,840 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:31:47,840 - main - INFO - 
==================================================
2025-05-03 09:31:47,840 - main - INFO - Running configuration 658/756:
2025-05-03 09:31:47,840 - main - INFO - Model: ConvNeXt
2025-05-03 09:31:47,840 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:31:47,840 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 09:31:47,840 - main - INFO - Loss Function: CrossEntropy
2025-05-03 09:31:47,840 - main - INFO - ==================================================
2025-05-03 09:31:47,840 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_658 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_658
2025-05-03 09:31:47,841 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_658 - INFO - Config: {
  "id": 658,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 09:31:48,640 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:31:48,640 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 658,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 09:31:48,640 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:31:49,094 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:31:49,095 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:31:49,098 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_658 - INFO - Starting model evaluation
2025-05-03 09:32:01,158 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_658 - INFO - Evaluation metrics:
  Accuracy:  0.9514
  Precision: 0.8937
  Recall:    0.9406
  F1 Score:  0.9165
  IoU:       0.8459
  mAP:       0.9644
  AUC:       0.9837
2025-05-03 09:32:01,159 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_658 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_658/final_results.json
2025-05-03 09:32:01,161 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_658 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_658/final_results.json
2025-05-03 09:32:01,161 - main - INFO - 
Summary for configuration 658:
2025-05-03 09:32:01,161 - main - INFO - Accuracy: 0.9514
2025-05-03 09:32:01,161 - main - INFO - Precision: 0.8937
2025-05-03 09:32:01,161 - main - INFO - Recall: 0.9406
2025-05-03 09:32:01,161 - main - INFO - F1 Score: 0.9165
2025-05-03 09:32:01,161 - main - INFO - IoU: 0.8459
2025-05-03 09:32:01,161 - main - INFO - mAP: 0.9644
2025-05-03 09:32:01,161 - main - INFO - AUC: 0.9837
2025-05-03 09:32:01,161 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:32:01,161 - main - INFO - 
==================================================
2025-05-03 09:32:01,161 - main - INFO - Running configuration 659/756:
2025-05-03 09:32:01,161 - main - INFO - Model: ConvNeXt
2025-05-03 09:32:01,161 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:32:01,161 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 09:32:01,161 - main - INFO - Loss Function: FocalLoss
2025-05-03 09:32:01,161 - main - INFO - ==================================================
2025-05-03 09:32:01,161 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_659 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_659
2025-05-03 09:32:01,161 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_659 - INFO - Config: {
  "id": 659,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 09:32:01,900 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:32:01,900 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 659,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 09:32:01,900 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:32:02,375 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:32:02,376 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:32:02,379 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_659 - INFO - Starting model evaluation
2025-05-03 09:32:13,722 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_659 - INFO - Evaluation metrics:
  Accuracy:  0.9495
  Precision: 0.8930
  Recall:    0.9336
  F1 Score:  0.9128
  IoU:       0.8396
  mAP:       0.9630
  AUC:       0.9823
2025-05-03 09:32:13,724 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_659 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_659/final_results.json
2025-05-03 09:32:13,726 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_659 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_659/final_results.json
2025-05-03 09:32:13,726 - main - INFO - 
Summary for configuration 659:
2025-05-03 09:32:13,726 - main - INFO - Accuracy: 0.9495
2025-05-03 09:32:13,726 - main - INFO - Precision: 0.8930
2025-05-03 09:32:13,726 - main - INFO - Recall: 0.9336
2025-05-03 09:32:13,726 - main - INFO - F1 Score: 0.9128
2025-05-03 09:32:13,726 - main - INFO - IoU: 0.8396
2025-05-03 09:32:13,726 - main - INFO - mAP: 0.9630
2025-05-03 09:32:13,726 - main - INFO - AUC: 0.9823
2025-05-03 09:32:13,726 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:32:13,726 - main - INFO - 
==================================================
2025-05-03 09:32:13,726 - main - INFO - Running configuration 660/756:
2025-05-03 09:32:13,726 - main - INFO - Model: ConvNeXt
2025-05-03 09:32:13,726 - main - INFO - Optimizer: SGD, Learning Rate: 0.01
2025-05-03 09:32:13,726 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 09:32:13,726 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 09:32:13,726 - main - INFO - ==================================================
2025-05-03 09:32:13,726 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_660 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01_id_660
2025-05-03 09:32:13,726 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_660 - INFO - Config: {
  "id": 660,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 09:32:14,472 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.01
2025-05-03 09:32:14,473 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Config: {
  "id": 660,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 09:32:14,473 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01/checkpoint.pt
2025-05-03 09:32:14,825 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.9708
2025-05-03 09:32:14,827 - training.model_ConvNeXt_opt_SGD_lr_0.01 - INFO - Training completed after 1994.41 seconds
2025-05-03 09:32:14,829 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_660 - INFO - Starting model evaluation
2025-05-03 09:32:26,535 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_660 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.9054
  Recall:    0.9371
  F1 Score:  0.9210
  IoU:       0.8535
  mAP:       0.9645
  AUC:       0.9829
2025-05-03 09:32:26,537 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_660 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_660/final_results.json
2025-05-03 09:32:26,538 - training.model_ConvNeXt_opt_SGD_lr_0.01_id_660 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.01_id_660/final_results.json
2025-05-03 09:32:26,538 - main - INFO - 
Summary for configuration 660:
2025-05-03 09:32:26,538 - main - INFO - Accuracy: 0.9544
2025-05-03 09:32:26,538 - main - INFO - Precision: 0.9054
2025-05-03 09:32:26,538 - main - INFO - Recall: 0.9371
2025-05-03 09:32:26,538 - main - INFO - F1 Score: 0.9210
2025-05-03 09:32:26,538 - main - INFO - IoU: 0.8535
2025-05-03 09:32:26,539 - main - INFO - mAP: 0.9645
2025-05-03 09:32:26,539 - main - INFO - AUC: 0.9829
2025-05-03 09:32:26,539 - main - INFO - Training time: 1994.41 seconds
2025-05-03 09:32:26,539 - main - INFO - 
==================================================
2025-05-03 09:32:26,539 - main - INFO - Running configuration 661/756:
2025-05-03 09:32:26,539 - main - INFO - Model: ConvNeXt
2025-05-03 09:32:26,539 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 09:32:26,539 - main - INFO - Scheduler: StepLR
2025-05-03 09:32:26,539 - main - INFO - Loss Function: CrossEntropy
2025-05-03 09:32:26,539 - main - INFO - ==================================================
2025-05-03 09:32:26,539 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_661 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_661
2025-05-03 09:32:26,539 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_661 - INFO - Config: {
  "id": 661,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 09:32:27,261 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 09:32:27,261 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 661,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 09:32:27,261 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-03 09:32:27,262 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 1/20
2025-05-03 09:33:12,116 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 09:34:07,535 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.8662
2025-05-03 09:34:07,710 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 1 completed in 100.45s - Train Loss: 0.5605, Train Acc: 0.7610, Val Loss: 0.4589, Val Acc: 0.8662
2025-05-03 09:34:08,283 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 09:34:08,283 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 2/20
2025-05-03 09:34:53,132 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 09:35:46,690 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.8662 to 0.8954
2025-05-03 09:35:46,965 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 2 completed in 98.68s - Train Loss: 0.4547, Train Acc: 0.8664, Val Loss: 0.4178, Val Acc: 0.8954
2025-05-03 09:35:47,485 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 09:35:47,486 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 3/20
2025-05-03 09:36:34,207 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 09:37:26,237 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.8954 to 0.9202
2025-05-03 09:37:26,522 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 3 completed in 99.04s - Train Loss: 0.4209, Train Acc: 0.8977, Val Loss: 0.4004, Val Acc: 0.9202
2025-05-03 09:37:27,044 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 09:37:27,045 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 4/20
2025-05-03 09:38:11,411 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 09:39:04,343 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9202 to 0.9262
2025-05-03 09:39:04,604 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 4 completed in 97.56s - Train Loss: 0.4046, Train Acc: 0.9123, Val Loss: 0.3846, Val Acc: 0.9262
2025-05-03 09:39:05,109 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 09:39:05,110 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 5/20
2025-05-03 09:39:48,928 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 09:40:41,295 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation loss improved from inf to 0.3884
2025-05-03 09:40:41,582 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 5 completed in 96.47s - Train Loss: 0.3861, Train Acc: 0.9344, Val Loss: 0.3884, Val Acc: 0.9237
2025-05-03 09:40:42,083 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 09:40:42,083 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 6/20
2025-05-03 09:41:26,764 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 09:42:19,376 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9262 to 0.9597
2025-05-03 09:42:19,656 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 6 completed in 97.57s - Train Loss: 0.3787, Train Acc: 0.9369, Val Loss: 0.3564, Val Acc: 0.9597
2025-05-03 09:42:20,179 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 09:42:20,179 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 7/20
2025-05-03 09:43:03,433 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 09:43:56,852 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3884 to 0.3561
2025-05-03 09:43:57,124 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 7 completed in 96.94s - Train Loss: 0.3709, Train Acc: 0.9438, Val Loss: 0.3561, Val Acc: 0.9588
2025-05-03 09:43:57,639 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 09:43:57,640 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 8/20
2025-05-03 09:44:40,981 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 09:45:32,026 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9597 to 0.9623
2025-05-03 09:45:32,295 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 8 completed in 94.66s - Train Loss: 0.3612, Train Acc: 0.9535, Val Loss: 0.3531, Val Acc: 0.9623
2025-05-03 09:45:32,799 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 09:45:32,799 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 9/20
2025-05-03 09:46:16,971 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 09:47:09,971 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9623 to 0.9700
2025-05-03 09:47:10,264 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 9 completed in 97.46s - Train Loss: 0.3588, Train Acc: 0.9556, Val Loss: 0.3472, Val Acc: 0.9700
2025-05-03 09:47:10,791 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 09:47:10,791 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 10/20
2025-05-03 09:47:55,516 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 09:48:47,690 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9700 to 0.9751
2025-05-03 09:48:47,957 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 10 completed in 97.17s - Train Loss: 0.3543, Train Acc: 0.9614, Val Loss: 0.3405, Val Acc: 0.9751
2025-05-03 09:48:48,467 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 09:48:48,467 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 11/20
2025-05-03 09:49:32,286 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 09:50:25,523 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9751 to 0.9768
2025-05-03 09:50:25,794 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 11 completed in 97.33s - Train Loss: 0.3396, Train Acc: 0.9766, Val Loss: 0.3371, Val Acc: 0.9768
2025-05-03 09:50:26,302 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 09:50:26,303 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 12/20
2025-05-03 09:51:10,773 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 09:52:02,906 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9768 to 0.9777
2025-05-03 09:52:03,184 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 12 completed in 96.88s - Train Loss: 0.3358, Train Acc: 0.9798, Val Loss: 0.3361, Val Acc: 0.9777
2025-05-03 09:52:03,712 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 09:52:03,712 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 09:52:47,205 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 09:53:39,206 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9777 to 0.9794
2025-05-03 09:53:39,481 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 13 completed in 95.77s - Train Loss: 0.3340, Train Acc: 0.9813, Val Loss: 0.3356, Val Acc: 0.9794
2025-05-03 09:53:40,008 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 09:53:40,008 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 09:54:25,705 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 09:55:19,377 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3561 to 0.3366
2025-05-03 09:55:19,673 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 14 completed in 99.67s - Train Loss: 0.3322, Train Acc: 0.9835, Val Loss: 0.3366, Val Acc: 0.9760
2025-05-03 09:55:20,199 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 09:55:20,199 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 09:56:05,245 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 09:56:58,309 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9794 to 0.9811
2025-05-03 09:56:58,595 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 15 completed in 98.40s - Train Loss: 0.3316, Train Acc: 0.9843, Val Loss: 0.3321, Val Acc: 0.9811
2025-05-03 09:56:59,118 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 09:56:59,119 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 09:57:43,385 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 09:58:36,307 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3366 to 0.3335
2025-05-03 09:58:36,590 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 16 completed in 97.47s - Train Loss: 0.3315, Train Acc: 0.9852, Val Loss: 0.3335, Val Acc: 0.9777
2025-05-03 09:58:37,121 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 09:58:37,121 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 09:59:22,360 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 10:00:15,321 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 17 completed in 98.20s - Train Loss: 0.3290, Train Acc: 0.9876, Val Loss: 0.3366, Val Acc: 0.9768
2025-05-03 10:00:15,888 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 10:00:15,889 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 10:00:59,635 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 10:01:51,626 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation accuracy improved from 0.9811 to 0.9820
2025-05-03 10:01:51,916 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 18 completed in 96.03s - Train Loss: 0.3299, Train Acc: 0.9863, Val Loss: 0.3314, Val Acc: 0.9820
2025-05-03 10:01:52,445 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 10:01:52,446 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 10:02:37,213 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 10:03:29,323 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 19 completed in 96.88s - Train Loss: 0.3279, Train Acc: 0.9893, Val Loss: 0.3346, Val Acc: 0.9786
2025-05-03 10:03:29,871 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 10:03:29,871 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 10:04:14,839 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 10:05:06,959 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Validation loss improved from 0.3335 to 0.3313
2025-05-03 10:05:07,225 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Epoch 20 completed in 97.35s - Train Loss: 0.3303, Train Acc: 0.9848, Val Loss: 0.3313, Val Acc: 0.9820
2025-05-03 10:05:07,742 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 10:05:07,743 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:05:07,746 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_661 - INFO - Starting model evaluation
2025-05-03 10:05:19,060 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_661 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9167
  Recall:    0.9615
  F1 Score:  0.9386
  IoU:       0.8842
  mAP:       0.9856
  AUC:       0.9940
2025-05-03 10:05:19,062 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_661 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_661/final_results.json
2025-05-03 10:05:19,064 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_661 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_661/final_results.json
2025-05-03 10:05:19,064 - main - INFO - 
Summary for configuration 661:
2025-05-03 10:05:19,064 - main - INFO - Accuracy: 0.9643
2025-05-03 10:05:19,064 - main - INFO - Precision: 0.9167
2025-05-03 10:05:19,064 - main - INFO - Recall: 0.9615
2025-05-03 10:05:19,064 - main - INFO - F1 Score: 0.9386
2025-05-03 10:05:19,064 - main - INFO - IoU: 0.8842
2025-05-03 10:05:19,064 - main - INFO - mAP: 0.9856
2025-05-03 10:05:19,064 - main - INFO - AUC: 0.9940
2025-05-03 10:05:19,064 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:05:19,064 - main - INFO - 
==================================================
2025-05-03 10:05:19,064 - main - INFO - Running configuration 662/756:
2025-05-03 10:05:19,064 - main - INFO - Model: ConvNeXt
2025-05-03 10:05:19,064 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:05:19,064 - main - INFO - Scheduler: StepLR
2025-05-03 10:05:19,064 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:05:19,064 - main - INFO - ==================================================
2025-05-03 10:05:19,064 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_662 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_662
2025-05-03 10:05:19,064 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_662 - INFO - Config: {
  "id": 662,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:05:19,799 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:05:19,799 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 662,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:05:19,800 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:05:20,227 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:05:20,228 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:05:20,231 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_662 - INFO - Starting model evaluation
2025-05-03 10:05:31,671 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_662 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9184
  Recall:    0.9441
  F1 Score:  0.9310
  IoU:       0.8710
  mAP:       0.9843
  AUC:       0.9931
2025-05-03 10:05:31,674 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_662 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_662/final_results.json
2025-05-03 10:05:31,678 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_662 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_662/final_results.json
2025-05-03 10:05:31,678 - main - INFO - 
Summary for configuration 662:
2025-05-03 10:05:31,678 - main - INFO - Accuracy: 0.9604
2025-05-03 10:05:31,678 - main - INFO - Precision: 0.9184
2025-05-03 10:05:31,678 - main - INFO - Recall: 0.9441
2025-05-03 10:05:31,678 - main - INFO - F1 Score: 0.9310
2025-05-03 10:05:31,678 - main - INFO - IoU: 0.8710
2025-05-03 10:05:31,678 - main - INFO - mAP: 0.9843
2025-05-03 10:05:31,678 - main - INFO - AUC: 0.9931
2025-05-03 10:05:31,678 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:05:31,678 - main - INFO - 
==================================================
2025-05-03 10:05:31,678 - main - INFO - Running configuration 663/756:
2025-05-03 10:05:31,678 - main - INFO - Model: ConvNeXt
2025-05-03 10:05:31,678 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:05:31,678 - main - INFO - Scheduler: StepLR
2025-05-03 10:05:31,678 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 10:05:31,678 - main - INFO - ==================================================
2025-05-03 10:05:31,678 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_663 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_663
2025-05-03 10:05:31,678 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_663 - INFO - Config: {
  "id": 663,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:05:32,446 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:05:32,446 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 663,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:05:32,446 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:05:32,632 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:05:32,633 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:05:32,636 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_663 - INFO - Starting model evaluation
2025-05-03 10:05:44,169 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_663 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9133
  Recall:    0.9580
  F1 Score:  0.9352
  IoU:       0.8782
  mAP:       0.9847
  AUC:       0.9933
2025-05-03 10:05:44,172 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_663 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_663/final_results.json
2025-05-03 10:05:44,174 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_663 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_663/final_results.json
2025-05-03 10:05:44,174 - main - INFO - 
Summary for configuration 663:
2025-05-03 10:05:44,174 - main - INFO - Accuracy: 0.9623
2025-05-03 10:05:44,174 - main - INFO - Precision: 0.9133
2025-05-03 10:05:44,174 - main - INFO - Recall: 0.9580
2025-05-03 10:05:44,174 - main - INFO - F1 Score: 0.9352
2025-05-03 10:05:44,174 - main - INFO - IoU: 0.8782
2025-05-03 10:05:44,174 - main - INFO - mAP: 0.9847
2025-05-03 10:05:44,174 - main - INFO - AUC: 0.9933
2025-05-03 10:05:44,174 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:05:44,174 - main - INFO - 
==================================================
2025-05-03 10:05:44,174 - main - INFO - Running configuration 664/756:
2025-05-03 10:05:44,174 - main - INFO - Model: ConvNeXt
2025-05-03 10:05:44,174 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:05:44,174 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 10:05:44,174 - main - INFO - Loss Function: CrossEntropy
2025-05-03 10:05:44,174 - main - INFO - ==================================================
2025-05-03 10:05:44,174 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_664 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_664
2025-05-03 10:05:44,174 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_664 - INFO - Config: {
  "id": 664,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:05:44,836 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:05:44,836 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 664,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:05:44,836 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:05:45,259 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:05:45,261 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:05:45,263 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_664 - INFO - Starting model evaluation
2025-05-03 10:05:56,989 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_664 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9220
  Recall:    0.9510
  F1 Score:  0.9363
  IoU:       0.8803
  mAP:       0.9851
  AUC:       0.9935
2025-05-03 10:05:56,991 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_664 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_664/final_results.json
2025-05-03 10:05:56,992 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_664 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_664/final_results.json
2025-05-03 10:05:56,992 - main - INFO - 
Summary for configuration 664:
2025-05-03 10:05:56,993 - main - INFO - Accuracy: 0.9633
2025-05-03 10:05:56,993 - main - INFO - Precision: 0.9220
2025-05-03 10:05:56,993 - main - INFO - Recall: 0.9510
2025-05-03 10:05:56,993 - main - INFO - F1 Score: 0.9363
2025-05-03 10:05:56,993 - main - INFO - IoU: 0.8803
2025-05-03 10:05:56,993 - main - INFO - mAP: 0.9851
2025-05-03 10:05:56,993 - main - INFO - AUC: 0.9935
2025-05-03 10:05:56,993 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:05:56,993 - main - INFO - 
==================================================
2025-05-03 10:05:56,993 - main - INFO - Running configuration 665/756:
2025-05-03 10:05:56,993 - main - INFO - Model: ConvNeXt
2025-05-03 10:05:56,993 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:05:56,993 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 10:05:56,993 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:05:56,993 - main - INFO - ==================================================
2025-05-03 10:05:56,993 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_665 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_665
2025-05-03 10:05:56,993 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_665 - INFO - Config: {
  "id": 665,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:05:57,962 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:05:57,962 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 665,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:05:57,962 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:05:58,149 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:05:58,150 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:05:58,153 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_665 - INFO - Starting model evaluation
2025-05-03 10:06:09,668 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_665 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9070
  Recall:    0.9545
  F1 Score:  0.9302
  IoU:       0.8694
  mAP:       0.9841
  AUC:       0.9930
2025-05-03 10:06:09,670 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_665 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_665/final_results.json
2025-05-03 10:06:09,671 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_665 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_665/final_results.json
2025-05-03 10:06:09,671 - main - INFO - 
Summary for configuration 665:
2025-05-03 10:06:09,671 - main - INFO - Accuracy: 0.9594
2025-05-03 10:06:09,671 - main - INFO - Precision: 0.9070
2025-05-03 10:06:09,671 - main - INFO - Recall: 0.9545
2025-05-03 10:06:09,671 - main - INFO - F1 Score: 0.9302
2025-05-03 10:06:09,671 - main - INFO - IoU: 0.8694
2025-05-03 10:06:09,671 - main - INFO - mAP: 0.9841
2025-05-03 10:06:09,671 - main - INFO - AUC: 0.9930
2025-05-03 10:06:09,671 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:06:09,671 - main - INFO - 
==================================================
2025-05-03 10:06:09,671 - main - INFO - Running configuration 666/756:
2025-05-03 10:06:09,671 - main - INFO - Model: ConvNeXt
2025-05-03 10:06:09,671 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:06:09,671 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 10:06:09,671 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 10:06:09,671 - main - INFO - ==================================================
2025-05-03 10:06:09,672 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_666 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_666
2025-05-03 10:06:09,672 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_666 - INFO - Config: {
  "id": 666,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:06:10,291 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:06:10,291 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 666,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:06:10,292 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:06:10,480 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:06:10,482 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:06:10,484 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_666 - INFO - Starting model evaluation
2025-05-03 10:06:21,907 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_666 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9254
  Recall:    0.9545
  F1 Score:  0.9398
  IoU:       0.8864
  mAP:       0.9865
  AUC:       0.9939
2025-05-03 10:06:21,909 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_666 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_666/final_results.json
2025-05-03 10:06:21,911 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_666 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_666/final_results.json
2025-05-03 10:06:21,911 - main - INFO - 
Summary for configuration 666:
2025-05-03 10:06:21,911 - main - INFO - Accuracy: 0.9653
2025-05-03 10:06:21,911 - main - INFO - Precision: 0.9254
2025-05-03 10:06:21,911 - main - INFO - Recall: 0.9545
2025-05-03 10:06:21,911 - main - INFO - F1 Score: 0.9398
2025-05-03 10:06:21,911 - main - INFO - IoU: 0.8864
2025-05-03 10:06:21,911 - main - INFO - mAP: 0.9865
2025-05-03 10:06:21,911 - main - INFO - AUC: 0.9939
2025-05-03 10:06:21,911 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:06:21,911 - main - INFO - 
==================================================
2025-05-03 10:06:21,911 - main - INFO - Running configuration 667/756:
2025-05-03 10:06:21,911 - main - INFO - Model: ConvNeXt
2025-05-03 10:06:21,911 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:06:21,911 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 10:06:21,911 - main - INFO - Loss Function: CrossEntropy
2025-05-03 10:06:21,911 - main - INFO - ==================================================
2025-05-03 10:06:21,911 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_667 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_667
2025-05-03 10:06:21,911 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_667 - INFO - Config: {
  "id": 667,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:06:22,593 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:06:22,593 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 667,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:06:22,593 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:06:22,772 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:06:22,774 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:06:22,776 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_667 - INFO - Starting model evaluation
2025-05-03 10:06:34,349 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_667 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9161
  Recall:    0.9545
  F1 Score:  0.9349
  IoU:       0.8778
  mAP:       0.9850
  AUC:       0.9937
2025-05-03 10:06:34,351 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_667 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_667/final_results.json
2025-05-03 10:06:34,353 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_667 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_667/final_results.json
2025-05-03 10:06:34,353 - main - INFO - 
Summary for configuration 667:
2025-05-03 10:06:34,353 - main - INFO - Accuracy: 0.9623
2025-05-03 10:06:34,353 - main - INFO - Precision: 0.9161
2025-05-03 10:06:34,353 - main - INFO - Recall: 0.9545
2025-05-03 10:06:34,353 - main - INFO - F1 Score: 0.9349
2025-05-03 10:06:34,353 - main - INFO - IoU: 0.8778
2025-05-03 10:06:34,353 - main - INFO - mAP: 0.9850
2025-05-03 10:06:34,353 - main - INFO - AUC: 0.9937
2025-05-03 10:06:34,353 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:06:34,353 - main - INFO - 
==================================================
2025-05-03 10:06:34,353 - main - INFO - Running configuration 668/756:
2025-05-03 10:06:34,353 - main - INFO - Model: ConvNeXt
2025-05-03 10:06:34,353 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:06:34,353 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 10:06:34,353 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:06:34,353 - main - INFO - ==================================================
2025-05-03 10:06:34,353 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_668 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_668
2025-05-03 10:06:34,353 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_668 - INFO - Config: {
  "id": 668,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:06:35,035 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:06:35,035 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 668,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:06:35,035 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:06:35,234 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:06:35,235 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:06:35,238 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_668 - INFO - Starting model evaluation
2025-05-03 10:06:46,919 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_668 - INFO - Evaluation metrics:
  Accuracy:  0.9663
  Precision: 0.9286
  Recall:    0.9545
  F1 Score:  0.9414
  IoU:       0.8893
  mAP:       0.9841
  AUC:       0.9926
2025-05-03 10:06:46,921 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_668 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_668/final_results.json
2025-05-03 10:06:46,923 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_668 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_668/final_results.json
2025-05-03 10:06:46,923 - main - INFO - 
Summary for configuration 668:
2025-05-03 10:06:46,923 - main - INFO - Accuracy: 0.9663
2025-05-03 10:06:46,923 - main - INFO - Precision: 0.9286
2025-05-03 10:06:46,923 - main - INFO - Recall: 0.9545
2025-05-03 10:06:46,923 - main - INFO - F1 Score: 0.9414
2025-05-03 10:06:46,923 - main - INFO - IoU: 0.8893
2025-05-03 10:06:46,923 - main - INFO - mAP: 0.9841
2025-05-03 10:06:46,923 - main - INFO - AUC: 0.9926
2025-05-03 10:06:46,923 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:06:46,923 - main - INFO - 
==================================================
2025-05-03 10:06:46,923 - main - INFO - Running configuration 669/756:
2025-05-03 10:06:46,923 - main - INFO - Model: ConvNeXt
2025-05-03 10:06:46,923 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:06:46,923 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 10:06:46,923 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 10:06:46,923 - main - INFO - ==================================================
2025-05-03 10:06:46,923 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_669 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_669
2025-05-03 10:06:46,923 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_669 - INFO - Config: {
  "id": 669,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:06:47,727 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:06:47,727 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 669,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:06:47,728 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:06:47,909 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:06:47,910 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:06:47,913 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_669 - INFO - Starting model evaluation
2025-05-03 10:06:59,494 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_669 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9254
  Recall:    0.9545
  F1 Score:  0.9398
  IoU:       0.8864
  mAP:       0.9867
  AUC:       0.9943
2025-05-03 10:06:59,496 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_669 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_669/final_results.json
2025-05-03 10:06:59,497 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_669 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_669/final_results.json
2025-05-03 10:06:59,497 - main - INFO - 
Summary for configuration 669:
2025-05-03 10:06:59,497 - main - INFO - Accuracy: 0.9653
2025-05-03 10:06:59,497 - main - INFO - Precision: 0.9254
2025-05-03 10:06:59,497 - main - INFO - Recall: 0.9545
2025-05-03 10:06:59,497 - main - INFO - F1 Score: 0.9398
2025-05-03 10:06:59,497 - main - INFO - IoU: 0.8864
2025-05-03 10:06:59,497 - main - INFO - mAP: 0.9867
2025-05-03 10:06:59,497 - main - INFO - AUC: 0.9943
2025-05-03 10:06:59,497 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:06:59,497 - main - INFO - 
==================================================
2025-05-03 10:06:59,497 - main - INFO - Running configuration 670/756:
2025-05-03 10:06:59,497 - main - INFO - Model: ConvNeXt
2025-05-03 10:06:59,498 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:06:59,498 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 10:06:59,498 - main - INFO - Loss Function: CrossEntropy
2025-05-03 10:06:59,498 - main - INFO - ==================================================
2025-05-03 10:06:59,498 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_670 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_670
2025-05-03 10:06:59,498 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_670 - INFO - Config: {
  "id": 670,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:07:00,239 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:07:00,240 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 670,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:07:00,240 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:07:00,421 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:07:00,422 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:07:00,425 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_670 - INFO - Starting model evaluation
2025-05-03 10:07:11,902 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_670 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9186
  Recall:    0.9476
  F1 Score:  0.9329
  IoU:       0.8742
  mAP:       0.9836
  AUC:       0.9925
2025-05-03 10:07:11,904 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_670 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_670/final_results.json
2025-05-03 10:07:11,905 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_670 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_670/final_results.json
2025-05-03 10:07:11,906 - main - INFO - 
Summary for configuration 670:
2025-05-03 10:07:11,906 - main - INFO - Accuracy: 0.9613
2025-05-03 10:07:11,906 - main - INFO - Precision: 0.9186
2025-05-03 10:07:11,906 - main - INFO - Recall: 0.9476
2025-05-03 10:07:11,906 - main - INFO - F1 Score: 0.9329
2025-05-03 10:07:11,906 - main - INFO - IoU: 0.8742
2025-05-03 10:07:11,906 - main - INFO - mAP: 0.9836
2025-05-03 10:07:11,906 - main - INFO - AUC: 0.9925
2025-05-03 10:07:11,906 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:07:11,906 - main - INFO - 
==================================================
2025-05-03 10:07:11,906 - main - INFO - Running configuration 671/756:
2025-05-03 10:07:11,906 - main - INFO - Model: ConvNeXt
2025-05-03 10:07:11,906 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:07:11,906 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 10:07:11,906 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:07:11,906 - main - INFO - ==================================================
2025-05-03 10:07:11,906 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_671 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_671
2025-05-03 10:07:11,906 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_671 - INFO - Config: {
  "id": 671,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:07:12,583 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:07:12,583 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 671,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:07:12,584 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:07:12,766 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:07:12,767 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:07:12,770 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_671 - INFO - Starting model evaluation
2025-05-03 10:07:24,287 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_671 - INFO - Evaluation metrics:
  Accuracy:  0.9633
  Precision: 0.9164
  Recall:    0.9580
  F1 Score:  0.9368
  IoU:       0.8810
  mAP:       0.9835
  AUC:       0.9929
2025-05-03 10:07:24,289 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_671 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_671/final_results.json
2025-05-03 10:07:24,290 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_671 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_671/final_results.json
2025-05-03 10:07:24,290 - main - INFO - 
Summary for configuration 671:
2025-05-03 10:07:24,290 - main - INFO - Accuracy: 0.9633
2025-05-03 10:07:24,290 - main - INFO - Precision: 0.9164
2025-05-03 10:07:24,290 - main - INFO - Recall: 0.9580
2025-05-03 10:07:24,290 - main - INFO - F1 Score: 0.9368
2025-05-03 10:07:24,290 - main - INFO - IoU: 0.8810
2025-05-03 10:07:24,290 - main - INFO - mAP: 0.9835
2025-05-03 10:07:24,290 - main - INFO - AUC: 0.9929
2025-05-03 10:07:24,290 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:07:24,290 - main - INFO - 
==================================================
2025-05-03 10:07:24,290 - main - INFO - Running configuration 672/756:
2025-05-03 10:07:24,290 - main - INFO - Model: ConvNeXt
2025-05-03 10:07:24,291 - main - INFO - Optimizer: SGD, Learning Rate: 0.001
2025-05-03 10:07:24,291 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 10:07:24,291 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 10:07:24,291 - main - INFO - ==================================================
2025-05-03 10:07:24,291 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_672 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001_id_672
2025-05-03 10:07:24,291 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_672 - INFO - Config: {
  "id": 672,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:07:25,061 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.001
2025-05-03 10:07:25,062 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Config: {
  "id": 672,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:07:25,062 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001/checkpoint.pt
2025-05-03 10:07:25,243 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9820
2025-05-03 10:07:25,244 - training.model_ConvNeXt_opt_SGD_lr_0.001 - INFO - Training completed after 1959.96 seconds
2025-05-03 10:07:25,247 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_672 - INFO - Starting model evaluation
2025-05-03 10:07:36,780 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_672 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.9125
  Recall:    0.9476
  F1 Score:  0.9297
  IoU:       0.8686
  mAP:       0.9842
  AUC:       0.9932
2025-05-03 10:07:36,782 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_672 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_672/final_results.json
2025-05-03 10:07:36,784 - training.model_ConvNeXt_opt_SGD_lr_0.001_id_672 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.001_id_672/final_results.json
2025-05-03 10:07:36,784 - main - INFO - 
Summary for configuration 672:
2025-05-03 10:07:36,784 - main - INFO - Accuracy: 0.9594
2025-05-03 10:07:36,784 - main - INFO - Precision: 0.9125
2025-05-03 10:07:36,784 - main - INFO - Recall: 0.9476
2025-05-03 10:07:36,784 - main - INFO - F1 Score: 0.9297
2025-05-03 10:07:36,784 - main - INFO - IoU: 0.8686
2025-05-03 10:07:36,784 - main - INFO - mAP: 0.9842
2025-05-03 10:07:36,784 - main - INFO - AUC: 0.9932
2025-05-03 10:07:36,784 - main - INFO - Training time: 1959.96 seconds
2025-05-03 10:07:36,784 - main - INFO - 
==================================================
2025-05-03 10:07:36,784 - main - INFO - Running configuration 673/756:
2025-05-03 10:07:36,784 - main - INFO - Model: ConvNeXt
2025-05-03 10:07:36,784 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:07:36,784 - main - INFO - Scheduler: StepLR
2025-05-03 10:07:36,784 - main - INFO - Loss Function: CrossEntropy
2025-05-03 10:07:36,784 - main - INFO - ==================================================
2025-05-03 10:07:36,784 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_673 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_673
2025-05-03 10:07:36,784 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_673 - INFO - Config: {
  "id": 673,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:07:37,354 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:07:37,354 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 673,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:07:37,354 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-03 10:07:37,354 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-03 10:08:22,184 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 10:09:14,932 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.8139
2025-05-03 10:09:15,118 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 1 completed in 97.76s - Train Loss: 0.6503, Train Acc: 0.7068, Val Loss: 0.5972, Val Acc: 0.8139
2025-05-03 10:09:15,651 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 10:09:15,651 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-03 10:10:00,607 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 10:10:52,874 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8139 to 0.8370
2025-05-03 10:10:53,144 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 2 completed in 97.49s - Train Loss: 0.5875, Train Acc: 0.8005, Val Loss: 0.5449, Val Acc: 0.8370
2025-05-03 10:10:53,677 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 10:10:53,678 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-03 10:11:38,560 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 10:12:31,247 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8370 to 0.8465
2025-05-03 10:12:31,517 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 3 completed in 97.84s - Train Loss: 0.5464, Train Acc: 0.8192, Val Loss: 0.5118, Val Acc: 0.8465
2025-05-03 10:12:32,054 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 10:12:32,055 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-03 10:13:17,225 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 10:14:09,654 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8465 to 0.8499
2025-05-03 10:14:09,928 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 4 completed in 97.87s - Train Loss: 0.5186, Train Acc: 0.8393, Val Loss: 0.4905, Val Acc: 0.8499
2025-05-03 10:14:10,460 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 10:14:10,460 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 10:14:55,219 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 10:15:46,482 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8499 to 0.8636
2025-05-03 10:15:46,752 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 5 completed in 96.29s - Train Loss: 0.4987, Train Acc: 0.8516, Val Loss: 0.4748, Val Acc: 0.8636
2025-05-03 10:15:47,271 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 10:15:47,272 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 10:16:31,329 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 10:17:24,157 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8636 to 0.8714
2025-05-03 10:17:24,445 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 6 completed in 97.17s - Train Loss: 0.4843, Train Acc: 0.8599, Val Loss: 0.4642, Val Acc: 0.8714
2025-05-03 10:17:24,982 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 10:17:24,983 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 10:18:09,097 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 10:19:01,928 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8714 to 0.8739
2025-05-03 10:19:02,196 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 7 completed in 97.21s - Train Loss: 0.4726, Train Acc: 0.8642, Val Loss: 0.4546, Val Acc: 0.8739
2025-05-03 10:19:02,699 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 10:19:02,699 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 10:19:47,492 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 10:20:40,067 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8739 to 0.8877
2025-05-03 10:20:40,354 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 8 completed in 97.65s - Train Loss: 0.4636, Train Acc: 0.8730, Val Loss: 0.4462, Val Acc: 0.8877
2025-05-03 10:20:40,878 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 10:20:40,878 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 10:21:24,744 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 10:22:17,342 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation loss improved from inf to 0.4423
2025-05-03 10:22:17,619 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 9 completed in 96.74s - Train Loss: 0.4540, Train Acc: 0.8767, Val Loss: 0.4423, Val Acc: 0.8842
2025-05-03 10:22:18,168 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 10:22:18,168 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 10:23:01,570 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 10:23:54,490 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4423 to 0.4361
2025-05-03 10:23:54,772 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 10 completed in 96.60s - Train Loss: 0.4456, Train Acc: 0.8923, Val Loss: 0.4361, Val Acc: 0.8842
2025-05-03 10:23:55,290 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 10:23:55,290 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 10:24:39,189 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 10:25:32,227 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8877 to 0.8971
2025-05-03 10:25:32,514 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 11 completed in 97.22s - Train Loss: 0.4403, Train Acc: 0.8927, Val Loss: 0.4318, Val Acc: 0.8971
2025-05-03 10:25:33,033 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 10:25:33,034 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 10:26:17,702 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 10:27:11,559 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4361 to 0.4307
2025-05-03 10:27:11,838 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 12 completed in 98.80s - Train Loss: 0.4374, Train Acc: 0.8940, Val Loss: 0.4307, Val Acc: 0.8954
2025-05-03 10:27:12,368 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 10:27:12,369 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 10:27:56,779 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 10:28:49,235 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8971 to 0.8979
2025-05-03 10:28:49,531 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 13 completed in 97.16s - Train Loss: 0.4366, Train Acc: 0.8927, Val Loss: 0.4294, Val Acc: 0.8979
2025-05-03 10:28:50,054 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 10:28:50,054 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 10:29:33,743 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 10:30:26,989 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4307 to 0.4288
2025-05-03 10:30:27,274 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 14 completed in 97.22s - Train Loss: 0.4373, Train Acc: 0.8964, Val Loss: 0.4288, Val Acc: 0.8954
2025-05-03 10:30:27,797 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 10:30:27,798 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 10:31:12,903 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 10:32:04,706 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation accuracy improved from 0.8979 to 0.9022
2025-05-03 10:32:04,976 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 15 completed in 97.18s - Train Loss: 0.4344, Train Acc: 0.8990, Val Loss: 0.4270, Val Acc: 0.9022
2025-05-03 10:32:05,511 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 10:32:05,512 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 10:32:50,176 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 10:33:42,694 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4288 to 0.4284
2025-05-03 10:33:42,987 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 16 completed in 97.48s - Train Loss: 0.4344, Train Acc: 0.8985, Val Loss: 0.4284, Val Acc: 0.8971
2025-05-03 10:33:43,511 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 10:33:43,511 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 10:34:28,004 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 10:35:21,520 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4284 to 0.4267
2025-05-03 10:35:21,806 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 17 completed in 98.29s - Train Loss: 0.4354, Train Acc: 0.8981, Val Loss: 0.4267, Val Acc: 0.8997
2025-05-03 10:35:22,330 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 10:35:22,330 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 10:36:06,263 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 10:36:58,217 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4267 to 0.4258
2025-05-03 10:36:58,489 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 18 completed in 96.16s - Train Loss: 0.4343, Train Acc: 0.8958, Val Loss: 0.4258, Val Acc: 0.9014
2025-05-03 10:36:58,995 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 10:36:58,996 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 10:37:43,916 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 10:38:37,259 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Validation loss improved from 0.4258 to 0.4251
2025-05-03 10:38:37,552 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 19 completed in 98.56s - Train Loss: 0.4325, Train Acc: 0.8994, Val Loss: 0.4251, Val Acc: 0.9014
2025-05-03 10:38:38,061 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 10:38:38,062 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 10:39:22,993 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 10:40:16,237 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Epoch 20 completed in 98.17s - Train Loss: 0.4333, Train Acc: 0.8953, Val Loss: 0.4252, Val Acc: 0.8928
2025-05-03 10:40:16,786 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 10:40:16,786 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:40:16,790 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_673 - INFO - Starting model evaluation
2025-05-03 10:40:28,310 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_673 - INFO - Evaluation metrics:
  Accuracy:  0.9019
  Precision: 0.7895
  Recall:    0.8916
  F1 Score:  0.8374
  IoU:       0.7203
  mAP:       0.9234
  AUC:       0.9584
2025-05-03 10:40:28,316 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_673 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_673/final_results.json
2025-05-03 10:40:28,318 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_673 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_673/final_results.json
2025-05-03 10:40:28,318 - main - INFO - 
Summary for configuration 673:
2025-05-03 10:40:28,318 - main - INFO - Accuracy: 0.9019
2025-05-03 10:40:28,318 - main - INFO - Precision: 0.7895
2025-05-03 10:40:28,318 - main - INFO - Recall: 0.8916
2025-05-03 10:40:28,318 - main - INFO - F1 Score: 0.8374
2025-05-03 10:40:28,318 - main - INFO - IoU: 0.7203
2025-05-03 10:40:28,318 - main - INFO - mAP: 0.9234
2025-05-03 10:40:28,318 - main - INFO - AUC: 0.9584
2025-05-03 10:40:28,318 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:40:28,318 - main - INFO - 
==================================================
2025-05-03 10:40:28,318 - main - INFO - Running configuration 674/756:
2025-05-03 10:40:28,318 - main - INFO - Model: ConvNeXt
2025-05-03 10:40:28,318 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:40:28,318 - main - INFO - Scheduler: StepLR
2025-05-03 10:40:28,318 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:40:28,318 - main - INFO - ==================================================
2025-05-03 10:40:28,318 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_674 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_674
2025-05-03 10:40:28,318 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_674 - INFO - Config: {
  "id": 674,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:40:29,096 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:40:29,096 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 674,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:40:29,096 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:40:29,284 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:40:29,285 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:40:29,288 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_674 - INFO - Starting model evaluation
2025-05-03 10:40:40,779 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_674 - INFO - Evaluation metrics:
  Accuracy:  0.8989
  Precision: 0.7840
  Recall:    0.8881
  F1 Score:  0.8328
  IoU:       0.7135
  mAP:       0.9259
  AUC:       0.9585
2025-05-03 10:40:40,782 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_674 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_674/final_results.json
2025-05-03 10:40:40,784 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_674 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_674/final_results.json
2025-05-03 10:40:40,784 - main - INFO - 
Summary for configuration 674:
2025-05-03 10:40:40,784 - main - INFO - Accuracy: 0.8989
2025-05-03 10:40:40,784 - main - INFO - Precision: 0.7840
2025-05-03 10:40:40,784 - main - INFO - Recall: 0.8881
2025-05-03 10:40:40,784 - main - INFO - F1 Score: 0.8328
2025-05-03 10:40:40,784 - main - INFO - IoU: 0.7135
2025-05-03 10:40:40,784 - main - INFO - mAP: 0.9259
2025-05-03 10:40:40,784 - main - INFO - AUC: 0.9585
2025-05-03 10:40:40,784 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:40:40,784 - main - INFO - 
==================================================
2025-05-03 10:40:40,784 - main - INFO - Running configuration 675/756:
2025-05-03 10:40:40,784 - main - INFO - Model: ConvNeXt
2025-05-03 10:40:40,784 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:40:40,784 - main - INFO - Scheduler: StepLR
2025-05-03 10:40:40,784 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 10:40:40,784 - main - INFO - ==================================================
2025-05-03 10:40:40,785 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_675 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_675
2025-05-03 10:40:40,785 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_675 - INFO - Config: {
  "id": 675,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:40:41,595 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:40:41,595 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 675,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:40:41,595 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:40:41,784 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:40:41,785 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:40:41,788 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_675 - INFO - Starting model evaluation
2025-05-03 10:40:53,221 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_675 - INFO - Evaluation metrics:
  Accuracy:  0.8999
  Precision: 0.7812
  Recall:    0.8986
  F1 Score:  0.8358
  IoU:       0.7179
  mAP:       0.9244
  AUC:       0.9591
2025-05-03 10:40:53,223 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_675 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_675/final_results.json
2025-05-03 10:40:53,224 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_675 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_675/final_results.json
2025-05-03 10:40:53,224 - main - INFO - 
Summary for configuration 675:
2025-05-03 10:40:53,224 - main - INFO - Accuracy: 0.8999
2025-05-03 10:40:53,224 - main - INFO - Precision: 0.7812
2025-05-03 10:40:53,224 - main - INFO - Recall: 0.8986
2025-05-03 10:40:53,224 - main - INFO - F1 Score: 0.8358
2025-05-03 10:40:53,224 - main - INFO - IoU: 0.7179
2025-05-03 10:40:53,224 - main - INFO - mAP: 0.9244
2025-05-03 10:40:53,224 - main - INFO - AUC: 0.9591
2025-05-03 10:40:53,224 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:40:53,224 - main - INFO - 
==================================================
2025-05-03 10:40:53,224 - main - INFO - Running configuration 676/756:
2025-05-03 10:40:53,224 - main - INFO - Model: ConvNeXt
2025-05-03 10:40:53,224 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:40:53,224 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 10:40:53,224 - main - INFO - Loss Function: CrossEntropy
2025-05-03 10:40:53,224 - main - INFO - ==================================================
2025-05-03 10:40:53,225 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_676 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_676
2025-05-03 10:40:53,225 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_676 - INFO - Config: {
  "id": 676,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:40:53,878 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:40:53,878 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 676,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:40:53,878 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:40:54,067 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:40:54,068 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:40:54,071 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_676 - INFO - Starting model evaluation
2025-05-03 10:41:05,747 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_676 - INFO - Evaluation metrics:
  Accuracy:  0.8979
  Precision: 0.7798
  Recall:    0.8916
  F1 Score:  0.8320
  IoU:       0.7123
  mAP:       0.9233
  AUC:       0.9580
2025-05-03 10:41:05,749 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_676 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_676/final_results.json
2025-05-03 10:41:05,751 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_676 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_676/final_results.json
2025-05-03 10:41:05,751 - main - INFO - 
Summary for configuration 676:
2025-05-03 10:41:05,752 - main - INFO - Accuracy: 0.8979
2025-05-03 10:41:05,752 - main - INFO - Precision: 0.7798
2025-05-03 10:41:05,752 - main - INFO - Recall: 0.8916
2025-05-03 10:41:05,752 - main - INFO - F1 Score: 0.8320
2025-05-03 10:41:05,752 - main - INFO - IoU: 0.7123
2025-05-03 10:41:05,752 - main - INFO - mAP: 0.9233
2025-05-03 10:41:05,752 - main - INFO - AUC: 0.9580
2025-05-03 10:41:05,752 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:41:05,752 - main - INFO - 
==================================================
2025-05-03 10:41:05,752 - main - INFO - Running configuration 677/756:
2025-05-03 10:41:05,752 - main - INFO - Model: ConvNeXt
2025-05-03 10:41:05,752 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:41:05,752 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 10:41:05,752 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:41:05,752 - main - INFO - ==================================================
2025-05-03 10:41:05,752 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_677 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_677
2025-05-03 10:41:05,752 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_677 - INFO - Config: {
  "id": 677,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:41:06,573 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:41:06,574 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 677,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:41:06,574 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:41:06,764 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:41:06,765 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:41:06,768 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_677 - INFO - Starting model evaluation
2025-05-03 10:41:18,147 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_677 - INFO - Evaluation metrics:
  Accuracy:  0.9009
  Precision: 0.7888
  Recall:    0.8881
  F1 Score:  0.8355
  IoU:       0.7175
  mAP:       0.9249
  AUC:       0.9596
2025-05-03 10:41:18,148 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_677 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_677/final_results.json
2025-05-03 10:41:18,150 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_677 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_677/final_results.json
2025-05-03 10:41:18,150 - main - INFO - 
Summary for configuration 677:
2025-05-03 10:41:18,150 - main - INFO - Accuracy: 0.9009
2025-05-03 10:41:18,150 - main - INFO - Precision: 0.7888
2025-05-03 10:41:18,150 - main - INFO - Recall: 0.8881
2025-05-03 10:41:18,150 - main - INFO - F1 Score: 0.8355
2025-05-03 10:41:18,150 - main - INFO - IoU: 0.7175
2025-05-03 10:41:18,150 - main - INFO - mAP: 0.9249
2025-05-03 10:41:18,150 - main - INFO - AUC: 0.9596
2025-05-03 10:41:18,150 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:41:18,150 - main - INFO - 
==================================================
2025-05-03 10:41:18,150 - main - INFO - Running configuration 678/756:
2025-05-03 10:41:18,150 - main - INFO - Model: ConvNeXt
2025-05-03 10:41:18,150 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:41:18,150 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 10:41:18,150 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 10:41:18,150 - main - INFO - ==================================================
2025-05-03 10:41:18,150 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_678 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_678
2025-05-03 10:41:18,150 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_678 - INFO - Config: {
  "id": 678,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:41:18,986 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:41:18,986 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 678,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:41:18,986 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:41:19,175 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:41:19,176 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:41:19,179 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_678 - INFO - Starting model evaluation
2025-05-03 10:41:30,612 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_678 - INFO - Evaluation metrics:
  Accuracy:  0.8979
  Precision: 0.7781
  Recall:    0.8951
  F1 Score:  0.8325
  IoU:       0.7131
  mAP:       0.9219
  AUC:       0.9571
2025-05-03 10:41:30,613 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_678 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_678/final_results.json
2025-05-03 10:41:30,615 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_678 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_678/final_results.json
2025-05-03 10:41:30,615 - main - INFO - 
Summary for configuration 678:
2025-05-03 10:41:30,615 - main - INFO - Accuracy: 0.8979
2025-05-03 10:41:30,615 - main - INFO - Precision: 0.7781
2025-05-03 10:41:30,615 - main - INFO - Recall: 0.8951
2025-05-03 10:41:30,615 - main - INFO - F1 Score: 0.8325
2025-05-03 10:41:30,615 - main - INFO - IoU: 0.7131
2025-05-03 10:41:30,615 - main - INFO - mAP: 0.9219
2025-05-03 10:41:30,615 - main - INFO - AUC: 0.9571
2025-05-03 10:41:30,615 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:41:30,615 - main - INFO - 
==================================================
2025-05-03 10:41:30,615 - main - INFO - Running configuration 679/756:
2025-05-03 10:41:30,615 - main - INFO - Model: ConvNeXt
2025-05-03 10:41:30,615 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:41:30,615 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 10:41:30,615 - main - INFO - Loss Function: CrossEntropy
2025-05-03 10:41:30,615 - main - INFO - ==================================================
2025-05-03 10:41:30,616 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_679 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_679
2025-05-03 10:41:30,616 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_679 - INFO - Config: {
  "id": 679,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:41:31,388 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:41:31,391 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 679,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:41:31,391 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:41:31,574 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:41:31,667 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:41:31,670 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_679 - INFO - Starting model evaluation
2025-05-03 10:41:43,244 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_679 - INFO - Evaluation metrics:
  Accuracy:  0.9009
  Precision: 0.7870
  Recall:    0.8916
  F1 Score:  0.8361
  IoU:       0.7183
  mAP:       0.9230
  AUC:       0.9582
2025-05-03 10:41:43,246 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_679 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_679/final_results.json
2025-05-03 10:41:43,247 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_679 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_679/final_results.json
2025-05-03 10:41:43,247 - main - INFO - 
Summary for configuration 679:
2025-05-03 10:41:43,247 - main - INFO - Accuracy: 0.9009
2025-05-03 10:41:43,247 - main - INFO - Precision: 0.7870
2025-05-03 10:41:43,247 - main - INFO - Recall: 0.8916
2025-05-03 10:41:43,247 - main - INFO - F1 Score: 0.8361
2025-05-03 10:41:43,247 - main - INFO - IoU: 0.7183
2025-05-03 10:41:43,247 - main - INFO - mAP: 0.9230
2025-05-03 10:41:43,247 - main - INFO - AUC: 0.9582
2025-05-03 10:41:43,247 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:41:43,247 - main - INFO - 
==================================================
2025-05-03 10:41:43,247 - main - INFO - Running configuration 680/756:
2025-05-03 10:41:43,247 - main - INFO - Model: ConvNeXt
2025-05-03 10:41:43,247 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:41:43,247 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 10:41:43,247 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:41:43,247 - main - INFO - ==================================================
2025-05-03 10:41:43,248 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_680 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_680
2025-05-03 10:41:43,248 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_680 - INFO - Config: {
  "id": 680,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:41:43,996 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:41:43,997 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 680,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:41:43,997 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:41:44,422 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:41:44,423 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:41:44,426 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_680 - INFO - Starting model evaluation
2025-05-03 10:41:55,765 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_680 - INFO - Evaluation metrics:
  Accuracy:  0.8989
  Precision: 0.7822
  Recall:    0.8916
  F1 Score:  0.8333
  IoU:       0.7143
  mAP:       0.9248
  AUC:       0.9594
2025-05-03 10:41:55,767 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_680 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_680/final_results.json
2025-05-03 10:41:55,768 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_680 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_680/final_results.json
2025-05-03 10:41:55,769 - main - INFO - 
Summary for configuration 680:
2025-05-03 10:41:55,769 - main - INFO - Accuracy: 0.8989
2025-05-03 10:41:55,769 - main - INFO - Precision: 0.7822
2025-05-03 10:41:55,769 - main - INFO - Recall: 0.8916
2025-05-03 10:41:55,769 - main - INFO - F1 Score: 0.8333
2025-05-03 10:41:55,769 - main - INFO - IoU: 0.7143
2025-05-03 10:41:55,769 - main - INFO - mAP: 0.9248
2025-05-03 10:41:55,769 - main - INFO - AUC: 0.9594
2025-05-03 10:41:55,769 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:41:55,769 - main - INFO - 
==================================================
2025-05-03 10:41:55,769 - main - INFO - Running configuration 681/756:
2025-05-03 10:41:55,769 - main - INFO - Model: ConvNeXt
2025-05-03 10:41:55,769 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:41:55,769 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 10:41:55,769 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 10:41:55,769 - main - INFO - ==================================================
2025-05-03 10:41:55,769 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_681 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_681
2025-05-03 10:41:55,769 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_681 - INFO - Config: {
  "id": 681,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:41:56,327 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:41:56,327 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 681,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:41:56,327 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:41:56,754 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:41:56,756 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:41:56,758 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_681 - INFO - Starting model evaluation
2025-05-03 10:42:08,181 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_681 - INFO - Evaluation metrics:
  Accuracy:  0.8999
  Precision: 0.7846
  Recall:    0.8916
  F1 Score:  0.8347
  IoU:       0.7163
  mAP:       0.9227
  AUC:       0.9578
2025-05-03 10:42:08,182 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_681 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_681/final_results.json
2025-05-03 10:42:08,184 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_681 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_681/final_results.json
2025-05-03 10:42:08,184 - main - INFO - 
Summary for configuration 681:
2025-05-03 10:42:08,184 - main - INFO - Accuracy: 0.8999
2025-05-03 10:42:08,184 - main - INFO - Precision: 0.7846
2025-05-03 10:42:08,184 - main - INFO - Recall: 0.8916
2025-05-03 10:42:08,184 - main - INFO - F1 Score: 0.8347
2025-05-03 10:42:08,184 - main - INFO - IoU: 0.7163
2025-05-03 10:42:08,184 - main - INFO - mAP: 0.9227
2025-05-03 10:42:08,184 - main - INFO - AUC: 0.9578
2025-05-03 10:42:08,184 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:42:08,184 - main - INFO - 
==================================================
2025-05-03 10:42:08,184 - main - INFO - Running configuration 682/756:
2025-05-03 10:42:08,184 - main - INFO - Model: ConvNeXt
2025-05-03 10:42:08,184 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:42:08,184 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 10:42:08,184 - main - INFO - Loss Function: CrossEntropy
2025-05-03 10:42:08,184 - main - INFO - ==================================================
2025-05-03 10:42:08,184 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_682 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_682
2025-05-03 10:42:08,184 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_682 - INFO - Config: {
  "id": 682,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:42:09,005 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:42:09,005 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 682,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:42:09,005 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:42:09,187 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:42:09,188 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:42:09,191 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_682 - INFO - Starting model evaluation
2025-05-03 10:42:20,934 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_682 - INFO - Evaluation metrics:
  Accuracy:  0.9009
  Precision: 0.7835
  Recall:    0.8986
  F1 Score:  0.8371
  IoU:       0.7199
  mAP:       0.9248
  AUC:       0.9591
2025-05-03 10:42:20,936 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_682 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_682/final_results.json
2025-05-03 10:42:20,937 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_682 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_682/final_results.json
2025-05-03 10:42:20,938 - main - INFO - 
Summary for configuration 682:
2025-05-03 10:42:20,938 - main - INFO - Accuracy: 0.9009
2025-05-03 10:42:20,938 - main - INFO - Precision: 0.7835
2025-05-03 10:42:20,938 - main - INFO - Recall: 0.8986
2025-05-03 10:42:20,938 - main - INFO - F1 Score: 0.8371
2025-05-03 10:42:20,938 - main - INFO - IoU: 0.7199
2025-05-03 10:42:20,938 - main - INFO - mAP: 0.9248
2025-05-03 10:42:20,938 - main - INFO - AUC: 0.9591
2025-05-03 10:42:20,938 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:42:20,938 - main - INFO - 
==================================================
2025-05-03 10:42:20,938 - main - INFO - Running configuration 683/756:
2025-05-03 10:42:20,938 - main - INFO - Model: ConvNeXt
2025-05-03 10:42:20,938 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:42:20,938 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 10:42:20,938 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:42:20,938 - main - INFO - ==================================================
2025-05-03 10:42:20,938 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_683 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_683
2025-05-03 10:42:20,938 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_683 - INFO - Config: {
  "id": 683,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:42:21,737 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:42:21,737 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 683,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:42:21,737 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:42:21,919 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:42:21,920 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:42:21,923 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_683 - INFO - Starting model evaluation
2025-05-03 10:42:33,327 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_683 - INFO - Evaluation metrics:
  Accuracy:  0.9039
  Precision: 0.7926
  Recall:    0.8951
  F1 Score:  0.8407
  IoU:       0.7252
  mAP:       0.9219
  AUC:       0.9584
2025-05-03 10:42:33,329 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_683 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_683/final_results.json
2025-05-03 10:42:33,331 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_683 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_683/final_results.json
2025-05-03 10:42:33,331 - main - INFO - 
Summary for configuration 683:
2025-05-03 10:42:33,331 - main - INFO - Accuracy: 0.9039
2025-05-03 10:42:33,331 - main - INFO - Precision: 0.7926
2025-05-03 10:42:33,331 - main - INFO - Recall: 0.8951
2025-05-03 10:42:33,331 - main - INFO - F1 Score: 0.8407
2025-05-03 10:42:33,331 - main - INFO - IoU: 0.7252
2025-05-03 10:42:33,331 - main - INFO - mAP: 0.9219
2025-05-03 10:42:33,331 - main - INFO - AUC: 0.9584
2025-05-03 10:42:33,331 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:42:33,331 - main - INFO - 
==================================================
2025-05-03 10:42:33,331 - main - INFO - Running configuration 684/756:
2025-05-03 10:42:33,331 - main - INFO - Model: ConvNeXt
2025-05-03 10:42:33,331 - main - INFO - Optimizer: SGD, Learning Rate: 0.0001
2025-05-03 10:42:33,331 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 10:42:33,331 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 10:42:33,331 - main - INFO - ==================================================
2025-05-03 10:42:33,331 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_684 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001_id_684
2025-05-03 10:42:33,331 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_684 - INFO - Config: {
  "id": 684,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:42:34,115 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_SGD_lr_0.0001
2025-05-03 10:42:34,115 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Config: {
  "id": 684,
  "model_name": "ConvNeXt",
  "optimizer": "SGD",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 10:42:34,116 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001/checkpoint.pt
2025-05-03 10:42:34,297 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9022
2025-05-03 10:42:34,300 - training.model_ConvNeXt_opt_SGD_lr_0.0001 - INFO - Training completed after 1958.88 seconds
2025-05-03 10:42:34,303 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_684 - INFO - Starting model evaluation
2025-05-03 10:42:45,843 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_684 - INFO - Evaluation metrics:
  Accuracy:  0.9039
  Precision: 0.7908
  Recall:    0.8986
  F1 Score:  0.8412
  IoU:       0.7260
  mAP:       0.9230
  AUC:       0.9580
2025-05-03 10:42:45,845 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_684 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_684/final_results.json
2025-05-03 10:42:45,847 - training.model_ConvNeXt_opt_SGD_lr_0.0001_id_684 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_SGD_lr_0.0001_id_684/final_results.json
2025-05-03 10:42:45,847 - main - INFO - 
Summary for configuration 684:
2025-05-03 10:42:45,847 - main - INFO - Accuracy: 0.9039
2025-05-03 10:42:45,847 - main - INFO - Precision: 0.7908
2025-05-03 10:42:45,847 - main - INFO - Recall: 0.8986
2025-05-03 10:42:45,847 - main - INFO - F1 Score: 0.8412
2025-05-03 10:42:45,847 - main - INFO - IoU: 0.7260
2025-05-03 10:42:45,847 - main - INFO - mAP: 0.9230
2025-05-03 10:42:45,847 - main - INFO - AUC: 0.9580
2025-05-03 10:42:45,847 - main - INFO - Training time: 1958.88 seconds
2025-05-03 10:42:45,847 - main - INFO - 
==================================================
2025-05-03 10:42:45,847 - main - INFO - Running configuration 685/756:
2025-05-03 10:42:45,847 - main - INFO - Model: ConvNeXt
2025-05-03 10:42:45,847 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 10:42:45,848 - main - INFO - Scheduler: StepLR
2025-05-03 10:42:45,848 - main - INFO - Loss Function: CrossEntropy
2025-05-03 10:42:45,848 - main - INFO - ==================================================
2025-05-03 10:42:45,848 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_685 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_685
2025-05-03 10:42:45,848 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_685 - INFO - Config: {
  "id": 685,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:42:46,729 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 10:42:46,729 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 685,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 10:42:46,729 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-03 10:42:46,730 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 1/20
2025-05-03 10:43:31,780 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 10:44:25,391 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.5043
2025-05-03 10:44:25,584 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 1 completed in 98.85s - Train Loss: 0.8100, Train Acc: 0.5000, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 10:44:26,410 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 10:44:26,410 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 2/20
2025-05-03 10:45:11,781 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 10:46:04,801 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Validation loss improved from inf to 0.8096
2025-05-03 10:46:05,069 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 2 completed in 98.66s - Train Loss: 0.8204, Train Acc: 0.4925, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 10:46:05,874 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 10:46:05,874 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 3/20
2025-05-03 10:46:51,736 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 10:47:45,024 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 3 completed in 99.15s - Train Loss: 0.8216, Train Acc: 0.4903, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 10:47:45,803 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 10:47:45,803 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 4/20
2025-05-03 10:48:30,653 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 10:49:24,497 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 4 completed in 98.69s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 10:49:25,296 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 10:49:25,296 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 5/20
2025-05-03 10:50:11,077 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 10:51:03,405 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 5 completed in 98.11s - Train Loss: 0.8164, Train Acc: 0.4964, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 10:51:04,179 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 10:51:04,180 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 6/20
2025-05-03 10:51:50,953 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 10:52:43,588 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 6 completed in 99.41s - Train Loss: 0.8022, Train Acc: 0.5105, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 10:52:44,405 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 10:52:44,405 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 7/20
2025-05-03 10:53:29,712 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 10:54:23,976 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 7 completed in 99.57s - Train Loss: 0.8048, Train Acc: 0.5092, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 10:54:24,829 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 10:54:24,830 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Early stopping triggered after 7 epochs
2025-05-03 10:54:24,830 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 697.25 seconds
2025-05-03 10:54:24,833 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_685 - INFO - Starting model evaluation
2025-05-03 10:54:36,399 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_685 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 10:54:36,400 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_685 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_685/final_results.json
2025-05-03 10:54:36,401 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_685 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_685/final_results.json
2025-05-03 10:54:36,401 - main - INFO - 
Summary for configuration 685:
2025-05-03 10:54:36,401 - main - INFO - Accuracy: 0.2834
2025-05-03 10:54:36,401 - main - INFO - Precision: 0.2834
2025-05-03 10:54:36,401 - main - INFO - Recall: 1.0000
2025-05-03 10:54:36,401 - main - INFO - F1 Score: 0.4417
2025-05-03 10:54:36,401 - main - INFO - IoU: 0.2834
2025-05-03 10:54:36,401 - main - INFO - mAP: 0.2834
2025-05-03 10:54:36,401 - main - INFO - AUC: 0.5000
2025-05-03 10:54:36,401 - main - INFO - Training time: 697.25 seconds
2025-05-03 10:54:36,401 - main - INFO - 
==================================================
2025-05-03 10:54:36,401 - main - INFO - Running configuration 686/756:
2025-05-03 10:54:36,401 - main - INFO - Model: ConvNeXt
2025-05-03 10:54:36,401 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 10:54:36,401 - main - INFO - Scheduler: StepLR
2025-05-03 10:54:36,401 - main - INFO - Loss Function: FocalLoss
2025-05-03 10:54:36,401 - main - INFO - ==================================================
2025-05-03 10:54:36,401 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_686 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_686
2025-05-03 10:54:36,401 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_686 - INFO - Config: {
  "id": 686,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:54:37,224 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 10:54:37,224 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 686,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 10:54:37,224 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 10:54:37,718 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 7, best validation accuracy: 0.5043
2025-05-03 10:54:37,719 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 8/20
2025-05-03 10:55:24,280 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 10:56:16,144 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.8096 to 0.0912
2025-05-03 10:56:16,444 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 8 completed in 98.72s - Train Loss: 0.0896, Train Acc: 0.5054, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 10:56:17,274 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 10:56:17,275 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 9/20
2025-05-03 10:57:02,830 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 10:57:55,093 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Validation loss improved from 0.0912 to 0.0899
2025-05-03 10:57:55,372 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 9 completed in 98.10s - Train Loss: 0.0898, Train Acc: 0.5045, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 10:57:56,182 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 10:57:56,183 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 10/20
2025-05-03 10:58:41,228 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 10:59:33,586 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 10 completed in 97.40s - Train Loss: 0.0909, Train Acc: 0.4989, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 10:59:34,385 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 10:59:34,386 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 11/20
2025-05-03 11:00:19,213 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 11:01:11,980 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 11 completed in 97.59s - Train Loss: 0.0906, Train Acc: 0.4987, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 11:01:12,798 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 11:01:12,799 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 12/20
2025-05-03 11:01:57,056 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 11:02:50,420 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 12 completed in 97.62s - Train Loss: 0.0905, Train Acc: 0.4998, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 11:02:51,240 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 11:02:51,241 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 11:03:36,735 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 11:04:30,671 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 13 completed in 99.43s - Train Loss: 0.0909, Train Acc: 0.4985, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 11:04:31,476 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 11:04:31,476 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 11:05:17,235 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 11:06:10,458 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 14 completed in 98.98s - Train Loss: 0.0899, Train Acc: 0.5034, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 11:06:11,285 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 11:06:11,286 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Early stopping triggered after 14 epochs
2025-05-03 11:06:11,286 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 1389.98 seconds
2025-05-03 11:06:11,289 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_686 - INFO - Starting model evaluation
2025-05-03 11:06:23,089 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_686 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2654
  AUC:       0.4915
2025-05-03 11:06:23,091 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_686 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_686/final_results.json
2025-05-03 11:06:23,093 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_686 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_686/final_results.json
2025-05-03 11:06:23,093 - main - INFO - 
Summary for configuration 686:
2025-05-03 11:06:23,093 - main - INFO - Accuracy: 0.7166
2025-05-03 11:06:23,093 - main - INFO - Precision: 0.0000
2025-05-03 11:06:23,093 - main - INFO - Recall: 0.0000
2025-05-03 11:06:23,093 - main - INFO - F1 Score: 0.0000
2025-05-03 11:06:23,093 - main - INFO - IoU: 0.0000
2025-05-03 11:06:23,093 - main - INFO - mAP: 0.2654
2025-05-03 11:06:23,093 - main - INFO - AUC: 0.4915
2025-05-03 11:06:23,093 - main - INFO - Training time: 1389.98 seconds
2025-05-03 11:06:23,093 - main - INFO - 
==================================================
2025-05-03 11:06:23,093 - main - INFO - Running configuration 687/756:
2025-05-03 11:06:23,093 - main - INFO - Model: ConvNeXt
2025-05-03 11:06:23,093 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:06:23,093 - main - INFO - Scheduler: StepLR
2025-05-03 11:06:23,093 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:06:23,093 - main - INFO - ==================================================
2025-05-03 11:06:23,093 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_687 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_687
2025-05-03 11:06:23,093 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_687 - INFO - Config: {
  "id": 687,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:06:23,818 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:06:23,818 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 687,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:06:23,818 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:06:24,325 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 14, best validation accuracy: 0.5043
2025-05-03 11:06:24,326 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 11:07:09,332 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 11:08:02,694 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 15 completed in 98.37s - Train Loss: 0.8107, Train Acc: 0.5032, Val Loss: 0.8103, Val Acc: 0.5043
2025-05-03 11:08:03,468 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 11:08:03,468 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Early stopping triggered after 15 epochs
2025-05-03 11:08:03,468 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 1488.35 seconds
2025-05-03 11:08:03,472 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_687 - INFO - Starting model evaluation
2025-05-03 11:08:15,362 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_687 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2789
  AUC:       0.5329
2025-05-03 11:08:15,364 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_687 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_687/final_results.json
2025-05-03 11:08:15,366 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_687 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_687/final_results.json
2025-05-03 11:08:15,366 - main - INFO - 
Summary for configuration 687:
2025-05-03 11:08:15,366 - main - INFO - Accuracy: 0.7166
2025-05-03 11:08:15,366 - main - INFO - Precision: 0.0000
2025-05-03 11:08:15,366 - main - INFO - Recall: 0.0000
2025-05-03 11:08:15,366 - main - INFO - F1 Score: 0.0000
2025-05-03 11:08:15,366 - main - INFO - IoU: 0.0000
2025-05-03 11:08:15,366 - main - INFO - mAP: 0.2789
2025-05-03 11:08:15,366 - main - INFO - AUC: 0.5329
2025-05-03 11:08:15,366 - main - INFO - Training time: 1488.35 seconds
2025-05-03 11:08:15,366 - main - INFO - 
==================================================
2025-05-03 11:08:15,366 - main - INFO - Running configuration 688/756:
2025-05-03 11:08:15,366 - main - INFO - Model: ConvNeXt
2025-05-03 11:08:15,366 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:08:15,366 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 11:08:15,366 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:08:15,366 - main - INFO - ==================================================
2025-05-03 11:08:15,366 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_688 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_688
2025-05-03 11:08:15,366 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_688 - INFO - Config: {
  "id": 688,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:08:15,949 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:08:15,949 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 688,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:08:15,949 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:08:16,523 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 15, best validation accuracy: 0.5043
2025-05-03 11:08:16,525 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 11:09:01,826 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 11:09:54,592 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 16 completed in 98.07s - Train Loss: 0.8088, Train Acc: 0.5043, Val Loss: 0.8096, Val Acc: 0.5043
2025-05-03 11:09:55,420 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 11:09:55,420 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Early stopping triggered after 16 epochs
2025-05-03 11:09:55,420 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 1586.42 seconds
2025-05-03 11:09:55,424 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_688 - INFO - Starting model evaluation
2025-05-03 11:10:07,198 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_688 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.3106
  AUC:       0.5749
2025-05-03 11:10:07,201 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_688 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_688/final_results.json
2025-05-03 11:10:07,204 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_688 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_688/final_results.json
2025-05-03 11:10:07,204 - main - INFO - 
Summary for configuration 688:
2025-05-03 11:10:07,204 - main - INFO - Accuracy: 0.7166
2025-05-03 11:10:07,204 - main - INFO - Precision: 0.0000
2025-05-03 11:10:07,204 - main - INFO - Recall: 0.0000
2025-05-03 11:10:07,204 - main - INFO - F1 Score: 0.0000
2025-05-03 11:10:07,204 - main - INFO - IoU: 0.0000
2025-05-03 11:10:07,204 - main - INFO - mAP: 0.3106
2025-05-03 11:10:07,204 - main - INFO - AUC: 0.5749
2025-05-03 11:10:07,204 - main - INFO - Training time: 1586.42 seconds
2025-05-03 11:10:07,204 - main - INFO - 
==================================================
2025-05-03 11:10:07,204 - main - INFO - Running configuration 689/756:
2025-05-03 11:10:07,204 - main - INFO - Model: ConvNeXt
2025-05-03 11:10:07,204 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:10:07,204 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 11:10:07,204 - main - INFO - Loss Function: FocalLoss
2025-05-03 11:10:07,204 - main - INFO - ==================================================
2025-05-03 11:10:07,204 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_689 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_689
2025-05-03 11:10:07,204 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_689 - INFO - Config: {
  "id": 689,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:10:07,894 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:10:07,895 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 689,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:10:07,895 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:10:08,412 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 16, best validation accuracy: 0.5043
2025-05-03 11:10:08,413 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 11:10:53,556 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 11:11:46,605 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Epoch 17 completed in 98.19s - Train Loss: 0.0922, Train Acc: 0.4888, Val Loss: 0.0899, Val Acc: 0.5043
2025-05-03 11:11:47,435 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 11:11:47,436 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Early stopping triggered after 17 epochs
2025-05-03 11:11:47,436 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 1684.61 seconds
2025-05-03 11:11:47,439 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_689 - INFO - Starting model evaluation
2025-05-03 11:11:59,104 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_689 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.3072
  AUC:       0.5719
2025-05-03 11:11:59,106 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_689 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_689/final_results.json
2025-05-03 11:11:59,108 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_689 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_689/final_results.json
2025-05-03 11:11:59,108 - main - INFO - 
Summary for configuration 689:
2025-05-03 11:11:59,108 - main - INFO - Accuracy: 0.7166
2025-05-03 11:11:59,108 - main - INFO - Precision: 0.0000
2025-05-03 11:11:59,108 - main - INFO - Recall: 0.0000
2025-05-03 11:11:59,108 - main - INFO - F1 Score: 0.0000
2025-05-03 11:11:59,108 - main - INFO - IoU: 0.0000
2025-05-03 11:11:59,108 - main - INFO - mAP: 0.3072
2025-05-03 11:11:59,108 - main - INFO - AUC: 0.5719
2025-05-03 11:11:59,108 - main - INFO - Training time: 1684.61 seconds
2025-05-03 11:11:59,108 - main - INFO - 
==================================================
2025-05-03 11:11:59,108 - main - INFO - Running configuration 690/756:
2025-05-03 11:11:59,108 - main - INFO - Model: ConvNeXt
2025-05-03 11:11:59,108 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:11:59,108 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 11:11:59,108 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:11:59,108 - main - INFO - ==================================================
2025-05-03 11:11:59,108 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_690 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_690
2025-05-03 11:11:59,108 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_690 - INFO - Config: {
  "id": 690,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:11:59,852 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:11:59,852 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 690,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:11:59,852 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:12:00,359 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 17, best validation accuracy: 0.5043
2025-05-03 11:12:00,361 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 11:12:00,542 - training.model_ConvNeXt_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.01 GiB is allocated by PyTorch, and 432.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:00,542 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-03 11:12:01,367 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 11:12:01,368 - main - ERROR - Error running configuration 690: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.01 GiB is allocated by PyTorch, and 432.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:01,368 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
                                ^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.01 GiB is allocated by PyTorch, and 432.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:01,370 - main - INFO - 
==================================================
2025-05-03 11:12:01,370 - main - INFO - Running configuration 691/756:
2025-05-03 11:12:01,370 - main - INFO - Model: ConvNeXt
2025-05-03 11:12:01,370 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:12:01,370 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 11:12:01,370 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:12:01,370 - main - INFO - ==================================================
2025-05-03 11:12:01,370 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_691 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_691
2025-05-03 11:12:01,371 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_691 - INFO - Config: {
  "id": 691,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:12:02,155 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:12:02,155 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 691,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:12:02,155 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:12:02,667 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 18, best validation accuracy: 0.5043
2025-05-03 11:12:02,669 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 11:12:03,223 - training.model_ConvNeXt_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 8.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.83 GiB is allocated by PyTorch, and 631.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:03,223 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-03 11:12:04,033 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 11:12:04,033 - main - ERROR - Error running configuration 691: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 8.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.83 GiB is allocated by PyTorch, and 631.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:04,033 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
                                ^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 8.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.83 GiB is allocated by PyTorch, and 631.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:04,035 - main - INFO - 
==================================================
2025-05-03 11:12:04,035 - main - INFO - Running configuration 692/756:
2025-05-03 11:12:04,035 - main - INFO - Model: ConvNeXt
2025-05-03 11:12:04,035 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:12:04,035 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 11:12:04,035 - main - INFO - Loss Function: FocalLoss
2025-05-03 11:12:04,035 - main - INFO - ==================================================
2025-05-03 11:12:04,035 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_692 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_692
2025-05-03 11:12:04,035 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_692 - INFO - Config: {
  "id": 692,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:12:04,848 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:12:04,848 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 692,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:12:04,848 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:12:05,319 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 19, best validation accuracy: 0.5043
2025-05-03 11:12:05,320 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 11:12:05,488 - training.model_ConvNeXt_opt_Adam_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 8.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.99 GiB is allocated by PyTorch, and 470.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:05,488 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-03 11:12:06,312 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 11:12:06,312 - main - ERROR - Error running configuration 692: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 8.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.99 GiB is allocated by PyTorch, and 470.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:06,312 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 8.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.99 GiB is allocated by PyTorch, and 470.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:12:06,314 - main - INFO - 
==================================================
2025-05-03 11:12:06,314 - main - INFO - Running configuration 693/756:
2025-05-03 11:12:06,314 - main - INFO - Model: ConvNeXt
2025-05-03 11:12:06,314 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:12:06,314 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 11:12:06,314 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:12:06,314 - main - INFO - ==================================================
2025-05-03 11:12:06,314 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_693 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_693
2025-05-03 11:12:06,314 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_693 - INFO - Config: {
  "id": 693,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:12:07,120 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:12:07,120 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 693,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:12:07,120 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:12:07,724 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 11:12:07,725 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 1685.52 seconds
2025-05-03 11:12:07,728 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_693 - INFO - Starting model evaluation
2025-05-03 11:12:19,381 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_693 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.3054
  AUC:       0.5674
2025-05-03 11:12:19,385 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_693 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_693/final_results.json
2025-05-03 11:12:19,386 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_693 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_693/final_results.json
2025-05-03 11:12:19,386 - main - INFO - 
Summary for configuration 693:
2025-05-03 11:12:19,386 - main - INFO - Accuracy: 0.7166
2025-05-03 11:12:19,386 - main - INFO - Precision: 0.0000
2025-05-03 11:12:19,386 - main - INFO - Recall: 0.0000
2025-05-03 11:12:19,386 - main - INFO - F1 Score: 0.0000
2025-05-03 11:12:19,386 - main - INFO - IoU: 0.0000
2025-05-03 11:12:19,386 - main - INFO - mAP: 0.3054
2025-05-03 11:12:19,386 - main - INFO - AUC: 0.5674
2025-05-03 11:12:19,386 - main - INFO - Training time: 1685.52 seconds
2025-05-03 11:12:19,386 - main - INFO - 
==================================================
2025-05-03 11:12:19,386 - main - INFO - Running configuration 694/756:
2025-05-03 11:12:19,387 - main - INFO - Model: ConvNeXt
2025-05-03 11:12:19,387 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:12:19,387 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 11:12:19,387 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:12:19,387 - main - INFO - ==================================================
2025-05-03 11:12:19,387 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_694 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_694
2025-05-03 11:12:19,387 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_694 - INFO - Config: {
  "id": 694,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:12:20,056 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:12:20,056 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 694,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:12:20,056 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:12:20,564 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 11:12:20,565 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 1685.52 seconds
2025-05-03 11:12:20,568 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_694 - INFO - Starting model evaluation
2025-05-03 11:12:31,970 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_694 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.3122
  AUC:       0.5684
2025-05-03 11:12:31,972 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_694 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_694/final_results.json
2025-05-03 11:12:31,974 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_694 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_694/final_results.json
2025-05-03 11:12:31,974 - main - INFO - 
Summary for configuration 694:
2025-05-03 11:12:31,974 - main - INFO - Accuracy: 0.7166
2025-05-03 11:12:31,974 - main - INFO - Precision: 0.0000
2025-05-03 11:12:31,974 - main - INFO - Recall: 0.0000
2025-05-03 11:12:31,974 - main - INFO - F1 Score: 0.0000
2025-05-03 11:12:31,974 - main - INFO - IoU: 0.0000
2025-05-03 11:12:31,974 - main - INFO - mAP: 0.3122
2025-05-03 11:12:31,974 - main - INFO - AUC: 0.5684
2025-05-03 11:12:31,974 - main - INFO - Training time: 1685.52 seconds
2025-05-03 11:12:31,974 - main - INFO - 
==================================================
2025-05-03 11:12:31,974 - main - INFO - Running configuration 695/756:
2025-05-03 11:12:31,974 - main - INFO - Model: ConvNeXt
2025-05-03 11:12:31,974 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:12:31,974 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 11:12:31,974 - main - INFO - Loss Function: FocalLoss
2025-05-03 11:12:31,974 - main - INFO - ==================================================
2025-05-03 11:12:31,974 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_695 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_695
2025-05-03 11:12:31,974 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_695 - INFO - Config: {
  "id": 695,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:12:32,605 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:12:32,605 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 695,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:12:32,605 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:12:33,129 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 11:12:33,130 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 1685.52 seconds
2025-05-03 11:12:33,133 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_695 - INFO - Starting model evaluation
2025-05-03 11:12:44,759 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_695 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.2969
  AUC:       0.5596
2025-05-03 11:12:44,760 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_695 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_695/final_results.json
2025-05-03 11:12:44,762 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_695 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_695/final_results.json
2025-05-03 11:12:44,762 - main - INFO - 
Summary for configuration 695:
2025-05-03 11:12:44,762 - main - INFO - Accuracy: 0.7166
2025-05-03 11:12:44,762 - main - INFO - Precision: 0.0000
2025-05-03 11:12:44,762 - main - INFO - Recall: 0.0000
2025-05-03 11:12:44,762 - main - INFO - F1 Score: 0.0000
2025-05-03 11:12:44,762 - main - INFO - IoU: 0.0000
2025-05-03 11:12:44,762 - main - INFO - mAP: 0.2969
2025-05-03 11:12:44,762 - main - INFO - AUC: 0.5596
2025-05-03 11:12:44,762 - main - INFO - Training time: 1685.52 seconds
2025-05-03 11:12:44,762 - main - INFO - 
==================================================
2025-05-03 11:12:44,762 - main - INFO - Running configuration 696/756:
2025-05-03 11:12:44,762 - main - INFO - Model: ConvNeXt
2025-05-03 11:12:44,762 - main - INFO - Optimizer: Adam, Learning Rate: 0.01
2025-05-03 11:12:44,762 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 11:12:44,762 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:12:44,762 - main - INFO - ==================================================
2025-05-03 11:12:44,762 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_696 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01_id_696
2025-05-03 11:12:44,762 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_696 - INFO - Config: {
  "id": 696,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:12:45,498 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.01
2025-05-03 11:12:45,501 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Config: {
  "id": 696,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:12:45,501 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01/checkpoint.pt
2025-05-03 11:12:46,003 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 11:12:46,004 - training.model_ConvNeXt_opt_Adam_lr_0.01 - INFO - Training completed after 1685.52 seconds
2025-05-03 11:12:46,007 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_696 - INFO - Starting model evaluation
2025-05-03 11:12:57,455 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_696 - INFO - Evaluation metrics:
  Accuracy:  0.7166
  Precision: 0.0000
  Recall:    0.0000
  F1 Score:  0.0000
  IoU:       0.0000
  mAP:       0.3076
  AUC:       0.5700
2025-05-03 11:12:57,457 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_696 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_696/final_results.json
2025-05-03 11:12:57,458 - training.model_ConvNeXt_opt_Adam_lr_0.01_id_696 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.01_id_696/final_results.json
2025-05-03 11:12:57,458 - main - INFO - 
Summary for configuration 696:
2025-05-03 11:12:57,458 - main - INFO - Accuracy: 0.7166
2025-05-03 11:12:57,458 - main - INFO - Precision: 0.0000
2025-05-03 11:12:57,458 - main - INFO - Recall: 0.0000
2025-05-03 11:12:57,458 - main - INFO - F1 Score: 0.0000
2025-05-03 11:12:57,458 - main - INFO - IoU: 0.0000
2025-05-03 11:12:57,459 - main - INFO - mAP: 0.3076
2025-05-03 11:12:57,459 - main - INFO - AUC: 0.5700
2025-05-03 11:12:57,459 - main - INFO - Training time: 1685.52 seconds
2025-05-03 11:12:57,459 - main - INFO - 
==================================================
2025-05-03 11:12:57,459 - main - INFO - Running configuration 697/756:
2025-05-03 11:12:57,459 - main - INFO - Model: ConvNeXt
2025-05-03 11:12:57,459 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:12:57,459 - main - INFO - Scheduler: StepLR
2025-05-03 11:12:57,459 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:12:57,459 - main - INFO - ==================================================
2025-05-03 11:12:57,459 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_697 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_697
2025-05-03 11:12:57,459 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_697 - INFO - Config: {
  "id": 697,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:12:58,334 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:12:58,334 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 697,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:12:58,334 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-03 11:12:58,335 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 1/20
2025-05-03 11:13:42,863 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 11:14:36,481 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.4957
2025-05-03 11:14:36,666 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 1 completed in 98.33s - Train Loss: 0.7059, Train Acc: 0.4974, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 11:14:37,495 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 11:14:37,495 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 2/20
2025-05-03 11:15:23,206 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 11:16:16,651 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Validation accuracy improved from 0.4957 to 0.5043
2025-05-03 11:16:16,925 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 2 completed in 99.43s - Train Loss: 0.6942, Train Acc: 0.5000, Val Loss: 0.6933, Val Acc: 0.5043
2025-05-03 11:16:17,751 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 11:16:17,752 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 3/20
2025-05-03 11:17:03,968 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 11:17:57,521 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Validation loss improved from inf to 0.6931
2025-05-03 11:17:57,809 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 3 completed in 100.06s - Train Loss: 0.6940, Train Acc: 0.4816, Val Loss: 0.6931, Val Acc: 0.4957
2025-05-03 11:17:58,635 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 11:17:58,636 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 4/20
2025-05-03 11:18:45,254 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 11:19:39,629 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 4 completed in 100.99s - Train Loss: 0.6935, Train Acc: 0.5011, Val Loss: 0.6951, Val Acc: 0.4957
2025-05-03 11:19:40,400 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 11:19:40,400 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 5/20
2025-05-03 11:20:27,163 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 11:21:20,715 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 5 completed in 100.32s - Train Loss: 0.6941, Train Acc: 0.4994, Val Loss: 0.6941, Val Acc: 0.4957
2025-05-03 11:21:21,521 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 11:21:21,522 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 6/20
2025-05-03 11:22:07,805 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 11:23:01,654 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 6 completed in 100.13s - Train Loss: 0.6936, Train Acc: 0.5118, Val Loss: 0.6935, Val Acc: 0.4957
2025-05-03 11:23:02,475 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 11:23:02,475 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 7/20
2025-05-03 11:23:47,780 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 11:24:41,995 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 7 completed in 99.52s - Train Loss: 0.6937, Train Acc: 0.5148, Val Loss: 0.6947, Val Acc: 0.5043
2025-05-03 11:24:42,807 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 11:24:42,808 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 8/20
2025-05-03 11:25:28,337 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 11:26:21,838 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 8 completed in 99.03s - Train Loss: 0.6936, Train Acc: 0.4916, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 11:26:22,651 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 11:26:22,652 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Early stopping triggered after 8 epochs
2025-05-03 11:26:22,652 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 803.50 seconds
2025-05-03 11:26:22,655 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_697 - INFO - Starting model evaluation
2025-05-03 11:26:34,427 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_697 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2722
  AUC:       0.5047
2025-05-03 11:26:34,429 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_697 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_697/final_results.json
2025-05-03 11:26:34,433 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_697 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_697/final_results.json
2025-05-03 11:26:34,433 - main - INFO - 
Summary for configuration 697:
2025-05-03 11:26:34,433 - main - INFO - Accuracy: 0.2834
2025-05-03 11:26:34,433 - main - INFO - Precision: 0.2834
2025-05-03 11:26:34,433 - main - INFO - Recall: 1.0000
2025-05-03 11:26:34,433 - main - INFO - F1 Score: 0.4417
2025-05-03 11:26:34,433 - main - INFO - IoU: 0.2834
2025-05-03 11:26:34,433 - main - INFO - mAP: 0.2722
2025-05-03 11:26:34,433 - main - INFO - AUC: 0.5047
2025-05-03 11:26:34,433 - main - INFO - Training time: 803.50 seconds
2025-05-03 11:26:34,433 - main - INFO - 
==================================================
2025-05-03 11:26:34,433 - main - INFO - Running configuration 698/756:
2025-05-03 11:26:34,433 - main - INFO - Model: ConvNeXt
2025-05-03 11:26:34,433 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:26:34,433 - main - INFO - Scheduler: StepLR
2025-05-03 11:26:34,433 - main - INFO - Loss Function: FocalLoss
2025-05-03 11:26:34,433 - main - INFO - ==================================================
2025-05-03 11:26:34,433 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_698 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_698
2025-05-03 11:26:34,433 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_698 - INFO - Config: {
  "id": 698,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:26:35,400 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:26:35,400 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 698,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:26:35,400 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:26:35,843 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 8, best validation accuracy: 0.5043
2025-05-03 11:26:35,844 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 9/20
2025-05-03 11:27:21,561 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 11:28:16,795 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Validation loss improved from 0.6931 to 0.0433
2025-05-03 11:28:17,082 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 9 completed in 101.24s - Train Loss: 0.0434, Train Acc: 0.4865, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 11:28:17,915 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 11:28:17,916 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 10/20
2025-05-03 11:29:03,134 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 11:29:56,477 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 10 completed in 98.56s - Train Loss: 0.0433, Train Acc: 0.5043, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 11:29:57,328 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 11:29:57,329 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 11/20
2025-05-03 11:30:43,019 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 11:31:35,617 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 11 completed in 98.29s - Train Loss: 0.0433, Train Acc: 0.4959, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 11:31:36,430 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 11:31:36,431 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 12/20
2025-05-03 11:32:22,558 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 11:33:15,967 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 12 completed in 99.54s - Train Loss: 0.0433, Train Acc: 0.5028, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 11:33:16,795 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 11:33:16,796 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 11:34:02,488 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 11:34:55,599 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 13 completed in 98.80s - Train Loss: 0.0433, Train Acc: 0.5000, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 11:34:56,419 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 11:34:56,419 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 11:35:42,180 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 11:36:36,141 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 14 completed in 99.72s - Train Loss: 0.0433, Train Acc: 0.4972, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 11:36:36,957 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 11:36:36,957 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Early stopping triggered after 14 epochs
2025-05-03 11:36:36,957 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1403.80 seconds
2025-05-03 11:36:36,961 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_698 - INFO - Starting model evaluation
2025-05-03 11:36:48,774 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_698 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2603
  AUC:       0.4818
2025-05-03 11:36:48,778 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_698 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_698/final_results.json
2025-05-03 11:36:48,779 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_698 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_698/final_results.json
2025-05-03 11:36:48,779 - main - INFO - 
Summary for configuration 698:
2025-05-03 11:36:48,779 - main - INFO - Accuracy: 0.2834
2025-05-03 11:36:48,779 - main - INFO - Precision: 0.2834
2025-05-03 11:36:48,779 - main - INFO - Recall: 1.0000
2025-05-03 11:36:48,779 - main - INFO - F1 Score: 0.4417
2025-05-03 11:36:48,779 - main - INFO - IoU: 0.2834
2025-05-03 11:36:48,779 - main - INFO - mAP: 0.2603
2025-05-03 11:36:48,779 - main - INFO - AUC: 0.4818
2025-05-03 11:36:48,779 - main - INFO - Training time: 1403.80 seconds
2025-05-03 11:36:48,779 - main - INFO - 
==================================================
2025-05-03 11:36:48,779 - main - INFO - Running configuration 699/756:
2025-05-03 11:36:48,779 - main - INFO - Model: ConvNeXt
2025-05-03 11:36:48,779 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:36:48,779 - main - INFO - Scheduler: StepLR
2025-05-03 11:36:48,779 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:36:48,779 - main - INFO - ==================================================
2025-05-03 11:36:48,780 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_699 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_699
2025-05-03 11:36:48,780 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_699 - INFO - Config: {
  "id": 699,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:36:49,575 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:36:49,575 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 699,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:36:49,575 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:36:50,068 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 14, best validation accuracy: 0.5043
2025-05-03 11:36:50,069 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 11:37:36,492 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 11:38:29,763 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 15 completed in 99.69s - Train Loss: 0.6932, Train Acc: 0.4961, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 11:38:30,562 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 11:38:30,562 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Early stopping triggered after 15 epochs
2025-05-03 11:38:30,562 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1503.49 seconds
2025-05-03 11:38:30,565 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_699 - INFO - Starting model evaluation
2025-05-03 11:38:42,045 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_699 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2571
  AUC:       0.4680
2025-05-03 11:38:42,047 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_699 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_699/final_results.json
2025-05-03 11:38:42,049 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_699 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_699/final_results.json
2025-05-03 11:38:42,049 - main - INFO - 
Summary for configuration 699:
2025-05-03 11:38:42,049 - main - INFO - Accuracy: 0.2834
2025-05-03 11:38:42,049 - main - INFO - Precision: 0.2834
2025-05-03 11:38:42,049 - main - INFO - Recall: 1.0000
2025-05-03 11:38:42,049 - main - INFO - F1 Score: 0.4417
2025-05-03 11:38:42,049 - main - INFO - IoU: 0.2834
2025-05-03 11:38:42,049 - main - INFO - mAP: 0.2571
2025-05-03 11:38:42,049 - main - INFO - AUC: 0.4680
2025-05-03 11:38:42,049 - main - INFO - Training time: 1503.49 seconds
2025-05-03 11:38:42,049 - main - INFO - 
==================================================
2025-05-03 11:38:42,049 - main - INFO - Running configuration 700/756:
2025-05-03 11:38:42,049 - main - INFO - Model: ConvNeXt
2025-05-03 11:38:42,049 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:38:42,049 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 11:38:42,049 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:38:42,049 - main - INFO - ==================================================
2025-05-03 11:38:42,049 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_700 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_700
2025-05-03 11:38:42,049 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_700 - INFO - Config: {
  "id": 700,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:38:42,970 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:38:42,970 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 700,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:38:42,970 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:38:43,444 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 15, best validation accuracy: 0.5043
2025-05-03 11:38:43,446 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 11:38:43,752 - training.model_ConvNeXt_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.96 GiB is allocated by PyTorch, and 484.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:38:43,752 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-03 11:38:44,560 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 11:38:44,561 - main - ERROR - Error running configuration 700: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.96 GiB is allocated by PyTorch, and 484.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:38:44,561 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
                                ^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.96 GiB is allocated by PyTorch, and 484.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:38:44,563 - main - INFO - 
==================================================
2025-05-03 11:38:44,563 - main - INFO - Running configuration 701/756:
2025-05-03 11:38:44,563 - main - INFO - Model: ConvNeXt
2025-05-03 11:38:44,563 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:38:44,563 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 11:38:44,563 - main - INFO - Loss Function: FocalLoss
2025-05-03 11:38:44,563 - main - INFO - ==================================================
2025-05-03 11:38:44,563 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_701 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_701
2025-05-03 11:38:44,563 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_701 - INFO - Config: {
  "id": 701,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:38:45,290 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:38:45,290 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 701,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:38:45,290 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:38:45,889 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 16, best validation accuracy: 0.5043
2025-05-03 11:38:45,890 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 11:38:46,092 - training.model_ConvNeXt_opt_Adam_lr_0.001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 9.76 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.07 GiB is allocated by PyTorch, and 392.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:38:46,092 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Saving checkpoint before exit
2025-05-03 11:38:46,872 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 11:38:46,872 - main - ERROR - Error running configuration 701: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 9.76 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.07 GiB is allocated by PyTorch, and 392.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:38:46,872 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
             ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 2.62 MiB is free. Including non-PyTorch memory, this process has 9.76 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.07 GiB is allocated by PyTorch, and 392.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:38:46,873 - main - INFO - 
==================================================
2025-05-03 11:38:46,873 - main - INFO - Running configuration 702/756:
2025-05-03 11:38:46,873 - main - INFO - Model: ConvNeXt
2025-05-03 11:38:46,873 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:38:46,873 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 11:38:46,873 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:38:46,873 - main - INFO - ==================================================
2025-05-03 11:38:46,873 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_702 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_702
2025-05-03 11:38:46,874 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_702 - INFO - Config: {
  "id": 702,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:38:47,820 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:38:47,820 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 702,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:38:47,820 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:38:48,280 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 17, best validation accuracy: 0.5043
2025-05-03 11:38:48,281 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 11:39:33,949 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 11:40:28,507 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 18 completed in 100.23s - Train Loss: 0.6932, Train Acc: 0.4940, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 11:40:29,304 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 11:40:29,304 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Early stopping triggered after 18 epochs
2025-05-03 11:40:29,304 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1604.23 seconds
2025-05-03 11:40:29,309 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_702 - INFO - Starting model evaluation
2025-05-03 11:40:41,977 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_702 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2590
  AUC:       0.4638
2025-05-03 11:40:41,979 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_702 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_702/final_results.json
2025-05-03 11:40:41,980 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_702 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_702/final_results.json
2025-05-03 11:40:41,980 - main - INFO - 
Summary for configuration 702:
2025-05-03 11:40:41,980 - main - INFO - Accuracy: 0.2834
2025-05-03 11:40:41,980 - main - INFO - Precision: 0.2834
2025-05-03 11:40:41,980 - main - INFO - Recall: 1.0000
2025-05-03 11:40:41,980 - main - INFO - F1 Score: 0.4417
2025-05-03 11:40:41,980 - main - INFO - IoU: 0.2834
2025-05-03 11:40:41,980 - main - INFO - mAP: 0.2590
2025-05-03 11:40:41,980 - main - INFO - AUC: 0.4638
2025-05-03 11:40:41,980 - main - INFO - Training time: 1604.23 seconds
2025-05-03 11:40:41,980 - main - INFO - 
==================================================
2025-05-03 11:40:41,980 - main - INFO - Running configuration 703/756:
2025-05-03 11:40:41,980 - main - INFO - Model: ConvNeXt
2025-05-03 11:40:41,980 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:40:41,980 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 11:40:41,980 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:40:41,980 - main - INFO - ==================================================
2025-05-03 11:40:41,981 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_703 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_703
2025-05-03 11:40:41,981 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_703 - INFO - Config: {
  "id": 703,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:40:42,919 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:40:42,919 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 703,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:40:42,919 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:40:43,435 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 18, best validation accuracy: 0.5043
2025-05-03 11:40:43,436 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 11:41:29,678 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 11:42:24,275 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 19 completed in 100.84s - Train Loss: 0.6932, Train Acc: 0.4903, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 11:42:25,048 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 11:42:25,048 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Early stopping triggered after 19 epochs
2025-05-03 11:42:25,048 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1705.07 seconds
2025-05-03 11:42:25,052 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_703 - INFO - Starting model evaluation
2025-05-03 11:42:36,777 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_703 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2771
  AUC:       0.5158
2025-05-03 11:42:36,779 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_703 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_703/final_results.json
2025-05-03 11:42:36,780 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_703 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_703/final_results.json
2025-05-03 11:42:36,780 - main - INFO - 
Summary for configuration 703:
2025-05-03 11:42:36,780 - main - INFO - Accuracy: 0.2834
2025-05-03 11:42:36,780 - main - INFO - Precision: 0.2834
2025-05-03 11:42:36,780 - main - INFO - Recall: 1.0000
2025-05-03 11:42:36,780 - main - INFO - F1 Score: 0.4417
2025-05-03 11:42:36,780 - main - INFO - IoU: 0.2834
2025-05-03 11:42:36,780 - main - INFO - mAP: 0.2771
2025-05-03 11:42:36,780 - main - INFO - AUC: 0.5158
2025-05-03 11:42:36,780 - main - INFO - Training time: 1705.07 seconds
2025-05-03 11:42:36,780 - main - INFO - 
==================================================
2025-05-03 11:42:36,780 - main - INFO - Running configuration 704/756:
2025-05-03 11:42:36,780 - main - INFO - Model: ConvNeXt
2025-05-03 11:42:36,781 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:42:36,781 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 11:42:36,781 - main - INFO - Loss Function: FocalLoss
2025-05-03 11:42:36,781 - main - INFO - ==================================================
2025-05-03 11:42:36,781 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_704 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_704
2025-05-03 11:42:36,781 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_704 - INFO - Config: {
  "id": 704,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:42:37,611 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:42:37,611 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 704,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:42:37,611 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:42:38,127 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 19, best validation accuracy: 0.5043
2025-05-03 11:42:38,128 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 11:43:23,173 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 11:44:16,622 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Epoch 20 completed in 98.49s - Train Loss: 0.0433, Train Acc: 0.5032, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 11:44:17,425 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 11:44:17,426 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Early stopping triggered after 20 epochs
2025-05-03 11:44:17,426 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1803.56 seconds
2025-05-03 11:44:17,429 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_704 - INFO - Starting model evaluation
2025-05-03 11:44:29,119 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_704 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2880
  AUC:       0.5322
2025-05-03 11:44:29,121 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_704 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_704/final_results.json
2025-05-03 11:44:29,122 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_704 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_704/final_results.json
2025-05-03 11:44:29,122 - main - INFO - 
Summary for configuration 704:
2025-05-03 11:44:29,122 - main - INFO - Accuracy: 0.2834
2025-05-03 11:44:29,123 - main - INFO - Precision: 0.2834
2025-05-03 11:44:29,123 - main - INFO - Recall: 1.0000
2025-05-03 11:44:29,123 - main - INFO - F1 Score: 0.4417
2025-05-03 11:44:29,123 - main - INFO - IoU: 0.2834
2025-05-03 11:44:29,123 - main - INFO - mAP: 0.2880
2025-05-03 11:44:29,123 - main - INFO - AUC: 0.5322
2025-05-03 11:44:29,123 - main - INFO - Training time: 1803.56 seconds
2025-05-03 11:44:29,123 - main - INFO - 
==================================================
2025-05-03 11:44:29,123 - main - INFO - Running configuration 705/756:
2025-05-03 11:44:29,123 - main - INFO - Model: ConvNeXt
2025-05-03 11:44:29,123 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:44:29,123 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 11:44:29,123 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:44:29,123 - main - INFO - ==================================================
2025-05-03 11:44:29,123 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_705 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_705
2025-05-03 11:44:29,123 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_705 - INFO - Config: {
  "id": 705,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:44:29,908 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:44:29,908 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 705,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:44:29,909 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:44:30,432 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 11:44:30,433 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1803.56 seconds
2025-05-03 11:44:30,436 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_705 - INFO - Starting model evaluation
2025-05-03 11:44:43,005 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_705 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2924
  AUC:       0.5471
2025-05-03 11:44:43,007 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_705 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_705/final_results.json
2025-05-03 11:44:43,009 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_705 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_705/final_results.json
2025-05-03 11:44:43,009 - main - INFO - 
Summary for configuration 705:
2025-05-03 11:44:43,009 - main - INFO - Accuracy: 0.2834
2025-05-03 11:44:43,009 - main - INFO - Precision: 0.2834
2025-05-03 11:44:43,009 - main - INFO - Recall: 1.0000
2025-05-03 11:44:43,009 - main - INFO - F1 Score: 0.4417
2025-05-03 11:44:43,009 - main - INFO - IoU: 0.2834
2025-05-03 11:44:43,009 - main - INFO - mAP: 0.2924
2025-05-03 11:44:43,009 - main - INFO - AUC: 0.5471
2025-05-03 11:44:43,009 - main - INFO - Training time: 1803.56 seconds
2025-05-03 11:44:43,009 - main - INFO - 
==================================================
2025-05-03 11:44:43,009 - main - INFO - Running configuration 706/756:
2025-05-03 11:44:43,009 - main - INFO - Model: ConvNeXt
2025-05-03 11:44:43,009 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:44:43,009 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 11:44:43,009 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:44:43,009 - main - INFO - ==================================================
2025-05-03 11:44:43,009 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_706 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_706
2025-05-03 11:44:43,009 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_706 - INFO - Config: {
  "id": 706,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:44:43,774 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:44:43,774 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 706,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:44:43,775 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:44:44,309 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 11:44:44,311 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1803.56 seconds
2025-05-03 11:44:44,313 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_706 - INFO - Starting model evaluation
2025-05-03 11:44:56,612 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_706 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2748
  AUC:       0.5151
2025-05-03 11:44:56,616 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_706 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_706/final_results.json
2025-05-03 11:44:56,617 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_706 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_706/final_results.json
2025-05-03 11:44:56,617 - main - INFO - 
Summary for configuration 706:
2025-05-03 11:44:56,617 - main - INFO - Accuracy: 0.2834
2025-05-03 11:44:56,617 - main - INFO - Precision: 0.2834
2025-05-03 11:44:56,617 - main - INFO - Recall: 1.0000
2025-05-03 11:44:56,617 - main - INFO - F1 Score: 0.4417
2025-05-03 11:44:56,617 - main - INFO - IoU: 0.2834
2025-05-03 11:44:56,617 - main - INFO - mAP: 0.2748
2025-05-03 11:44:56,617 - main - INFO - AUC: 0.5151
2025-05-03 11:44:56,617 - main - INFO - Training time: 1803.56 seconds
2025-05-03 11:44:56,617 - main - INFO - 
==================================================
2025-05-03 11:44:56,617 - main - INFO - Running configuration 707/756:
2025-05-03 11:44:56,617 - main - INFO - Model: ConvNeXt
2025-05-03 11:44:56,617 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:44:56,617 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 11:44:56,617 - main - INFO - Loss Function: FocalLoss
2025-05-03 11:44:56,617 - main - INFO - ==================================================
2025-05-03 11:44:56,617 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_707 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_707
2025-05-03 11:44:56,617 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_707 - INFO - Config: {
  "id": 707,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:44:57,567 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:44:57,568 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 707,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:44:57,568 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:44:58,091 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 11:44:58,093 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1803.56 seconds
2025-05-03 11:44:58,096 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_707 - INFO - Starting model evaluation
2025-05-03 11:45:10,725 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_707 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2907
  AUC:       0.5320
2025-05-03 11:45:10,726 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_707 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_707/final_results.json
2025-05-03 11:45:10,728 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_707 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_707/final_results.json
2025-05-03 11:45:10,728 - main - INFO - 
Summary for configuration 707:
2025-05-03 11:45:10,728 - main - INFO - Accuracy: 0.2834
2025-05-03 11:45:10,728 - main - INFO - Precision: 0.2834
2025-05-03 11:45:10,728 - main - INFO - Recall: 1.0000
2025-05-03 11:45:10,728 - main - INFO - F1 Score: 0.4417
2025-05-03 11:45:10,728 - main - INFO - IoU: 0.2834
2025-05-03 11:45:10,728 - main - INFO - mAP: 0.2907
2025-05-03 11:45:10,728 - main - INFO - AUC: 0.5320
2025-05-03 11:45:10,728 - main - INFO - Training time: 1803.56 seconds
2025-05-03 11:45:10,728 - main - INFO - 
==================================================
2025-05-03 11:45:10,728 - main - INFO - Running configuration 708/756:
2025-05-03 11:45:10,728 - main - INFO - Model: ConvNeXt
2025-05-03 11:45:10,728 - main - INFO - Optimizer: Adam, Learning Rate: 0.001
2025-05-03 11:45:10,728 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 11:45:10,728 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:45:10,728 - main - INFO - ==================================================
2025-05-03 11:45:10,728 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_708 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001_id_708
2025-05-03 11:45:10,728 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_708 - INFO - Config: {
  "id": 708,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:45:11,604 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.001
2025-05-03 11:45:11,604 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Config: {
  "id": 708,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:45:11,604 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001/checkpoint.pt
2025-05-03 11:45:12,136 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 11:45:12,137 - training.model_ConvNeXt_opt_Adam_lr_0.001 - INFO - Training completed after 1803.56 seconds
2025-05-03 11:45:12,140 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_708 - INFO - Starting model evaluation
2025-05-03 11:45:25,402 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_708 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2915
  AUC:       0.5332
2025-05-03 11:45:25,404 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_708 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_708/final_results.json
2025-05-03 11:45:25,406 - training.model_ConvNeXt_opt_Adam_lr_0.001_id_708 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.001_id_708/final_results.json
2025-05-03 11:45:25,406 - main - INFO - 
Summary for configuration 708:
2025-05-03 11:45:25,406 - main - INFO - Accuracy: 0.2834
2025-05-03 11:45:25,406 - main - INFO - Precision: 0.2834
2025-05-03 11:45:25,406 - main - INFO - Recall: 1.0000
2025-05-03 11:45:25,406 - main - INFO - F1 Score: 0.4417
2025-05-03 11:45:25,406 - main - INFO - IoU: 0.2834
2025-05-03 11:45:25,406 - main - INFO - mAP: 0.2915
2025-05-03 11:45:25,406 - main - INFO - AUC: 0.5332
2025-05-03 11:45:25,406 - main - INFO - Training time: 1803.56 seconds
2025-05-03 11:45:25,406 - main - INFO - 
==================================================
2025-05-03 11:45:25,406 - main - INFO - Running configuration 709/756:
2025-05-03 11:45:25,406 - main - INFO - Model: ConvNeXt
2025-05-03 11:45:25,406 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 11:45:25,406 - main - INFO - Scheduler: StepLR
2025-05-03 11:45:25,406 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:45:25,406 - main - INFO - ==================================================
2025-05-03 11:45:25,407 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_709 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_709
2025-05-03 11:45:25,407 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_709 - INFO - Config: {
  "id": 709,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:45:26,214 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 11:45:26,214 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 709,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:45:26,214 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-03 11:45:26,215 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-03 11:45:26,450 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.78 GiB is allocated by PyTorch, and 674.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:26,450 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-03 11:45:26,632 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 11:45:26,632 - main - ERROR - Error running configuration 709: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.78 GiB is allocated by PyTorch, and 674.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:26,632 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
                                ^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.78 GiB is allocated by PyTorch, and 674.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:26,634 - main - INFO - 
==================================================
2025-05-03 11:45:26,634 - main - INFO - Running configuration 710/756:
2025-05-03 11:45:26,634 - main - INFO - Model: ConvNeXt
2025-05-03 11:45:26,634 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 11:45:26,634 - main - INFO - Scheduler: StepLR
2025-05-03 11:45:26,634 - main - INFO - Loss Function: FocalLoss
2025-05-03 11:45:26,634 - main - INFO - ==================================================
2025-05-03 11:45:26,634 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_710 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_710
2025-05-03 11:45:26,634 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_710 - INFO - Config: {
  "id": 710,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:45:27,288 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 11:45:27,288 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 710,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 11:45:27,288 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 11:45:27,560 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 1, best validation accuracy: 0.0000
2025-05-03 11:45:27,562 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-03 11:45:27,846 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 691.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:27,847 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-03 11:45:28,170 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 11:45:28,170 - main - ERROR - Error running configuration 710: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 691.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:28,171 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
                                ^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 18.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 691.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:28,172 - main - INFO - 
==================================================
2025-05-03 11:45:28,172 - main - INFO - Running configuration 711/756:
2025-05-03 11:45:28,172 - main - INFO - Model: ConvNeXt
2025-05-03 11:45:28,172 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 11:45:28,172 - main - INFO - Scheduler: StepLR
2025-05-03 11:45:28,172 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 11:45:28,172 - main - INFO - ==================================================
2025-05-03 11:45:28,172 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_711 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_711
2025-05-03 11:45:28,173 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_711 - INFO - Config: {
  "id": 711,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:45:29,063 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 11:45:29,063 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 711,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 11:45:29,063 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 11:45:29,163 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 2, best validation accuracy: 0.0000
2025-05-03 11:45:29,164 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-03 11:45:29,523 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.98 GiB is allocated by PyTorch, and 472.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:29,524 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Saving checkpoint before exit
2025-05-03 11:45:29,796 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 11:45:29,796 - main - ERROR - Error running configuration 711: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.98 GiB is allocated by PyTorch, and 472.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:29,796 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
                                ^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.98 GiB is allocated by PyTorch, and 472.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 11:45:29,798 - main - INFO - 
==================================================
2025-05-03 11:45:29,798 - main - INFO - Running configuration 712/756:
2025-05-03 11:45:29,798 - main - INFO - Model: ConvNeXt
2025-05-03 11:45:29,798 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 11:45:29,798 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 11:45:29,798 - main - INFO - Loss Function: CrossEntropy
2025-05-03 11:45:29,798 - main - INFO - ==================================================
2025-05-03 11:45:29,798 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_712 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_712
2025-05-03 11:45:29,798 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_712 - INFO - Config: {
  "id": 712,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:45:30,558 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 11:45:30,559 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 712,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 11:45:30,559 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 11:45:30,659 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 3, best validation accuracy: 0.0000
2025-05-03 11:45:30,660 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-03 11:46:17,083 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 11:47:12,681 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9503
2025-05-03 11:47:12,870 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 4 completed in 102.21s - Train Loss: 0.4197, Train Acc: 0.8934, Val Loss: 0.3591, Val Acc: 0.9503
2025-05-03 11:47:13,670 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 11:47:13,670 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 11:48:00,001 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 11:48:55,619 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9503 to 0.9605
2025-05-03 11:48:55,908 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 5 completed in 102.24s - Train Loss: 0.3578, Train Acc: 0.9571, Val Loss: 0.3521, Val Acc: 0.9605
2025-05-03 11:48:56,726 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 11:48:56,726 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 11:49:43,019 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 11:50:36,459 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9605 to 0.9674
2025-05-03 11:50:36,737 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 6 completed in 100.01s - Train Loss: 0.3504, Train Acc: 0.9612, Val Loss: 0.3453, Val Acc: 0.9674
2025-05-03 11:50:37,563 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 11:50:37,563 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 11:51:23,172 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 11:52:16,964 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation loss improved from inf to 0.3490
2025-05-03 11:52:17,252 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 7 completed in 99.69s - Train Loss: 0.3496, Train Acc: 0.9625, Val Loss: 0.3490, Val Acc: 0.9631
2025-05-03 11:52:18,034 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 11:52:18,035 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 11:53:02,056 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 11:53:54,233 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 8 completed in 96.20s - Train Loss: 0.3433, Train Acc: 0.9683, Val Loss: 0.3645, Val Acc: 0.9477
2025-05-03 11:53:55,064 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 11:53:55,064 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 11:54:41,493 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 11:55:35,486 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 9 completed in 100.42s - Train Loss: 0.3385, Train Acc: 0.9740, Val Loss: 0.3534, Val Acc: 0.9571
2025-05-03 11:55:36,322 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 11:55:36,322 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 11:56:21,654 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 11:57:15,889 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 10 completed in 99.57s - Train Loss: 0.3446, Train Acc: 0.9685, Val Loss: 0.3583, Val Acc: 0.9545
2025-05-03 11:57:16,723 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 11:57:16,723 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 11:58:02,535 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 11:58:55,977 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 11 completed in 99.25s - Train Loss: 0.3414, Train Acc: 0.9710, Val Loss: 0.3543, Val Acc: 0.9571
2025-05-03 11:58:56,758 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 11:58:56,758 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 11:59:42,361 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 12:00:35,882 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 12 completed in 99.12s - Train Loss: 0.3408, Train Acc: 0.9715, Val Loss: 0.3579, Val Acc: 0.9545
2025-05-03 12:00:36,670 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 12:00:36,671 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Early stopping triggered after 12 epochs
2025-05-03 12:00:36,671 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 906.10 seconds
2025-05-03 12:00:36,674 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_712 - INFO - Starting model evaluation
2025-05-03 12:00:48,458 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_712 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9395
  Recall:    0.9231
  F1 Score:  0.9312
  IoU:       0.8713
  mAP:       0.9770
  AUC:       0.9883
2025-05-03 12:00:48,459 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_712 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_712/final_results.json
2025-05-03 12:00:48,461 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_712 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_712/final_results.json
2025-05-03 12:00:48,461 - main - INFO - 
Summary for configuration 712:
2025-05-03 12:00:48,461 - main - INFO - Accuracy: 0.9613
2025-05-03 12:00:48,461 - main - INFO - Precision: 0.9395
2025-05-03 12:00:48,461 - main - INFO - Recall: 0.9231
2025-05-03 12:00:48,461 - main - INFO - F1 Score: 0.9312
2025-05-03 12:00:48,461 - main - INFO - IoU: 0.8713
2025-05-03 12:00:48,461 - main - INFO - mAP: 0.9770
2025-05-03 12:00:48,461 - main - INFO - AUC: 0.9883
2025-05-03 12:00:48,461 - main - INFO - Training time: 906.10 seconds
2025-05-03 12:00:48,461 - main - INFO - 
==================================================
2025-05-03 12:00:48,461 - main - INFO - Running configuration 713/756:
2025-05-03 12:00:48,461 - main - INFO - Model: ConvNeXt
2025-05-03 12:00:48,461 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 12:00:48,461 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 12:00:48,461 - main - INFO - Loss Function: FocalLoss
2025-05-03 12:00:48,461 - main - INFO - ==================================================
2025-05-03 12:00:48,462 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_713 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_713
2025-05-03 12:00:48,462 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_713 - INFO - Config: {
  "id": 713,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:00:49,208 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 12:00:49,208 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 713,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:00:49,208 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 12:00:49,735 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 12, best validation accuracy: 0.9674
2025-05-03 12:00:49,736 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 12:01:35,625 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 12:02:31,055 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9674 to 0.9708
2025-05-03 12:02:31,353 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 13 completed in 101.62s - Train Loss: 0.0093, Train Acc: 0.9768, Val Loss: 0.0102, Val Acc: 0.9708
2025-05-03 12:02:32,140 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 12:02:32,140 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 12:03:17,773 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 12:04:11,160 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9708 to 0.9811
2025-05-03 12:04:11,451 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 14 completed in 99.31s - Train Loss: 0.0086, Train Acc: 0.9809, Val Loss: 0.0088, Val Acc: 0.9811
2025-05-03 12:04:12,237 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 12:04:12,238 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 12:04:57,699 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 12:05:50,689 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.3490 to 0.0087
2025-05-03 12:05:50,971 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 15 completed in 98.73s - Train Loss: 0.0076, Train Acc: 0.9856, Val Loss: 0.0087, Val Acc: 0.9803
2025-05-03 12:05:51,764 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 12:05:51,764 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 12:06:37,871 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 12:07:31,942 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9811 to 0.9828
2025-05-03 12:07:32,226 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 16 completed in 100.46s - Train Loss: 0.0073, Train Acc: 0.9893, Val Loss: 0.0083, Val Acc: 0.9828
2025-05-03 12:07:33,061 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 12:07:33,062 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 12:08:18,268 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 12:09:13,490 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation accuracy improved from 0.9828 to 0.9854
2025-05-03 12:09:13,783 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 17 completed in 100.72s - Train Loss: 0.0074, Train Acc: 0.9886, Val Loss: 0.0077, Val Acc: 0.9854
2025-05-03 12:09:14,610 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 12:09:14,611 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 12:09:59,919 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 12:10:54,679 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.0087 to 0.0085
2025-05-03 12:10:54,959 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 18 completed in 100.35s - Train Loss: 0.0070, Train Acc: 0.9912, Val Loss: 0.0085, Val Acc: 0.9820
2025-05-03 12:10:55,775 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 12:10:55,775 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 12:11:40,444 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 12:12:34,702 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.0085 to 0.0082
2025-05-03 12:12:34,987 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 19 completed in 99.21s - Train Loss: 0.0068, Train Acc: 0.9927, Val Loss: 0.0082, Val Acc: 0.9828
2025-05-03 12:12:35,758 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 12:12:35,758 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 12:13:21,099 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 12:14:15,877 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Validation loss improved from 0.0082 to 0.0079
2025-05-03 12:14:16,141 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Epoch 20 completed in 100.38s - Train Loss: 0.0065, Train Acc: 0.9944, Val Loss: 0.0079, Val Acc: 0.9854
2025-05-03 12:14:16,933 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 12:14:16,934 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 1712.51 seconds
2025-05-03 12:14:16,937 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_713 - INFO - Starting model evaluation
2025-05-03 12:14:28,401 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_713 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9252
  Recall:    0.9510
  F1 Score:  0.9379
  IoU:       0.8831
  mAP:       0.9803
  AUC:       0.9922
2025-05-03 12:14:28,403 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_713 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_713/final_results.json
2025-05-03 12:14:28,404 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_713 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_713/final_results.json
2025-05-03 12:14:28,405 - main - INFO - 
Summary for configuration 713:
2025-05-03 12:14:28,405 - main - INFO - Accuracy: 0.9643
2025-05-03 12:14:28,405 - main - INFO - Precision: 0.9252
2025-05-03 12:14:28,405 - main - INFO - Recall: 0.9510
2025-05-03 12:14:28,405 - main - INFO - F1 Score: 0.9379
2025-05-03 12:14:28,405 - main - INFO - IoU: 0.8831
2025-05-03 12:14:28,405 - main - INFO - mAP: 0.9803
2025-05-03 12:14:28,405 - main - INFO - AUC: 0.9922
2025-05-03 12:14:28,405 - main - INFO - Training time: 1712.51 seconds
2025-05-03 12:14:28,405 - main - INFO - 
==================================================
2025-05-03 12:14:28,405 - main - INFO - Running configuration 714/756:
2025-05-03 12:14:28,405 - main - INFO - Model: ConvNeXt
2025-05-03 12:14:28,405 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 12:14:28,405 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 12:14:28,405 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 12:14:28,405 - main - INFO - ==================================================
2025-05-03 12:14:28,405 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_714 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_714
2025-05-03 12:14:28,405 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_714 - INFO - Config: {
  "id": 714,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:14:29,102 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 12:14:29,102 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 714,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:14:29,102 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 12:14:29,627 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 12:14:29,628 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 1712.51 seconds
2025-05-03 12:14:29,631 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_714 - INFO - Starting model evaluation
2025-05-03 12:14:41,043 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_714 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9223
  Recall:    0.9545
  F1 Score:  0.9381
  IoU:       0.8835
  mAP:       0.9809
  AUC:       0.9912
2025-05-03 12:14:41,045 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_714 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_714/final_results.json
2025-05-03 12:14:41,046 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_714 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_714/final_results.json
2025-05-03 12:14:41,046 - main - INFO - 
Summary for configuration 714:
2025-05-03 12:14:41,046 - main - INFO - Accuracy: 0.9643
2025-05-03 12:14:41,046 - main - INFO - Precision: 0.9223
2025-05-03 12:14:41,046 - main - INFO - Recall: 0.9545
2025-05-03 12:14:41,046 - main - INFO - F1 Score: 0.9381
2025-05-03 12:14:41,046 - main - INFO - IoU: 0.8835
2025-05-03 12:14:41,046 - main - INFO - mAP: 0.9809
2025-05-03 12:14:41,046 - main - INFO - AUC: 0.9912
2025-05-03 12:14:41,046 - main - INFO - Training time: 1712.51 seconds
2025-05-03 12:14:41,046 - main - INFO - 
==================================================
2025-05-03 12:14:41,046 - main - INFO - Running configuration 715/756:
2025-05-03 12:14:41,046 - main - INFO - Model: ConvNeXt
2025-05-03 12:14:41,046 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 12:14:41,046 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 12:14:41,046 - main - INFO - Loss Function: CrossEntropy
2025-05-03 12:14:41,046 - main - INFO - ==================================================
2025-05-03 12:14:41,047 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_715 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_715
2025-05-03 12:14:41,047 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_715 - INFO - Config: {
  "id": 715,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:14:41,713 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 12:14:41,713 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 715,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:14:41,713 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 12:14:42,191 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 12:14:42,193 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 1712.51 seconds
2025-05-03 12:14:42,195 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_715 - INFO - Starting model evaluation
2025-05-03 12:14:53,801 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_715 - INFO - Evaluation metrics:
  Accuracy:  0.9623
  Precision: 0.9189
  Recall:    0.9510
  F1 Score:  0.9347
  IoU:       0.8774
  mAP:       0.9804
  AUC:       0.9913
2025-05-03 12:14:53,802 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_715 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_715/final_results.json
2025-05-03 12:14:53,804 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_715 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_715/final_results.json
2025-05-03 12:14:53,804 - main - INFO - 
Summary for configuration 715:
2025-05-03 12:14:53,804 - main - INFO - Accuracy: 0.9623
2025-05-03 12:14:53,804 - main - INFO - Precision: 0.9189
2025-05-03 12:14:53,804 - main - INFO - Recall: 0.9510
2025-05-03 12:14:53,804 - main - INFO - F1 Score: 0.9347
2025-05-03 12:14:53,804 - main - INFO - IoU: 0.8774
2025-05-03 12:14:53,804 - main - INFO - mAP: 0.9804
2025-05-03 12:14:53,804 - main - INFO - AUC: 0.9913
2025-05-03 12:14:53,804 - main - INFO - Training time: 1712.51 seconds
2025-05-03 12:14:53,804 - main - INFO - 
==================================================
2025-05-03 12:14:53,804 - main - INFO - Running configuration 716/756:
2025-05-03 12:14:53,804 - main - INFO - Model: ConvNeXt
2025-05-03 12:14:53,805 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 12:14:53,805 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 12:14:53,805 - main - INFO - Loss Function: FocalLoss
2025-05-03 12:14:53,805 - main - INFO - ==================================================
2025-05-03 12:14:53,805 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_716 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_716
2025-05-03 12:14:53,805 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_716 - INFO - Config: {
  "id": 716,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:14:54,526 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 12:14:54,526 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 716,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:14:54,526 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 12:14:55,054 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 12:14:55,055 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 1712.51 seconds
2025-05-03 12:14:55,058 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_716 - INFO - Starting model evaluation
2025-05-03 12:15:06,506 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_716 - INFO - Evaluation metrics:
  Accuracy:  0.9653
  Precision: 0.9254
  Recall:    0.9545
  F1 Score:  0.9398
  IoU:       0.8864
  mAP:       0.9812
  AUC:       0.9921
2025-05-03 12:15:06,508 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_716 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_716/final_results.json
2025-05-03 12:15:06,510 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_716 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_716/final_results.json
2025-05-03 12:15:06,510 - main - INFO - 
Summary for configuration 716:
2025-05-03 12:15:06,510 - main - INFO - Accuracy: 0.9653
2025-05-03 12:15:06,510 - main - INFO - Precision: 0.9254
2025-05-03 12:15:06,510 - main - INFO - Recall: 0.9545
2025-05-03 12:15:06,510 - main - INFO - F1 Score: 0.9398
2025-05-03 12:15:06,510 - main - INFO - IoU: 0.8864
2025-05-03 12:15:06,510 - main - INFO - mAP: 0.9812
2025-05-03 12:15:06,510 - main - INFO - AUC: 0.9921
2025-05-03 12:15:06,510 - main - INFO - Training time: 1712.51 seconds
2025-05-03 12:15:06,510 - main - INFO - 
==================================================
2025-05-03 12:15:06,510 - main - INFO - Running configuration 717/756:
2025-05-03 12:15:06,510 - main - INFO - Model: ConvNeXt
2025-05-03 12:15:06,510 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 12:15:06,510 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 12:15:06,510 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 12:15:06,510 - main - INFO - ==================================================
2025-05-03 12:15:06,510 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_717 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_717
2025-05-03 12:15:06,510 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_717 - INFO - Config: {
  "id": 717,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:15:07,298 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 12:15:07,298 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 717,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:15:07,298 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 12:15:07,815 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 12:15:07,816 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 1712.51 seconds
2025-05-03 12:15:07,819 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_717 - INFO - Starting model evaluation
2025-05-03 12:15:19,499 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_717 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9281
  Recall:    0.9476
  F1 Score:  0.9377
  IoU:       0.8827
  mAP:       0.9818
  AUC:       0.9922
2025-05-03 12:15:19,501 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_717 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_717/final_results.json
2025-05-03 12:15:19,503 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_717 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_717/final_results.json
2025-05-03 12:15:19,503 - main - INFO - 
Summary for configuration 717:
2025-05-03 12:15:19,503 - main - INFO - Accuracy: 0.9643
2025-05-03 12:15:19,503 - main - INFO - Precision: 0.9281
2025-05-03 12:15:19,503 - main - INFO - Recall: 0.9476
2025-05-03 12:15:19,503 - main - INFO - F1 Score: 0.9377
2025-05-03 12:15:19,503 - main - INFO - IoU: 0.8827
2025-05-03 12:15:19,503 - main - INFO - mAP: 0.9818
2025-05-03 12:15:19,503 - main - INFO - AUC: 0.9922
2025-05-03 12:15:19,503 - main - INFO - Training time: 1712.51 seconds
2025-05-03 12:15:19,503 - main - INFO - 
==================================================
2025-05-03 12:15:19,503 - main - INFO - Running configuration 718/756:
2025-05-03 12:15:19,503 - main - INFO - Model: ConvNeXt
2025-05-03 12:15:19,503 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 12:15:19,503 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 12:15:19,503 - main - INFO - Loss Function: CrossEntropy
2025-05-03 12:15:19,503 - main - INFO - ==================================================
2025-05-03 12:15:19,503 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_718 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_718
2025-05-03 12:15:19,503 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_718 - INFO - Config: {
  "id": 718,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:15:20,293 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 12:15:20,293 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 718,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:15:20,293 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 12:15:20,796 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 12:15:20,798 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 1712.51 seconds
2025-05-03 12:15:20,800 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_718 - INFO - Starting model evaluation
2025-05-03 12:15:32,438 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_718 - INFO - Evaluation metrics:
  Accuracy:  0.9693
  Precision: 0.9381
  Recall:    0.9545
  F1 Score:  0.9463
  IoU:       0.8980
  mAP:       0.9822
  AUC:       0.9927
2025-05-03 12:15:32,440 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_718 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_718/final_results.json
2025-05-03 12:15:32,442 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_718 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_718/final_results.json
2025-05-03 12:15:32,442 - main - INFO - 
Summary for configuration 718:
2025-05-03 12:15:32,442 - main - INFO - Accuracy: 0.9693
2025-05-03 12:15:32,442 - main - INFO - Precision: 0.9381
2025-05-03 12:15:32,442 - main - INFO - Recall: 0.9545
2025-05-03 12:15:32,442 - main - INFO - F1 Score: 0.9463
2025-05-03 12:15:32,442 - main - INFO - IoU: 0.8980
2025-05-03 12:15:32,442 - main - INFO - mAP: 0.9822
2025-05-03 12:15:32,442 - main - INFO - AUC: 0.9927
2025-05-03 12:15:32,442 - main - INFO - Training time: 1712.51 seconds
2025-05-03 12:15:32,442 - main - INFO - 
==================================================
2025-05-03 12:15:32,442 - main - INFO - Running configuration 719/756:
2025-05-03 12:15:32,442 - main - INFO - Model: ConvNeXt
2025-05-03 12:15:32,442 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 12:15:32,442 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 12:15:32,442 - main - INFO - Loss Function: FocalLoss
2025-05-03 12:15:32,442 - main - INFO - ==================================================
2025-05-03 12:15:32,443 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_719 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_719
2025-05-03 12:15:32,443 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_719 - INFO - Config: {
  "id": 719,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:15:33,264 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 12:15:33,264 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 719,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:15:33,264 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 12:15:33,749 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 12:15:33,751 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 1712.51 seconds
2025-05-03 12:15:33,753 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_719 - INFO - Starting model evaluation
2025-05-03 12:15:45,460 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_719 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9223
  Recall:    0.9545
  F1 Score:  0.9381
  IoU:       0.8835
  mAP:       0.9866
  AUC:       0.9938
2025-05-03 12:15:45,462 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_719 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_719/final_results.json
2025-05-03 12:15:45,463 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_719 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_719/final_results.json
2025-05-03 12:15:45,463 - main - INFO - 
Summary for configuration 719:
2025-05-03 12:15:45,463 - main - INFO - Accuracy: 0.9643
2025-05-03 12:15:45,463 - main - INFO - Precision: 0.9223
2025-05-03 12:15:45,463 - main - INFO - Recall: 0.9545
2025-05-03 12:15:45,463 - main - INFO - F1 Score: 0.9381
2025-05-03 12:15:45,463 - main - INFO - IoU: 0.8835
2025-05-03 12:15:45,464 - main - INFO - mAP: 0.9866
2025-05-03 12:15:45,464 - main - INFO - AUC: 0.9938
2025-05-03 12:15:45,464 - main - INFO - Training time: 1712.51 seconds
2025-05-03 12:15:45,464 - main - INFO - 
==================================================
2025-05-03 12:15:45,464 - main - INFO - Running configuration 720/756:
2025-05-03 12:15:45,464 - main - INFO - Model: ConvNeXt
2025-05-03 12:15:45,464 - main - INFO - Optimizer: Adam, Learning Rate: 0.0001
2025-05-03 12:15:45,464 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 12:15:45,464 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 12:15:45,464 - main - INFO - ==================================================
2025-05-03 12:15:45,464 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_720 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001_id_720
2025-05-03 12:15:45,464 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_720 - INFO - Config: {
  "id": 720,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:15:46,186 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_Adam_lr_0.0001
2025-05-03 12:15:46,186 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Config: {
  "id": 720,
  "model_name": "ConvNeXt",
  "optimizer": "Adam",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:15:46,187 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001/checkpoint.pt
2025-05-03 12:15:46,669 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 12:15:46,670 - training.model_ConvNeXt_opt_Adam_lr_0.0001 - INFO - Training completed after 1712.51 seconds
2025-05-03 12:15:46,673 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_720 - INFO - Starting model evaluation
2025-05-03 12:15:58,268 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_720 - INFO - Evaluation metrics:
  Accuracy:  0.9643
  Precision: 0.9252
  Recall:    0.9510
  F1 Score:  0.9379
  IoU:       0.8831
  mAP:       0.9825
  AUC:       0.9927
2025-05-03 12:15:58,269 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_720 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_720/final_results.json
2025-05-03 12:15:58,271 - training.model_ConvNeXt_opt_Adam_lr_0.0001_id_720 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_Adam_lr_0.0001_id_720/final_results.json
2025-05-03 12:15:58,271 - main - INFO - 
Summary for configuration 720:
2025-05-03 12:15:58,271 - main - INFO - Accuracy: 0.9643
2025-05-03 12:15:58,271 - main - INFO - Precision: 0.9252
2025-05-03 12:15:58,271 - main - INFO - Recall: 0.9510
2025-05-03 12:15:58,271 - main - INFO - F1 Score: 0.9379
2025-05-03 12:15:58,271 - main - INFO - IoU: 0.8831
2025-05-03 12:15:58,271 - main - INFO - mAP: 0.9825
2025-05-03 12:15:58,271 - main - INFO - AUC: 0.9927
2025-05-03 12:15:58,271 - main - INFO - Training time: 1712.51 seconds
2025-05-03 12:15:58,271 - main - INFO - 
==================================================
2025-05-03 12:15:58,271 - main - INFO - Running configuration 721/756:
2025-05-03 12:15:58,271 - main - INFO - Model: ConvNeXt
2025-05-03 12:15:58,271 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:15:58,271 - main - INFO - Scheduler: StepLR
2025-05-03 12:15:58,271 - main - INFO - Loss Function: CrossEntropy
2025-05-03 12:15:58,271 - main - INFO - ==================================================
2025-05-03 12:15:58,272 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_721 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_721
2025-05-03 12:15:58,272 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_721 - INFO - Config: {
  "id": 721,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:15:59,104 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:15:59,104 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 721,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:15:59,104 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - No checkpoint found, starting fresh training
2025-05-03 12:15:59,105 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 1/20
2025-05-03 12:16:45,019 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 12:17:39,240 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Validation accuracy improved from 0.0000 to 0.4957
2025-05-03 12:17:39,414 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 1 completed in 100.31s - Train Loss: 0.8119, Train Acc: 0.5002, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 12:17:40,211 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 0
2025-05-03 12:17:40,211 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 2/20
2025-05-03 12:18:26,079 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 12:19:18,867 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Validation loss improved from inf to 0.8169
2025-05-03 12:19:19,152 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 2 completed in 98.94s - Train Loss: 0.8116, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 12:19:19,942 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 1
2025-05-03 12:19:19,943 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 3/20
2025-05-03 12:20:05,151 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 12:20:57,973 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 3 completed in 98.03s - Train Loss: 0.8116, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 12:20:58,805 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 2
2025-05-03 12:20:58,806 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 4/20
2025-05-03 12:21:44,237 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 12:22:38,408 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 4 completed in 99.60s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 12:22:39,254 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 3
2025-05-03 12:22:39,254 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 5/20
2025-05-03 12:23:24,765 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 12:24:18,417 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 5 completed in 99.16s - Train Loss: 0.8128, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 12:24:19,221 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 4
2025-05-03 12:24:19,221 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 6/20
2025-05-03 12:25:04,909 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 12:25:58,011 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 6 completed in 98.79s - Train Loss: 0.8135, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 12:25:58,748 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 5
2025-05-03 12:25:58,749 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 7/20
2025-05-03 12:26:44,787 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 12:27:38,208 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 7 completed in 99.46s - Train Loss: 0.8135, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 12:27:38,985 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 6
2025-05-03 12:27:38,985 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 7 epochs
2025-05-03 12:27:38,985 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 699.10 seconds
2025-05-03 12:27:38,988 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_721 - INFO - Starting model evaluation
2025-05-03 12:27:50,514 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_721 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:27:50,515 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_721 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_721/final_results.json
2025-05-03 12:27:50,516 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_721 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_721/final_results.json
2025-05-03 12:27:50,516 - main - INFO - 
Summary for configuration 721:
2025-05-03 12:27:50,516 - main - INFO - Accuracy: 0.2834
2025-05-03 12:27:50,516 - main - INFO - Precision: 0.2834
2025-05-03 12:27:50,516 - main - INFO - Recall: 1.0000
2025-05-03 12:27:50,516 - main - INFO - F1 Score: 0.4417
2025-05-03 12:27:50,516 - main - INFO - IoU: 0.2834
2025-05-03 12:27:50,516 - main - INFO - mAP: 0.2834
2025-05-03 12:27:50,516 - main - INFO - AUC: 0.5000
2025-05-03 12:27:50,516 - main - INFO - Training time: 699.10 seconds
2025-05-03 12:27:50,516 - main - INFO - 
==================================================
2025-05-03 12:27:50,516 - main - INFO - Running configuration 722/756:
2025-05-03 12:27:50,516 - main - INFO - Model: ConvNeXt
2025-05-03 12:27:50,516 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:27:50,516 - main - INFO - Scheduler: StepLR
2025-05-03 12:27:50,516 - main - INFO - Loss Function: FocalLoss
2025-05-03 12:27:50,516 - main - INFO - ==================================================
2025-05-03 12:27:50,516 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_722 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_722
2025-05-03 12:27:50,516 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_722 - INFO - Config: {
  "id": 722,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:27:51,395 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:27:51,395 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 722,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:27:51,395 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:27:51,869 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 7, best validation accuracy: 0.4957
2025-05-03 12:27:51,870 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 8/20
2025-05-03 12:28:36,891 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 12:29:30,142 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Validation loss improved from 0.8169 to 0.0912
2025-05-03 12:29:30,412 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 8 completed in 98.54s - Train Loss: 0.0905, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 12:29:31,182 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 7
2025-05-03 12:29:31,183 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 9/20
2025-05-03 12:30:17,248 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 12:31:10,759 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 9 completed in 99.58s - Train Loss: 0.0905, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 12:31:11,585 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 8
2025-05-03 12:31:11,586 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 10/20
2025-05-03 12:31:56,792 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 12:32:49,920 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 10 completed in 98.33s - Train Loss: 0.0904, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 12:32:50,743 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 9
2025-05-03 12:32:50,744 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 11/20
2025-05-03 12:33:36,640 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 12:34:30,153 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 11 completed in 99.41s - Train Loss: 0.0904, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 12:34:30,976 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 10
2025-05-03 12:34:30,977 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 12/20
2025-05-03 12:35:16,057 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 12:36:10,182 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 12 completed in 99.21s - Train Loss: 0.0903, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 12:36:11,005 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 11
2025-05-03 12:36:11,005 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 13/20
2025-05-03 12:36:56,403 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 12:37:49,574 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 13 completed in 98.57s - Train Loss: 0.0902, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 12:37:50,352 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 12
2025-05-03 12:37:50,353 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 13 epochs
2025-05-03 12:37:50,353 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 1296.81 seconds
2025-05-03 12:37:50,356 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_722 - INFO - Starting model evaluation
2025-05-03 12:38:01,697 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_722 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:38:01,698 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_722 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_722/final_results.json
2025-05-03 12:38:01,700 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_722 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_722/final_results.json
2025-05-03 12:38:01,700 - main - INFO - 
Summary for configuration 722:
2025-05-03 12:38:01,700 - main - INFO - Accuracy: 0.2834
2025-05-03 12:38:01,700 - main - INFO - Precision: 0.2834
2025-05-03 12:38:01,700 - main - INFO - Recall: 1.0000
2025-05-03 12:38:01,700 - main - INFO - F1 Score: 0.4417
2025-05-03 12:38:01,700 - main - INFO - IoU: 0.2834
2025-05-03 12:38:01,700 - main - INFO - mAP: 0.2834
2025-05-03 12:38:01,700 - main - INFO - AUC: 0.5000
2025-05-03 12:38:01,700 - main - INFO - Training time: 1296.81 seconds
2025-05-03 12:38:01,700 - main - INFO - 
==================================================
2025-05-03 12:38:01,700 - main - INFO - Running configuration 723/756:
2025-05-03 12:38:01,700 - main - INFO - Model: ConvNeXt
2025-05-03 12:38:01,700 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:38:01,700 - main - INFO - Scheduler: StepLR
2025-05-03 12:38:01,700 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 12:38:01,700 - main - INFO - ==================================================
2025-05-03 12:38:01,700 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_723 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_723
2025-05-03 12:38:01,700 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_723 - INFO - Config: {
  "id": 723,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:38:02,339 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:38:02,339 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 723,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:38:02,339 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:38:02,843 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 13, best validation accuracy: 0.4957
2025-05-03 12:38:02,844 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 14/20
2025-05-03 12:38:47,554 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 12:39:43,229 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 14 completed in 100.39s - Train Loss: 0.8119, Train Acc: 0.5011, Val Loss: 0.8162, Val Acc: 0.4957
2025-05-03 12:39:44,522 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 13
2025-05-03 12:39:44,523 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 14 epochs
2025-05-03 12:39:44,523 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 1397.19 seconds
2025-05-03 12:39:44,527 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_723 - INFO - Starting model evaluation
2025-05-03 12:39:57,024 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_723 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:39:57,025 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_723 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_723/final_results.json
2025-05-03 12:39:57,027 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_723 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_723/final_results.json
2025-05-03 12:39:57,027 - main - INFO - 
Summary for configuration 723:
2025-05-03 12:39:57,027 - main - INFO - Accuracy: 0.2834
2025-05-03 12:39:57,027 - main - INFO - Precision: 0.2834
2025-05-03 12:39:57,027 - main - INFO - Recall: 1.0000
2025-05-03 12:39:57,027 - main - INFO - F1 Score: 0.4417
2025-05-03 12:39:57,027 - main - INFO - IoU: 0.2834
2025-05-03 12:39:57,027 - main - INFO - mAP: 0.2834
2025-05-03 12:39:57,027 - main - INFO - AUC: 0.5000
2025-05-03 12:39:57,027 - main - INFO - Training time: 1397.19 seconds
2025-05-03 12:39:57,027 - main - INFO - 
==================================================
2025-05-03 12:39:57,027 - main - INFO - Running configuration 724/756:
2025-05-03 12:39:57,027 - main - INFO - Model: ConvNeXt
2025-05-03 12:39:57,027 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:39:57,028 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 12:39:57,028 - main - INFO - Loss Function: CrossEntropy
2025-05-03 12:39:57,028 - main - INFO - ==================================================
2025-05-03 12:39:57,028 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_724 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_724
2025-05-03 12:39:57,028 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_724 - INFO - Config: {
  "id": 724,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:39:57,913 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:39:57,913 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 724,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:39:57,913 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:39:58,545 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 14, best validation accuracy: 0.4957
2025-05-03 12:39:58,547 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 15/20
2025-05-03 12:40:44,208 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 12:41:38,846 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 15 completed in 100.30s - Train Loss: 0.8122, Train Acc: 0.5011, Val Loss: 0.8169, Val Acc: 0.4957
2025-05-03 12:41:39,642 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 14
2025-05-03 12:41:39,642 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 15 epochs
2025-05-03 12:41:39,642 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 1497.49 seconds
2025-05-03 12:41:39,646 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_724 - INFO - Starting model evaluation
2025-05-03 12:41:51,168 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_724 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:41:51,169 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_724 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_724/final_results.json
2025-05-03 12:41:51,170 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_724 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_724/final_results.json
2025-05-03 12:41:51,170 - main - INFO - 
Summary for configuration 724:
2025-05-03 12:41:51,170 - main - INFO - Accuracy: 0.2834
2025-05-03 12:41:51,170 - main - INFO - Precision: 0.2834
2025-05-03 12:41:51,170 - main - INFO - Recall: 1.0000
2025-05-03 12:41:51,170 - main - INFO - F1 Score: 0.4417
2025-05-03 12:41:51,170 - main - INFO - IoU: 0.2834
2025-05-03 12:41:51,170 - main - INFO - mAP: 0.2834
2025-05-03 12:41:51,170 - main - INFO - AUC: 0.5000
2025-05-03 12:41:51,170 - main - INFO - Training time: 1497.49 seconds
2025-05-03 12:41:51,170 - main - INFO - 
==================================================
2025-05-03 12:41:51,170 - main - INFO - Running configuration 725/756:
2025-05-03 12:41:51,170 - main - INFO - Model: ConvNeXt
2025-05-03 12:41:51,170 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:41:51,170 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 12:41:51,170 - main - INFO - Loss Function: FocalLoss
2025-05-03 12:41:51,170 - main - INFO - ==================================================
2025-05-03 12:41:51,171 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_725 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_725
2025-05-03 12:41:51,171 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_725 - INFO - Config: {
  "id": 725,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:41:52,050 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:41:52,050 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 725,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:41:52,051 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:41:52,679 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 15, best validation accuracy: 0.4957
2025-05-03 12:41:52,681 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 16/20
2025-05-03 12:42:39,821 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 12:43:34,666 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 16 completed in 101.99s - Train Loss: 0.0902, Train Acc: 0.5011, Val Loss: 0.0912, Val Acc: 0.4957
2025-05-03 12:43:35,496 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 15
2025-05-03 12:43:35,497 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 16 epochs
2025-05-03 12:43:35,497 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 1599.48 seconds
2025-05-03 12:43:35,500 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_725 - INFO - Starting model evaluation
2025-05-03 12:43:47,352 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_725 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:43:47,353 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_725 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_725/final_results.json
2025-05-03 12:43:47,354 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_725 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_725/final_results.json
2025-05-03 12:43:47,354 - main - INFO - 
Summary for configuration 725:
2025-05-03 12:43:47,354 - main - INFO - Accuracy: 0.2834
2025-05-03 12:43:47,354 - main - INFO - Precision: 0.2834
2025-05-03 12:43:47,354 - main - INFO - Recall: 1.0000
2025-05-03 12:43:47,354 - main - INFO - F1 Score: 0.4417
2025-05-03 12:43:47,354 - main - INFO - IoU: 0.2834
2025-05-03 12:43:47,354 - main - INFO - mAP: 0.2834
2025-05-03 12:43:47,354 - main - INFO - AUC: 0.5000
2025-05-03 12:43:47,354 - main - INFO - Training time: 1599.48 seconds
2025-05-03 12:43:47,354 - main - INFO - 
==================================================
2025-05-03 12:43:47,354 - main - INFO - Running configuration 726/756:
2025-05-03 12:43:47,354 - main - INFO - Model: ConvNeXt
2025-05-03 12:43:47,354 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:43:47,354 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 12:43:47,354 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 12:43:47,354 - main - INFO - ==================================================
2025-05-03 12:43:47,354 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_726 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_726
2025-05-03 12:43:47,354 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_726 - INFO - Config: {
  "id": 726,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:43:48,098 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:43:48,098 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 726,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:43:48,098 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:43:48,723 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 16, best validation accuracy: 0.4957
2025-05-03 12:43:48,724 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 17/20
2025-05-03 12:44:34,819 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 12:45:28,453 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Epoch 17 completed in 99.73s - Train Loss: 0.8124, Train Acc: 0.5011, Val Loss: 0.8162, Val Acc: 0.4957
2025-05-03 12:45:29,282 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 16
2025-05-03 12:45:29,282 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Early stopping triggered after 17 epochs
2025-05-03 12:45:29,282 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 1699.21 seconds
2025-05-03 12:45:29,286 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_726 - INFO - Starting model evaluation
2025-05-03 12:45:41,137 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_726 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:45:41,139 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_726 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_726/final_results.json
2025-05-03 12:45:41,140 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_726 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_726/final_results.json
2025-05-03 12:45:41,140 - main - INFO - 
Summary for configuration 726:
2025-05-03 12:45:41,140 - main - INFO - Accuracy: 0.2834
2025-05-03 12:45:41,140 - main - INFO - Precision: 0.2834
2025-05-03 12:45:41,140 - main - INFO - Recall: 1.0000
2025-05-03 12:45:41,140 - main - INFO - F1 Score: 0.4417
2025-05-03 12:45:41,140 - main - INFO - IoU: 0.2834
2025-05-03 12:45:41,140 - main - INFO - mAP: 0.2834
2025-05-03 12:45:41,140 - main - INFO - AUC: 0.5000
2025-05-03 12:45:41,140 - main - INFO - Training time: 1699.21 seconds
2025-05-03 12:45:41,140 - main - INFO - 
==================================================
2025-05-03 12:45:41,140 - main - INFO - Running configuration 727/756:
2025-05-03 12:45:41,140 - main - INFO - Model: ConvNeXt
2025-05-03 12:45:41,140 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:45:41,140 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 12:45:41,140 - main - INFO - Loss Function: CrossEntropy
2025-05-03 12:45:41,140 - main - INFO - ==================================================
2025-05-03 12:45:41,140 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_727 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_727
2025-05-03 12:45:41,140 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_727 - INFO - Config: {
  "id": 727,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:45:41,947 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:45:41,947 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 727,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:45:41,947 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:45:42,594 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 17, best validation accuracy: 0.4957
2025-05-03 12:45:42,596 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 18/20
2025-05-03 12:45:42,837 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.96 GiB is allocated by PyTorch, and 494.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:42,837 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-03 12:45:43,682 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 17
2025-05-03 12:45:43,683 - main - ERROR - Error running configuration 727: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.96 GiB is allocated by PyTorch, and 494.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:43,683 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 64, in forward
    result = self.stochastic_depth(result)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/ops/stochastic_depth.py", line 62, in forward
    return stochastic_depth(input, self.p, self.mode, self.training)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/ops/stochastic_depth.py", line 44, in stochastic_depth
    return input * noise
           ~~~~~~^~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 10.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.96 GiB is allocated by PyTorch, and 494.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:43,685 - main - INFO - 
==================================================
2025-05-03 12:45:43,685 - main - INFO - Running configuration 728/756:
2025-05-03 12:45:43,685 - main - INFO - Model: ConvNeXt
2025-05-03 12:45:43,685 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:45:43,685 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 12:45:43,685 - main - INFO - Loss Function: FocalLoss
2025-05-03 12:45:43,685 - main - INFO - ==================================================
2025-05-03 12:45:43,685 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_728 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_728
2025-05-03 12:45:43,685 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_728 - INFO - Config: {
  "id": 728,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:45:44,435 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:45:44,435 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 728,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:45:44,435 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:45:45,061 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 18, best validation accuracy: 0.4957
2025-05-03 12:45:45,062 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 19/20
2025-05-03 12:45:45,230 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 6.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 701.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:45,230 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-03 12:45:46,057 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 18
2025-05-03 12:45:46,057 - main - ERROR - Error running configuration 728: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 6.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 701.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:46,057 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
                                ^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 6.62 MiB is free. Including non-PyTorch memory, this process has 9.75 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 701.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:46,059 - main - INFO - 
==================================================
2025-05-03 12:45:46,059 - main - INFO - Running configuration 729/756:
2025-05-03 12:45:46,059 - main - INFO - Model: ConvNeXt
2025-05-03 12:45:46,059 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:45:46,059 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 12:45:46,059 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 12:45:46,059 - main - INFO - ==================================================
2025-05-03 12:45:46,059 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_729 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_729
2025-05-03 12:45:46,059 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_729 - INFO - Config: {
  "id": 729,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:45:46,924 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:45:46,924 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 729,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:45:46,925 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:45:47,659 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 19, best validation accuracy: 0.4957
2025-05-03 12:45:47,661 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Starting epoch 20/20
2025-05-03 12:45:47,833 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - ERROR - Training interrupted: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.02 GiB is allocated by PyTorch, and 423.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:47,833 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Saving checkpoint before exit
2025-05-03 12:45:48,674 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Checkpoint saved at epoch 19
2025-05-03 12:45:48,675 - main - ERROR - Error running configuration 729: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.02 GiB is allocated by PyTorch, and 423.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:48,675 - main - ERROR - Exception details:
Traceback (most recent call last):
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1576, in main
    best_model, history, training_time = train_model_with_resume(
                                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1285, in train_model_with_resume
    raise e
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 1173, in train_model_with_resume
    output = model(data)
             ^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/ivory/irap-bh-root/BigDataCourse/vehicle_collision2.py", line 175, in forward
    x = self.model(x)
        ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 176, in forward
    return self._forward_impl(x)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 170, in _forward_impl
    x = self.features(x)
        ^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torchvision/models/convnext.py", line 63, in forward
    result = self.layer_scale * self.block(input)
                                ^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mshahid/anaconda3/envs/swinvenv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 16.62 MiB is free. Including non-PyTorch memory, this process has 9.74 GiB memory in use. Process 1701407 has 5.91 GiB memory in use. Of the allocated memory 9.02 GiB is allocated by PyTorch, and 423.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-05-03 12:45:48,676 - main - INFO - 
==================================================
2025-05-03 12:45:48,676 - main - INFO - Running configuration 730/756:
2025-05-03 12:45:48,676 - main - INFO - Model: ConvNeXt
2025-05-03 12:45:48,676 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:45:48,676 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 12:45:48,676 - main - INFO - Loss Function: CrossEntropy
2025-05-03 12:45:48,676 - main - INFO - ==================================================
2025-05-03 12:45:48,676 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_730 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_730
2025-05-03 12:45:48,676 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_730 - INFO - Config: {
  "id": 730,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:45:49,449 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:45:49,450 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 730,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:45:49,450 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:45:50,320 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 12:45:50,322 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 1699.79 seconds
2025-05-03 12:45:50,325 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_730 - INFO - Starting model evaluation
2025-05-03 12:46:01,987 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_730 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:46:01,989 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_730 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_730/final_results.json
2025-05-03 12:46:01,990 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_730 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_730/final_results.json
2025-05-03 12:46:01,990 - main - INFO - 
Summary for configuration 730:
2025-05-03 12:46:01,990 - main - INFO - Accuracy: 0.2834
2025-05-03 12:46:01,990 - main - INFO - Precision: 0.2834
2025-05-03 12:46:01,990 - main - INFO - Recall: 1.0000
2025-05-03 12:46:01,990 - main - INFO - F1 Score: 0.4417
2025-05-03 12:46:01,990 - main - INFO - IoU: 0.2834
2025-05-03 12:46:01,990 - main - INFO - mAP: 0.2834
2025-05-03 12:46:01,990 - main - INFO - AUC: 0.5000
2025-05-03 12:46:01,990 - main - INFO - Training time: 1699.79 seconds
2025-05-03 12:46:01,990 - main - INFO - 
==================================================
2025-05-03 12:46:01,990 - main - INFO - Running configuration 731/756:
2025-05-03 12:46:01,990 - main - INFO - Model: ConvNeXt
2025-05-03 12:46:01,990 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:46:01,990 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 12:46:01,990 - main - INFO - Loss Function: FocalLoss
2025-05-03 12:46:01,990 - main - INFO - ==================================================
2025-05-03 12:46:01,990 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_731 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_731
2025-05-03 12:46:01,990 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_731 - INFO - Config: {
  "id": 731,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:46:02,647 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:46:02,647 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 731,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 12:46:02,647 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:46:03,273 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 12:46:03,274 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 1699.79 seconds
2025-05-03 12:46:03,277 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_731 - INFO - Starting model evaluation
2025-05-03 12:46:15,142 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_731 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:46:15,144 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_731 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_731/final_results.json
2025-05-03 12:46:15,145 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_731 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_731/final_results.json
2025-05-03 12:46:15,145 - main - INFO - 
Summary for configuration 731:
2025-05-03 12:46:15,145 - main - INFO - Accuracy: 0.2834
2025-05-03 12:46:15,145 - main - INFO - Precision: 0.2834
2025-05-03 12:46:15,145 - main - INFO - Recall: 1.0000
2025-05-03 12:46:15,145 - main - INFO - F1 Score: 0.4417
2025-05-03 12:46:15,145 - main - INFO - IoU: 0.2834
2025-05-03 12:46:15,145 - main - INFO - mAP: 0.2834
2025-05-03 12:46:15,145 - main - INFO - AUC: 0.5000
2025-05-03 12:46:15,145 - main - INFO - Training time: 1699.79 seconds
2025-05-03 12:46:15,145 - main - INFO - 
==================================================
2025-05-03 12:46:15,145 - main - INFO - Running configuration 732/756:
2025-05-03 12:46:15,145 - main - INFO - Model: ConvNeXt
2025-05-03 12:46:15,145 - main - INFO - Optimizer: AdamW, Learning Rate: 0.01
2025-05-03 12:46:15,145 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 12:46:15,145 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 12:46:15,145 - main - INFO - ==================================================
2025-05-03 12:46:15,145 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_732 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01_id_732
2025-05-03 12:46:15,145 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_732 - INFO - Config: {
  "id": 732,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:46:15,889 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.01
2025-05-03 12:46:15,889 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Config: {
  "id": 732,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.01,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 12:46:15,889 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01/checkpoint.pt
2025-05-03 12:46:16,502 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Resuming from epoch 20, best validation accuracy: 0.4957
2025-05-03 12:46:16,507 - training.model_ConvNeXt_opt_AdamW_lr_0.01 - INFO - Training completed after 1699.79 seconds
2025-05-03 12:46:16,510 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_732 - INFO - Starting model evaluation
2025-05-03 12:46:28,457 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_732 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2834
  AUC:       0.5000
2025-05-03 12:46:28,458 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_732 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_732/final_results.json
2025-05-03 12:46:28,459 - training.model_ConvNeXt_opt_AdamW_lr_0.01_id_732 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.01_id_732/final_results.json
2025-05-03 12:46:28,459 - main - INFO - 
Summary for configuration 732:
2025-05-03 12:46:28,459 - main - INFO - Accuracy: 0.2834
2025-05-03 12:46:28,459 - main - INFO - Precision: 0.2834
2025-05-03 12:46:28,459 - main - INFO - Recall: 1.0000
2025-05-03 12:46:28,459 - main - INFO - F1 Score: 0.4417
2025-05-03 12:46:28,459 - main - INFO - IoU: 0.2834
2025-05-03 12:46:28,459 - main - INFO - mAP: 0.2834
2025-05-03 12:46:28,459 - main - INFO - AUC: 0.5000
2025-05-03 12:46:28,459 - main - INFO - Training time: 1699.79 seconds
2025-05-03 12:46:28,459 - main - INFO - 
==================================================
2025-05-03 12:46:28,459 - main - INFO - Running configuration 733/756:
2025-05-03 12:46:28,459 - main - INFO - Model: ConvNeXt
2025-05-03 12:46:28,459 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 12:46:28,459 - main - INFO - Scheduler: StepLR
2025-05-03 12:46:28,459 - main - INFO - Loss Function: CrossEntropy
2025-05-03 12:46:28,459 - main - INFO - ==================================================
2025-05-03 12:46:28,459 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_733 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_733
2025-05-03 12:46:28,459 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_733 - INFO - Config: {
  "id": 733,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:46:29,269 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 12:46:29,269 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 733,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 12:46:29,269 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - No checkpoint found, starting fresh training
2025-05-03 12:46:29,270 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 1/20
2025-05-03 12:47:15,839 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 12:48:11,362 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.0000 to 0.4957
2025-05-03 12:48:11,534 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 1 completed in 102.26s - Train Loss: 0.7049, Train Acc: 0.5030, Val Loss: 0.6959, Val Acc: 0.4957
2025-05-03 12:48:12,332 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 0
2025-05-03 12:48:12,333 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 2/20
2025-05-03 12:48:59,304 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 12:49:54,407 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from inf to 0.6936
2025-05-03 12:49:54,691 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 2 completed in 102.36s - Train Loss: 0.6950, Train Acc: 0.4822, Val Loss: 0.6936, Val Acc: 0.4957
2025-05-03 12:49:55,515 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 1
2025-05-03 12:49:55,515 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 3/20
2025-05-03 12:50:41,697 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 12:51:37,534 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 3 completed in 102.02s - Train Loss: 0.6940, Train Acc: 0.5002, Val Loss: 0.6947, Val Acc: 0.4957
2025-05-03 12:51:38,321 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 2
2025-05-03 12:51:38,321 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 4/20
2025-05-03 12:52:24,766 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 12:53:21,010 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation accuracy improved from 0.4957 to 0.5043
2025-05-03 12:53:21,270 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 4 completed in 102.95s - Train Loss: 0.6947, Train Acc: 0.4936, Val Loss: 0.6932, Val Acc: 0.5043
2025-05-03 12:53:22,041 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 3
2025-05-03 12:53:22,041 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 5/20
2025-05-03 12:54:08,750 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 12:55:04,382 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.6936 to 0.6934
2025-05-03 12:55:04,674 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 5 completed in 102.63s - Train Loss: 0.6941, Train Acc: 0.4981, Val Loss: 0.6934, Val Acc: 0.4957
2025-05-03 12:55:05,505 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 4
2025-05-03 12:55:05,505 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 6/20
2025-05-03 12:55:53,014 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 12:56:47,901 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.6934 to 0.6933
2025-05-03 12:56:48,164 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 6 completed in 102.66s - Train Loss: 0.6933, Train Acc: 0.5066, Val Loss: 0.6933, Val Acc: 0.4957
2025-05-03 12:56:48,948 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 5
2025-05-03 12:56:48,948 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 7/20
2025-05-03 12:57:36,259 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 12:58:30,573 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 7 completed in 101.63s - Train Loss: 0.6943, Train Acc: 0.4921, Val Loss: 0.6934, Val Acc: 0.5043
2025-05-03 12:58:31,401 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 6
2025-05-03 12:58:31,401 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 8/20
2025-05-03 12:59:18,494 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 13:00:12,504 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.6933 to 0.6932
2025-05-03 13:00:12,782 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 8 completed in 101.38s - Train Loss: 0.6942, Train Acc: 0.4833, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 13:00:13,631 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 7
2025-05-03 13:00:13,631 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 9/20
2025-05-03 13:00:59,706 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 13:01:54,989 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.6932 to 0.6931
2025-05-03 13:01:55,275 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 9 completed in 101.64s - Train Loss: 0.6936, Train Acc: 0.5004, Val Loss: 0.6931, Val Acc: 0.5043
2025-05-03 13:01:56,101 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 8
2025-05-03 13:01:56,102 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 10/20
2025-05-03 13:02:42,323 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 13:03:36,998 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.6931 to 0.6931
2025-05-03 13:03:37,282 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 10 completed in 101.18s - Train Loss: 0.6937, Train Acc: 0.5066, Val Loss: 0.6931, Val Acc: 0.5043
2025-05-03 13:03:38,114 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 9
2025-05-03 13:03:38,114 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 11/20
2025-05-03 13:04:23,115 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 13:05:17,486 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 11 completed in 99.37s - Train Loss: 0.6933, Train Acc: 0.4858, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 13:05:18,299 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 10
2025-05-03 13:05:18,299 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 12/20
2025-05-03 13:06:05,227 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 13:07:00,089 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 12 completed in 101.79s - Train Loss: 0.6932, Train Acc: 0.5054, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 13:07:00,873 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 11
2025-05-03 13:07:00,874 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 13/20
2025-05-03 13:07:47,010 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 13:08:42,146 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 13 completed in 101.27s - Train Loss: 0.6932, Train Acc: 0.4981, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 13:08:42,978 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 12
2025-05-03 13:08:42,978 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 14/20
2025-05-03 13:09:29,804 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 13:10:23,498 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 14 completed in 100.52s - Train Loss: 0.6931, Train Acc: 0.5009, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 13:10:24,339 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 13
2025-05-03 13:10:24,339 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 15/20
2025-05-03 13:11:11,072 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 13:12:03,663 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 15 completed in 99.32s - Train Loss: 0.6932, Train Acc: 0.4957, Val Loss: 0.6932, Val Acc: 0.4957
2025-05-03 13:12:04,511 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 14
2025-05-03 13:12:04,511 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Early stopping triggered after 15 epochs
2025-05-03 13:12:04,511 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 1534.39 seconds
2025-05-03 13:12:04,514 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_733 - INFO - Starting model evaluation
2025-05-03 13:12:16,404 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_733 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2723
  AUC:       0.4962
2025-05-03 13:12:16,406 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_733 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_733/final_results.json
2025-05-03 13:12:16,407 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_733 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_733/final_results.json
2025-05-03 13:12:16,407 - main - INFO - 
Summary for configuration 733:
2025-05-03 13:12:16,407 - main - INFO - Accuracy: 0.2834
2025-05-03 13:12:16,407 - main - INFO - Precision: 0.2834
2025-05-03 13:12:16,407 - main - INFO - Recall: 1.0000
2025-05-03 13:12:16,407 - main - INFO - F1 Score: 0.4417
2025-05-03 13:12:16,407 - main - INFO - IoU: 0.2834
2025-05-03 13:12:16,407 - main - INFO - mAP: 0.2723
2025-05-03 13:12:16,408 - main - INFO - AUC: 0.4962
2025-05-03 13:12:16,408 - main - INFO - Training time: 1534.39 seconds
2025-05-03 13:12:16,408 - main - INFO - 
==================================================
2025-05-03 13:12:16,408 - main - INFO - Running configuration 734/756:
2025-05-03 13:12:16,408 - main - INFO - Model: ConvNeXt
2025-05-03 13:12:16,408 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:12:16,408 - main - INFO - Scheduler: StepLR
2025-05-03 13:12:16,408 - main - INFO - Loss Function: FocalLoss
2025-05-03 13:12:16,408 - main - INFO - ==================================================
2025-05-03 13:12:16,408 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_734 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_734
2025-05-03 13:12:16,408 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_734 - INFO - Config: {
  "id": 734,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:12:17,266 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:12:17,266 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 734,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:12:17,266 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:12:17,695 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 15, best validation accuracy: 0.5043
2025-05-03 13:12:17,696 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 16/20
2025-05-03 13:13:04,058 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 13:13:57,679 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.6931 to 0.0433
2025-05-03 13:13:57,983 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 16 completed in 100.29s - Train Loss: 0.0433, Train Acc: 0.4953, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 13:13:58,821 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 15
2025-05-03 13:13:58,822 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 17/20
2025-05-03 13:14:45,710 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 13:15:40,195 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.0433 to 0.0433
2025-05-03 13:15:40,422 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 17 completed in 101.60s - Train Loss: 0.0433, Train Acc: 0.4938, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 13:15:41,215 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 16
2025-05-03 13:15:41,216 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 18/20
2025-05-03 13:16:26,785 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 13:17:21,303 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.0433 to 0.0433
2025-05-03 13:17:21,597 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 18 completed in 100.38s - Train Loss: 0.0433, Train Acc: 0.4955, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 13:17:22,437 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 17
2025-05-03 13:17:22,438 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 19/20
2025-05-03 13:18:07,888 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 13:19:00,964 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 19 completed in 98.53s - Train Loss: 0.0433, Train Acc: 0.5017, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 13:19:01,771 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 18
2025-05-03 13:19:01,772 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Starting epoch 20/20
2025-05-03 13:19:47,598 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 13:20:42,293 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Validation loss improved from 0.0433 to 0.0433
2025-05-03 13:20:42,567 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Epoch 20 completed in 100.79s - Train Loss: 0.0433, Train Acc: 0.5062, Val Loss: 0.0433, Val Acc: 0.4957
2025-05-03 13:20:43,383 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Checkpoint saved at epoch 19
2025-05-03 13:20:43,384 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:20:43,387 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_734 - INFO - Starting model evaluation
2025-05-03 13:20:55,630 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_734 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2812
  AUC:       0.4895
2025-05-03 13:20:55,631 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_734 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_734/final_results.json
2025-05-03 13:20:55,633 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_734 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_734/final_results.json
2025-05-03 13:20:55,633 - main - INFO - 
Summary for configuration 734:
2025-05-03 13:20:55,633 - main - INFO - Accuracy: 0.2834
2025-05-03 13:20:55,633 - main - INFO - Precision: 0.2834
2025-05-03 13:20:55,633 - main - INFO - Recall: 1.0000
2025-05-03 13:20:55,633 - main - INFO - F1 Score: 0.4417
2025-05-03 13:20:55,633 - main - INFO - IoU: 0.2834
2025-05-03 13:20:55,633 - main - INFO - mAP: 0.2812
2025-05-03 13:20:55,633 - main - INFO - AUC: 0.4895
2025-05-03 13:20:55,633 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:20:55,633 - main - INFO - 
==================================================
2025-05-03 13:20:55,633 - main - INFO - Running configuration 735/756:
2025-05-03 13:20:55,633 - main - INFO - Model: ConvNeXt
2025-05-03 13:20:55,633 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:20:55,633 - main - INFO - Scheduler: StepLR
2025-05-03 13:20:55,633 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 13:20:55,633 - main - INFO - ==================================================
2025-05-03 13:20:55,633 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_735 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_735
2025-05-03 13:20:55,633 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_735 - INFO - Config: {
  "id": 735,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:20:56,529 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:20:56,529 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 735,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:20:56,529 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:20:57,034 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:20:57,035 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:20:57,038 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_735 - INFO - Starting model evaluation
2025-05-03 13:21:08,748 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_735 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2810
  AUC:       0.4884
2025-05-03 13:21:08,749 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_735 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_735/final_results.json
2025-05-03 13:21:08,751 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_735 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_735/final_results.json
2025-05-03 13:21:08,751 - main - INFO - 
Summary for configuration 735:
2025-05-03 13:21:08,751 - main - INFO - Accuracy: 0.2834
2025-05-03 13:21:08,751 - main - INFO - Precision: 0.2834
2025-05-03 13:21:08,751 - main - INFO - Recall: 1.0000
2025-05-03 13:21:08,751 - main - INFO - F1 Score: 0.4417
2025-05-03 13:21:08,751 - main - INFO - IoU: 0.2834
2025-05-03 13:21:08,751 - main - INFO - mAP: 0.2810
2025-05-03 13:21:08,751 - main - INFO - AUC: 0.4884
2025-05-03 13:21:08,751 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:21:08,751 - main - INFO - 
==================================================
2025-05-03 13:21:08,751 - main - INFO - Running configuration 736/756:
2025-05-03 13:21:08,751 - main - INFO - Model: ConvNeXt
2025-05-03 13:21:08,751 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:21:08,751 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 13:21:08,751 - main - INFO - Loss Function: CrossEntropy
2025-05-03 13:21:08,751 - main - INFO - ==================================================
2025-05-03 13:21:08,751 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_736 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_736
2025-05-03 13:21:08,751 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_736 - INFO - Config: {
  "id": 736,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:21:09,391 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:21:09,391 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 736,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:21:09,391 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:21:09,915 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:21:09,916 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:21:09,919 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_736 - INFO - Starting model evaluation
2025-05-03 13:21:23,069 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_736 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2771
  AUC:       0.4786
2025-05-03 13:21:23,071 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_736 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_736/final_results.json
2025-05-03 13:21:23,073 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_736 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_736/final_results.json
2025-05-03 13:21:23,073 - main - INFO - 
Summary for configuration 736:
2025-05-03 13:21:23,073 - main - INFO - Accuracy: 0.2834
2025-05-03 13:21:23,073 - main - INFO - Precision: 0.2834
2025-05-03 13:21:23,073 - main - INFO - Recall: 1.0000
2025-05-03 13:21:23,073 - main - INFO - F1 Score: 0.4417
2025-05-03 13:21:23,073 - main - INFO - IoU: 0.2834
2025-05-03 13:21:23,073 - main - INFO - mAP: 0.2771
2025-05-03 13:21:23,073 - main - INFO - AUC: 0.4786
2025-05-03 13:21:23,073 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:21:23,073 - main - INFO - 
==================================================
2025-05-03 13:21:23,073 - main - INFO - Running configuration 737/756:
2025-05-03 13:21:23,073 - main - INFO - Model: ConvNeXt
2025-05-03 13:21:23,073 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:21:23,073 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 13:21:23,073 - main - INFO - Loss Function: FocalLoss
2025-05-03 13:21:23,073 - main - INFO - ==================================================
2025-05-03 13:21:23,073 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_737 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_737
2025-05-03 13:21:23,073 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_737 - INFO - Config: {
  "id": 737,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:21:23,822 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:21:23,822 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 737,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:21:23,822 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:21:24,342 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:21:24,343 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:21:24,346 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_737 - INFO - Starting model evaluation
2025-05-03 13:21:35,982 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_737 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2830
  AUC:       0.4896
2025-05-03 13:21:35,984 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_737 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_737/final_results.json
2025-05-03 13:21:35,985 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_737 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_737/final_results.json
2025-05-03 13:21:35,985 - main - INFO - 
Summary for configuration 737:
2025-05-03 13:21:35,986 - main - INFO - Accuracy: 0.2834
2025-05-03 13:21:35,986 - main - INFO - Precision: 0.2834
2025-05-03 13:21:35,986 - main - INFO - Recall: 1.0000
2025-05-03 13:21:35,986 - main - INFO - F1 Score: 0.4417
2025-05-03 13:21:35,986 - main - INFO - IoU: 0.2834
2025-05-03 13:21:35,986 - main - INFO - mAP: 0.2830
2025-05-03 13:21:35,986 - main - INFO - AUC: 0.4896
2025-05-03 13:21:35,986 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:21:35,986 - main - INFO - 
==================================================
2025-05-03 13:21:35,986 - main - INFO - Running configuration 738/756:
2025-05-03 13:21:35,986 - main - INFO - Model: ConvNeXt
2025-05-03 13:21:35,986 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:21:35,986 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 13:21:35,986 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 13:21:35,986 - main - INFO - ==================================================
2025-05-03 13:21:35,986 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_738 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_738
2025-05-03 13:21:35,986 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_738 - INFO - Config: {
  "id": 738,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:21:36,648 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:21:36,648 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 738,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:21:36,648 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:21:37,231 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:21:37,232 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:21:37,235 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_738 - INFO - Starting model evaluation
2025-05-03 13:21:49,158 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_738 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2803
  AUC:       0.4874
2025-05-03 13:21:49,159 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_738 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_738/final_results.json
2025-05-03 13:21:49,161 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_738 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_738/final_results.json
2025-05-03 13:21:49,161 - main - INFO - 
Summary for configuration 738:
2025-05-03 13:21:49,161 - main - INFO - Accuracy: 0.2834
2025-05-03 13:21:49,161 - main - INFO - Precision: 0.2834
2025-05-03 13:21:49,161 - main - INFO - Recall: 1.0000
2025-05-03 13:21:49,161 - main - INFO - F1 Score: 0.4417
2025-05-03 13:21:49,161 - main - INFO - IoU: 0.2834
2025-05-03 13:21:49,161 - main - INFO - mAP: 0.2803
2025-05-03 13:21:49,161 - main - INFO - AUC: 0.4874
2025-05-03 13:21:49,161 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:21:49,161 - main - INFO - 
==================================================
2025-05-03 13:21:49,161 - main - INFO - Running configuration 739/756:
2025-05-03 13:21:49,161 - main - INFO - Model: ConvNeXt
2025-05-03 13:21:49,161 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:21:49,161 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 13:21:49,161 - main - INFO - Loss Function: CrossEntropy
2025-05-03 13:21:49,161 - main - INFO - ==================================================
2025-05-03 13:21:49,161 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_739 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_739
2025-05-03 13:21:49,161 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_739 - INFO - Config: {
  "id": 739,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:21:49,968 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:21:49,968 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 739,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:21:49,968 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:21:50,483 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:21:50,485 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:21:50,487 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_739 - INFO - Starting model evaluation
2025-05-03 13:22:02,671 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_739 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2849
  AUC:       0.4924
2025-05-03 13:22:02,672 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_739 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_739/final_results.json
2025-05-03 13:22:02,674 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_739 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_739/final_results.json
2025-05-03 13:22:02,674 - main - INFO - 
Summary for configuration 739:
2025-05-03 13:22:02,674 - main - INFO - Accuracy: 0.2834
2025-05-03 13:22:02,674 - main - INFO - Precision: 0.2834
2025-05-03 13:22:02,674 - main - INFO - Recall: 1.0000
2025-05-03 13:22:02,674 - main - INFO - F1 Score: 0.4417
2025-05-03 13:22:02,674 - main - INFO - IoU: 0.2834
2025-05-03 13:22:02,674 - main - INFO - mAP: 0.2849
2025-05-03 13:22:02,674 - main - INFO - AUC: 0.4924
2025-05-03 13:22:02,674 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:22:02,674 - main - INFO - 
==================================================
2025-05-03 13:22:02,674 - main - INFO - Running configuration 740/756:
2025-05-03 13:22:02,674 - main - INFO - Model: ConvNeXt
2025-05-03 13:22:02,674 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:22:02,674 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 13:22:02,674 - main - INFO - Loss Function: FocalLoss
2025-05-03 13:22:02,674 - main - INFO - ==================================================
2025-05-03 13:22:02,674 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_740 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_740
2025-05-03 13:22:02,674 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_740 - INFO - Config: {
  "id": 740,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:22:03,389 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:22:03,389 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 740,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:22:03,390 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:22:03,912 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:22:03,914 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:22:03,916 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_740 - INFO - Starting model evaluation
2025-05-03 13:22:15,474 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_740 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2822
  AUC:       0.4904
2025-05-03 13:22:15,475 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_740 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_740/final_results.json
2025-05-03 13:22:15,477 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_740 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_740/final_results.json
2025-05-03 13:22:15,477 - main - INFO - 
Summary for configuration 740:
2025-05-03 13:22:15,477 - main - INFO - Accuracy: 0.2834
2025-05-03 13:22:15,477 - main - INFO - Precision: 0.2834
2025-05-03 13:22:15,477 - main - INFO - Recall: 1.0000
2025-05-03 13:22:15,477 - main - INFO - F1 Score: 0.4417
2025-05-03 13:22:15,477 - main - INFO - IoU: 0.2834
2025-05-03 13:22:15,477 - main - INFO - mAP: 0.2822
2025-05-03 13:22:15,477 - main - INFO - AUC: 0.4904
2025-05-03 13:22:15,477 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:22:15,477 - main - INFO - 
==================================================
2025-05-03 13:22:15,477 - main - INFO - Running configuration 741/756:
2025-05-03 13:22:15,477 - main - INFO - Model: ConvNeXt
2025-05-03 13:22:15,477 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:22:15,477 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 13:22:15,477 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 13:22:15,477 - main - INFO - ==================================================
2025-05-03 13:22:15,477 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_741 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_741
2025-05-03 13:22:15,477 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_741 - INFO - Config: {
  "id": 741,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:22:16,206 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:22:16,207 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 741,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:22:16,207 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:22:16,704 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:22:16,706 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:22:16,708 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_741 - INFO - Starting model evaluation
2025-05-03 13:22:28,258 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_741 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2807
  AUC:       0.4853
2025-05-03 13:22:28,259 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_741 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_741/final_results.json
2025-05-03 13:22:28,261 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_741 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_741/final_results.json
2025-05-03 13:22:28,261 - main - INFO - 
Summary for configuration 741:
2025-05-03 13:22:28,261 - main - INFO - Accuracy: 0.2834
2025-05-03 13:22:28,261 - main - INFO - Precision: 0.2834
2025-05-03 13:22:28,261 - main - INFO - Recall: 1.0000
2025-05-03 13:22:28,261 - main - INFO - F1 Score: 0.4417
2025-05-03 13:22:28,261 - main - INFO - IoU: 0.2834
2025-05-03 13:22:28,261 - main - INFO - mAP: 0.2807
2025-05-03 13:22:28,261 - main - INFO - AUC: 0.4853
2025-05-03 13:22:28,261 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:22:28,261 - main - INFO - 
==================================================
2025-05-03 13:22:28,261 - main - INFO - Running configuration 742/756:
2025-05-03 13:22:28,261 - main - INFO - Model: ConvNeXt
2025-05-03 13:22:28,261 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:22:28,261 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 13:22:28,261 - main - INFO - Loss Function: CrossEntropy
2025-05-03 13:22:28,261 - main - INFO - ==================================================
2025-05-03 13:22:28,261 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_742 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_742
2025-05-03 13:22:28,261 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_742 - INFO - Config: {
  "id": 742,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:22:29,100 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:22:29,100 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 742,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:22:29,100 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:22:29,590 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:22:29,592 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:22:29,594 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_742 - INFO - Starting model evaluation
2025-05-03 13:22:40,985 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_742 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2822
  AUC:       0.4921
2025-05-03 13:22:40,987 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_742 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_742/final_results.json
2025-05-03 13:22:40,988 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_742 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_742/final_results.json
2025-05-03 13:22:40,988 - main - INFO - 
Summary for configuration 742:
2025-05-03 13:22:40,988 - main - INFO - Accuracy: 0.2834
2025-05-03 13:22:40,988 - main - INFO - Precision: 0.2834
2025-05-03 13:22:40,988 - main - INFO - Recall: 1.0000
2025-05-03 13:22:40,988 - main - INFO - F1 Score: 0.4417
2025-05-03 13:22:40,988 - main - INFO - IoU: 0.2834
2025-05-03 13:22:40,988 - main - INFO - mAP: 0.2822
2025-05-03 13:22:40,988 - main - INFO - AUC: 0.4921
2025-05-03 13:22:40,988 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:22:40,988 - main - INFO - 
==================================================
2025-05-03 13:22:40,988 - main - INFO - Running configuration 743/756:
2025-05-03 13:22:40,988 - main - INFO - Model: ConvNeXt
2025-05-03 13:22:40,988 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:22:40,988 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 13:22:40,988 - main - INFO - Loss Function: FocalLoss
2025-05-03 13:22:40,988 - main - INFO - ==================================================
2025-05-03 13:22:40,989 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_743 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_743
2025-05-03 13:22:40,989 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_743 - INFO - Config: {
  "id": 743,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:22:41,646 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:22:41,646 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 743,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:22:41,646 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:22:42,163 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:22:42,164 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:22:42,167 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_743 - INFO - Starting model evaluation
2025-05-03 13:22:53,866 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_743 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2804
  AUC:       0.4882
2025-05-03 13:22:53,868 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_743 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_743/final_results.json
2025-05-03 13:22:53,869 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_743 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_743/final_results.json
2025-05-03 13:22:53,869 - main - INFO - 
Summary for configuration 743:
2025-05-03 13:22:53,869 - main - INFO - Accuracy: 0.2834
2025-05-03 13:22:53,869 - main - INFO - Precision: 0.2834
2025-05-03 13:22:53,869 - main - INFO - Recall: 1.0000
2025-05-03 13:22:53,869 - main - INFO - F1 Score: 0.4417
2025-05-03 13:22:53,869 - main - INFO - IoU: 0.2834
2025-05-03 13:22:53,869 - main - INFO - mAP: 0.2804
2025-05-03 13:22:53,869 - main - INFO - AUC: 0.4882
2025-05-03 13:22:53,869 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:22:53,869 - main - INFO - 
==================================================
2025-05-03 13:22:53,869 - main - INFO - Running configuration 744/756:
2025-05-03 13:22:53,869 - main - INFO - Model: ConvNeXt
2025-05-03 13:22:53,869 - main - INFO - Optimizer: AdamW, Learning Rate: 0.001
2025-05-03 13:22:53,869 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 13:22:53,869 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 13:22:53,869 - main - INFO - ==================================================
2025-05-03 13:22:53,870 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_744 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001_id_744
2025-05-03 13:22:53,870 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_744 - INFO - Config: {
  "id": 744,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:22:54,714 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.001
2025-05-03 13:22:54,715 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Config: {
  "id": 744,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:22:54,715 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001/checkpoint.pt
2025-05-03 13:22:55,207 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Resuming from epoch 20, best validation accuracy: 0.5043
2025-05-03 13:22:55,208 - training.model_ConvNeXt_opt_AdamW_lr_0.001 - INFO - Training completed after 2039.26 seconds
2025-05-03 13:22:55,211 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_744 - INFO - Starting model evaluation
2025-05-03 13:23:07,028 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_744 - INFO - Evaluation metrics:
  Accuracy:  0.2834
  Precision: 0.2834
  Recall:    1.0000
  F1 Score:  0.4417
  IoU:       0.2834
  mAP:       0.2836
  AUC:       0.4906
2025-05-03 13:23:07,029 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_744 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_744/final_results.json
2025-05-03 13:23:07,031 - training.model_ConvNeXt_opt_AdamW_lr_0.001_id_744 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.001_id_744/final_results.json
2025-05-03 13:23:07,031 - main - INFO - 
Summary for configuration 744:
2025-05-03 13:23:07,031 - main - INFO - Accuracy: 0.2834
2025-05-03 13:23:07,031 - main - INFO - Precision: 0.2834
2025-05-03 13:23:07,031 - main - INFO - Recall: 1.0000
2025-05-03 13:23:07,031 - main - INFO - F1 Score: 0.4417
2025-05-03 13:23:07,031 - main - INFO - IoU: 0.2834
2025-05-03 13:23:07,031 - main - INFO - mAP: 0.2836
2025-05-03 13:23:07,031 - main - INFO - AUC: 0.4906
2025-05-03 13:23:07,031 - main - INFO - Training time: 2039.26 seconds
2025-05-03 13:23:07,031 - main - INFO - 
==================================================
2025-05-03 13:23:07,031 - main - INFO - Running configuration 745/756:
2025-05-03 13:23:07,031 - main - INFO - Model: ConvNeXt
2025-05-03 13:23:07,031 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:23:07,031 - main - INFO - Scheduler: StepLR
2025-05-03 13:23:07,031 - main - INFO - Loss Function: CrossEntropy
2025-05-03 13:23:07,031 - main - INFO - ==================================================
2025-05-03 13:23:07,031 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_745 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_745
2025-05-03 13:23:07,032 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_745 - INFO - Config: {
  "id": 745,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:23:07,883 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:23:07,883 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 745,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:23:07,883 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - No checkpoint found, starting fresh training
2025-05-03 13:23:07,884 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 1/20
2025-05-03 13:23:53,609 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 13:24:48,498 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.0000 to 0.9563
2025-05-03 13:24:48,682 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 1 completed in 100.80s - Train Loss: 0.4212, Train Acc: 0.8876, Val Loss: 0.3590, Val Acc: 0.9563
2025-05-03 13:24:49,484 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 0
2025-05-03 13:24:49,484 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 2/20
2025-05-03 13:25:34,990 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 13:26:29,324 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9563 to 0.9580
2025-05-03 13:26:29,613 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 2 completed in 100.13s - Train Loss: 0.3563, Train Acc: 0.9562, Val Loss: 0.3534, Val Acc: 0.9580
2025-05-03 13:26:30,424 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 1
2025-05-03 13:26:30,424 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 3/20
2025-05-03 13:27:16,579 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 13:28:09,107 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9580 to 0.9640
2025-05-03 13:28:09,401 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 3 completed in 98.98s - Train Loss: 0.3523, Train Acc: 0.9603, Val Loss: 0.3463, Val Acc: 0.9640
2025-05-03 13:28:10,214 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 2
2025-05-03 13:28:10,214 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 4/20
2025-05-03 13:28:56,498 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 13:29:49,345 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9640 to 0.9683
2025-05-03 13:29:49,633 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 4 completed in 99.42s - Train Loss: 0.3418, Train Acc: 0.9715, Val Loss: 0.3434, Val Acc: 0.9683
2025-05-03 13:29:50,449 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 3
2025-05-03 13:29:50,449 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 5/20
2025-05-03 13:30:35,864 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 13:31:29,216 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from inf to 0.3460
2025-05-03 13:31:29,487 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 5 completed in 99.04s - Train Loss: 0.3399, Train Acc: 0.9734, Val Loss: 0.3460, Val Acc: 0.9666
2025-05-03 13:31:30,313 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 4
2025-05-03 13:31:30,313 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 6/20
2025-05-03 13:32:15,831 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 13:33:09,160 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9683 to 0.9700
2025-05-03 13:33:09,430 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 6 completed in 99.12s - Train Loss: 0.3386, Train Acc: 0.9734, Val Loss: 0.3420, Val Acc: 0.9700
2025-05-03 13:33:10,209 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 5
2025-05-03 13:33:10,210 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 7/20
2025-05-03 13:33:56,378 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 13:34:50,564 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3460 to 0.3455
2025-05-03 13:34:50,833 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 7 completed in 100.62s - Train Loss: 0.3356, Train Acc: 0.9766, Val Loss: 0.3455, Val Acc: 0.9657
2025-05-03 13:34:51,608 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 6
2025-05-03 13:34:51,608 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 8/20
2025-05-03 13:35:36,856 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 13:36:29,571 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9700 to 0.9708
2025-05-03 13:36:29,839 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 8 completed in 98.23s - Train Loss: 0.3336, Train Acc: 0.9792, Val Loss: 0.3419, Val Acc: 0.9708
2025-05-03 13:36:30,614 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 7
2025-05-03 13:36:30,614 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 9/20
2025-05-03 13:37:16,339 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 13:38:09,521 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9708 to 0.9837
2025-05-03 13:38:09,793 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 9 completed in 99.18s - Train Loss: 0.3341, Train Acc: 0.9788, Val Loss: 0.3297, Val Acc: 0.9837
2025-05-03 13:38:10,552 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 8
2025-05-03 13:38:10,552 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 10/20
2025-05-03 13:38:56,738 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 13:39:51,719 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3455 to 0.3446
2025-05-03 13:39:52,004 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 10 completed in 101.45s - Train Loss: 0.3285, Train Acc: 0.9841, Val Loss: 0.3446, Val Acc: 0.9683
2025-05-03 13:39:52,821 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 9
2025-05-03 13:39:52,822 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 11/20
2025-05-03 13:40:39,182 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 13:41:33,962 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3446 to 0.3343
2025-05-03 13:41:34,228 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 11 completed in 101.41s - Train Loss: 0.3279, Train Acc: 0.9852, Val Loss: 0.3343, Val Acc: 0.9794
2025-05-03 13:41:35,023 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 10
2025-05-03 13:41:35,023 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 12/20
2025-05-03 13:42:20,342 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 13:43:13,733 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3343 to 0.3341
2025-05-03 13:43:14,008 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 12 completed in 98.98s - Train Loss: 0.3225, Train Acc: 0.9906, Val Loss: 0.3341, Val Acc: 0.9794
2025-05-03 13:43:14,775 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 11
2025-05-03 13:43:14,776 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 13/20
2025-05-03 13:43:59,445 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 13:44:53,114 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3341 to 0.3323
2025-05-03 13:44:53,370 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 13 completed in 98.59s - Train Loss: 0.3204, Train Acc: 0.9931, Val Loss: 0.3323, Val Acc: 0.9803
2025-05-03 13:44:54,143 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 12
2025-05-03 13:44:54,144 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 14/20
2025-05-03 13:45:38,627 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 13:46:32,690 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 14 completed in 98.55s - Train Loss: 0.3209, Train Acc: 0.9923, Val Loss: 0.3331, Val Acc: 0.9803
2025-05-03 13:46:33,479 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 13
2025-05-03 13:46:33,480 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 15/20
2025-05-03 13:47:18,720 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 13:48:11,013 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3323 to 0.3313
2025-05-03 13:48:11,282 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 15 completed in 97.80s - Train Loss: 0.3222, Train Acc: 0.9910, Val Loss: 0.3313, Val Acc: 0.9803
2025-05-03 13:48:12,080 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 14
2025-05-03 13:48:12,080 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 16/20
2025-05-03 13:48:57,839 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 13:49:51,830 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3313 to 0.3309
2025-05-03 13:49:52,102 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 16 completed in 100.02s - Train Loss: 0.3208, Train Acc: 0.9921, Val Loss: 0.3309, Val Acc: 0.9820
2025-05-03 13:49:52,871 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 15
2025-05-03 13:49:52,871 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 17/20
2025-05-03 13:50:38,658 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 13:51:31,915 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation accuracy improved from 0.9837 to 0.9854
2025-05-03 13:51:32,194 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 17 completed in 99.32s - Train Loss: 0.3188, Train Acc: 0.9946, Val Loss: 0.3283, Val Acc: 0.9854
2025-05-03 13:51:32,995 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 16
2025-05-03 13:51:32,996 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 18/20
2025-05-03 13:52:19,168 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 13:53:12,244 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Validation loss improved from 0.3309 to 0.3274
2025-05-03 13:53:12,510 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 18 completed in 99.51s - Train Loss: 0.3180, Train Acc: 0.9953, Val Loss: 0.3274, Val Acc: 0.9846
2025-05-03 13:53:13,282 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 17
2025-05-03 13:53:13,282 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 19/20
2025-05-03 13:53:59,122 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 13:54:52,864 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 19 completed in 99.58s - Train Loss: 0.3188, Train Acc: 0.9942, Val Loss: 0.3297, Val Acc: 0.9837
2025-05-03 13:54:53,691 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 18
2025-05-03 13:54:53,692 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Starting epoch 20/20
2025-05-03 13:55:38,963 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 13:56:33,981 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Epoch 20 completed in 100.29s - Train Loss: 0.3186, Train Acc: 0.9944, Val Loss: 0.3299, Val Acc: 0.9828
2025-05-03 13:56:34,789 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Checkpoint saved at epoch 19
2025-05-03 13:56:34,790 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:56:34,793 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_745 - INFO - Starting model evaluation
2025-05-03 13:56:46,142 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_745 - INFO - Evaluation metrics:
  Accuracy:  0.9584
  Precision: 0.9013
  Recall:    0.9580
  F1 Score:  0.9288
  IoU:       0.8671
  mAP:       0.9801
  AUC:       0.9921
2025-05-03 13:56:46,144 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_745 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_745/final_results.json
2025-05-03 13:56:46,146 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_745 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_745/final_results.json
2025-05-03 13:56:46,146 - main - INFO - 
Summary for configuration 745:
2025-05-03 13:56:46,146 - main - INFO - Accuracy: 0.9584
2025-05-03 13:56:46,146 - main - INFO - Precision: 0.9013
2025-05-03 13:56:46,146 - main - INFO - Recall: 0.9580
2025-05-03 13:56:46,146 - main - INFO - F1 Score: 0.9288
2025-05-03 13:56:46,146 - main - INFO - IoU: 0.8671
2025-05-03 13:56:46,146 - main - INFO - mAP: 0.9801
2025-05-03 13:56:46,146 - main - INFO - AUC: 0.9921
2025-05-03 13:56:46,146 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:56:46,146 - main - INFO - 
==================================================
2025-05-03 13:56:46,146 - main - INFO - Running configuration 746/756:
2025-05-03 13:56:46,146 - main - INFO - Model: ConvNeXt
2025-05-03 13:56:46,146 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:56:46,146 - main - INFO - Scheduler: StepLR
2025-05-03 13:56:46,146 - main - INFO - Loss Function: FocalLoss
2025-05-03 13:56:46,146 - main - INFO - ==================================================
2025-05-03 13:56:46,146 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_746 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_746
2025-05-03 13:56:46,146 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_746 - INFO - Config: {
  "id": 746,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:56:46,971 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:56:46,972 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 746,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:56:46,972 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:56:47,484 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:56:47,488 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:56:47,491 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_746 - INFO - Starting model evaluation
2025-05-03 13:56:59,093 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_746 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.9037
  Recall:    0.9510
  F1 Score:  0.9267
  IoU:       0.8635
  mAP:       0.9775
  AUC:       0.9899
2025-05-03 13:56:59,095 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_746 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_746/final_results.json
2025-05-03 13:56:59,096 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_746 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_746/final_results.json
2025-05-03 13:56:59,096 - main - INFO - 
Summary for configuration 746:
2025-05-03 13:56:59,096 - main - INFO - Accuracy: 0.9574
2025-05-03 13:56:59,096 - main - INFO - Precision: 0.9037
2025-05-03 13:56:59,096 - main - INFO - Recall: 0.9510
2025-05-03 13:56:59,096 - main - INFO - F1 Score: 0.9267
2025-05-03 13:56:59,096 - main - INFO - IoU: 0.8635
2025-05-03 13:56:59,096 - main - INFO - mAP: 0.9775
2025-05-03 13:56:59,096 - main - INFO - AUC: 0.9899
2025-05-03 13:56:59,096 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:56:59,096 - main - INFO - 
==================================================
2025-05-03 13:56:59,096 - main - INFO - Running configuration 747/756:
2025-05-03 13:56:59,096 - main - INFO - Model: ConvNeXt
2025-05-03 13:56:59,097 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:56:59,097 - main - INFO - Scheduler: StepLR
2025-05-03 13:56:59,097 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 13:56:59,097 - main - INFO - ==================================================
2025-05-03 13:56:59,097 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_747 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_747
2025-05-03 13:56:59,097 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_747 - INFO - Config: {
  "id": 747,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:56:59,928 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:56:59,928 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 747,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "StepLR",
    "step_size": 10,
    "gamma": 0.1
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:56:59,928 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:57:00,447 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:57:00,448 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:57:00,451 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_747 - INFO - Starting model evaluation
2025-05-03 13:57:12,040 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_747 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.9033
  Recall:    0.9476
  F1 Score:  0.9249
  IoU:       0.8603
  mAP:       0.9785
  AUC:       0.9903
2025-05-03 13:57:12,042 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_747 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_747/final_results.json
2025-05-03 13:57:12,043 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_747 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_747/final_results.json
2025-05-03 13:57:12,043 - main - INFO - 
Summary for configuration 747:
2025-05-03 13:57:12,043 - main - INFO - Accuracy: 0.9564
2025-05-03 13:57:12,043 - main - INFO - Precision: 0.9033
2025-05-03 13:57:12,043 - main - INFO - Recall: 0.9476
2025-05-03 13:57:12,043 - main - INFO - F1 Score: 0.9249
2025-05-03 13:57:12,043 - main - INFO - IoU: 0.8603
2025-05-03 13:57:12,043 - main - INFO - mAP: 0.9785
2025-05-03 13:57:12,043 - main - INFO - AUC: 0.9903
2025-05-03 13:57:12,043 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:57:12,044 - main - INFO - 
==================================================
2025-05-03 13:57:12,044 - main - INFO - Running configuration 748/756:
2025-05-03 13:57:12,044 - main - INFO - Model: ConvNeXt
2025-05-03 13:57:12,044 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:57:12,044 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 13:57:12,044 - main - INFO - Loss Function: CrossEntropy
2025-05-03 13:57:12,044 - main - INFO - ==================================================
2025-05-03 13:57:12,044 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_748 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_748
2025-05-03 13:57:12,044 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_748 - INFO - Config: {
  "id": 748,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:57:12,890 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:57:12,890 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 748,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:57:12,890 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:57:13,386 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:57:13,388 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:57:13,390 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_748 - INFO - Starting model evaluation
2025-05-03 13:57:24,957 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_748 - INFO - Evaluation metrics:
  Accuracy:  0.9594
  Precision: 0.8990
  Recall:    0.9650
  F1 Score:  0.9309
  IoU:       0.8707
  mAP:       0.9800
  AUC:       0.9922
2025-05-03 13:57:24,958 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_748 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_748/final_results.json
2025-05-03 13:57:24,960 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_748 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_748/final_results.json
2025-05-03 13:57:24,960 - main - INFO - 
Summary for configuration 748:
2025-05-03 13:57:24,960 - main - INFO - Accuracy: 0.9594
2025-05-03 13:57:24,960 - main - INFO - Precision: 0.8990
2025-05-03 13:57:24,960 - main - INFO - Recall: 0.9650
2025-05-03 13:57:24,960 - main - INFO - F1 Score: 0.9309
2025-05-03 13:57:24,960 - main - INFO - IoU: 0.8707
2025-05-03 13:57:24,960 - main - INFO - mAP: 0.9800
2025-05-03 13:57:24,960 - main - INFO - AUC: 0.9922
2025-05-03 13:57:24,960 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:57:24,960 - main - INFO - 
==================================================
2025-05-03 13:57:24,960 - main - INFO - Running configuration 749/756:
2025-05-03 13:57:24,960 - main - INFO - Model: ConvNeXt
2025-05-03 13:57:24,960 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:57:24,960 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 13:57:24,960 - main - INFO - Loss Function: FocalLoss
2025-05-03 13:57:24,960 - main - INFO - ==================================================
2025-05-03 13:57:24,960 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_749 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_749
2025-05-03 13:57:24,961 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_749 - INFO - Config: {
  "id": 749,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:57:25,859 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:57:25,859 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 749,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:57:25,859 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:57:26,360 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:57:26,362 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:57:26,364 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_749 - INFO - Starting model evaluation
2025-05-03 13:57:37,881 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_749 - INFO - Evaluation metrics:
  Accuracy:  0.9534
  Precision: 0.8944
  Recall:    0.9476
  F1 Score:  0.9202
  IoU:       0.8522
  mAP:       0.9776
  AUC:       0.9897
2025-05-03 13:57:37,883 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_749 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_749/final_results.json
2025-05-03 13:57:37,885 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_749 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_749/final_results.json
2025-05-03 13:57:37,885 - main - INFO - 
Summary for configuration 749:
2025-05-03 13:57:37,885 - main - INFO - Accuracy: 0.9534
2025-05-03 13:57:37,885 - main - INFO - Precision: 0.8944
2025-05-03 13:57:37,885 - main - INFO - Recall: 0.9476
2025-05-03 13:57:37,885 - main - INFO - F1 Score: 0.9202
2025-05-03 13:57:37,885 - main - INFO - IoU: 0.8522
2025-05-03 13:57:37,885 - main - INFO - mAP: 0.9776
2025-05-03 13:57:37,885 - main - INFO - AUC: 0.9897
2025-05-03 13:57:37,885 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:57:37,885 - main - INFO - 
==================================================
2025-05-03 13:57:37,885 - main - INFO - Running configuration 750/756:
2025-05-03 13:57:37,885 - main - INFO - Model: ConvNeXt
2025-05-03 13:57:37,885 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:57:37,885 - main - INFO - Scheduler: ReduceLROnPlateau
2025-05-03 13:57:37,885 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 13:57:37,885 - main - INFO - ==================================================
2025-05-03 13:57:37,885 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_750 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_750
2025-05-03 13:57:37,885 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_750 - INFO - Config: {
  "id": 750,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:57:38,703 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:57:38,703 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 750,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "ReduceLROnPlateau",
    "patience": 5,
    "factor": 0.5
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:57:38,703 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:57:39,216 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:57:39,217 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:57:39,220 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_750 - INFO - Starting model evaluation
2025-05-03 13:57:50,895 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_750 - INFO - Evaluation metrics:
  Accuracy:  0.9604
  Precision: 0.9046
  Recall:    0.9615
  F1 Score:  0.9322
  IoU:       0.8730
  mAP:       0.9801
  AUC:       0.9918
2025-05-03 13:57:50,897 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_750 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_750/final_results.json
2025-05-03 13:57:50,899 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_750 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_750/final_results.json
2025-05-03 13:57:50,899 - main - INFO - 
Summary for configuration 750:
2025-05-03 13:57:50,899 - main - INFO - Accuracy: 0.9604
2025-05-03 13:57:50,899 - main - INFO - Precision: 0.9046
2025-05-03 13:57:50,899 - main - INFO - Recall: 0.9615
2025-05-03 13:57:50,899 - main - INFO - F1 Score: 0.9322
2025-05-03 13:57:50,899 - main - INFO - IoU: 0.8730
2025-05-03 13:57:50,899 - main - INFO - mAP: 0.9801
2025-05-03 13:57:50,899 - main - INFO - AUC: 0.9918
2025-05-03 13:57:50,899 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:57:50,899 - main - INFO - 
==================================================
2025-05-03 13:57:50,899 - main - INFO - Running configuration 751/756:
2025-05-03 13:57:50,899 - main - INFO - Model: ConvNeXt
2025-05-03 13:57:50,899 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:57:50,899 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 13:57:50,899 - main - INFO - Loss Function: CrossEntropy
2025-05-03 13:57:50,899 - main - INFO - ==================================================
2025-05-03 13:57:50,899 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_751 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_751
2025-05-03 13:57:50,899 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_751 - INFO - Config: {
  "id": 751,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:57:51,649 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:57:51,650 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 751,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:57:51,650 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:57:52,161 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:57:52,162 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:57:52,165 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_751 - INFO - Starting model evaluation
2025-05-03 13:58:04,260 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_751 - INFO - Evaluation metrics:
  Accuracy:  0.9574
  Precision: 0.8984
  Recall:    0.9580
  F1 Score:  0.9272
  IoU:       0.8644
  mAP:       0.9774
  AUC:       0.9897
2025-05-03 13:58:04,262 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_751 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_751/final_results.json
2025-05-03 13:58:04,264 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_751 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_751/final_results.json
2025-05-03 13:58:04,264 - main - INFO - 
Summary for configuration 751:
2025-05-03 13:58:04,264 - main - INFO - Accuracy: 0.9574
2025-05-03 13:58:04,264 - main - INFO - Precision: 0.8984
2025-05-03 13:58:04,264 - main - INFO - Recall: 0.9580
2025-05-03 13:58:04,264 - main - INFO - F1 Score: 0.9272
2025-05-03 13:58:04,264 - main - INFO - IoU: 0.8644
2025-05-03 13:58:04,264 - main - INFO - mAP: 0.9774
2025-05-03 13:58:04,264 - main - INFO - AUC: 0.9897
2025-05-03 13:58:04,264 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:58:04,264 - main - INFO - 
==================================================
2025-05-03 13:58:04,264 - main - INFO - Running configuration 752/756:
2025-05-03 13:58:04,264 - main - INFO - Model: ConvNeXt
2025-05-03 13:58:04,264 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:58:04,264 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 13:58:04,264 - main - INFO - Loss Function: FocalLoss
2025-05-03 13:58:04,264 - main - INFO - ==================================================
2025-05-03 13:58:04,264 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_752 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_752
2025-05-03 13:58:04,264 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_752 - INFO - Config: {
  "id": 752,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:58:04,983 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:58:04,984 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 752,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:58:04,984 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:58:05,486 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:58:05,487 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:58:05,490 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_752 - INFO - Starting model evaluation
2025-05-03 13:58:17,678 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_752 - INFO - Evaluation metrics:
  Accuracy:  0.9584
  Precision: 0.9040
  Recall:    0.9545
  F1 Score:  0.9286
  IoU:       0.8667
  mAP:       0.9782
  AUC:       0.9898
2025-05-03 13:58:17,680 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_752 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_752/final_results.json
2025-05-03 13:58:17,683 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_752 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_752/final_results.json
2025-05-03 13:58:17,683 - main - INFO - 
Summary for configuration 752:
2025-05-03 13:58:17,683 - main - INFO - Accuracy: 0.9584
2025-05-03 13:58:17,683 - main - INFO - Precision: 0.9040
2025-05-03 13:58:17,683 - main - INFO - Recall: 0.9545
2025-05-03 13:58:17,683 - main - INFO - F1 Score: 0.9286
2025-05-03 13:58:17,683 - main - INFO - IoU: 0.8667
2025-05-03 13:58:17,683 - main - INFO - mAP: 0.9782
2025-05-03 13:58:17,683 - main - INFO - AUC: 0.9898
2025-05-03 13:58:17,683 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:58:17,683 - main - INFO - 
==================================================
2025-05-03 13:58:17,683 - main - INFO - Running configuration 753/756:
2025-05-03 13:58:17,683 - main - INFO - Model: ConvNeXt
2025-05-03 13:58:17,683 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:58:17,683 - main - INFO - Scheduler: CosineAnnealingLR
2025-05-03 13:58:17,683 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 13:58:17,683 - main - INFO - ==================================================
2025-05-03 13:58:17,683 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_753 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_753
2025-05-03 13:58:17,683 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_753 - INFO - Config: {
  "id": 753,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:58:18,682 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:58:18,682 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 753,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingLR",
    "T_max": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:58:18,682 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:58:19,295 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:58:19,297 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:58:19,299 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_753 - INFO - Starting model evaluation
2025-05-03 13:58:31,008 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_753 - INFO - Evaluation metrics:
  Accuracy:  0.9613
  Precision: 0.9049
  Recall:    0.9650
  F1 Score:  0.9340
  IoU:       0.8762
  mAP:       0.9791
  AUC:       0.9915
2025-05-03 13:58:31,010 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_753 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_753/final_results.json
2025-05-03 13:58:31,011 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_753 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_753/final_results.json
2025-05-03 13:58:31,012 - main - INFO - 
Summary for configuration 753:
2025-05-03 13:58:31,012 - main - INFO - Accuracy: 0.9613
2025-05-03 13:58:31,012 - main - INFO - Precision: 0.9049
2025-05-03 13:58:31,012 - main - INFO - Recall: 0.9650
2025-05-03 13:58:31,012 - main - INFO - F1 Score: 0.9340
2025-05-03 13:58:31,012 - main - INFO - IoU: 0.8762
2025-05-03 13:58:31,012 - main - INFO - mAP: 0.9791
2025-05-03 13:58:31,012 - main - INFO - AUC: 0.9915
2025-05-03 13:58:31,012 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:58:31,012 - main - INFO - 
==================================================
2025-05-03 13:58:31,012 - main - INFO - Running configuration 754/756:
2025-05-03 13:58:31,012 - main - INFO - Model: ConvNeXt
2025-05-03 13:58:31,012 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:58:31,012 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 13:58:31,012 - main - INFO - Loss Function: CrossEntropy
2025-05-03 13:58:31,012 - main - INFO - ==================================================
2025-05-03 13:58:31,012 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_754 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_754
2025-05-03 13:58:31,012 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_754 - INFO - Config: {
  "id": 754,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:58:31,836 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:58:31,838 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 754,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "CrossEntropy"
  }
}
2025-05-03 13:58:31,838 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:58:32,324 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:58:32,326 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:58:32,329 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_754 - INFO - Starting model evaluation
2025-05-03 13:58:44,013 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_754 - INFO - Evaluation metrics:
  Accuracy:  0.9564
  Precision: 0.8980
  Recall:    0.9545
  F1 Score:  0.9254
  IoU:       0.8612
  mAP:       0.9781
  AUC:       0.9900
2025-05-03 13:58:44,014 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_754 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_754/final_results.json
2025-05-03 13:58:44,016 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_754 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_754/final_results.json
2025-05-03 13:58:44,016 - main - INFO - 
Summary for configuration 754:
2025-05-03 13:58:44,016 - main - INFO - Accuracy: 0.9564
2025-05-03 13:58:44,016 - main - INFO - Precision: 0.8980
2025-05-03 13:58:44,016 - main - INFO - Recall: 0.9545
2025-05-03 13:58:44,016 - main - INFO - F1 Score: 0.9254
2025-05-03 13:58:44,016 - main - INFO - IoU: 0.8612
2025-05-03 13:58:44,016 - main - INFO - mAP: 0.9781
2025-05-03 13:58:44,016 - main - INFO - AUC: 0.9900
2025-05-03 13:58:44,016 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:58:44,016 - main - INFO - 
==================================================
2025-05-03 13:58:44,016 - main - INFO - Running configuration 755/756:
2025-05-03 13:58:44,016 - main - INFO - Model: ConvNeXt
2025-05-03 13:58:44,016 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:58:44,016 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 13:58:44,016 - main - INFO - Loss Function: FocalLoss
2025-05-03 13:58:44,016 - main - INFO - ==================================================
2025-05-03 13:58:44,016 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_755 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_755
2025-05-03 13:58:44,017 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_755 - INFO - Config: {
  "id": 755,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:58:44,888 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:58:44,888 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 755,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "FocalLoss",
    "alpha": 0.25,
    "gamma": 2.0
  }
}
2025-05-03 13:58:44,888 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:58:45,371 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:58:45,372 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:58:45,375 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_755 - INFO - Starting model evaluation
2025-05-03 13:58:56,978 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_755 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.8947
  Recall:    0.9510
  F1 Score:  0.9220
  IoU:       0.8553
  mAP:       0.9782
  AUC:       0.9902
2025-05-03 13:58:56,980 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_755 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_755/final_results.json
2025-05-03 13:58:56,982 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_755 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_755/final_results.json
2025-05-03 13:58:56,982 - main - INFO - 
Summary for configuration 755:
2025-05-03 13:58:56,982 - main - INFO - Accuracy: 0.9544
2025-05-03 13:58:56,982 - main - INFO - Precision: 0.8947
2025-05-03 13:58:56,982 - main - INFO - Recall: 0.9510
2025-05-03 13:58:56,982 - main - INFO - F1 Score: 0.9220
2025-05-03 13:58:56,982 - main - INFO - IoU: 0.8553
2025-05-03 13:58:56,982 - main - INFO - mAP: 0.9782
2025-05-03 13:58:56,982 - main - INFO - AUC: 0.9902
2025-05-03 13:58:56,982 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:58:56,982 - main - INFO - 
==================================================
2025-05-03 13:58:56,982 - main - INFO - Running configuration 756/756:
2025-05-03 13:58:56,982 - main - INFO - Model: ConvNeXt
2025-05-03 13:58:56,982 - main - INFO - Optimizer: AdamW, Learning Rate: 0.0001
2025-05-03 13:58:56,982 - main - INFO - Scheduler: CosineAnnealingWarmRestarts
2025-05-03 13:58:56,982 - main - INFO - Loss Function: LabelSmoothing
2025-05-03 13:58:56,982 - main - INFO - ==================================================
2025-05-03 13:58:56,982 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_756 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001_id_756
2025-05-03 13:58:56,982 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_756 - INFO - Config: {
  "id": 756,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:58:57,766 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Initialized training manager for model_ConvNeXt_opt_AdamW_lr_0.0001
2025-05-03 13:58:57,766 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Config: {
  "id": 756,
  "model_name": "ConvNeXt",
  "optimizer": "AdamW",
  "learning_rate": 0.0001,
  "scheduler": {
    "name": "CosineAnnealingWarmRestarts",
    "T_0": 10
  },
  "loss_fn": {
    "name": "LabelSmoothing",
    "smoothing": 0.1
  }
}
2025-05-03 13:58:57,766 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Loading checkpoint from model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001/checkpoint.pt
2025-05-03 13:58:58,255 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Resuming from epoch 20, best validation accuracy: 0.9854
2025-05-03 13:58:58,256 - training.model_ConvNeXt_opt_AdamW_lr_0.0001 - INFO - Training completed after 2006.10 seconds
2025-05-03 13:58:58,259 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_756 - INFO - Starting model evaluation
2025-05-03 13:59:09,909 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_756 - INFO - Evaluation metrics:
  Accuracy:  0.9544
  Precision: 0.8922
  Recall:    0.9545
  F1 Score:  0.9223
  IoU:       0.8558
  mAP:       0.9780
  AUC:       0.9900
2025-05-03 13:59:09,910 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_756 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_756/final_results.json
2025-05-03 13:59:09,912 - training.model_ConvNeXt_opt_AdamW_lr_0.0001_id_756 - INFO - Final results saved to model_results_v2/model_ConvNeXt_opt_AdamW_lr_0.0001_id_756/final_results.json
2025-05-03 13:59:09,912 - main - INFO - 
Summary for configuration 756:
2025-05-03 13:59:09,912 - main - INFO - Accuracy: 0.9544
2025-05-03 13:59:09,912 - main - INFO - Precision: 0.8922
2025-05-03 13:59:09,912 - main - INFO - Recall: 0.9545
2025-05-03 13:59:09,912 - main - INFO - F1 Score: 0.9223
2025-05-03 13:59:09,912 - main - INFO - IoU: 0.8558
2025-05-03 13:59:09,912 - main - INFO - mAP: 0.9780
2025-05-03 13:59:09,912 - main - INFO - AUC: 0.9900
2025-05-03 13:59:09,912 - main - INFO - Training time: 2006.10 seconds
2025-05-03 13:59:09,912 - main - INFO - Compiling results summary...
2025-05-03 13:59:10,199 - main - INFO - Creating visualizations...
